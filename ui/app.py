"""
Phase-1 Clean Point Design UI (Streamlit)

Professional, single-command UI for:
- Point Designer: evaluate one operating point and show pass/fail constraint dashboard
- Scan Lab: run parameter scans and explore results
- Results Explorer: filter/sort/export feasible points

Design goals:
- No JS toolchain required (runs on pure Python).
- Physics and models live in src/ (imported as a library).
- All models remain explicit proxies (Phase-1), with conservative pass/fail gates.
"""

from __future__ import annotations

import subprocess


# --- Branding (v175.4) ---
APP_NAME = 'Tokamak 0-D Design Studio'
APP_SUBTITLE = 'Feasibility-first, constraint-authoritative 0-D tokamak design'
APP_AUTHOR = 'Afshin Arjhangmehr'
APP_YEAR = 2026
APP_COPYRIGHT = 'Â© 2026 Afshin Arjhangmehr'

def _render_branding_header():
    # Keep the main title crisp and readable across zoom levels.
    st.markdown(f"# {APP_NAME}")
    st.caption(APP_SUBTITLE)

def _render_footer():
    """Render a persistent footer (safe HTML/CSS injection)."""
    st.markdown(
        f"""
        <style>
        .shams-footer {{
            position: fixed;
            bottom: 0;
            left: 0;
            right: 0;
            text-align: center;
            font-size: 12px;
            color: rgba(49,51,63,0.7);
            padding: 6px 0;
            background: rgba(255,255,255,0.92);
            border-top: 1px solid rgba(49,51,63,0.15);
            z-index: 1000;
        }}
        .block-container {{
            padding-bottom: 3rem;
        }}
        </style>
        <div class="shams-footer">{APP_COPYRIGHT}</div>
        """,
        unsafe_allow_html=True,
    )


def _attach_common_metadata(d: dict) -> dict:
    """Attach canonical author/software metadata to an artifact-like dict.

    v264.0 adds governance overlays:
    - citation completeness (for authority overrides)
    - experimental evidence anchoring on constraint records
    """
    if not isinstance(d, dict):
        return d

    d.setdefault("software", APP_NAME)
    d.setdefault("author", APP_AUTHOR)
    d.setdefault("year", APP_YEAR)
    d.setdefault("copyright", APP_COPYRIGHT)

    # Point Designer governance (frozen): embed in every artifact so that
    # downstream exports and bundles always carry the evaluation/exploration boundary.
    d.setdefault(
        "point_designer",
        {
            "status": "frozen",
            "frozen_since": "v179.2",
            "role": "constraint-authoritative 0-D operating point evaluator",
            "note": "No optimization, relaxation, or exploration occurs in Point Designer. Exploration is performed in Systems Mode, which calls Point Designer as a fixed evaluator.",
            "non_goals": [
                "optimization",
                "transport",
                "time evolution",
                "design space exploration",
                "equilibrium solve",
                "SOL / edge code replacement",
                "neutronics Monte-Carlo replacement",
            ],
        },
    )

    # Reproducibility stamp: keep Point Designer artifacts audit-ready.
    # (UI-only; does not affect physics.)
    try:
        from pathlib import Path as _Path
        import platform as _platform
        import sys as _sys
        import datetime as _dt
        _ver_path = _Path(__file__).resolve().parents[1] / "VERSION"
        _ver = "unknown"
        if _ver_path.exists():
            _ver = _ver_path.read_text(encoding="utf-8").strip().splitlines()[0]
        d.setdefault("shams_version", _ver)
        d.setdefault("build_utc", _dt.datetime.utcnow().replace(microsecond=0).isoformat() + "Z")
        d.setdefault("python", _sys.version.split("\n")[0])
        d.setdefault("platform", _platform.platform())
    except Exception:
        pass

    # --- Governance overlays (v264.0) ---
    try:
        from src.governance.citations import validate_authority_overrides, summarize_citation_completeness
        issues = validate_authority_overrides(d.get("authority_overrides", {}))
        d["citation_completeness"] = summarize_citation_completeness(issues)
    except Exception:
        pass

    try:
        from pathlib import Path as _Path
        from src.governance.experimental_anchoring import load_anchoring_db, annotate_constraints, summarize_evidence
        dbp = _Path(ROOT) / "benchmarks" / "experimental" / "data" / "anchors_default.json"
        if dbp.exists() and isinstance(d.get("constraints"), list):
            db = load_anchoring_db(dbp)
            d["constraints"] = annotate_constraints(d["constraints"], db)
            d["experimental_evidence_summary"] = summarize_evidence(d["constraints"])
    except Exception:
        pass

    return d
# ---- import path bootstrap (must be before any local imports) ----
import os as _os, sys as _sys
_ROOT = _os.path.abspath(_os.path.join(_os.path.dirname(__file__), ".."))
_SRC = _os.path.join(_ROOT, "src")
if _SRC not in _sys.path:
    _sys.path.insert(0, _SRC)
# ---------------------------------------------------------------

# Expose repo root for downstream helpers
ROOT = _ROOT
SRC = _SRC

import io
import json
import os
import sys
import math

# --- JSON export helpers (cycle-safe, deterministic) ---
def _shams_json_sanitize(obj, _seen=None, _depth: int = 0, _max_depth: int = 25):
    import math
    if _seen is None:
        _seen = set()
    if _depth > _max_depth:
        return "<max_depth>"
    if obj is None or isinstance(obj, (str, int, bool)):
        return obj
    if isinstance(obj, float):
        if not math.isfinite(obj):
            return None
        return obj
    oid = id(obj)
    if oid in _seen:
        return "<circular_ref>"
    if isinstance(obj, dict):
        _seen.add(oid)
        out = {}
        for k, v in obj.items():
            try:
                kk = str(k)
            except Exception:
                kk = "<key>"
            if kk in {"_session_state", "session_state", "__streamlit__", "__ctx__"}:
                continue
            out[kk] = _shams_json_sanitize(v, _seen, _depth + 1, _max_depth)
        _seen.discard(oid)
        return out
    if isinstance(obj, (list, tuple, set)):
        _seen.add(oid)
        out = [_shams_json_sanitize(v, _seen, _depth + 1, _max_depth) for v in list(obj)]
        _seen.discard(oid)
        return out
    try:
        import dataclasses
        if dataclasses.is_dataclass(obj):
            return _shams_json_sanitize(dataclasses.asdict(obj), _seen, _depth + 1, _max_depth)
    except Exception:
        pass
    try:
        return str(obj)
    except Exception:
        return "<unserializable>"

def _shams_json_dumps(obj, **kwargs):
    import json as _json
    safe = _shams_json_sanitize(obj)
    return _json.dumps(safe, **kwargs)

# ----------------------------------------------------

import random
import datetime
import time
import concurrent.futures
import queue
import threading
from dataclasses import asdict, fields, replace
from typing import Dict, Any, List, Tuple

import html

import pandas as pd
import numpy as np
from decision.kpis import headline_kpis

import streamlit as st
from ui.tablekit import install_expandable_tables

# --- Optional plotting dependency (matplotlib) ---
# Matplotlib is intentionally optional in SHAMS. The UI must never crash if it is missing.
try:  # pragma: no cover (availability depends on user environment)
    import matplotlib.pyplot as plt  # type: ignore
    _HAVE_MPL = True
except Exception:  # pragma: no cover
    plt = None  # type: ignore
    _HAVE_MPL = False

# --- Global UI preference: make all tables collapsible to prevent scroll walls
if "ui_tablekit_enabled" not in st.session_state:
    st.session_state["ui_tablekit_enabled"] = True
if "ui_tablekit_default_expanded" not in st.session_state:
    st.session_state["ui_tablekit_default_expanded"] = False

# --- v327.0: Design State Graph (DSG) â€” inter-panel continuity (exploration layer)
_DSG_SNAPSHOT_PATH = "artifacts/dsg/current_dsg.json"
from typing import Any, Optional
try:
    from src.dsg import DesignStateGraph  # type: ignore
except Exception:
    try:
        from dsg import DesignStateGraph  # type: ignore
    except Exception:
        DesignStateGraph = None  # type: ignore

if "_shams_dsg" not in st.session_state and DesignStateGraph is not None:
    try:
        st.session_state["_shams_dsg"] = DesignStateGraph.load(_DSG_SNAPSHOT_PATH)
    except Exception:
        st.session_state["_shams_dsg"] = DesignStateGraph()

def _dsg_save_best_effort() -> None:
    try:
        if DesignStateGraph is None:
            return
        g = st.session_state.get("_shams_dsg")
        if g is None:
            return
        g.save(_DSG_SNAPSHOT_PATH)
    except Exception:
        return

def _dsg_record_best_effort(*, inp: Any, res: Any, origin: str, parents: Optional[list[str]] = None, tags: Optional[list[str]] = None, edge_kind: Optional[str] = None, edge_note: str = "") -> None:
    """Record an evaluation into the DSG if available.

    This is an exploration-layer side effect only; it must not affect truth.
    """
    try:
        if DesignStateGraph is None:
            return
        g = st.session_state.get("_shams_dsg")
        if g is None:
            return
        out = getattr(res, "out", {}) if res is not None else {}
        ok = bool(getattr(res, "ok", True))
        msg = str(getattr(res, "message", "") or "")
        el = float(getattr(res, "elapsed_s", 0.0) or 0.0)
        node = g.record(inp=inp, out=out, ok=ok, message=msg, elapsed_s=el, origin=str(origin), parents=parents, tags=tags, edge_kind=edge_kind, edge_note=edge_note)
        st.session_state["active_design_node_id"] = node.node_id
        _dsg_save_best_effort()
    except Exception:
        return

def _dsg_evaluator(*, origin: str = "UI", **evaluator_kwargs: Any):
    """Construct an Evaluator wrapper that records evaluations into DSG.

    This is an exploration-layer adapter only.
    """
    try:
        from src.evaluator.core import Evaluator  # type: ignore
    except Exception:
        from evaluator.core import Evaluator  # type: ignore

    # Evaluator construction can be expensive. Cache the base evaluator instance deterministically
    # so UI reruns (tab switches, widget edits) do not rebuild heavy objects.
    @st.cache_resource(show_spinner=False)
    def _cached_evaluator(**kwargs: Any):
        return Evaluator(**kwargs)

    ev = _cached_evaluator(**evaluator_kwargs)

    class _Wrap:
        def __init__(self, _ev: Any, _origin: str):
            self._ev = _ev
            self._origin = str(_origin)

        def evaluate(self, inp: Any):
            res = self._ev.evaluate(inp)
            parent = st.session_state.get("dsg_selected_node_id") or st.session_state.get("active_design_node_id")
            kind = st.session_state.get("dsg_context_edge_kind") or "derived"
            parents = [str(parent)] if parent else None
            _dsg_record_best_effort(inp=inp, res=res, origin=self._origin, parents=parents, edge_kind=str(kind))
            return res

        def get(self, *args: Any, **kwargs: Any):
            return self._ev.get(*args, **kwargs)

        def cache_stats(self):
            return self._ev.cache_stats()

        def reset_cache_stats(self):
            return self._ev.reset_cache_stats()

    return _Wrap(ev, origin)
install_expandable_tables(st)

from ui.icons import label as ui_label, render_mode_scope
from ui.dsg_panel import render_dsg_sidebar
# ---- SHAMS UX guardrails: global run lock + fusion-expert notifications
from ui import runlock as _shams_runlock
import atexit as _shams_atexit
import time as _shams_time
import uuid as _shams_uuid
import re as _shams_re

if "_shams_app_start_ts" not in st.session_state:
    st.session_state["_shams_app_start_ts"] = _shams_time.time()
if "_shams_owner_token" not in st.session_state:
    st.session_state["_shams_owner_token"] = str(_shams_uuid.uuid4())

def _shams_lock_banner():
    locked, task, started, is_owner = _shams_runlock.status(st.session_state.get("_shams_owner_token"), app_start_ts=st.session_state.get("_shams_app_start_ts"))
    if locked and task:
        age_s = int(_shams_time.time() - float(started or _shams_time.time()))
        badge = "âš¡ Shot in Progress" if not is_owner else "âš¡ Running Sequence"
        st.sidebar.info(f"{badge}: **{task}**  Â·  t+{age_s}s")
    return locked, task, started, is_owner

def _shams_is_solver_label(label: str) -> bool:
    # Freeze only *solver* actions, not navigation / downloads.
    return bool(_shams_re.search(r"\b(evaluate|solve|run|build|compute|scan|pareto|search|atlas|trajectory|candidates)\b", label, flags=_shams_re.I))

# Monkeypatch st.button to enforce lock on solver actions
_shams__orig_button = st.button
def _shams_button(label, *args, **kwargs):
    locked, task, started, is_owner = _shams_runlock.status(st.session_state.get("_shams_owner_token"), app_start_ts=st.session_state.get("_shams_app_start_ts"))
    label_str = str(label)
    if locked and _shams_is_solver_label(label_str) and not is_owner:
        kwargs["disabled"] = True
        return _shams__orig_button(label, *args, **kwargs)

    clicked = _shams__orig_button(label, *args, **kwargs)
    if clicked and _shams_is_solver_label(label_str):
        # Do NOT acquire the global run-lock here.
        # Streamlit reruns can cause a deadlock if acquisition happens before the solver call stack.
        locked2, task2, started2, is_owner2 = _shams_runlock.status(
            st.session_state.get("_shams_owner_token"),
            app_start_ts=st.session_state.get("_shams_app_start_ts"),
        )
        if locked2 and (not is_owner2):
            try:
                st.toast("â›” Another sequence is already running. Wait for the Blackâ€‘Box Chronicle to clear.", icon="â›”")
            except Exception:
                pass
            return False
    return clicked

st.button = _shams_button

# Show lock banner early (Control Ledger will inherit this)
_shams_lock_banner()

# --- v327.1: DSG selector + lineage breadcrumb (sidebar)
try:
    from ui.dsg_panel import render_dsg_sidebar  # type: ignore
    _g = st.session_state.get("_shams_dsg")
    if _g is not None:
        render_dsg_sidebar(_g)
        try:
            _sel = st.session_state.get("dsg_selected_node_id") or getattr(_g, "active_node_id", None)
            if _sel:
                st.caption(f"ðŸ§¬ Active design node: `{_sel}`  Â·  lineage edge kind: `{st.session_state.get('dsg_context_edge_kind','derived')}`")
        except Exception:
            pass
except Exception:
    pass


# ---- Systems solver adapter imports (v178.9) ----
# UI directly builds SolverRequest objects for Systems Mode solves.
# Some deployments expose `src/` modules as top-level packages; keep a robust fallback.
try:
    from solvers import SolverRequest, DefaultTargetSolverBackend, solve_request  # type: ignore
except Exception:  # pragma: no cover
    from src.solvers import SolverRequest, DefaultTargetSolverBackend, solve_request  # type: ignore

from pathlib import Path
from tools.activity_log import get_logger as _get_activity_logger

# Global repo root + activity logger (Asia/Tehran)
REPO_ROOT = Path(__file__).resolve().parent.parent

# Canonical base directory for UI-resolved repo paths.
# Some panels historically referenced BASE_DIR; keep it stable and deterministic.
BASE_DIR = REPO_ROOT

def _activity_logger():
    lg = _get_activity_logger(st, REPO_ROOT, tz_name="Asia/Tehran")
    # One-time marker so users can confirm logging is working
    if "activity_log_inited" not in st.session_state:
        st.session_state["activity_log_inited"] = True
        try:
            if bool(st.session_state.get("activity_log_auto", True)):
                lg.log_event("UI", "LogInitialized", {"tz": "Asia/Tehran"})
        except Exception:
            pass
    return lg

def _alog(mode: str, action: str, payload: dict | None = None):
    try:
        if bool(st.session_state.get("activity_log_auto", True)):
            _activity_logger().log_event(mode, action, payload or {})
    except Exception:
        pass



def _invalidate_mode_caches(reason: str = "") -> None:
    """Clear cached outputs/artifacts across modes when a truth-relevant UI contract changes.

    This prevents stale Telemetry (e.g., NaN KPIs) after changing intent/machine type/policy.
    Deterministic: only clears UI caches; never changes evaluator physics.
    """
    try:
        keys = [
            # Point Designer
            "pd_last_outputs","pd_last_artifact","pd_last_log_lines","pd_last_run_ts","pd_last_inputs_hash",
            "pd_last_forensics","pd_last_forensics_inputs_hash",
            "last_point_out","last_point_inp","last_solver_log",
            # Systems Mode
            "systems_last_solve_artifact","systems_last_outputs","systems_last_inputs_hash",
            "systems_targets","systems_variables","systems_last_log_lines",
            # Scan Lab / Cartography
            "scan_cartography_artifact","scan_last_run","scan_last_outputs",
            # Pareto / Optimizer
            "opt_last_run_id","opt_last_meta","opt_last_records","opt_last_best",
            # Publication Benchmarks UI caches
            "pb_last_pack","pb_last_delta","pb_last_topology",
            # Compare
            "cmp_slot_A","cmp_slot_B","cmp_slot_A_meta","cmp_slot_B_meta",
        ]
        for k in keys:
            st.session_state.pop(k, None)
        if reason:
            st.session_state["ui_last_invalidation_reason"] = str(reason)
    except Exception:
        pass



def _alog_exc(mode: str, action: str, exc: BaseException):
    try:
        if bool(st.session_state.get("activity_log_auto", True)):
            _activity_logger().log_exception(mode, action, exc)
    except Exception:
        pass


# --- Forward-definition bootstrap (v175.6) ---
# --- Forward-definition bootstrap (v175.6.2) ---
def _bootstrap_forward_defs(target_names=None) -> None:
    """Predefine later top-level defs so early UI can reference them.

    Streamlit executes this script top-to-bottom. Some panels are defined later
    in this file but referenced earlier (e.g. by Panel Availability Map).
    This helper AST-parses *this* file and execs top-level defs (functions/classes)
    into the current globals() *without* executing their bodies.

    If `target_names` is provided, only those defs are loaded.
    """
    try:
        import ast
        from pathlib import Path

        src = Path(__file__).read_text(encoding="utf-8")
        mod = ast.parse(src)
        g = globals()

        targets = set(target_names) if target_names else None

        def _allow(name: str) -> bool:
            if not name:
                return False
            if targets is not None and name not in targets:
                return False
            # Only bootstrap panels and key helpers; avoid random internals.
            if name.startswith("_v"):
                return True
            if name in {"_render_with_contract", "_resolve_panel_function"}:
                return True
            return False

        for node in mod.body:
            if not isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
                continue
            name = getattr(node, "name", None)
            if not _allow(name):
                continue
            if name in g:
                continue
            try:
                code = compile(ast.Module([node], type_ignores=[]), filename=str(__file__), mode="exec")
                exec(code, g, g)
            except Exception:
                # Best-effort; skip anything that can't be safely defined.
                continue
    except Exception:
        return

_bootstrap_forward_defs()

from ui.panel_availability import PanelState, PanelStatus, default_status
from ui.panel_contracts import get_panel_contracts
from ui.state import SessionStateModel


# =====================
# Phase-1 UI Stabilization: early-safe state helpers (v372.8)
# =====================
# Rationale: prevent forward-reference failures and tab-scope leakage under Streamlit rerun semantics.
# These helpers are intentionally minimal and deterministic; they do not modify physics truth.

def _v92_state_get():
    import streamlit as st
    st.session_state.setdefault("shams_state", SessionStateModel())
    s = st.session_state["shams_state"]
    if getattr(s, "run_history", None) is None:
        s.run_history = []
    if getattr(s, "pinned_run_ids", None) is None:
        s.pinned_run_ids = []
    return s

def _v92_state_clear_point():
    import streamlit as st
    s = _v92_state_get()
    s.last_point_inputs = None
    s.last_point_outputs = None
    s.last_point_artifact = None
    s.last_point_radial_png = None
    for k in ["pd_last_outputs", "pd_last_artifact", "pd_last_radial_png_bytes"]:
        if k in st.session_state:
            del st.session_state[k]

def _phase1_stabilize_cache_aliases() -> None:
    """Maintain canonical cache keys while preserving backward-compat keys.

    Canonical (Phase-1 contract):
      - pd_last_outputs
      - systems_last_solution
      - scan_last_grid
      - pareto_last_front
    """
    import streamlit as st
    ss = st.session_state

    # Systems
    if "systems_last_solution" not in ss and "last_systems_solution" in ss:
        ss["systems_last_solution"] = ss.get("last_systems_solution")

    # Scan
    if "scan_last_grid" not in ss:
        if "scan_last_outputs" in ss:
            ss["scan_last_grid"] = ss.get("scan_last_outputs")
        elif "scan_cartography_artifact" in ss:
            ss["scan_last_grid"] = ss.get("scan_cartography_artifact")

    # Pareto
    if "pareto_last_front" not in ss and "pareto_last" in ss:
        ss["pareto_last_front"] = ss.get("pareto_last")

    # Point Designer: keep legacy aliases alive
    if "pd_last_outputs" in ss and "last_point_out" not in ss:
        ss["last_point_out"] = ss.get("pd_last_outputs")
    if "pd_last_artifact" in ss and "last_point_artifact" not in ss:
        ss["last_point_artifact"] = ss.get("pd_last_artifact")


from ui.pareto_language import PARETO_LOCK_LINE, PARETO_OPTIMAL_DEF, TRUST_BOUNDARIES, FREEZE_STAMP
from ui.optimizer_console import render_external_optimizer_launcher, render_optimizer_evidence_packs
from ui.language_freeze import CANON as _LANG, FORBIDDEN_PHRASES as _FORBIDDEN

# --- UI helpers (v87, additive) ---

# --- Panel Availability Map helpers (v174, additive) ---
def _dedupe_checks(checks_list):
    """Remove duplicate checks while preserving order."""
    seen = set()
    out = []
    for c in (checks_list or []):
        key = None
        try:
            if isinstance(c, dict):
                key = c.get("id") or c.get("name") or c.get("title")
            else:
                key = getattr(c, "id", None) or getattr(c, "name", None) or getattr(c, "title", None)
        except Exception:
            key = None
        if key is None:
            key = repr(c)
        if key in seen:
            continue
        seen.add(key)
        out.append(c)
    return out

_PANEL_FN_CACHE = {}

def _resolve_panel_function(fn_name: str):
    """Resolve a panel function by name across the codebase.
    Streamlit runs ui/app.py as __main__, so we must search broadly.
    Uses a cache to avoid repeated module walks.
    """
    if not fn_name:
        return None
    if fn_name in _PANEL_FN_CACHE:
        return _PANEL_FN_CACHE[fn_name]

    # Fast paths
    try:
        cand = globals().get(fn_name)
        if callable(cand):
            _PANEL_FN_CACHE[fn_name] = cand
            return cand
    except Exception:
        pass

    import sys
    try:
        main_mod = sys.modules.get("__main__")
        cand = getattr(main_mod, fn_name, None) if main_mod is not None else None
        if callable(cand):
            _PANEL_FN_CACHE[fn_name] = cand
            return cand
    except Exception:
        pass

    
    # If still not found, try bootstrapping the specific def from later in this file.
    try:
        _bootstrap_forward_defs([fn_name])
        cand = globals().get(fn_name)
        if callable(cand):
            _PANEL_FN_CACHE[fn_name] = cand
            return cand
    except Exception:
        pass

# Robust path: import-walk ui.* modules and search for attribute
    try:
        import ui as _ui_pkg
        import pkgutil as _pkgutil
        import importlib as _importlib
        for m in _pkgutil.walk_packages(_ui_pkg.__path__, _ui_pkg.__name__ + "."):
            modname = m.name
            try:
                mod = _importlib.import_module(modname)
            except Exception:
                continue
            try:
                cand = getattr(mod, fn_name, None)
                if callable(cand):
                    _PANEL_FN_CACHE[fn_name] = cand
                    return cand
            except Exception:
                continue
    except Exception:
        pass

    _PANEL_FN_CACHE[fn_name] = None
    return None


def _render_panel_status_card(status: PanelStatus):
    import streamlit as st
    if status.state == PanelState.AVAILABLE:
        return
    if status.state == PanelState.NOT_GENERATED:
        st.info(status.message)
        if status.missing:
            st.write("Missing artifacts:")
            st.code("\n".join(status.missing))
        return
    if status.state == PanelState.BLOCKED:
        st.warning(status.message)
        return
    if status.state == PanelState.NOT_APPLICABLE:
        st.caption(status.message)
        return
    if status.state == PanelState.DEMO_SUBSTITUTED:
        st.caption(status.message)
        return

def _render_with_contract(panel_fn_name: str, panel_callable):
    import streamlit as st
    contracts = get_panel_contracts()
    c = contracts.get(panel_fn_name)
    if c is None:
        # No contract registered: still render, but never allow silent emptiness
        try:
            panel_callable()
        except Exception as e:
            st.warning(f"Panel error: {e}")
        return
    from ui.panel_availability import evaluate_contract
    status = evaluate_contract(c, st.session_state)
    if status.state != PanelState.AVAILABLE:
        _render_panel_status_card(status)
        return
    try:
        panel_callable()
    except Exception as e:
        st.warning(f"Panel error: {e}")



def _v175_panel_availability_map_panel():
    import streamlit as st
    from ui.panel_contracts import get_panel_contracts
    from ui.panel_availability import evaluate_contract, PanelState

    st.subheader("Panel Availability Map")
    st.caption("Self-explaining UI: every panel reports whether it is available, missing required artifacts, blocked, or not applicable.")

    contracts = get_panel_contracts()
    rows = []
    counts = {s.name: 0 for s in PanelState}

    for fn_name, c in contracts.items():
        status = evaluate_contract(c, st.session_state)
        counts[status.state.name] = counts.get(status.state.name, 0) + 1
        rows.append({
            "panel_fn": fn_name,
            "title": c.title,
            "state": status.state.name,
            "missing_required": ", ".join(status.missing or []),
        })

    c1, c2, c3, c4, c5 = st.columns(5)
    c1.metric("Available", counts.get("AVAILABLE", 0))
    c2.metric("Not generated", counts.get("NOT_GENERATED", 0))
    c3.metric("Blocked", counts.get("BLOCKED", 0))
    c4.metric("Not applicable", counts.get("NOT_APPLICABLE", 0))
    c5.metric("Demo-substituted", counts.get("DEMO_SUBSTITUTED", 0))

    states = ["ALL"] + sorted({r["state"] for r in rows})
    pick = st.selectbox("Filter by state", states, index=0, key="v175_pam_filter")
    show = [r for r in rows if pick == "ALL" or r["state"] == pick]

    st.write("Panels:")
    st.dataframe(show, use_container_width=True, hide_index=True)

    st.divider()
    st.markdown("### Open a panel here")
    opts = ["(select)"] + [f'{r["title"]}  -  {r["panel_fn"]}' for r in rows]
    sel = st.selectbox("Choose panel", opts, index=0, key="v175_pam_open_pick")
    if sel != "(select)":
        fn_name = sel.rsplit("-", 1)[-1].strip()
        st.session_state["v175_pam_focus"] = fn_name

    focus = st.session_state.get("v175_pam_focus")
    if focus:
        st.markdown(f"#### Focus: `{focus}`")
        fn = _resolve_panel_function(focus)
        if callable(fn):
            _render_with_contract(focus, fn)
        else:
            st.warning("Selected panel function not found in this UI build.")

def _render_provenance_sidebar():
    with st.sidebar:
        st.subheader("Run Provenance")
        try:
            v = (BASE_DIR / "VERSION").read_text().strip()
        except Exception:
            v = "unknown"
        st.code(v)
        st.markdown("---")
        st.subheader("Session")
        _exit_confirm = st.checkbox("Confirm exit", value=False, key="shams_exit_confirm")
        if st.button("ðŸ”´ Exit SHAMS", type="primary", use_container_width=True, disabled=not _exit_confirm, key="shams_exit_btn"):
            st.info("SHAMS UI shutdown requested by user.")
            # Hard-exit is the only reliable cross-platform Streamlit shutdown mechanism.
            _os._exit(0)
        st.caption("Authoritative feasibility lives in SHAMS core. Sandbox results are non-authoritative.")

def _feasibility_narrative(point):
    feas = point.get("feasible", False)
    mins = point.get("min_signed_margin", None)
    acts = point.get("active_constraints", [])
    if feas:
        return f"Feasible. Min margin = {mins:.3g}." if isinstance(mins, (int,float)) else "Feasible."
    return f"Infeasible. Limiting constraints: {', '.join(acts[:3]) if acts else 'unknown'}."

def _margin_waterfall(records):
    # records: list of dicts with name + signed_margin
    import pandas as pd
    rows = []
    for r in records:
        name = r.get("name","")
        sm = r.get("signed_margin", None)
        if name and isinstance(sm,(int,float)):
            rows.append({"constraint": name, "signed_margin": sm})
    if not rows:
        st.info("No constraint margins available.")
        return
    df = pd.DataFrame(rows).sort_values("signed_margin")
    st.bar_chart(df.set_index("constraint"))


# ---------------------------------------------------------------------------
# Session-state initialization (prevents AttributeError on first run)
# ---------------------------------------------------------------------------
def _init_session_state() -> None:

    # NOTE: phase1_core import happens later in this file. We defensively import
    # PointInputs here to avoid NameError during early session-state init.
    try:
        from phase1_core import PointInputs as _PointInputs
    except Exception:
        _PointInputs = None

    defaults = {
        # If PointInputs is not available for any reason, fall back to None and let
        # downstream code initialize it in a controlled path.
        "last_point_inp": (_PointInputs(R0_m=1.85, a_m=0.57, kappa=1.8, Bt_T=12.2, Ip_MA=8.0, Ti_keV=15.0, fG=0.8, Paux_MW=20.0)
                           if _PointInputs is not None else None),
        "last_point_out": None,
        "last_solver_log": None,
        "explain_mode": True,
        "expert_mode": False,
        "design_intent": "Power Reactor (net-electric)",
        "scan_df": None,
        "scan_meta": None,
        "scan_log_lines": [],
        "scan_log_text": "",
        "scan_progress": 0.0,
        "scan_queue": [],
        "scan_running": False,
        "scan_future": None,
        "scan_executor": None,

        # Phase 7+ UI persistence
        "de_best_inputs": None,
        "de_best_out": None,
        "de_history": None,
        "robustness_result": None,
    }
    for k, v in defaults.items():
        if k not in st.session_state:
            st.session_state[k] = v

_init_session_state()
# _sync_point_designer_from_last_point_inp()


# Stable Streamlit keys for Point Designer widgets (presets rely on these)
PD_KEYS = {
    "R0_m": "pd_R0_m",
    "a_m": "pd_a_m",
    "kappa": "pd_kappa",
    "delta": "pd_delta",
    "Bt_T": "pd_Bt_T",
    "Ti_keV": "pd_Ti_keV",
    "Paux_MW": "pd_Paux_MW",
    "H98_tgt": "pd_target_H98",
    "Q_tgt": "pd_target_Q",
    "Ip_lo": "pd_Ip_lo",
    "Ip_hi": "pd_Ip_hi",
    "fG_lo": "pd_fG_lo",
    "fG_hi": "pd_fG_hi",
    # Additional stable keys (used for preset propagation)
    "Ti_over_Te": "pd_Ti_over_Te",
    "Paux_for_Q": "pd_Paux_for_Q",
    "magnet_technology": "pd_magnet_technology",
    "Tcoil_K": "pd_Tcoil_K",

    # v318.0 profile bundle knobs (stable keys for presets and promotion)
    "profile_mode": "pd_profile_mode",
    "profile_alpha_T": "pd_profile_alpha_T",
    "profile_alpha_n": "pd_profile_alpha_n",
    "profile_shear_shape": "pd_profile_shear_shape",
    "pedestal_enabled": "pd_pedestal_enabled",
    "pedestal_width_a": "pd_pedestal_width_a",
    "include_bootstrap_pressure_selfconsistency": "pd_include_bootstrap_pressure_selfconsistency",
    "f_bootstrap_consistency_abs_max": "pd_f_bootstrap_consistency_abs_max",

    # v371.0 transport contract library keys
    "include_transport_contracts_v371": "pd_include_transport_contracts_v371",
    "H_required_max_optimistic": "pd_H_required_max_optimistic",
    "H_required_max_robust": "pd_H_required_max_robust",
    # v372.0 neutronicsâ€“materials coupling keys
    "include_neutronics_materials_coupling_v372": "pd_include_nm_coupling_v372",
    "nm_material_class_v372": "pd_nm_material_class_v372",
    "nm_spectrum_class_v372": "pd_nm_spectrum_class_v372",
    "nm_T_oper_C_v372": "pd_nm_T_oper_C_v372",
    "dpa_rate_eff_max_v372": "pd_dpa_rate_eff_max_v372",
    "damage_margin_min_v372": "pd_damage_margin_min_v372",
}


def _push_point_inputs_to_pd_widget_keys(base: Any) -> None:
    """Push a PointInputs-like object into Point Designer widget keys.

    Streamlit evaluates the script top-to-bottom on each rerun.
    If a preset is loaded via a button click, the preset application happens
    *after* the early one-shot sync. To avoid requiring a second click, we
    provide a direct, deterministic propagation routine that can be called
    immediately from the button handler.

    UI-only: this function only mutates widget/session state.
    """
    if base is None:
        return
    try:
        st.session_state[PD_KEYS["R0_m"]] = float(getattr(base, "R0_m"))
        st.session_state[PD_KEYS["a_m"]] = float(getattr(base, "a_m"))
        st.session_state[PD_KEYS["kappa"]] = float(getattr(base, "kappa"))
        st.session_state[PD_KEYS["delta"]] = float(getattr(base, "delta", 0.0) or 0.0)
        st.session_state[PD_KEYS["Bt_T"]] = float(getattr(base, "Bt_T"))
        st.session_state[PD_KEYS["Ti_keV"]] = float(getattr(base, "Ti_keV"))
        st.session_state[PD_KEYS["Paux_MW"]] = float(getattr(base, "Paux_MW"))
    except Exception:
        pass
    # Bounds from preset Ip/fG
    try:
        ip = float(getattr(base, "Ip_MA"))
        st.session_state[PD_KEYS["Ip_lo"]] = max(0.1, 0.80 * ip)
        st.session_state[PD_KEYS["Ip_hi"]] = max(0.2, 1.20 * ip)
    except Exception:
        pass
    try:
        fg = float(getattr(base, "fG"))
        st.session_state[PD_KEYS["fG_lo"]] = max(0.0, fg - 0.20)
        st.session_state[PD_KEYS["fG_hi"]] = min(2.0, fg + 0.20)
    except Exception:
        pass
    # Aux/Q denominator and Ti/Te
    try:
        st.session_state[PD_KEYS["Paux_for_Q"]] = float(getattr(base, "Paux_MW"))
    except Exception:
        pass
    try:
        st.session_state[PD_KEYS["Ti_over_Te"]] = float(getattr(base, "Ti_over_Te", 1.0))
    except Exception:
        pass

    # v371.0 transport contracts
    try:
        st.session_state[PD_KEYS["include_transport_contracts_v371"]] = bool(getattr(base, "include_transport_contracts_v371", False))
        st.session_state[PD_KEYS["H_required_max_optimistic"]] = float(getattr(base, "H_required_max_optimistic", float("nan")))
        st.session_state[PD_KEYS["H_required_max_robust"]] = float(getattr(base, "H_required_max_robust", float("nan")))
    except Exception:
        pass

    # Magnet technology axis (optional in older presets)
    try:
        st.session_state[PD_KEYS["magnet_technology"]] = str(getattr(base, "magnet_technology", "HTS_REBCO") or "HTS_REBCO")
    except Exception:
        pass

    # v318.0: profile bundle knobs (optional in older presets)
    try:
        st.session_state[PD_KEYS["profile_mode"]] = bool(getattr(base, "profile_mode", False))
        st.session_state[PD_KEYS["profile_alpha_T"]] = float(getattr(base, "profile_alpha_T", 1.5))
        st.session_state[PD_KEYS["profile_alpha_n"]] = float(getattr(base, "profile_alpha_n", 1.0))
        st.session_state[PD_KEYS["profile_shear_shape"]] = float(getattr(base, "profile_shear_shape", 0.5))
        st.session_state[PD_KEYS["pedestal_enabled"]] = bool(getattr(base, "pedestal_enabled", False))
        st.session_state[PD_KEYS["pedestal_width_a"]] = float(getattr(base, "pedestal_width_a", 0.05))
        st.session_state[PD_KEYS["include_bootstrap_pressure_selfconsistency"]] = bool(getattr(base, "include_bootstrap_pressure_selfconsistency", False))
        st.session_state[PD_KEYS["f_bootstrap_consistency_abs_max"]] = float(getattr(base, "f_bootstrap_consistency_abs_max", float("nan")))
    except Exception:
        pass
    try:
        st.session_state[PD_KEYS["Tcoil_K"]] = float(getattr(base, "Tcoil_K", 20.0))
    except Exception:
        pass


def apply_reference_preset(ref_key: str) -> None:
    """Load a catalog reference preset into the workspace.

    This is a *UI state* operation:
      - Sets ``last_point_inp`` to the preset PointInputs
      - Sets design intent (reactor vs research) from the catalog metadata
      - Arms a one-shot sync to propagate values into Point Designer widget keys
    """
    cat = reference_catalog()
    if ref_key not in cat:
        raise KeyError(f"Unknown reference preset: {ref_key}")
    ent = cat[ref_key]
    base = ent.get("inputs")
    if base is None:
        raise ValueError(f"Reference preset missing inputs: {ref_key}")

    # Set design intent from catalog
    intent = str(ent.get("intent", "")).strip().lower()
    if intent.startswith("research"):
        st.session_state["design_intent"] = "Experimental Device (research)"
    elif intent:
        st.session_state["design_intent"] = "Power Reactor (net-electric)"

    # Persist as the active workspace input object
    st.session_state["last_point_inp"] = base

    # Immediate propagation into Point Designer widget keys so the Configure
    # panel updates on the same click (no second click required).
    _push_point_inputs_to_pd_widget_keys(base)
    st.session_state["pd_needs_sync"] = False


def apply_legacy_reference_machine(name: str) -> None:
    """Load legacy REFERENCE_MACHINES entry (dict-like) into the workspace."""
    if name not in REFERENCE_MACHINES:
        raise KeyError(f"Unknown legacy preset: {name}")
    d = dict(REFERENCE_MACHINES[name] or {})
    try:
        base = make_point_inputs(**d)
    except Exception as e:
        raise ValueError(f"Legacy preset could not be converted to PointInputs: {name} ({e})")
    st.session_state["last_point_inp"] = base
    _push_point_inputs_to_pd_widget_keys(base)
    st.session_state["pd_needs_sync"] = False


def _compute_run_summary_from_out(out: Dict[str, Any]) -> Dict[str, Any]:
    """Compute a deterministic, UI-safe run summary from outputs.

    Important: this must not depend on which Telemetry deck is visible.
    """
    try:
        cons = evaluate_constraints(out or {})
    except Exception:
        cons = []
    # Hard constraints only
    hard = []
    for c in (cons or []):
        try:
            if str(getattr(c, "severity", "hard")).strip().lower() == "hard":
                hard.append(c)
        except Exception:
            pass
    # Sort worst-first (most negative margin)
    try:
        hard_sorted = sorted(hard, key=lambda c: float(getattr(c, "margin", float("inf"))))
    except Exception:
        hard_sorted = list(hard)

    tight: List[Dict[str, Any]] = []
    for c in hard_sorted[:8]:
        try:
            tight.append(
                {
                    "name": str(getattr(c, "name", "")),
                    "passed": bool(getattr(c, "passed", False)),
                    "margin_frac": float(getattr(c, "margin", float("nan"))),
                    "value": getattr(c, "value", None),
                    "limit": getattr(c, "limit", None),
                    "units": getattr(c, "units", None),
                    "sense": getattr(c, "sense", None),
                    "group": getattr(c, "group", "general"),
                }
            )
        except Exception:
            pass

    # Power closure diagnostic
    closure = float("nan")
    try:
        pin = float(out.get("Pin_MW", float("nan")))
        ploss = float(out.get("Ploss_MW", float("nan")))
        if np.isfinite(pin) and np.isfinite(ploss):
            closure = pin - ploss
    except Exception:
        pass

    return {
        "headline": {
            "Q_DT_eqv": float(out.get("Q_DT_eqv", float("nan"))) if isinstance(out, dict) else float("nan"),
            "H98": float(out.get("H98", float("nan"))) if isinstance(out, dict) else float("nan"),
            "P_net_e_MW": float(out.get("P_net_e_MW", float("nan"))) if isinstance(out, dict) else float("nan"),
        },
        "power_closure_MW": closure,
        "tightest_hard_constraints": tight,
    }
# One-shot synchronization: when a preset is loaded we set this flag, and on the next
# rerun we push preset values into Point Designer widget keys.
def _render_magnet_authority_panel(out: Dict[str, Any]) -> None:
    """Render Magnet Technology Authority panel (v328.0).

    UI-only: reads frozen truth outputs and displays contract-driven limits and margins.
    """
    if not isinstance(out, dict) or not out:
        return

    with st.expander("ðŸ§² Magnet Authority â€” Technology Regime (v328.0)", expanded=False):
        regime = str(out.get("magnet_regime", "UNKNOWN"))
        tech = str(out.get("magnet_technology", ""))
        contract_sha = str(out.get("magnet_contract_sha256", ""))[:12]
        cls = str(out.get("magnet_fragility_class", "UNKNOWN"))
        mmin = out.get("magnet_margin_min", float("nan"))

        c1, c2, c3, c4 = st.columns(4)
        with c1:
            st.metric("Regime", regime)
        with c2:
            st.metric("Tech string", tech if tech else "â€”")
        with c3:
            st.metric("Class", cls)
        with c4:
            try:
                st.metric("Min margin (frac)", f"{float(mmin):.3g}")
            except Exception:
                st.metric("Min margin (frac)", str(mmin))

        st.caption(f"Contract SHA-256 (prefix): {contract_sha if contract_sha else 'â€”'}")

        # Build a compact table of key limits and values
        rows = []
        def _add(name: str, vkey: str, lkey: str, units: str):
            v = out.get(vkey, float('nan'))
            l = out.get(lkey, float('nan'))
            try:
                v_f = float(v)
                l_f = float(l)
                if (v_f == v_f) or (l_f == l_f):
                    margin = (l_f - v_f) / max(abs(l_f), 1e-9) if (v_f == v_f and l_f == l_f) else float('nan')
                else:
                    margin = float('nan')
            except Exception:
                v_f, l_f, margin = v, l, float('nan')
            rows.append({
                "Quantity": name,
                "Value": v_f,
                "Limit": l_f,
                "Margin(frac)": margin,
                "Units": units,
            })

        _add("TF peak field", "B_peak_T", "B_peak_allow_T", "T")
        _add("TF von Mises stress", "sigma_vm_MPa", "sigma_allow_MPa", "MPa")
        _add("TF engineering J", "J_eng_A_mm2", "J_eng_max_A_mm2", "A/mm^2")
        _add("Coil nuclear heat", "coil_heat_nuclear_MW", "coil_heat_nuclear_max_MW", "MW")
        # SC margin (>=)
        try:
            rows.append({
                "Quantity": "SC critical-surface margin",
                "Value": float(out.get("hts_margin", float("nan"))),
                "Limit": float(out.get("hts_margin_min", float("nan"))),
                "Margin(frac)": float(out.get("hts_margin", float("nan"))) - float(out.get("hts_margin_min", float("nan"))),
                "Units": "-"
            })
        except Exception:
            pass
        try:
            rows.append({
                "Quantity": "Quench proxy margin",
                "Value": float(out.get("quench_proxy_margin", float("nan"))),
                "Limit": float(out.get("quench_proxy_min", float("nan"))),
                "Margin(frac)": float(out.get("quench_proxy_margin", float("nan"))) - float(out.get("quench_proxy_min", float("nan"))),
                "Units": "-"
            })
        except Exception:
            pass

        try:
            import pandas as pd  # type: ignore
            df = pd.DataFrame(rows)
            st.dataframe(df, use_container_width=True, hide_index=True)
        except Exception:
            st.write(rows)

        # Deterministic repair hints (high-level; detailed mapping lives in contract artifact)
        st.markdown("**Deterministic repair levers (non-exhaustive):**")
        st.markdown("- Decrease **Bt** or increase **R0** (reduces required ampere-turns and peak field).")
        st.markdown("- Increase **TF build** (winding/structure) to improve **J** and **stress** margins.")
        st.markdown("- Increase **shielding/build** to reduce **nuclear heat** to coils (when coupled).")

def _sync_point_designer_from_last_point_inp() -> None:
    if not st.session_state.get("pd_needs_sync", False):
        return
    base = st.session_state.get("last_point_inp", None)
    if base is None:
        st.session_state["pd_needs_sync"] = False
        return
    _push_point_inputs_to_pd_widget_keys(base)
    st.session_state["pd_needs_sync"] = False




_sync_point_designer_from_last_point_inp()  # deferred until function is defined

# One-shot candidate apply: any panel may stage a candidate dict for Point Designer.
# This preserves SHAMS law: panels propose; Point Designer remains the frozen truth console.
def _consume_pd_candidate_apply() -> None:
    cand = st.session_state.pop("pd_candidate_apply", None)
    if not isinstance(cand, dict) or not cand:
        return
    try:
        base = make_point_inputs(**cand)
    except Exception:
        try:
            base = PointInputs(**cand)  # type: ignore
        except Exception:
            return
    st.session_state["last_point_inp"] = base
    # Force PD widget sync on next rerun; also push immediately so the same click updates widgets.
    try:
        _push_point_inputs_to_pd_widget_keys(base)
        st.session_state["pd_needs_sync"] = False
    except Exception:
        st.session_state["pd_needs_sync"] = True


def stage_pd_candidate_apply(cand: dict, source: str, note: str | None = None) -> None:
    """Canonical cross-panel handoff to Point Designer + provenance breadcrumb.

    Panels propose candidates. Point Designer evaluates frozen truth.

    v327.4: staging is delegated to ui.handoff to ensure deterministic DSG node-id propagation
    without requiring evaluator execution.
    """
    try:
        from ui.handoff import stage_pd_candidate_apply as _stage  # type: ignore
    except Exception:
        # fallback relative import for environments that package ui as a module
        from .handoff import stage_pd_candidate_apply as _stage  # type: ignore
    _stage(cand=cand, source=source, note=note)

def _verification_report_paths():
    rep = os.path.join(ROOT, "verification", "report.json")
    reqs = os.path.join(ROOT, "requirements", "SHAMS_REQS.yaml")
    reqs_json = os.path.join(ROOT, "requirements", "SHAMS_REQS.json")
    runner = os.path.join(ROOT, "verification", "run_verification.py")
    return rep, reqs, reqs_json, runner

def _verification_needs_run():
    rep, reqs, reqs_json, runner = _verification_report_paths()
    if not os.path.exists(rep):
        return True
    try:
        rep_m = os.path.getmtime(rep)
        deps = [p for p in [reqs, reqs_json, runner] if os.path.exists(p)]
        if not deps:
            return False
        dep_m = max(os.path.getmtime(p) for p in deps)
        return rep_m < dep_m
    except Exception:
        return False

def _run_verification_capture():
    """
    Run verification runner using the current Python interpreter.
    Returns: (ok: bool, stdout: str, stderr: str, seconds: float)
    """
    rep, reqs, reqs_json, runner = _verification_report_paths()
    t0 = time.time()
    if not os.path.exists(runner):
        return False, "", f"Missing verification runner: {runner}", 0.0
    try:
        proc = subprocess.run(
            [sys.executable, runner],
            cwd=ROOT,
            capture_output=True,
            text=True,
            check=False,
        )
        dt = time.time() - t0
        ok = (proc.returncode == 0) and os.path.exists(rep)
        return ok, (proc.stdout or ""), (proc.stderr or ""), dt
    except Exception as e:
        dt = time.time() - t0
        return False, "", f"{type(e).__name__}: {e}", dt


from phase1_core import (
    PointInputs,
    hot_ion_point,
    solve_Ip_for_H98_with_Q_match,
    solve_Ip_for_H98_with_Q_match_stream,
    solve_sparc_envelope,
    solve_for_targets,
    solve_for_targets_stream,
    SolveResult,
    optimize_design,
)
from frontier.frontier import find_nearest_feasible
from models.reference_machines import REFERENCE_MACHINES, reference_catalog
from phase1_models import BH_COEFFS
from constraints.constraints import evaluate_constraints
from solvers.optimize import scan_feasible_and_pareto, pareto_front
from docs.variable_registry import registry_dataframe
from shams_io.run_artifact import build_run_artifact
from shams_io.plotting import plot_radial_build_from_artifact, plot_summary_pdf
from solvers.sensitivity import finite_difference_sensitivities


# --- Defensive constructor: UI may pass knobs that are absent in older/newer src/PointInputs
# This keeps the UI stable across PointInputs refactors (extra fields are ignored).
_POINTINPUTS_FIELDS = {f.name for f in fields(PointInputs)}

def make_point_inputs(**kwargs) -> PointInputs:
    """Create PointInputs using only supported dataclass fields."""
    filtered = {k: v for k, v in kwargs.items() if k in _POINTINPUTS_FIELDS}
    return PointInputs(**filtered)



# -----------------------------
# UI helpers
# -----------------------------
def _safe_get(obj, key: str, default=None):
    """Dict/dataclass-safe getter.

    SHAMS UI sometimes handles base objects loaded from JSON artifacts (dict)
    or from in-memory dataclass/namespace objects. This helper preserves
    deterministic fallback semantics without attribute-access crashes.
    """
    if isinstance(obj, dict):
        return obj.get(key, default)
    return getattr(obj, key, default)


def _num(label: str, value: float, step: float, fmt: str = None, help: str = None, min_value=None, max_value=None, key: str = None):
    # Streamlit raises if default value is outside [min_value, max_value].
    # Clamp defensively so the UI remains robust even if bounds change.
    v = float(value)
    if min_value is not None:
        try:
            v = max(v, float(min_value))
        except Exception:
            pass
    if max_value is not None:
        try:
            v = min(v, float(max_value))
        except Exception:
            pass
    kwargs = {}
    if fmt: kwargs["format"] = fmt
    if help: kwargs["help"] = help
    if min_value is not None: kwargs["min_value"] = min_value
    if max_value is not None: kwargs["max_value"] = max_value
    return st.number_input(label, value=v, step=float(step), key=key, **kwargs) if key else st.number_input(label, value=v, step=float(step), **kwargs)


def _warn_unrealistic_point_inputs(pi: Any, context: str = "") -> None:
    """Non-blocking, UI-only warnings for obviously unrealistic user inputs.

    This must not change any model/solver behavior; it only surfaces warnings.
    """
    if pi is None:
        return
    # (lo, hi, message)
    checks = [
        ("R0_m", 0.5, 15.0, "Major radius R0 [m] looks unusual"),
        ("a_m", 0.1, 5.0, "Minor radius a [m] looks unusual"),
        ("kappa", 1.0, 3.5, "Elongation Îº looks unusual"),
        ("delta", -0.8, 0.8, "Triangularity Î´ looks unusual"),
        ("Bt_T", 0.5, 25.0, "Toroidal field Bt [T] looks unusual"),
        ("Ip_MA", 0.1, 30.0, "Plasma current Ip [MA] looks unusual"),
        ("Ti_keV", 0.1, 40.0, "Ion temperature Ti [keV] looks unusual"),
        ("fG", 0.05, 1.5, "Greenwald fraction fG looks unusual"),
        ("Paux_MW", 0.0, 300.0, "Auxiliary power Paux [MW] looks unusual"),
        ("t_shield_m", 0.05, 2.0, "Shield thickness t_shield [m] looks unusual"),
    ]
    warns: List[str] = []
    for name, lo, hi, msg in checks:
        if not hasattr(pi, name):
            continue
        try:
            v = float(getattr(pi, name))
        except Exception:
            continue
        if (v < lo) or (v > hi):
            warns.append(f"- {msg}: **{name}={v:g}** (expected roughly {lo:g}â€“{hi:g})")
    if warns:
        title = "Unrealistic inputs" + (f" ({context})" if context else "")
        st.warning(title + "\n" + "\n".join(warns))


# -----------------------------
# Scan Lab parameter metadata (UI-only)
# -----------------------------

# Human-friendly physics block names (used in tooltips + mapping table)
_PHYS_BLOCKS: Dict[str, str] = {
    "Geometry": "Machine geometry / size assumptions",
    "Magnets & radial build": "TF/HTS coil build, inboard stack closure, peak field mapping, stress",
    "0-D plasma core": "0-D profiles, fusion power, temperatures, density, basic scalings",
    "ðŸŒ€ Confinement": "Energy confinement (IPB98-like) + confinement multipliers",
    "H-mode access": "L-H threshold (Martin-08-like) + margin screening",
    "Stability & limits": "q95, \u03b2N, bootstrap fraction and related operational screens",
    "Power balance & radiation": "Zeff/dilution/radiation and alpha deposition assumptions",
    "Divertor / SOL": "SOL power loading proxy (PSOL/R) and divertor heat-flux screen",
    "â˜¢ï¸ Neutronics": "TBR proxy + HTS fluence/lifetime proxy",
    "Electrical balance": "Recirculating power closure and net electric power screen",
    "Numerics": "Solver bounds/tolerance and feasibility filtering",
}

# For Scan Lab UI: which parameters are mandatory vs optional + which physics blocks they affect.
_SCAN_PARAM_META: Dict[str, Dict[str, Any]] = {
    # Machine / plasma assumptions
    "R0": {"req": True, "blocks": ["Geometry", "Magnets & radial build", "0-D plasma core", "Divertor / SOL"]},
    "B0": {"req": True, "blocks": ["Magnets & radial build", "0-D plasma core", "ðŸŒ€ Confinement", "Stability & limits"]},
    "tshield": {"req": True, "blocks": ["Magnets & radial build", "â˜¢ï¸ Neutronics"]},
    "Paux": {"req": True, "blocks": ["0-D plasma core", "Power balance & radiation", "H-mode access", "Electrical balance"]},
    "Paux_for_Q": {"req": True, "blocks": ["0-D plasma core"]},
    "Ti_over_Te": {"req": True, "blocks": ["0-D plasma core", "Power balance & radiation"]},

    # Axes
    "Ti": {"req": True, "blocks": ["0-D plasma core", "ðŸŒ€ Confinement", "Power balance & radiation"]},
    "H98": {"req": True, "blocks": ["ðŸŒ€ Confinement"]},
    "a": {"req": True, "blocks": ["Geometry", "0-D plasma core", "Stability & limits", "Divertor / SOL", "Magnets & radial build"]},
    "Q": {"req": True, "blocks": ["0-D plasma core", "Electrical balance"]},
    "g_conf": {"req": True, "blocks": ["ðŸŒ€ Confinement"]},

    # Solver bounds & screens
    "Ip_bounds": {"req": True, "blocks": ["Numerics", "0-D plasma core", "Stability & limits", "Magnets & radial build"]},
    "fG_bounds": {"req": True, "blocks": ["Numerics", "0-D plasma core"]},
    "tol": {"req": True, "blocks": ["Numerics"]},

    # Screening knobs (plasma)
    "Zeff": {"req": True, "blocks": ["Power balance & radiation"]},
    "dilution_fuel": {"req": True, "blocks": ["Power balance & radiation", "0-D plasma core"]},
    "extra_rad_factor": {"req": True, "blocks": ["Power balance & radiation"]},
    "alpha_loss_frac": {"req": True, "blocks": ["Power balance & radiation"]},
    "kappa": {"req": True, "blocks": ["Stability & limits", "0-D plasma core"]},
    "q95_min": {"req": True, "blocks": ["Stability & limits"]},
    "betaN_max": {"req": True, "blocks": ["Stability & limits"]},
    "C_bs": {"req": True, "blocks": ["Stability & limits"]},
    "f_bs_max": {"req": True, "blocks": ["Stability & limits"]},
    "PSOL_over_R_max": {"req": True, "blocks": ["Divertor / SOL"]},

    # Optional toggle
    "require_Hmode": {"req": False, "blocks": ["H-mode access"]},
    "PLH_margin": {"req": False, "blocks": ["H-mode access"]},

    # Clean design knobs (engineering screens)
    "tblanket_m": {"req": False, "blocks": ["Magnets & radial build", "â˜¢ï¸ Neutronics"]},
    "t_vv_m": {"req": False, "blocks": ["Magnets & radial build"]},
    "t_gap_m": {"req": False, "blocks": ["Magnets & radial build"]},
    "t_tf_struct_m": {"req": False, "blocks": ["Magnets & radial build"]},
    "t_tf_wind_m": {"req": False, "blocks": ["Magnets & radial build"]},
    "Bpeak_factor": {"req": False, "blocks": ["Magnets & radial build"]},
    "sigma_allow_MPa": {"req": False, "blocks": ["Magnets & radial build"]},
    "Tcoil_K": {"req": False, "blocks": ["Magnets & radial build"]},
    "hts_margin_min": {"req": False, "blocks": ["Magnets & radial build"]},
    "Vmax_kV": {"req": False, "blocks": ["Magnets & radial build"]},
    "q_div_max_MW_m2": {"req": False, "blocks": ["Divertor / SOL"]},
    "q_midplane_max_MW_m2": {"req": False, "blocks": ["Divertor / SOL"]},
    "TBR_min": {"req": False, "blocks": ["â˜¢ï¸ Neutronics"]},
    "hts_lifetime_min_yr": {"req": False, "blocks": ["â˜¢ï¸ Neutronics"]},
    "P_net_min_MW": {"req": False, "blocks": ["Electrical balance"]},
}


def _scan_badge(param_key: str) -> str:
    meta = _SCAN_PARAM_META.get(param_key)
    # Default to optional if unknown
    is_req = bool(meta.get("req")) if isinstance(meta, dict) else False
    return "ðŸŸ¥ Mandatory" if is_req else "â¬œ Optional"


def _scan_blocks(param_key: str) -> List[str]:
    meta = _SCAN_PARAM_META.get(param_key)
    if not isinstance(meta, dict):
        return []
    return [b for b in meta.get("blocks", []) if b in _PHYS_BLOCKS]


def _scan_label(base: str, param_key: str) -> str:
    # number_input labels do not render markdown; keep it simple + consistent.
    return f"{base}  Â·  {_scan_badge(param_key)}"


def _scan_help(base_help: str, param_key: str) -> str:
    blocks = _scan_blocks(param_key)
    if not blocks:
        return base_help
    lines = [base_help.strip(), "", "Maps to physics blocks:"]
    for b in blocks:
        lines.append(f"- {b}: {_PHYS_BLOCKS[b]}")
    return "\n".join(lines).strip()

def kpi_row(items: List[Tuple[str, Any]]):
    cols = st.columns(len(items))
    for c, (k, v) in zip(cols, items):
        c.metric(k, v)


def _numeric_cols(df: pd.DataFrame) -> List[str]:
    cols = []
    for c in df.columns:
        try:
            if pd.api.types.is_numeric_dtype(df[c]):
                cols.append(c)
        except Exception:
            pass
    return cols


def plot_scatter(df: pd.DataFrame, x: str, y: str, color: str | None = None, title: str | None = None):
    """Matplotlib scatter with optional numeric color."""
    if df is None or df.empty:
        st.info("No data to plot.")
        return
    if x not in df or y not in df:
        st.warning("Select valid x/y columns.")
        return

    d = df[[x, y] + ([color] if color and color in df else [])].dropna()
    if d.empty:
        st.info("No finite rows for this plot (after dropping NaNs).")
        return
    # Matplotlib is optional: if missing, fall back to Streamlit's built-in charts.
    if not _HAVE_MPL:
        st.warning("Plotting is limited because 'matplotlib' is not installed. Install it (pip install matplotlib) for full plotting.")
        # Streamlit fallback (no colorbar support)
        try:
            st.scatter_chart(d, x=x, y=y)
        except Exception:
            st.line_chart(d[[x, y]].rename(columns={x: "x", y: "y"}))
        return

    fig = plt.figure(figsize=(6.8, 4.6))
    ax = plt.gca()
    if color and color in d and pd.api.types.is_numeric_dtype(d[color]):
        sc = ax.scatter(d[x], d[y], c=d[color], s=22, alpha=0.85)
        cb = plt.colorbar(sc, ax=ax)
        cb.set_label(color)
    else:
        ax.scatter(d[x], d[y], s=22, alpha=0.85)

    ax.set_xlabel(x)
    ax.set_ylabel(y)
    if title:
        ax.set_title(title)
    ax.grid(True, alpha=0.25)
    st.pyplot(fig, clear_figure=True)


def plot_bars(values: Dict[str, float], title: str):
    keys = [k for k in values.keys() if isinstance(values.get(k), (int, float)) and math.isfinite(float(values.get(k))) ]
    if not keys:
        st.caption("No plottable values available.")
        return
    if not _HAVE_MPL:
        st.warning("Bar charts are limited because 'matplotlib' is not installed. Install it (pip install matplotlib) for full plotting.")
        import pandas as _pd
        s = _pd.Series({k: float(values[k]) for k in keys})
        try:
            st.bar_chart(s)
        except Exception:
            st.write(s)
        return

    fig = plt.figure(figsize=(6.8, 4.4))
    ax = plt.gca()
    ax.bar(range(len(keys)), [float(values[k]) for k in keys])
    ax.set_xticks(range(len(keys)), keys, rotation=35, ha="right")
    ax.set_title(title)
    ax.grid(True, axis="y", alpha=0.25)
    st.pyplot(fig, clear_figure=True)

def badge(check):
    """Render PASS/FAIL/WARN/SKIPPED badge.
    Accepts either a check dict with 'status' or a legacy ok flag.
    """
    if isinstance(check, dict):
        stt = check.get('status')
        if stt == 'SKIPPED':
            return 'âšª SKIPPED'
        if stt == 'WARN':
            return 'ðŸŸ¡ WARN'
        if stt == 'FAIL':
            return ' FAIL'
        if stt == 'PASS':
            return ' PASS'
        # fallback
        ok = check.get('ok')
    else:
        ok = check
    if ok is None:
        return 'âšª SKIPPED'
    return ' PASS' if ok else ' FAIL'


def finite(x):
    return isinstance(x, (int, float)) and math.isfinite(x)

def top_violations(checks: List[Dict[str, Any]], n: int = 3) -> List[Dict[str, Any]]:
    bad = [c for c in checks if c.get('status') == 'FAIL']

    # sort by relative violation if available
    def score(c):
        if c.get("value") is None or c.get("limit") is None:
            return 0.0
        v, lim = c["value"], c["limit"]
        if not (finite(v) and finite(lim)) or lim == 0:
            return 0.0
        if c.get("sense") == "max":
            return (v - lim) / abs(lim)
        if c.get("sense") == "min":
            return (lim - v) / abs(lim)
        return 0.0
    bad.sort(key=score, reverse=True)
    return bad[:n]

def top_warnings(checks: List[Dict[str, Any]], n: int = 3) -> List[Dict[str, Any]]:
    ws = [c for c in checks if c.get('status') == 'WARN']
    def score(c):
        v = c.get('value'); lim = c.get('limit'); sense = c.get('sense')
        if v is None or lim is None or not finite(v) or not finite(lim):
            return 0.0
        if sense == 'max':
            return max(0.0, (v - lim) / abs(lim))
        if sense == 'min':
            return max(0.0, (lim - v) / abs(lim))
        return 0.0
    ws.sort(key=score, reverse=True)
    return ws[:n]

def compute_checks(out: Dict[str, float]) -> List[Dict[str, Any]]:
    """
    Turn flat outputs into a structured constraint list.

    IMPORTANT:
    - Checks are only evaluated when the needed physics is enabled *and* the value is finite.
    - If a model/physics block is disabled (or produced NaN), the check is marked SKIPPED.
    - Checks can be WARNING-level or HARD FAIL depending on how far they are from the limit.
    """
    checks: List[Dict[str, Any]] = []


    # global warning behavior knobs (can be overridden per-check via explicit warn_limit)
    warn_frac_max = float(out.get("_warn_frac_max", 0.90))  # for max constraints: WARN if v > warn_frac*limit
    warn_frac_min = float(out.get("_warn_frac_min", 1.10))  # for min constraints: WARN if v < warn_frac*limit

    def add(name, status, value=None, limit=None, sense=None, notes="", severity="hard", warn_limit=None):
        """status in {'PASS','FAIL','WARN','SKIPPED'}"""
        ok = None
        if status == "PASS":
            ok = True
        elif status == "FAIL":
            ok = False
        elif status == "WARN":
            ok = True  # warn is still 'ok' for legacy consumers
        elif status == "SKIPPED":
            ok = None
        checks.append({
            "name": name,
            "status": status,
            "ok": ok,
            "value": value,
            "limit": limit,
            "warn_limit": warn_limit,
            "sense": sense,
            "notes": notes,
            "severity": severity,
        })

        # ok can be True/False/None (None => skipped)
        checks.append({"name": name, "ok": ok if ok in (True, False, None) else bool(ok),
                       "value": value, "limit": limit, "sense": sense, "notes": notes})

    def fin(x) -> bool:
        return isinstance(x, (int, float)) and math.isfinite(x)
    def eval_max(name, key_value, key_limit, notes="", severity="hard", warn_limit=None):
        v = out.get(key_value)
        lim = out.get(key_limit)
        if (not fin(v)) or (not fin(lim)):
            add(name, "SKIPPED", v, lim, "max", notes, severity=severity, warn_limit=warn_limit)
            return
        wl = warn_limit
        if wl is None:
            wl = warn_frac_max * lim
        if v > lim:
            add(name, "FAIL", v, lim, "max", notes, severity=severity, warn_limit=wl)
        elif v > wl:
            add(name, "WARN", v, lim, "max", notes, severity=severity, warn_limit=wl)
        else:
            add(name, "PASS", v, lim, "max", notes, severity=severity, warn_limit=wl)

    def eval_min(name, key_value, key_limit, notes="", severity="hard", warn_limit=None):
        v = out.get(key_value)
        lim = out.get(key_limit)
        if (not fin(v)) or (not fin(lim)):
            add(name, "SKIPPED", v, lim, "min", notes, severity=severity, warn_limit=warn_limit)
            return
        wl = warn_limit
        if wl is None:
            wl = warn_frac_min * lim
        # for min constraints, warn if v is between lim and wl (wl > lim)
        if v < lim:
            add(name, "FAIL", v, lim, "min", notes, severity=severity, warn_limit=wl)
        elif v < wl:
            add(name, "WARN", v, lim, "min", notes, severity=severity, warn_limit=wl)
        else:
            add(name, "PASS", v, lim, "min", notes, severity=severity, warn_limit=wl)    # --- Build closure ---
    if "radial_build_ok" in out:
        v = out.get("radial_build_ok")
        if not fin(v):
            add("Radial build closure", "SKIPPED", out.get("rb_total_inboard_m"), out.get("rinboard_available_m"), "max",
                "Requires inboard stack thickness â‰¤ available inboard space (R0 - a).")
        else:
            add("Radial build closure", "PASS" if (v > 0.5) else "FAIL", out.get("rb_total_inboard_m"), out.get("rinboard_available_m"), "max",
                "Requires inboard stack thickness â‰¤ available inboard space (R0 - a).")

    # --- Magnet stress ---


    if "sigma_hoop_MPa" in out and "sigma_allow_MPa" in out:
        eval_max("TF hoop stress", "sigma_hoop_MPa", "sigma_allow_MPa",
                 "Hoop stress proxy must be below allowable structural stress.")

    # --- HTS margin ---
    if "hts_margin" in out and "hts_margin_min" in out:
        eval_min("HTS margin", "hts_margin", "hts_margin_min",
                 "HTS operating margin proxy vs (B,T) must exceed minimum.")

    # --- Dump voltage ---
    if "V_dump_kV" in out and "Vmax_kV" in out:
        eval_max("Dump voltage", "V_dump_kV", "Vmax_kV",
                 "Fast discharge voltage must not exceed protection limit.")

    # --- Divertor heat flux ---

    if "q_div_MW_m2" in out and "q_div_max_MW_m2" in out:
        eval_max("Divertor heat flux", "q_div_MW_m2", "q_div_max_MW_m2",
                 "Peak divertor heat flux proxy must be below limit.")

    # --- Tritium breeding ratio ---
    if "TBR" in out and "TBR_min" in out:
        eval_min("TBR", "TBR", "TBR_min",
                 "Tritium breeding ratio proxy must exceed minimum.")

    # --- TBR proxy validity-domain (v321) ---
    if "TBR_domain_ok" in out:
        ok = out.get("TBR_domain_ok")
        enf = out.get("neutronics_domain_enforce", 0.0)
        if not fin(ok):
            add("TBR validity domain", "SKIPPED", ok, 1.0, "min", "Proxy validity-domain check not available.")
        else:
            if (fin(enf) and enf > 0.5):
                add("TBR validity domain", "PASS" if (ok > 0.5) else "FAIL", ok, 1.0, "min", "If enforced, TBR proxy must be inside declared validity domain.")
            else:
                add("TBR validity domain", "PASS" if (ok > 0.5) else "WARN", ok, 1.0, "min", "Not enforced by default; WARN means the proxy is out-of-domain.")

    # --- HTS lifetime ---
    if "hts_lifetime_yr" in out and "hts_lifetime_min_yr" in out:
        eval_min("HTS lifetime", "hts_lifetime_yr", "hts_lifetime_min_yr",
                 "Neutron lifetime proxy of HTS must exceed minimum.")

    # --- Net electric power ---
    if "P_net_e_MW" in out and "P_net_min_MW" in out:
        eval_min("Net electric power", "P_net_e_MW", "P_net_min_MW",
                 "Net electric power must exceed minimum (system closure).")

    # --- H-mode access (only if enforced) ---
    # If require_Hmode is False or physics is disabled, this becomes SKIPPED.
    if "require_Hmode" in out and "LH_ok" in out:
        req = out.get("require_Hmode")
        lh_ok = out.get("LH_ok")
        if not (fin(req) and req > 0.5):
            add("Hâ€‘mode access", None, lh_ok, 1.0, "min",
                "Hâ€‘mode not required (or LH physics disabled).")
        else:
            if not fin(lh_ok):
                add("Hâ€‘mode access", None, lh_ok, 1.0, "min",
                    "Hâ€‘mode required, but LH physics not available for this point.")
            else:
                add("Hâ€‘mode access", lh_ok > 0.5, lh_ok, 1.0, "min",
                    "If Hâ€‘mode required, point must be above LH threshold with margin.")

    return checks


# -----------------------------
# Scan runner (UI-native)
# -----------------------------
def frange(start: float, stop: float, step: float) -> List[float]:
    vals: List[float] = []
    if step == 0:
        return [start]
    x = start
    if step > 0:
        while x <= stop + 1e-12:
            vals.append(float(x))
            x += step
    else:
        while x >= stop - 1e-12:
            vals.append(float(x))
            x += step
    return vals

def run_scan(spec: Dict[str, Any]) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    """
    A refactor of the CLI scan loop into a UI-callable function.

    Returns:
      df_feasible: rows of extended-feasible points (same spirit as 'feasible_ext' sheet)
      meta: dict with scan settings + summary stats
    """
    Ti_grid = frange(spec["Ti_start"], spec["Ti_stop"], spec["Ti_step"] if spec["Ti_stop"] >= spec["Ti_start"] else -abs(spec["Ti_step"]))
    H_grid = frange(spec["H98_start"], spec["H98_stop"], abs(spec["H98_step"]))
    a_grid = frange(spec["a_min"], spec["a_max"], abs(spec["a_step"]))
    Q_grid = frange(spec["Q_start"], spec["Q_stop"], abs(spec["Q_step"]))
    g_grid = frange(spec["gconf_start"], spec["gconf_stop"], abs(spec["gconf_step"]))


    # --- Scan Lab: optional UI progress + logging hooks (kept no-op for non-UI use) ---
    progress_cb = spec.get("_progress_cb")  # callable(fraction: float, info: dict) -> None
    log_cb = spec.get("_log_cb")            # callable(line: str) -> None
    log_lines: List[str] = []

    def _log(line: str) -> None:
        ts = datetime.datetime.now().strftime("%H:%M:%S")
        s = f"[{ts}] {line}"
        log_lines.append(s)
        if callable(log_cb):
            try:
                log_cb(s)
            except Exception:
                pass

    def _progress(i: int, n: int, **info: Any) -> None:
        if callable(progress_cb) and n > 0:
            try:
                progress_cb(min(max(i / n, 0.0), 1.0), info)
            except Exception:
                pass

    n_total = max(1, len(g_grid) * len(Ti_grid) * len(H_grid) * len(a_grid) * len(Q_grid))
    _log(f"Scan initialized: {len(g_grid)} g_conf Ã— {len(Ti_grid)} Ti Ã— {len(H_grid)} H98 Ã— {len(a_grid)} a Ã— {len(Q_grid)} Q  => {n_total} evaluations")
    i_eval = 0

    rows: List[Dict[str, Any]] = []
    best_g = None

    # --- v327.3: pipeline DSG edge automation (best-effort; does not change truth) ---
    dsg_parent = spec.get("_dsg_parent_node_id")
    if not dsg_parent:
        try:
            dsg_parent = st.session_state.get("dsg_selected_node_id") or st.session_state.get("active_design_node_id")
        except Exception:
            dsg_parent = None

    scan_node_ids: List[str] = []
    scan_edge_note = f"scan: g_conf[{len(g_grid)}] Ti[{len(Ti_grid)}] H98[{len(H_grid)}] a[{len(a_grid)}] Q[{len(Q_grid)}]"


    for g_conf in g_grid:
        for Ti in Ti_grid:
            for Hreq in H_grid:
                for a in a_grid:
                    for Qtar in Q_grid:
                        # Solve at reduced target (same logic as CLI driver)

                        i_eval += 1
                        _progress(i_eval, n_total,
                                  stage="setup",
                                  g_conf=float(g_conf), Ti_keV=float(Ti), H98_req=float(Hreq), a_m=float(a), Q_target=float(Qtar))
                        _log(f"Eval {i_eval}/{n_total}: g_conf={g_conf:.3g}, Ti={Ti:.3g} keV, H98_req={Hreq:.3g}, a={a:.3g} m, Q={Qtar:.3g}")
                        _log("  - Building point inputs (geometry, fields, density/temperature assumptions)")

                        H_base_target = Hreq / max(g_conf, 1e-9)

                        base = make_point_inputs(
                            R0_m=spec["R0"],
                            a_m=a,
                            kappa=spec["kappa"],
                            Bt_T=spec["B0"],
                            Ip_MA=0.5*(spec["Ip_min"]+spec["Ip_max"]),
                            Ti_keV=Ti,
                            fG=0.8,
                            t_shield_m=spec["tshield"],
                            Paux_MW=spec["Paux"],
                            Ti_over_Te=spec["Ti_over_Te"],
                            zeff=spec["Zeff"],
                            dilution_fuel=spec["dilution_fuel"],
                            fuel_mode="DT",
                            include_secondary_DT=include_secondary_DT,
                            tritium_retention=0.5,
                            tau_T_loss_s=5.0,
                            extra_rad_factor=spec["extra_rad_factor"],
                            alpha_loss_frac=spec["alpha_loss_frac"],
                            C_bs=spec["C_bs"],
                            require_Hmode=spec["require_Hmode"],
                            PLH_margin=spec["PLH_margin"],
                            # --- Clean design knobs (passed through PointInputs defaults if present in your src)
                            **spec.get("clean_knobs", {}),
                        )


                        _log("  - Solving nested system: outer Ip for H98, inner fG for Q (bisection)")
                        _progress(i_eval, n_total, stage="solve")
                        sol_inp, sol_out, ok = solve_Ip_for_H98_with_Q_match(
                            base=base,
                            target_H98=H_base_target,
                            target_Q=Qtar,
                            Ip_min=spec["Ip_min"],
                            Ip_max=spec["Ip_max"],
                            fG_min=spec["fG_min"],
                            fG_max=spec["fG_max"],
                            tol=spec["tol"],
                            Paux_for_Q_MW=spec["Paux_for_Q"],
                        )
                        if not ok:
                            _log("  - Solver failed to bracket/converge for this combo (skipping)")
                            continue

                        # Effective confinement
                        H98_eff = g_conf * sol_out["H98"]
                        sol_out["H98_eff"] = H98_eff


                        _log("  - Evaluating physics proxies (power balance, confinement, operational limits)")
                        _progress(i_eval, n_total, stage="evaluate")
                        # Standard ext checks from CLI
                        ok_ext = True
                        if sol_out["ne20"] > 1.2:
                            ok_ext = False
                        if sol_out.get('q95_proxy', 1e9) < spec["q95_min"]:
                            ok_ext = False
                        if sol_out.get("betaN_proxy", 0.0) > spec["betaN_max"]:
                            ok_ext = False
                        if sol_out.get("f_bs_proxy", 0.0) > spec["f_bs_max"]:
                            ok_ext = False
                        PSOL_over_R = sol_out["Ploss_MW"] / spec["R0"]
                        sol_out["PSOL_over_R"] = PSOL_over_R
                        if PSOL_over_R > spec["PSOL_over_R_max"]:
                            ok_ext = False
                        if spec["require_Hmode"] and sol_out.get("LH_ok", 1.0) < 0.5:
                            ok_ext = False
                        if H98_eff < Hreq:
                            ok_ext = False
                        if sol_out["Q_DT_eqv"] < Qtar:
                            ok_ext = False

                        # Clean design checks (if present)
                        checks = compute_checks(sol_out)
                        if any((not c["ok"]) for c in checks):
                            ok_ext = False

                        if not ok_ext:
                            _log("  - Failed screening checks (skipping)")
                            continue

                        if best_g is None or g_conf < best_g:
                            best_g = g_conf

                        _log("  - Feasible point found âœ“ (adding to results)")
                        _progress(i_eval, n_total, stage="record")
                        row = dict(sol_out)
                        row.update({
                            "g_conf": g_conf,
                            "Ti_keV": Ti,
                            "Q_target": Qtar,
                            "H98_required": Hreq,
                            "a_m": a,
                            "Ip_MA": sol_inp.Ip_MA,
                            "f_G": sol_inp.fG,
                            "Paux_MW": sol_inp.Paux_MW,
                            "Paux_for_Q_MW": spec["Paux_for_Q"],
                            "H98_eff": H98_eff,
                        })

                        # --- v327.3: DSG pipeline capture for scan points (best-effort) ---
                        try:
                            g = st.session_state.get("_shams_dsg")
                            if g is not None:
                                node = g.record(
                                    inp=sol_inp,
                                    out=dict(sol_out),
                                    ok=True,
                                    message="scan_feasible",
                                    elapsed_s=0.0,
                                    origin="ScanLab",
                                    parents=[str(dsg_parent)] if dsg_parent else None,
                                    tags=["scan", "feasible"],
                                    edge_kind="scan",
                                    edge_note=scan_edge_note,
                                )
                                scan_node_ids.append(node.node_id)
                                st.session_state["active_design_node_id"] = node.node_id
                        except Exception:
                            pass                        # --- v327.4: pipeline-native dsg_node_id column for scan tables ---
                        try:
                            if "dsg_node_id" not in row:
                                _nid = None
                                try:
                                    _nid = node.node_id  # type: ignore[name-defined]
                                except Exception:
                                    _nid = None
                                if not _nid:
                                    from evaluator.cache_key import sha256_cache_key
                                    _nid = sha256_cache_key(sol_inp)
                                row["dsg_node_id"] = str(_nid)
                        except Exception:
                            pass

                        rows.append(row)

    df = pd.DataFrame(rows)
    meta = dict(spec)
    meta["best_g_conf_found"] = best_g if best_g is not None else "NONE"
    meta["n_feasible"] = int(len(df))

    # Strip non-serializable UI callbacks from meta and attach log text
    meta.pop("_progress_cb", None)
    meta.pop("_log_cb", None)
    meta["scan_log_text"] = "\n".join(log_lines)
    # --- v327.3: expose scan DSG node IDs for downstream panels ---
    try:
        meta["dsg_parent_node_id"] = str(dsg_parent) if dsg_parent else ""
        meta["dsg_scan_node_ids"] = list(scan_node_ids)
        st.session_state["scan_last_node_ids"] = list(scan_node_ids)
        st.session_state["scan_last_parent_node_id"] = str(dsg_parent) if dsg_parent else ""
        _dsg_save_best_effort()
    except Exception:
        pass
    _log(f"Scan complete: feasible={meta['n_feasible']}  best_g_conf_found={meta['best_g_conf_found']}")

    return df, meta

def df_to_excel_bytes(df: pd.DataFrame, meta: Dict[str, Any]) -> bytes:
    """
    Export feasible dataframe + meta into an Excel workbook (in-memory).
    """
    import openpyxl
    from openpyxl.styles import Font

    wb = openpyxl.Workbook()
    ws = wb.active
    ws.title = "feasible_ext"

    if df.empty:
        ws.append(["NO_FEASIBLE_POINTS"])
    else:
        ws.append(list(df.columns))
        for c in range(1, len(df.columns)+1):
            ws.cell(row=1, column=c).font = Font(bold=True)
        ws.freeze_panes = "A2"
        for _, r in df.iterrows():
            ws.append([r.get(c) for c in df.columns])

    # meta sheet
    wsM = wb.create_sheet("meta")
    wsM.append(["key", "value"])
    wsM["A1"].font = Font(bold=True)
    for k, v in meta.items():
        # keep meta compact
        if isinstance(v, dict):
            continue
        if isinstance(v, list):
            continue
        wsM.append([k, v])
    wsM.freeze_panes = "A2"

    bio = io.BytesIO()
    wb.save(bio)
    return bio.getvalue()


# -----------------------------
# Streamlit UI
# -----------------------------
st.set_page_config(page_title=APP_NAME, layout="wide")


# v276: keep last run artifact for cross-panels
if "last_run_artifact" not in st.session_state:
    st.session_state["last_run_artifact"] = {}

_render_branding_header()
_shams_status_strip_placeholder = st.empty()

def _shams_status_strip():
    # Top-of-page professional status strip (Review-room instrumentation)
    locked, task, started, is_owner = _shams_runlock.status(st.session_state.get("_shams_owner_token"), app_start_ts=st.session_state.get("_shams_app_start_ts"))
    prev_locked = bool(st.session_state.get("_shams_prev_locked", False))
    prev_task = st.session_state.get("_shams_prev_task")

    # Completion detection: was locked, now unlocked
    if prev_locked and not locked and prev_task:
        try:
            if not st.session_state.get("silence_mode", False):
                st.toast(f" Sequence Complete: {prev_task}")
        except Exception:
            pass

    st.session_state["_shams_prev_locked"] = locked
    st.session_state["_shams_prev_task"] = task

    with _shams_status_strip_placeholder.container():
        if locked and task:
            age_s = int(_shams_time.time() - float(started or _shams_time.time()))
            st.info(f"âš¡ **Running Sequence** Â· {task} Â· t+{age_s}s Â· All other solver actions are locked.", icon="âš¡")
            # Show a concise tail of the Black-Box Chronicle
            try:
                _lg = _activity_logger()
                tail_text = _lg.path.read_text(encoding="utf-8", errors="replace") if _lg.path.exists() else ""
                tail_lines = tail_text.splitlines()[-20:] if tail_text else []
                if tail_lines:
                    with st.expander("Black-Box Chronicle tail (live)", expanded=False):
                        st.code("\n".join(tail_lines), language="")
            except Exception:
                pass
        else:
            # Calm "ready" strip for experts
            st.caption("Status: Ready Â· Helm Console armed Â· Awaiting next sequence.")

_shams_status_strip()

st.session_state.setdefault('shams_state', SessionStateModel())
# ---- UI polish (CSS) ----
# Streamlit theming is usually done via .streamlit/config.toml, but we keep it self-contained.
st.markdown(
    """
<style>
  /* Slightly tighter overall spacing */
  .block-container { padding-top: 2.2rem; padding-bottom: 3.4rem; }
  h1 { margin-top: 0.25rem; line-height: 1.15; }

  /* Header spacing: avoid negative letter-spacing which can overlap glyphs at some zoom levels */
  h1, h2, h3 { letter-spacing: 0; }

  /* Metric cards: increase contrast and soften edges */
  [data-testid="stMetric"] {
    padding: 0.75rem 0.75rem;
    border-radius: 16px;
    border: 1px solid rgba(49, 51, 63, 0.15);
    background: rgba(255, 255, 255, 0.03);
  }

  /* Buttons */
  div.stButton > button {
    border-radius: 14px;
    padding: 0.55rem 0.9rem;
    font-weight: 650;
  }

  /* Expander */
  details {
    border-radius: 14px;
    border: 1px solid rgba(49, 51, 63, 0.12);
    padding: 0.25rem 0.4rem;
  }

  /* Code blocks */
  pre {
    border-radius: 14px;
  }

  /* Dataframes */
  .stDataFrame { border-radius: 14px; overflow: hidden; }
</style>
    """,
    unsafe_allow_html=True,
)

# Persistent footer (render early so st.stop paths still show it)
_render_footer()
st.sidebar.markdown("## Helm Console - Expert Navigation")

# Session & Authority (professional, read-mostly posture)
_forge_review_mode = bool(st.session_state.get("forge_review_mode", False))
_posture = "Review Mode (locked)" if _forge_review_mode else "Explore Mode"
st.sidebar.caption("Captainâ€™s Ledger")
st.sidebar.markdown(f"""- **Posture:** {_posture}
- **Authority:** Frozen evaluator
- **Workspace:** Non-authoritative""")

st.session_state.explain_mode = st.sidebar.toggle(
    "Explain mode (show equations & reasons)",
    value=bool(st.session_state.get("explain_mode", True)),
    help="Teaching mode: show model equations, assumptions, and why constraints bind.",
)

with st.sidebar.expander("Advanced controls", expanded=False):
    st.session_state.expert_mode = st.toggle(
        "Expert controls",
        value=bool(st.session_state.get("expert_mode", False)),
        disabled=_forge_review_mode,
        help="Expose solver tolerances and optimizer internals. Disabled in Review Mode.",
    )

# ---------------------------------------------------------------------------
# ---------------------------------------------------------------------------
# Design Intent - influences what counts as "hard" in Systems/Optimization
# ---------------------------------------------------------------------------
_design_intent_prev = st.session_state.get("design_intent", "Power Reactor (net-electric)")
with st.sidebar.expander("Reactor Covenant", expanded=False):
    st.session_state["design_intent"] = st.selectbox(
        "Intent",
        ["Power Reactor (net-electric)", "Experimental Device (research)"],
        index=0 if ("reactor" in _design_intent_prev.lower() or _design_intent_prev.lower().startswith("power")) else 1,
        help="Reactor intent enforces strict engineering/plant constraints (e.g., TBR, stress, heat flux). Research intent relaxes/softens some constraints to explore physics-relevant machines.",
    )

# (Design contract continues below in Verification/Fidelity panels)

if st.session_state.get("design_intent") != _design_intent_prev:

    _invalidate_mode_caches("design_intent_changed")

    try:
        _alog("UI", "DesignIntentChanged", {"from": _design_intent_prev, "to": st.session_state.get("design_intent")})
    except Exception:
        pass

def _design_intent_key() -> str:
    s = str(st.session_state.get("design_intent", "Power Reactor (net-electric)")).strip().lower()
    if s.startswith("experimental") or s.startswith("research") or ("research" in s):
        return "research"
    return "reactor"

# Canonical constraint enforcement sets (names must match constraint 'name' strings)
# NOTE: q95 is enforced hard in both intents per user request.
_INTENT_HARD = {
    "reactor": {"q95", "q_div", "P_SOL/R", "sigma_vm", "B_peak", "HTS margin", "TBR", "NWL"},
    "research": {"q95"},
}
_INTENT_SOFT = {
    "reactor": set(),
    "research": {"q_div", "P_SOL/R", "sigma_vm", "B_peak", "HTS margin", "NWL"},  # shown, not enforced as hard
}
_INTENT_IGNORE = {
    "reactor": set(),
    "research": {"TBR"},  # research machines may not aim for breeding blanket
}

def _hard_constraint_names_for_intent() -> set[str]:
    k = _design_intent_key()
    return set(_INTENT_HARD.get(k, set()))

def _ignored_constraint_names_for_intent() -> set[str]:
    k = _design_intent_key()
    return set(_INTENT_IGNORE.get(k, set()))


def _constraint_policy_snapshot() -> dict:
    """Return an intent-aware constraint policy snapshot (UI/exports)."""
    k = _design_intent_key()
    return {
        "design_intent": str(st.session_state.get("design_intent", "Power Reactor (net-electric)")),
        "intent_key": k,
        "hard_blocking": sorted([str(x) for x in _INTENT_HARD.get(k, set())]),
        "diagnostic_only": sorted([str(x) for x in _INTENT_SOFT.get(k, set())]),
        "ignored": sorted([str(x) for x in _INTENT_IGNORE.get(k, set())]),
    }


def _classify_failed_constraints(failed_names: list[str] | None) -> dict:
    """Classify failed constraint names into blocking/diagnostic/ignored per current intent."""
    failed = [str(x) for x in (failed_names or [])]
    hard_set = _hard_constraint_names_for_intent()
    ign_set = _ignored_constraint_names_for_intent()
    blocking = [c for c in failed if c in hard_set]
    ignored = [c for c in failed if c in ign_set]
    diagnostic = [c for c in failed if (c not in blocking and c not in ignored)]
    return {"blocking": blocking, "diagnostic": diagnostic, "ignored": ignored}








# ---------------------------------------------------------------------------
# Integrity gate (requirements & health)
# ---------------------------------------------------------------------------
with st.sidebar.expander("Integrity Gate - Requirements & Health", expanded=False):
    """A reviewer-safe, deterministic health/requirements check.

    Important: Streamlit executes UI code even when an expander is closed; therefore we avoid
    auto-running anything here. The user explicitly presses the run button.
    """

    rep_path, reqs_path, reqs_json_path, runner_path = _verification_report_paths()

    # Lightweight health checks (no subprocess): these are instantaneous and always safe.
    def _health_rows() -> list[dict[str, Any]]:
        rows: list[dict[str, Any]] = []
        try:
            rows.append({"Check": "Python", "Status": f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"})
        except Exception:
            rows.append({"Check": "Python", "Status": "unknown"})

        try:
            repo_ok = os.path.exists(os.path.join(ROOT_DIR, "MANIFEST_SHA256.txt"))
            rows.append({"Check": "Repo manifest", "Status": "present" if repo_ok else "missing"})
        except Exception:
            rows.append({"Check": "Repo manifest", "Status": "unknown"})

        try:
            out_dir = os.path.join(ROOT_DIR, "benchmarks", "publication")
            rows.append({"Check": "Benchmarks folder", "Status": "present" if os.path.isdir(out_dir) else "missing"})
        except Exception:
            rows.append({"Check": "Benchmarks folder", "Status": "unknown"})

        try:
            # Write-permission probe in the current working directory (deterministic + harmless)
            probe_dir = os.path.join(ROOT_DIR, ".shams_probe")
            os.makedirs(probe_dir, exist_ok=True)
            probe_file = os.path.join(probe_dir, "write_test.txt")
            with open(probe_file, "w", encoding="utf-8") as f:
                f.write("ok")
            rows.append({"Check": "Write access", "Status": "ok"})
        except Exception:
            rows.append({"Check": "Write access", "Status": "blocked"})
        return rows

    needs = _verification_needs_run()
    report_exists = os.path.exists(rep_path)
    status_line = (
        "ðŸŸ¢ Evidence report: up-to-date" if (report_exists and not needs) else
        "ðŸŸ  Evidence report: needs update" if report_exists else
        "ðŸ”´ Evidence report: missing"
    )
    st.caption("Run this only when you want an explicit, archived compliance report. Nothing runs automatically.")
    st.markdown(status_line)

    c1, c2 = st.columns([1, 1])
    force = c1.button("Run gatecheck", use_container_width=True)
    show_logs = c2.toggle("Show logs", value=bool(st.session_state.get("verify_show_logs", False)))
    st.session_state["verify_show_logs"] = show_logs

    # Provide the latest report (if present)
    if report_exists:
        try:
            rep_bytes = Path(rep_path).read_bytes()
            st.download_button(
                "Download evidence report (JSON)",
                data=rep_bytes,
                file_name="shams_verification_report.json",
                mime="application/json",
                use_container_width=True,
            )
        except Exception:
            st.caption("Evidence report download unavailable (read error).")

    with st.expander("Instant health snapshot", expanded=False):
        try:
            st.dataframe(pd.DataFrame(_health_rows()), hide_index=True, use_container_width=True)
        except Exception:
            st.json(_health_rows(), expanded=False)

    if force:
        with st.spinner("Running gatecheck (verification/run_verification.py)..."):
            ok, out, err, dt = _run_verification_capture()
        st.session_state["_last_verify_ok"] = ok
        st.session_state["_last_verify_out"] = out
        st.session_state["_last_verify_err"] = err
        st.session_state["_last_verify_dt"] = dt
        st.rerun()

    if st.session_state.get("_last_verify_dt") is not None:
        ok = bool(st.session_state.get("_last_verify_ok", False))
        (st.success if ok else st.error)(
            f"Last gatecheck: {'PASS' if ok else 'FAIL'} ({st.session_state.get('_last_verify_dt', 0.0):.2f}s)"
        )
        # If failing, surface a short, high-signal reason even when logs are hidden.
        if not ok:
            _e = (st.session_state.get("_last_verify_err", "") or "").strip()
            _o = (st.session_state.get("_last_verify_out", "") or "").strip()
            _msg = ""
            if _e:
                _msg = _e.splitlines()[0]
            elif _o:
                _msg = _o.splitlines()[0]
            if _msg:
                st.caption(f"Gatecheck detail: {_msg}  (toggle **Show logs** for full output)")

    if show_logs:
        st.text_area("stdout", value=str(st.session_state.get("_last_verify_out", "")), height=160)
        st.text_area("stderr", value=str(st.session_state.get("_last_verify_err", "")), height=160)


# ---------------------------------------------------------------------------
# Fidelity + Calibration (transparent (systems-code-inspired), transparent)
# ---------------------------------------------------------------------------
with st.sidebar.expander("Model Authority & Closures", expanded=False):
    fid = st.session_state.get("fidelity_config", {})
    plasma = st.selectbox("ðŸ”¥ Plasma", ["0D","1/2D"], index=0 if fid.get("plasma","0D")=="0D" else 1)
    magnets = st.selectbox("ðŸ§² Magnets", ["limits","stress"], index=0 if fid.get("magnets","limits")=="limits" else 1)
    exhaust = st.selectbox("ðŸ’¨ Exhaust", ["proxy","enriched"], index=0 if fid.get("exhaust","proxy")=="proxy" else 1)
    neutronics = st.selectbox("â˜¢ï¸ Neutronics", ["proxy","enriched"], index=0 if fid.get("neutronics","proxy")=="proxy" else 1)
    profiles = st.selectbox("Profiles", ["off","analytic"], index=0 if fid.get("profiles","off")=="off" else 1)
    economics = st.selectbox("ðŸ’° Economics", ["proxy","enriched"], index=0 if fid.get("economics","proxy")=="proxy" else 1)
    st.session_state["fidelity_config"] = {
        "plasma": plasma,
        "magnets": magnets,
        "exhaust": exhaust,
        "neutronics": neutronics,
        "profiles": profiles,
        "economics": economics,
    }


with st.sidebar.expander("Reference calibration (optional)", expanded=False):
    st.caption("Transparent multiplicative factors (default 1.0). These are **declared knobs** (not a blackâ€‘box fit). "
               "They are recorded into artifacts for reviewer-safe reproducibility.")

    cL, cR = st.columns([1, 1])
    with cL:
        if st.button("Reset factors to 1.0", use_container_width=True):
            st.session_state["calib_confinement"] = 1.0
            st.session_state["calib_divertor"] = 1.0
            st.session_state["calib_bootstrap"] = 1.0
    with cR:
        st.markdown("**Scope**")
        st.markdown("- Confinement â†’ Ï„â‚‘ / power balance")
        st.markdown("- Divertor â†’ exhaust proxy severity")
        st.markdown("- Bootstrap â†’ I_bs closure")

    st.session_state["calib_confinement"] = st.slider(
        "Confinement factor (H-like multiplier)",
        0.5, 1.5,
        float(st.session_state.get("calib_confinement", 1.0)),
        0.01,
        help="Multiplies the confinement closure used by the frozen evaluator. Use for reference calibration / sensitivity, not tuning for feasibility."
    )
    st.session_state["calib_divertor"] = st.slider(
        "Divertor factor (exhaust proxy multiplier)",
        0.5, 1.5,
        float(st.session_state.get("calib_divertor", 1.0)),
        0.01,
        help="Scales the exhaust/proxy model severity (e.g., qâŠ¥-like gate). Recorded in artifacts."
    )
    st.session_state["calib_bootstrap"] = st.slider(
        "Bootstrap factor (I_bs multiplier)",
        0.5, 1.5,
        float(st.session_state.get("calib_bootstrap", 1.0)),
        0.01,
        help="Scales the bootstrap-current closure output prior to current-drive accounting. Recorded in artifacts."
    )


with st.sidebar.expander("Policy Contracts (feasibility semantics)", expanded=False):
    st.caption("Explicit, reviewer-visible enforcement tiering. This does **not** change physics outputs; it only changes whether selected limits are **blocking** or **diagnostic**.")
    _q95_prev = str(st.session_state.get("q95_enforcement","hard"))
    _fg_prev = str(st.session_state.get("greenwald_enforcement","hard"))
    st.session_state["q95_enforcement"] = st.selectbox(
        "q95 enforcement",
        ["hard", "diagnostic"],
        index=0 if str(st.session_state.get("q95_enforcement","hard")).lower().strip()=="hard" else 1,
        help="hard: blocking feasibility gate. diagnostic: computed and reported, but non-blocking (soft).",
    )
    st.session_state["greenwald_enforcement"] = st.selectbox(
        "Greenwald (fG) enforcement",
        ["hard", "diagnostic"],
        index=0 if str(st.session_state.get("greenwald_enforcement","hard")).lower().strip()=="hard" else 1,
        help="hard: blocking feasibility gate. diagnostic: computed and reported, but non-blocking (soft).",
    )
    st.markdown("**Contract**")
    st.markdown("- These settings affect constraint tiering only (HARD vs SOFT).")
    st.markdown("- They are recorded in artifacts under `_policy_contract`.")
    st.markdown("- No hidden iteration, no softening of physics.")

    if (str(st.session_state.get("q95_enforcement","hard")) != _q95_prev) or (str(st.session_state.get("greenwald_enforcement","hard")) != _fg_prev):
        _invalidate_mode_caches("policy_contract_changed")


with st.sidebar.expander("Technology Readiness (TRL Contracts)", expanded=False):
    st.caption("Explicit maturity tiering for governance and evidence packs. This does **not** re-solve physics; it records assumption tier and suggested caps for optional constraints.")
    _tier_prev = str(st.session_state.get("tech_tier", "TRL7"))
    _tiers = ["TRL3", "TRL5", "TRL7", "TRL9"]
    _tier = str(st.session_state.get("tech_tier", "TRL7")).upper().strip()
    if _tier not in _tiers:
        _tier = "TRL7"
    st.session_state["tech_tier"] = st.selectbox(
        "Technology readiness tier",
        _tiers,
        index=_tiers.index(_tier),
        help="Used to label maturity assumptions and (optionally) suggest default caps such as f_recirc_max and TBR_min. Recorded in outputs as _maturity_contract.",
    )
    try:
        from contracts.tech_tiers import suggested_defaults  # type: ignore
        _sug = dict(suggested_defaults(str(st.session_state.get("tech_tier","TRL7"))))
    except Exception:
        _sug = {}
    if _sug:
        with st.expander("Suggested defaults (optional)", expanded=False):
            st.json(_sug)
            st.caption("These are *suggestions* for optional caps; SHAMS truth is unchanged unless you explicitly apply them to inputs.")
    if str(st.session_state.get("tech_tier","TRL7")) != _tier_prev:
        _invalidate_mode_caches("tech_tier_changed")

with st.sidebar.expander("Benchmark Vault", expanded=False):
    tabs = st.tabs(["ðŸ›ï¸ Presets", "ðŸ“¦ Benchmarks"])
    with tabs[0]:
        st.caption("Load frozen, reviewer-safe reference machines into the workspace. Presets do **not** modify physics - they set inputs.")
        try:
            _ref_catalog = reference_catalog()
            _ref_keys = sorted(list(_ref_catalog.keys()))
        except Exception:
            _ref_catalog = {}
            _ref_keys = []

        _legacy_names = list(REFERENCE_MACHINES.keys())
        _choices = _ref_keys if _ref_keys else _legacy_names

        _sel = st.selectbox("Preset", _choices, index=0 if _choices else None)
        if _sel and _sel in _ref_catalog:
            _suite = str(_ref_catalog[_sel].get('suite','n/a'))
            _cls = str(_ref_catalog[_sel].get('class','n/a'))
            # User preference: keep details collapsed by default; only user expands.
            with st.expander(f"Suite: {_suite} Â· Class: {_cls}", expanded=False):
                st.code(_shams_json_dumps(_ref_catalog[_sel], indent=2), language="json")
        elif _sel and _sel in REFERENCE_MACHINES:
            st.markdown("**Legacy preset** (inline table).")
        if st.button("Load preset", use_container_width=True, disabled=not bool(_sel)):
            try:
                if _sel in _ref_catalog:
                    apply_reference_preset(_sel)
                else:
                    apply_legacy_reference_machine(_sel)
                st.success("Preset loaded into workspace inputs.")
            except Exception as e:
                st.error(f"Preset load failed: {e}")

    with tabs[1]:
        st.caption("Benchmark packs are deterministic evidence generators. Use **Publication Benchmarks** for full packs.")
        st.markdown("**Quick actions**")
        c1, c2 = st.columns(2)
        with c1:
            if st.button("Show last benchmark pack (if any)", use_container_width=True):
                _p = st.session_state.get("publication_benchmark_last_outdir")
                if _p:
                    st.code(str(_p))
                else:
                    st.info("No benchmark pack recorded in this session yet.")
        with c2:
            st.markdown("Tip: Run ` Publication Benchmarks â†’ Generate Pack` for reviewer-safe CSV+JSON+hashes.")
# (Benchmark Vault rendered above)
_activity_log_sidebar_placeholder = st.sidebar.empty()


def _render_activity_log_sidebar() -> None:
    try:
        _lg = _activity_logger()
        with _activity_log_sidebar_placeholder.container():
            st.markdown("### Black-Box Chronicle")
            st.toggle(
                "Auto-log (recommended)",
                value=bool(st.session_state.get("activity_log_auto", True)),
                key="activity_log_auto",
            )
            _tail = st.number_input(
                "Show last N lines",
                min_value=50,
                max_value=2000,
                value=int(st.session_state.get("activity_log_tail", 200)),
                step=50,
                key="activity_log_tail",
            )

            # Always read from disk so the view reflects everything logged,
            # even if it happened later in this same run.
            try:
                tail_text = _lg.path.read_text(encoding="utf-8", errors="replace") if _lg.path.exists() else ""
            except Exception:
                tail_text = ""
            if int(_tail) > 0 and tail_text:
                tail_lines = tail_text.splitlines()[-int(_tail) :]
                tail_text = "\n".join(tail_lines)

            st.text_area(
                "Log (tail)",
                value=tail_text,
                height=220,
                key="activity_log_view",
                disabled=True,
            )

            _c1, _c2 = st.columns(2)
            with _c1:
                st.download_button(
                    "Download log",
                    data=(tail_text + "\n" if tail_text else ""),
                    file_name="activity.log",
                    mime="text/plain",
                    use_container_width=True,
                    key="activity_log_download",
                )
            with _c2:
                if st.button("Clear log", use_container_width=True, key="activity_log_clear_btn"):
                    try:
                        _alog("UI", "ClearLog", {})
                    except Exception:
                        pass

                    # Clear log on disk (authoritative: reflect immediately).
                    try:
                        _lg.clear()
                    except Exception:
                        pass
                    try:
                        st.session_state["activity_log_view"] = ""
                    except Exception:
                        pass
                    st.rerun()

            # -------------------------------------------------------------------
            # Session shutdown (Exit SHAMS) â€” MUST be visible in UI
            # -------------------------------------------------------------------
            st.markdown("---")
            st.markdown("### Session shutdown")
            _exit_confirm = st.checkbox(
                "Confirm exit",
                value=bool(st.session_state.get("shams_exit_confirm", False)),
                key="shams_exit_confirm",
                help="Safety latch to prevent accidental shutdown.",
            )
            if st.button(
                "ðŸ”´ Exit SHAMS",
                type="primary",
                use_container_width=True,
                disabled=not bool(_exit_confirm),
                key="shams_exit_btn",
                help="Hard-exit Streamlit process. Only reliable cross-platform shutdown.",
            ):
                try:
                    _alog("UI", "ExitRequested", {})
                except Exception:
                    pass
                st.info("SHAMS UI shutdown requested by user.")
                _os._exit(0)
    except Exception:
        # Never block UI.
        return

# Render once immediately so the panel is visible even if a downstream
# st.stop() occurs (some panels intentionally stop execution). We re-render
# again at end-of-file to include same-run events.
try:
    _render_activity_log_sidebar()
except Exception:
    pass

# ---------------------------------------------------------------------------
# Forward-definition bootstrap
#
# This UI file contains many panel functions defined *after* the main routing
# section. Streamlit executes top-to-bottom; if a panel is requested before its
# def is reached, it will appear as "not found". We proactively bootstrap the
# panel defs that are referenced by contracts/layer registry.
# ---------------------------------------------------------------------------
try:
    from ui.layer_registry import get_layer_registry
    _reg = get_layer_registry()
    _panel_names = []
    for _layer in _reg.get("layers", []):
        for _p in _layer.get("panels", []):
            n = _p.get("fn") or _p.get("panel_fn") or _p.get("fn_name")
            if isinstance(n, str) and n.startswith("_v"):
                _panel_names.append(n)
    # Also include all contract-registered panels.
    try:
        from ui.panel_contracts import get_panel_contracts
        _panel_names.extend(list(get_panel_contracts().keys()))
    except Exception:
        pass
    _panel_names = _unique(_panel_names)
    if _panel_names:
        _bootstrap_forward_defs(_panel_names)
except Exception:
    # Never block UI on bootstrap failure.
    pass


# --- Deck Navigation (v372.2 hotfix) ---
# Streamlit tabs reset to the first tab on reruns, which can cause the UI to "jump" back
# to Point Designer when interacting with other decks (and can unbind solver parameter
# names on button-click reruns). We therefore use a deterministic, persisted deck selector.
_DECK_LABELS = [
    "ðŸ§­ Point Designer",
    "ðŸ§  Systems Mode",
    "ðŸ—ºï¸ Scan Lab",
    "ðŸ“ˆ Pareto Lab",
    "ðŸ§ª Trade Study Studio",
    "âš’ï¸ Reactor Design Forge",
    "ðŸ§° System Suite",
    "ðŸ†š Compare",
    "ðŸ“š Publication Benchmarks",
    "ðŸŽ›ï¸ Control Room",
]
with st.sidebar:
    st.markdown("## Navigation")
    _sel = st.radio(
        "Deck",
        _DECK_LABELS,
        index=int(st.session_state.get("nav_deck_index", 0)),
        label_visibility="collapsed",
        key="nav_deck_label",
    )
    st.session_state["nav_deck_index"] = _DECK_LABELS.index(_sel)
_deck = _sel

# Phase-1 cache aliasing (no compute, no truth mutation)
try:
    _phase1_stabilize_cache_aliases()
except Exception:
    pass




if _deck == "ðŸ§° System Suite":
    st.header("ðŸ§° System Suite")
    st.caption("System-code diagnostics as *read-only overlays* on the frozen Point Designer truth.")
    render_mode_scope("suite")

    # Pull the most recent Point Designer artifact from Streamlit session state.
    # (Do not depend on later-defined internal state helpers; keep this block early-safe.)
    _point_art = st.session_state.get("pd_last_artifact", None)
    if not isinstance(_point_art, dict):
        _point_art = st.session_state.get("last_point_artifact", None)

    _point_inp = None
    _point_out = None
    if isinstance(_point_art, dict):
        _point_inp = _point_art.get("inputs")
        _point_out = _point_art.get("outputs")
    if not isinstance(_point_out, dict):
        _point_out = st.session_state.get("last_point_out", None)
    if not isinstance(_point_inp, dict):
        _point_inp = st.session_state.get("last_point_inp", None)

    if not isinstance(_point_out, dict):
        st.info("Run **Point Designer** first to populate System Suite diagnostics.")
    else:
        try:
            from tools.system_suite import (
                power_closure_overlay,
                trajectory_diagnostics_client,
                lifetime_and_fuel_overlay,
                ops_availability_overlay,
                thermal_network_diagnostics_client,
            )
        except Exception as _e:
            st.error(f"System Suite import failed: {_e}")
            power_closure_overlay = None  # type: ignore
            trajectory_diagnostics_client = None  # type: ignore
            lifetime_and_fuel_overlay = None  # type: ignore
            ops_availability_overlay = None  # type: ignore
            thermal_network_diagnostics_client = None  # type: ignore

        t_closure, t_auth, t_ops, t_traj, t_life, t_phase, t_regimes, t_prof, t_campaign, t_parity, t_uq = st.tabs([
            "ðŸ§® Closure Ledger",
            "ðŸ” Authority Vault",
            "ðŸ”¥ Ops & Thermal",
            "â±ï¸ Trajectory Lab",
            "ðŸ§¬ Lifetime & Fuel",
            "ðŸ—ºï¸ Phase Envelopes",
            "ðŸ§­ Regime Transitions",
            "ðŸ“œ Profile Contracts 2.0",
            "ðŸš€ Campaign Pack",
            "ðŸ†š Benchmark & Parity Harness 3.0",
            "ðŸ›¡ï¸ Uncertainty Contracts",
        ])

        with t_closure:
            st.subheader("Closure Ledger")
            if power_closure_overlay is None:
                st.warning("Power closure overlay unavailable.")
            else:
                rep = power_closure_overlay(_point_out, _point_inp if isinstance(_point_inp, dict) else None)
                c1, c2, c3, c4 = st.columns(4)
                c1.metric("Gross electric (MW)", f"{rep.Pe_gross_MW:.2f}" if math.isfinite(rep.Pe_gross_MW) else "-")
                c2.metric("Recirc (MW)", f"{rep.Precirc_MW:.2f}" if math.isfinite(rep.Precirc_MW) else "-")
                c3.metric("Net electric (MW)", f"{rep.Pe_net_MW:.2f}" if math.isfinite(rep.Pe_net_MW) else "-")
                c4.metric("Recirc fraction", f"{100.0*rep.recirc_frac:.1f}%" if math.isfinite(rep.recirc_frac) else "-")
                st.caption(f"Stamp: {rep.stamp_sha256[:12]}â€¦")
                with st.expander("Breakdown (diagnostic)", expanded=False):
                    st.json(rep.breakdown, expanded=False)

        with t_auth:
            st.subheader("Authority Vault")
            st.caption("Deterministic, versioned scenario libraries. These affect *robustness screening* only.")
            try:
                from tools.scenario_library import preset_names, get_preset
                presets = preset_names()
                sel = st.selectbox("Scenario preset", ["(select)"] + presets, index=0, key="system_suite_scen_preset_v250")
                if sel != "(select)":
                    st.code(json.dumps(get_preset(sel), indent=2, sort_keys=True), language="json")
            except Exception as _e:
                st.warning(f"Scenario library unavailable: {_e}")

            with st.expander("Authority ladder (policy)", expanded=False):
                st.markdown(
                    """
                    **Proxy** â†’ conservative screening models\
                    **Parametric** â†’ PROCESS-style regressions/closures\
                    **External (hashed)** â†’ imported authoritative results with SHA-256 stamping\
                    
                    SHAMS rule: *authority changes never modify feasibility truth silently; they are stamped and visible.*
                    """
                )

        with t_ops:
            st.subheader("Operations & Thermal")
            st.caption("System operations overlays (duty cycle, availability) and thermal envelope diagnostics. Read-only.")
            t_ops_duty, t_ops_therm = st.tabs(["ðŸ•“ Duty & Availability", "ðŸŒ¡ï¸ Thermal Network"])

            with t_ops_duty:
                if ops_availability_overlay is None:
                    st.warning("Operations overlay unavailable.")
                else:
                    # Expert-friendly: user-controlled availability, with deterministic defaults shown.
                    default_av = None
                    av = st.slider("Availability (fraction)", 0.0, 1.0, 0.75, 0.01, key="ops_availability_slider_v253")
                    rep = ops_availability_overlay(_point_out, _point_inp if isinstance(_point_inp, dict) else None, availability=float(av))
                    c1, c2, c3, c4 = st.columns(4)
                    c1.metric("Duty cycle", f"{100.0*rep.duty_cycle:.1f}%")
                    c2.metric("Availability", f"{100.0*rep.availability:.1f}%")
                    c3.metric("Avg delivered (MW)", f"{rep.avg_delivered_MW:.2f}")
                    c4.metric("Annual energy (GWh)", f"{rep.annual_energy_GWh:.1f}")
                    st.caption(f"Stamp: {rep.stamp_sha256[:12]}â€¦")
                    with st.expander("Breakdown (diagnostic)", expanded=False):
                        st.json(rep.breakdown, expanded=False)

            with t_ops_therm:
                if thermal_network_diagnostics_client is None:
                    st.warning("Thermal diagnostics unavailable.")
                else:
                    tr = thermal_network_diagnostics_client(_point_out, _point_inp if isinstance(_point_inp, dict) else None)
                    st.caption(f"Stamp: {tr.stamp_sha256[:12]}â€¦")
                    df = pd.DataFrame({"t_s": tr.t_s})
                    for k, v in tr.nodes_K.items():
                        df[f"T_{k}_K"] = v
                    st.line_chart(df, x="t_s", y=[c for c in df.columns if c != "t_s"], height=220)
                    if tr.violations:
                        st.error("Thermal violations detected (diagnostic).")
                        st.dataframe(pd.DataFrame(tr.violations), use_container_width=True, hide_index=True)
                    else:
                        st.success("No thermal violations detected (within available limits).")
                    with st.expander("Thermal meta", expanded=False):
                        st.json(tr.meta, expanded=False)

        with t_traj:
            st.subheader("Trajectory Lab")
            st.caption("Deterministic envelope trajectory diagnostic (not a control solver).")
            if trajectory_diagnostics_client is None:
                st.warning("Trajectory diagnostics unavailable.")
            else:
                tr = trajectory_diagnostics_client(_point_out, _point_inp if isinstance(_point_inp, dict) else None)
                c1, c2, c3, c4 = st.columns(4)
                c1.metric("Net peak (MW)", f"{tr.meta.get('Pnet_peak_MW', 0.0):.2f}")
                c2.metric("Net avg (MW)", f"{tr.meta.get('Pnet_avg_MW', 0.0):.2f}")
                c3.metric("Recirc peak (MW)", f"{tr.meta.get('Precirc_peak_MW', 0.0):.2f}")
                c4.metric("Recirc energy (MJ)", f"{tr.meta.get('Erecirc_MJ', 0.0):.1f}")
                st.caption(f"Stamp: {tr.stamp_sha256[:12]}â€¦")
                df_tr = pd.DataFrame({"t_s": tr.t_s, "P_net_MW": tr.Pe_net_MW, "P_recirc_MW": tr.Precirc_MW})
                st.line_chart(df_tr, x="t_s", y=["P_net_MW", "P_recirc_MW"], height=220)

                if tr.violations:
                    st.error("Trajectory violations detected (diagnostic).")
                    st.dataframe(pd.DataFrame(tr.violations), use_container_width=True, hide_index=True)
                else:
                    st.success("No trajectory violations detected (within available limits).")

        with t_life:
            st.subheader("Lifetime & Fuel")
            st.caption("Static feasibility overlays: lifetime budgets, pulsed fatigue proxy, and tritium closure.")
            if lifetime_and_fuel_overlay is None:
                st.warning("Lifetime/fuel overlay unavailable.")
            else:
                lr = lifetime_and_fuel_overlay(_point_out, _point_inp if isinstance(_point_inp, dict) else None)
                c1, c2, c3 = st.columns(3)
                c1.metric("FW dpa/yr", f"{lr.fw_dpa_per_year:.2f}" if math.isfinite(lr.fw_dpa_per_year) else "-")
                c2.metric("FW dpa max", f"{lr.fw_dpa_max_per_year:.2f}" if math.isfinite(lr.fw_dpa_max_per_year) else "-")
                c3.metric("FW margin", f"{lr.fw_dpa_margin:.2f}" if math.isfinite(lr.fw_dpa_margin) else "-")

                d1, d2, d3 = st.columns(3)
                d1.metric("Cycles/yr", f"{lr.cycles_per_year:.0f}" if math.isfinite(lr.cycles_per_year) else "-")
                d2.metric("Cycles max", f"{lr.cycles_max:.0f}" if math.isfinite(lr.cycles_max) else "-")
                d3.metric("Cycle margin", f"{lr.cycles_margin:.2f}" if math.isfinite(lr.cycles_margin) else "-")

                e1, e2, e3 = st.columns(3)
                e1.metric("TBR", f"{lr.tbr:.3f}" if math.isfinite(lr.tbr) else "-")
                e2.metric("TBR min", f"{lr.tbr_min:.3f}" if math.isfinite(lr.tbr_min) else "-")
                e3.metric("TBR margin", f"{lr.tbr_margin:.3f}" if math.isfinite(lr.tbr_margin) else "-")

                st.caption(f"Stamp: {lr.stamp_sha256[:12]}â€¦")
                with st.expander("Raw overlay JSON", expanded=False):
                    st.json({"fw": {"dpa_per_year": lr.fw_dpa_per_year, "dpa_max": lr.fw_dpa_max_per_year, "margin": lr.fw_dpa_margin},
                             "cycles": {"per_year": lr.cycles_per_year, "max": lr.cycles_max, "margin": lr.cycles_margin},
                             "tbr": {"tbr": lr.tbr, "min": lr.tbr_min, "margin": lr.tbr_margin},
                             "stamp": lr.stamp_sha256}, expanded=False)

        with t_phase:
            st.subheader("Phase Envelopes")
            st.caption("Outer-loop quasi-static phases evaluated against the frozen truth. Worst-phase determines verdict.")
            try:
                from ui.phase_envelopes import render_phase_envelopes_panel
                render_phase_envelopes_panel(
                    REPO_ROOT,
                    point_artifact=_point_art if isinstance(_point_art, dict) else None,
                    ui_key_prefix="pd_phase_env",
                )
            except Exception as _e:
                st.error(f"Phase Envelopes panel import failed: {_e}")

        with t_regimes:
            st.subheader("Regime Transitions")
            st.caption("Deterministic labels and near-boundary flags derived from the last Point Designer artifact. Read-only.")

            _rt = None
            if isinstance(_point_art, dict):
                _rt = _point_art.get("regime_transitions")
            if not isinstance(_rt, dict):
                try:
                    from src.analysis.regime_transition_detector_v353 import evaluate_regime_transitions
                except Exception:
                    try:
                        from analysis.regime_transition_detector_v353 import evaluate_regime_transitions  # type: ignore
                    except Exception:
                        evaluate_regime_transitions = None  # type: ignore
                if evaluate_regime_transitions is not None:
                    try:
                        _rt = evaluate_regime_transitions(
                            inputs=_point_inp if isinstance(_point_inp, dict) else {},
                            outputs=_point_out if isinstance(_point_out, dict) else {},
                        )
                    except Exception:
                        _rt = None

            if not isinstance(_rt, dict):
                st.warning("Regime transition detector unavailable.")
            else:
                st.info(str(_rt.get("regime_summary", "")) or "")
                labels = _rt.get("labels", {}) if isinstance(_rt.get("labels"), dict) else {}
                c1, c2, c3, c4, c5 = st.columns(5)
                c1.metric("Confinement", str(labels.get("confinement_regime", "-")))
                c2.metric("Exhaust", str(labels.get("exhaust_regime", "-")))
                c3.metric("Magnet", str(labels.get("magnet_regime", "-")))
                c4.metric("Greenwald", str(labels.get("greenwald_state", "-")))
                c5.metric("Î²N", str(labels.get("betaN_state", "-")))

                with st.expander("Near-boundary flags", expanded=False):
                    st.json(_rt.get("near_boundaries", []), expanded=False)
                with st.expander("Detector context", expanded=False):
                    st.json(_rt.get("context", {}), expanded=False)

        with t_prof:
            st.subheader("Profile Contracts 2.0")
            st.caption("Robust vs optimistic feasibility under certified profile/transport envelopes (finite corners).")
            render_mode_scope("profile_contracts")

            try:
                from src.analysis.profile_contracts_v362 import evaluate_profile_contracts_v362
            except Exception:
                try:
                    from analysis.profile_contracts_v362 import evaluate_profile_contracts_v362  # type: ignore
                except Exception:
                    evaluate_profile_contracts_v362 = None  # type: ignore

            if evaluate_profile_contracts_v362 is None:
                st.error("Profile Contracts module unavailable.")
            elif not isinstance(_point_inp, dict):
                st.info("Run **Point Designer** first (inputs required).")
            else:
                try:
                    from src.models.inputs import PointInputs
                except Exception:
                    from models.inputs import PointInputs  # type: ignore

                c1, c2, c3 = st.columns(3)
                preset = c1.selectbox("Corner preset", ["C8", "C16", "C32"], index=0, key="pc_preset_v362")
                tier = c2.selectbox("Contract tier", ["both", "optimistic", "robust"], index=0, key="pc_tier_v362")
                include_disabled = c3.checkbox(
                    "Force-enable v358 profile family", value=False, key="pc_force_enable_v358_v362"
                )

                st.caption("Tip: robust-feasible implies envelope-certified; optimistic-feasible but robust-infeasible is a MIRAGE.")

                run_btn = st.button("Run Profile Contracts", key="pc_run_v362")
                if run_btn:
                    # Ensure profile family library is active if user requests.
                    d = dict(_point_inp)
                    if include_disabled:
                        d["include_profile_family_v358"] = True
                    inp = PointInputs.from_dict(d)
                    rep = evaluate_profile_contracts_v362(inp, preset=str(preset), tier=str(tier))
                    rep_d = rep.to_dict()
                    st.session_state["profile_contracts_v362_last"] = rep_d

                rep_d = st.session_state.get("profile_contracts_v362_last", None)
                if isinstance(rep_d, dict):
                    v_rob = bool(rep_d.get("robust_feasible"))
                    v_opt = bool(rep_d.get("optimistic_feasible"))
                    mir = bool(rep_d.get("mirage"))

                    k1, k2, k3, k4 = st.columns(4)
                    k1.metric("Optimistic feasible", "YES" if v_opt else "NO")
                    k2.metric("Robust feasible", "YES" if v_rob else "NO")
                    k3.metric("MIRAGE", "YES" if mir else "NO")
                    k4.metric("Corners", str(rep_d.get("corner_count", "-")))

                    st.caption(f"Contract SHA-256: {str(rep_d.get('contract_sha256',''))[:12]}â€¦ | Run fingerprint: {str(rep_d.get('run_fingerprint_sha256',''))[:12]}â€¦")

                    with st.expander("Summary", expanded=False):
                        st.json(rep_d.get("summary", {}), expanded=False)

                    with st.expander("Corners (expandable)", expanded=False):
                        # Light table: omit heavy constraints payload by default.
                        rows = []
                        for c in rep_d.get("corners", []) or []:
                            if not isinstance(c, dict):
                                continue
                            rows.append({
                                "tier": c.get("tier"),
                                "corner": c.get("corner_index"),
                                "hard_feasible": c.get("hard_feasible"),
                                "min_margin_frac": c.get("min_margin_frac"),
                                **{f"ax_{k}": v for k, v in (c.get("axes") or {}).items()},
                            })
                        try:
                            st.dataframe(rows, use_container_width=True)
                        except Exception:
                            st.json(rows, expanded=False)

                    with st.expander("Full report JSON", expanded=False):
                        st.json(rep_d, expanded=False)

                    # Optional ZIP export (deterministic)
                    try:
                        from tools.profile_contracts_v362 import export_profile_contracts_zip
                        from pathlib import Path
                        import tempfile
                        if st.button("Export Profile Contracts ZIP", key="pc_export_zip_v362"):
                            td = Path(tempfile.gettempdir()) / "shams_profile_contracts"
                            td.mkdir(parents=True, exist_ok=True)
                            out_zip = td / "profile_contracts_v362_report.zip"
                            export_profile_contracts_zip(rep_d, out_zip)
                            st.download_button(
                                "Download ZIP",
                                data=out_zip.read_bytes(),
                                file_name="profile_contracts_v362_report.zip",
                                mime="application/zip",
                                key="pc_dl_zip_v362",
                            )
                    except Exception:
                        pass

        with t_campaign:
            st.subheader("Campaign Pack")
            st.caption("Deterministic campaign exports for external optimizers (firewalled).")
            render_mode_scope("campaign_pack")

            try:
                from tools.campaign_pack_v363 import render_campaign_pack_panel
            except Exception as _e:
                render_campaign_pack_panel = None  # type: ignore
                st.error(f"Campaign Pack import failed: {_e}")

            if render_campaign_pack_panel is None:
                st.info("Campaign Pack panel unavailable.")
            else:
                render_campaign_pack_panel(
                    repo_root=REPO_ROOT,
                    point_inputs=_point_inp if isinstance(_point_inp, dict) else None,
                )


        with t_parity:
            st.subheader("Benchmark & Parity Harness 3.0")
            st.caption("Deterministic parity study harness. PROCESS results are optional user-supplied references.")
            render_mode_scope("parity_harness")
            try:
                from tools.benchmark_parity_harness_v364 import render_benchmark_parity_harness_v364
            except Exception as _e:
                render_benchmark_parity_harness_v364 = None  # type: ignore
                st.warning(f"Parity harness unavailable: {_e}")
            if render_benchmark_parity_harness_v364 is None:
                st.info("Parity harness module not available.")
            else:
                render_benchmark_parity_harness_v364()

        with t_uq:
            st.subheader("Uncertainty Contracts")
            st.caption("Outer-loop deterministic interval corners (2^N). Verdict: ROBUST_PASS / FRAGILE / FAIL.")
            try:
                from ui.uncertainty_contracts import render_uncertainty_contracts_panel
                render_uncertainty_contracts_panel(
                    REPO_ROOT,
                    point_artifact=_point_art if isinstance(_point_art, dict) else None,
                    ui_key_prefix="pd_uq_contracts",
                )
            except Exception as _e:
                st.error(f"Uncertainty Contracts panel import failed: {_e}")




if _deck == "ðŸ“š Publication Benchmarks":
    st.header(" Benchmarks")
    st.caption("Benchmark suite: publication tables and the Tokamak Constitutional Atlas (preset-driven, intent-aware).")

    _pb_tabs = st.tabs([
        "ðŸ—ºï¸ Tokamak Constitutional Atlas",
        "ðŸ§­ Crossâ€‘Code Constitutions",
        "ðŸ“¦ Publication Benchmarks",
        "ðŸ§¾ Contract Studio",
        "ðŸ§¾ Regulatory Evidence Pack Builder (v387)",
    ])
    with _pb_tabs[0]:
        st.subheader("ðŸ—ºï¸ Tokamak Constitutional Atlas")
        st.caption("Select a famous tokamak preset and evaluate under **Research** or **Reactor** intent. No tuning. Deterministic, reviewer-safe.")

        try:
            from benchmarks.constitutional.atlas import evaluate_atlas_case, local_fragility_scan
            from benchmarks.constitutional.constitutions import intent_to_constitution, constitution_diff, pretty_clause
        except Exception as e:
            st.error("Constitutional Atlas modules are unavailable. Check installation / repo integrity.")
            st.exception(e)
        else:
            cat = reference_catalog()
            # Build categories (expert-friendly, stable ordering)
            _items = []
            for k, ent in cat.items():
                _items.append((k, str(ent.get("intent","")), str(ent.get("family","")), str(ent.get("label",k))))
            # deterministic sorting
            _items.sort(key=lambda x: (x[1], x[2], x[3]))

            # Categorize
            def _bucket(intent: str, family: str) -> str:
                it = intent.strip().upper()
                fam = family.strip().upper()
                if it.startswith("RESEARCH"):
                    return "Experimental Devices"
                if fam in ("ITER","JET","DIIID","DIII-D","EAST","KSTAR","JT-60SA","ASDEX","ASDEX-U","AUG","NSTX-U","MAST-U"):
                    return "Large-Scale & Program"
                if fam in ("SPARC","ARC"):
                    return "Compact / HTS"
                return "Reactor Concepts"

            buckets = {}
            for k,intent,family,label in _items:
                b=_bucket(intent,family)
                buckets.setdefault(b, []).append((k,label,intent,family))

            left, right = st.columns([0.32, 0.68], gap="large")
            with left:
                st.markdown("### Preset")
                _bucket_names = list(buckets.keys())
                _bucket_sel = st.radio("Category", _bucket_names, index=0, horizontal=False, label_visibility="collapsed")
                opts = buckets[_bucket_sel]
                labels = [o[1] for o in opts]
                keys = [o[0] for o in opts]
                sel_label = st.radio("Tokamak", labels, index=0)
                preset_key = keys[labels.index(sel_label)]

                st.markdown("### Intent")
                intent_sel = st.radio("Intent", ["Research", "Reactor"], index=0, horizontal=True)

                st.caption("Preset key")
                st.code(preset_key)

            with right:
                # Run evaluation
                with st.spinner("Evaluating preset under selected intentâ€¦"):
                    res = evaluate_atlas_case(preset_key, intent_sel)

                # Decks
                decks = st.tabs([" Verdict", "ðŸ§¾ Constitution Diff", "ðŸ“ Envelope & Fragility", "ðŸ“¦ Evidence"])
                with decks[0]:
                    run = res.run or {}
                    verdict = str(run.get("verdict","")).upper() if isinstance(run, dict) else ""
                    dom_mech = run.get("dominant_mechanism","")
                    dom_con = run.get("dominant_constraint","")
                    # v258.0: epoch feasibility summary (if present)
                    epoch_overall = ""
                    epoch_rows = []
                    try:
                        _ef = ((run.get("artifact") or {}).get("epoch_feasibility") or {}) if isinstance(run, dict) else {}
                        epoch_overall = str(_ef.get("overall","") or "")
                        for e in (_ef.get("epochs") or []):
                            if not isinstance(e, dict):
                                continue
                            epoch_rows.append({
                                "epoch": str(e.get("epoch","")),
                                "verdict": str(e.get("verdict","")),
                            })
                    except Exception:
                        epoch_overall = ""
                        epoch_rows = []

                    # derive worst hard margin if present
                    worst = None
                    led = (run.get("constraints", {}) or {}) if isinstance(run, dict) else {}
                    for _,c in led.items():
                        if isinstance(c, dict) and c.get("severity","").lower()=="hard":
                            m = c.get("margin")
                            if isinstance(m,(int,float)):
                                worst = m if worst is None else min(worst, m)
                    st.markdown(f"**Status:** {verdict or '-'}")
                    st.markdown(f"**Dominant mechanism:** {dom_mech or '-'}")
                    st.markdown(f"**Dominant constraint:** {dom_con or '-'}")
                    st.markdown(f"**Worst hard margin:** {('%.3f'%worst) if isinstance(worst,(int,float)) else '-'}")
                    if epoch_overall:
                        st.markdown(f"**Epoch feasibility (overall):** `{epoch_overall}`")
                    if epoch_rows:
                        st.dataframe(epoch_rows, use_container_width=True, hide_index=True)
                    # v256.0: authority confidence badge (trust ledger)
                    try:
                        _art = (run.get("artifact") or {}) if isinstance(run, dict) else {}
                        _ac = (_art.get("authority_confidence") or {}) if isinstance(_art, dict) else {}
                        _dc = str((_ac.get("design") or {}).get("design_confidence_class", "UNKNOWN"))
                        st.markdown(f"**Design confidence:** `{_dc}`")

                        # v366.0: multi-fidelity tier stamp (reviewer-facing)
                        _ft = (_art.get("fidelity_tiers") or {}) if isinstance(_art, dict) else {}
                        _fl = str((_ft.get("design") or {}).get("design_fidelity_label", ""))
                        if _fl:
                            st.markdown(f"**Fidelity tier:** `{_fl}`")

                        _dcon = (_art.get("decision_consequences") or {}) if isinstance(_art, dict) else {}
                        _post = str(_dcon.get("decision_posture", "UNKNOWN"))
                        _risk = str(_dcon.get("primary_risk_driver", "") or "")
                        st.markdown(f"**Decision posture:** `{_post}`")
                        if _risk:
                            st.markdown(f"**Primary risk driver:** `{_risk}`")
                    except Exception:
                        st.markdown("**Design confidence:** `UNKNOWN`")
                    st.caption(f"Native preset intent: **{res.native_intent}** â€¢ Stamp: `{res.stamp_sha256[:12]}â€¦`")

                with decks[1]:
                    st.markdown("**Selected intent constitution**")
                    st.json(res.constitution_selected, expanded=False)
                    st.markdown("**Preset native intent constitution**")
                    st.json(res.constitution_native, expanded=False)

                    diff = res.constitution_diff or []
                    st.markdown("### Diff (selected â†’ native)")
                    if not diff:
                        st.success("No constitutional differences. (Selected intent matches the presetâ€™s native intent semantics.)")
                    else:
                        # Present as compact diff table
                        rows=[]
                        for d in diff:
                            rows.append({
                                "Clause": pretty_clause(d.get("key","")),
                                "Selected": d.get("from",""),
                                "Native": d.get("to",""),
                            })
                        st.table(rows)

                with decks[2]:
                    st.caption("Deterministic local neighborhood scan (small grid) to classify robustness/fragility.")
                    # Choose knobs based on typical availability
                    # Use fG and Paux_MW if present; fall back to H98 and fG
                    base_in = dict(reference_catalog()[preset_key]["inputs"].to_dict())
                    knobs={}
                    if "fG" in base_in:
                        knobs["fG"] = (float(base_in["fG"]), 0.05, 0.05)
                    if "Paux_MW" in base_in:
                        knobs["Paux_MW"] = (float(base_in["Paux_MW"]), 0.10, 0.10)
                    elif "H98" in base_in:
                        knobs["H98"] = (float(base_in["H98"]), 0.05, 0.05)
                    scan = local_fragility_scan(preset_key, intent_sel, knobs)
                    st.markdown(f"**Pass fraction:** {scan.get('pass_fraction',0):.2f}")
                    st.markdown(f"**Mechanism stable:** {'Yes' if scan.get('mechanism_stable', True) else 'No'}")
                    wm = scan.get("worst_margin_min")
                    st.markdown(f"**Worst margin (min):** {('%.3f'%wm) if isinstance(wm,(int,float)) else '-'}")
                    st.json(scan, expanded=False)

                with decks[3]:
                    payload = {
                        "schema": res.schema,
                        "preset_key": res.preset_key,
                        "preset_label": res.preset_label,
                        "selected_intent": res.selected_intent,
                        "native_intent": res.native_intent,
                        "constitution_selected": res.constitution_selected,
                        "constitution_native": res.constitution_native,
                        "constitution_diff": res.constitution_diff,
                        "run": res.run,
                        "stamp_sha256": res.stamp_sha256,
                    }
                    st.download_button(
                        "â¬‡ï¸ Download Atlas Evidence (JSON)",
                        data=json.dumps(payload, indent=2),
                        file_name=f"atlas_{res.selected_intent.lower()}_{res.preset_key.replace('|','_')}.json",
                        mime="application/json",
                        use_container_width=True,
                    )
                    st.caption("This is a deterministic, single-case evidence capsule: inputs, outputs, ledger, constitution semantics, and a stable SHA-256 stamp.")

    

    with _pb_tabs[1]:
        st.subheader("ðŸ§­ Crossâ€‘Code Constitutions")
        st.caption("Documentation-level comparison: map other system codes' effective enforcement semantics against SHAMS intent constitutions. This does not execute external codes; it records and diffs declared clause semantics, with citations.")
        try:
            from benchmarks.crosscode.crosscode_compare import list_crosscode_constitutions, load_crosscode_constitution, compare_to_shams_intent
        except Exception as e:
            st.error(f"Cross-code module import failed: {e}")
        else:
            _items = list_crosscode_constitutions()
            if not _items:
                st.info("No cross-code constitution records found.")
            else:
                labels = [k for k,_ in _items]
                paths_map = {k:p for k,p in _items}
                colA, colB = st.columns([2,1], vertical_alignment="top")
                with colA:
                    code_key = st.selectbox("External code", labels, index=0, key="cc_code_sel")
                with colB:
                    intent = st.radio("SHAMS intent", ["research","reactor"], horizontal=True, key="cc_intent_sel")
                cc = load_crosscode_constitution(paths_map[code_key])
                comp = compare_to_shams_intent(intent, cc)
                c1, c2, c3 = st.columns(3)
                c1.metric("Unknown clauses", comp["unknown_clause_count"])
                c2.metric("Clauses total", len(comp["crosscode_constitution"]["clauses"]))
                c3.metric("Diff entries", len(comp["diff"]))
                st.markdown("#### Notes")
                st.write(cc.source_notes)
                if cc.citations:
                    st.markdown("#### Citations")
                    for c in cc.citations:
                        st.write(f"- {c}")
                st.markdown("#### Constitution Diff")
                st.json(comp["diff"], expanded=False)
                st.markdown("#### Clause Table")
                st.dataframe(
                    [{"clause": k, "shams": comp["baseline_constitution"].get(k, "(missing)"), "external": v}
                     for k, v in sorted(comp["crosscode_constitution"]["clauses"].items())],
                    use_container_width=True,
                    hide_index=True,
                )
                st.markdown("#### ðŸ“¤ Export")
                st.download_button(
                    "Download comparison (JSON)",
                    data=json.dumps(comp, indent=2).encode("utf-8"),
                    file_name=f"crosscode_comparison__{code_key}__{intent}.json",
                    mime="application/json",
                )

    with _pb_tabs[2]:

                # Status pill (simple, honest)
                _status = "ðŸŸ¢ Ready"
                _status_help = "Benchmarks are available. Runs use the frozen evaluator and configured machine list."
                st.markdown(f"**Status:** {_status}")
                st.caption(_status_help)

                # Topology regression (CI-grade): shows whether feasibility topology drifted vs baseline.
                with st.expander(" Topology regression (robust/fragile/empty stability)", expanded=False):
                    rep_path = Path(ROOT) / "verification" / "topology_regression_report.json"
                    if rep_path.exists():
                        try:
                            rep = json.loads(rep_path.read_text(encoding="utf-8"))
                        except Exception:
                            rep = {}
                        ok = bool(rep.get("ok", False))
                        st.markdown(f"**Result:** {' PASS' if ok else ' FAIL'}")
                        if not ok:
                            st.warning("Topology regression failed. See report details below; run gatecheck for full context.")
                        st.json(rep, expanded=False)
                    else:
                        st.info("No topology regression report found yet. Run **Run gatecheck** (or run `python verification/topology_regression.py`).")

                with st.container(border=True):
                    st.subheader("What this does")
                    st.write(
                        "Generates paper-ready benchmark tables and per-machine artifacts by evaluating the configured "
                        "reference machines with the frozen Point Designer. No physics, constraints, or policies are modified."
                    )
                    st.markdown("**Outputs include:**")
                    st.markdown("- CSV benchmark tables (Research & Reactor)\n- Per-machine JSON artifacts (inputs, outputs, constraint ledger)\n- Run metadata (timestamp, version, hash)")

                    cL, cR = st.columns(2, gap="large")
                    with cL:
                        st.markdown("###  Research Machines")
                        st.caption("Policy: Research Intent â€¢ q95 hard â€¢ Plant constraints diagnostic/ignored")
                        st.markdown("- ITER / JET / DIII-D / EAST / KSTAR / JT-60SA\n- SPARC / NSTX-U / MAST-U (as configured)")
                    with cR:
                        st.markdown("### âš¡ Reactor & Pilot Plants")
                        st.caption("Policy: Reactor Intent â€¢ Full feasibility gates enforced")
                        st.markdown("- ARC / ARIES-class\n- EU DEMO (incl. low-A variants)\n- STEP Prototype Plant (as configured)")

                    st.divider()

                    st.markdown("### Reproducibility contract")
                    st.markdown("- âœ” Frozen Point Designer evaluator\n- âœ” Deterministic run (no stochastic elements)\n- âœ” Explicit Design Intent policies\n- âœ” Versioned artifact schema")
                    st.caption("Every run is replayable. Every table is traceable.")

                    st.divider()

                    # One-button launcher with explicit acknowledgment (keeps it review-safe)
                    if "pubbench_ack" not in st.session_state:
                        st.session_state.pubbench_ack = False
                    if "pubbench_running" not in st.session_state:
                        st.session_state.pubbench_running = False
                    if "pubbench_last_outdir" not in st.session_state:
                        st.session_state.pubbench_last_outdir = None
                    if "pubbench_last_rc" not in st.session_state:
                        st.session_state.pubbench_last_rc = None

                    st.markdown("### Action")
                    st.checkbox("I understand this is a non-interactive, audit-grade run.", key="pubbench_ack")

                    run_btn = st.button("â–¶ Generate Publication Benchmark Pack", type="primary", disabled=(not st.session_state.pubbench_ack or st.session_state.pubbench_running))
                    prog = st.empty()
                    log_box = st.empty()

                    if run_btn:
                        st.session_state.pubbench_running = True
                        try:
                            import sys, subprocess, time, os
                            ts = time.strftime("%Y%m%d_%H%M%S")
                            outdir = os.path.join("benchmarks", "publication", "out_ui", ts)
                            os.makedirs(outdir, exist_ok=True)
                            cases = os.path.join("benchmarks", "publication", "cases_point_designer.json")
                            runner = os.path.join("benchmarks", "publication", "run_point_designer_benchmarks.py")

                            cmd = [sys.executable, runner, "--cases", cases, "--outdir", outdir, "--also-run-opposite-intent"]
                            prog.progress(0.05)
                            p = subprocess.run(cmd, capture_output=True, text=True)
                            prog.progress(1.0)

                            st.session_state.pubbench_last_outdir = outdir
                            st.session_state.pubbench_last_rc = int(p.returncode)

                            _out = (p.stdout or "").strip()
                            _err = (p.stderr or "").strip()
                            if _out:
                                log_box.code(_out[:8000])
                            if _err:
                                st.warning("Runner warnings/errors (stderr):")
                                st.code(_err[:8000])

                            if p.returncode == 0:
                                st.success(f"Completed. Output: {outdir}")
                            else:
                                st.error(f"Benchmark run failed (exit code {p.returncode}). Output folder: {outdir}")
                        except Exception as e:
                            st.error(f"Benchmark run failed: {e}")
                        finally:
                            st.session_state.pubbench_running = False

                    if st.session_state.pubbench_last_outdir:
                        st.caption(f"Last output: {st.session_state.pubbench_last_outdir} (rc={st.session_state.pubbench_last_rc})")

                    if st.session_state.pubbench_last_outdir and int(st.session_state.pubbench_last_rc or 0) == 0:
                        st.markdown("#### ðŸ§¾ Pack inspection (topology + delta)")
                        outdir = st.session_state.pubbench_last_outdir
                        # B3: topology
                        try:
                            import os, json
                            topo_p = os.path.join(outdir, "topology.json")
                            if os.path.exists(topo_p):
                                topo = json.loads(open(topo_p, "r", encoding="utf-8").read())
                                cA, cB, cC, cD = st.columns(4)
                                fr = (topo.get("fractions") or {})
                                cA.metric("Pass frac", f"{float(fr.get('pass',0.0)):.2f}")
                                cB.metric("Robust frac", f"{float(fr.get('robust',0.0)):.2f}")
                                cC.metric("Fragile frac", f"{float(fr.get('fragile',0.0)):.2f}")
                                cD.metric("Fail frac", f"{float(fr.get('fail',0.0)):.2f}")
                                st.json({"dominant_mechanism_hist": topo.get("dominant_mechanism_hist", {})})
                        except Exception:
                            pass

                        # B2: delta explainer
                        st.markdown("##### Explain delta vs baseline")
                        try:
                            import os, glob
                            base_dir = os.path.join("benchmarks", "publication", "baselines")
                            opts = []
                            if os.path.isdir(base_dir):
                                # allow baseline packs as directories, else fall back to shipped CSV baseline
                                for d in sorted(glob.glob(os.path.join(base_dir, "*"))):
                                    opts.append(d)
                            baseline_sel = st.selectbox("Baseline pack/folder", opts if opts else [base_dir], index=0, key="pubbench_delta_base_v235")
                            if st.button("ðŸ§  Explain delta (baseline â†’ last pack)", use_container_width=True, key="pubbench_delta_btn_v235"):
                                runner = os.path.join("benchmarks", "publication", "explain_delta.py")
                                cmd = [sys.executable, runner, "--baseline", baseline_sel, "--candidate", outdir]
                                p = subprocess.run(cmd, capture_output=True, text=True)
                                if p.returncode == 0:
                                    st.success("Delta explanation written to delta.md in the candidate pack.")
                                    delta_path = os.path.join(outdir, "delta.md")
                                    if os.path.exists(delta_path):
                                        st.code(open(delta_path, "r", encoding="utf-8").read()[:12000])
                                else:
                                    st.error(f"Delta explainer failed (rc={p.returncode}).")
                                    if p.stderr:
                                        st.code(p.stderr[:8000])
                        except Exception as e:
                            st.warning(f"Delta tools unavailable: {e}")

                st.caption("Exploration happens elsewhere in the UI. Evidence is generated here.")

                st.divider()
                st.markdown("### Evidence exports")
                st.caption("Generate reviewer/regulator and licensing packs from the current session artifact (read-only).")

                with st.expander("ðŸ§¾ Regulatory & Reviewer Evidence Packs (v334)", expanded=False):
                    try:
                        from ui.regulatory_evidence_pack import render_regulatory_evidence_pack_panel
                        render_regulatory_evidence_pack_panel(REPO_ROOT)
                    except Exception as _e:
                        st.error(f"Regulatory evidence pack panel import failed: {_e}")

                with st.expander("ðŸ›ï¸ Licensing Evidence Tier 2 (v355)", expanded=False):
                    try:
                        from ui.licensing_evidence_tier2 import render_licensing_evidence_tier2_panel
                        render_licensing_evidence_tier2_panel(REPO_ROOT)
                    except Exception as _e:
                        st.error(f"Licensing Tier 2 panel import failed: {_e}")

    with _pb_tabs[3]:
        st.subheader("ðŸ§¾ Contract Studio")
        st.caption("Validate and export governance contracts (read-only).")
        try:
            from ui.contract_studio import render_contract_studio
            from pathlib import Path
            _repo_root = Path(__file__).resolve().parents[1]
            render_contract_studio(_repo_root, ui_key_prefix="pb_contract_studio")
        except Exception as e:
            try:
                st.error(f"Contract Studio import failed: {e}")
            except Exception:
                pass

    with _pb_tabs[4]:
        st.subheader("ðŸ§¾ Regulatory Evidence Pack Builder (v387)")
        st.caption("Deterministic, hash-locked evidence ZIP from cached runs (export-only).")

        try:
            from src.tools.evidence_pack_v387 import build_evidence_pack_v387
        except Exception as _e:
            st.error(f"Evidence pack builder import failed: {_e}")
        else:
            # Cache-only: do not compute here; export whatever is already in session cache.
            cache_sources = {
                "pd_last_outputs": st.session_state.get("pd_last_outputs"),
                "systems_last_solution": st.session_state.get("systems_last_solution"),
                "scan_last_artifact": st.session_state.get("scan_last_artifact"),
                "pareto_last_front": st.session_state.get("pareto_last_front"),
                "extopt_last_run": st.session_state.get("extopt_last_run"),
                "surrogate_v386_last_screening_run": st.session_state.get("surrogate_v386_last_screening_run"),
            }

            st.markdown("### Select cached sources")
            include_flags: Dict[str, bool] = {}
            cols = st.columns(2)
            for i, k in enumerate(sorted(cache_sources.keys())):
                v = cache_sources.get(k)
                avail = isinstance(v, (dict, list))
                with cols[i % 2]:
                    include_flags[k] = st.checkbox(
                        f"Include `{k}`{' âœ…' if avail else ' (missing)'}",
                        value=bool(avail),
                        disabled=not avail,
                        key=f"pb_v387_inc_{k}",
                    )

            notes = st.text_area(
                "Reviewer notes (optional)",
                value=str(st.session_state.get("pb_v387_notes", "")),
                key="pb_v387_notes",
                height=120,
            )
            st.session_state["pb_v387_notes"] = notes

            run_btn = st.button("â–¶ Build Evidence Pack", type="primary", key="pb_v387_build")
            if run_btn:
                shams_version = (REPO_ROOT / "VERSION").read_text(encoding="utf-8").strip().splitlines()[0]
                out_dir = REPO_ROOT / "ui_runs" / "evidence_packs_v387"
                out_dir.mkdir(parents=True, exist_ok=True)
                out_zip = out_dir / "evidence_pack_v387.zip"
                with st.spinner("Building deterministic evidence ZIPâ€¦"):
                    res = build_evidence_pack_v387(
                        out_zip,
                        shams_version=shams_version,
                        sources=cache_sources,
                        include=include_flags,
                        notes=notes,
                    )
                st.session_state["evidence_pack_v387_last_zip"] = str(res.zip_path)
                st.session_state["evidence_pack_v387_last_index"] = res.index

            # Render from cache
            idx = st.session_state.get("evidence_pack_v387_last_index")
            zpath = st.session_state.get("evidence_pack_v387_last_zip")
            if isinstance(idx, dict):
                with st.expander("Pack index", expanded=False):
                    st.json(idx, expanded=False)
            if isinstance(zpath, str) and zpath:
                try:
                    from pathlib import Path
                    p = Path(zpath)
                    if p.exists():
                        st.download_button(
                            "â¬‡ï¸ Download Evidence Pack (ZIP)",
                            data=p.read_bytes(),
                            file_name=p.name,
                            mime="application/zip",
                            use_container_width=True,
                            key="pb_v387_dl",
                        )
                except Exception as _e:
                    st.error(f"Unable to load evidence ZIP: {_e}")

if _deck == "ðŸŽ›ï¸ Control Room":
    st.header("ðŸ›¡ï¸ Control Room")
    st.caption("Governance, provenance, exports, and expert diagnostics - organized as compact decks (no scroll walls).")

    deck_orient, deck_const, deck_prov, deck_art, deck_diag, deck_chron = st.tabs([
        "ðŸ§­ Orientation",
        "ðŸ§¾ Constitution",
        "ðŸ” Provenance",
        "ðŸ“¦ Artifacts",
        " Diagnostics",
        "ðŸ•“ Chronicle",
    ])

    with deck_orient:
        st.subheader("Orientation")
        st.caption("Quick-start workflows and reviewer-facing scope anchors (UI-only; does not modify truth).")
        o_launch, o_vocab, o_gallery, o_scope = st.tabs(["Launchpad", "Vocabulary", "Reference Gallery", "Scope"])

        with o_launch:
            st.subheader("Launchpad - First 30 Minutes")
            st.caption("A guided entry path for fusion experts: choose intent, then follow a minimal, honest workflow. UI scaffolding only.")
            _path = st.radio(
                "I want toâ€¦",
                [
                    "Understand feasibility limits (cartography)",
                    "Explore reactor concepts (Forge)",
                    "Review a finished case (Review Room)",
                    "Compare designs (Artifacts)",
                ],
                index=0,
                key="launchpad_path",
                horizontal=False,
            )
            if _path == "Understand feasibility limits (cartography)":
                st.info("Recommended: Scan Lab â†’ build Scan Atlas â†’ inspect first-failure topology.", icon="ðŸ—ºï¸")
                st.markdown("""
- Start with **Scan Lab - Cartography Deck**
- Choose a compact 2D scan
- Export the Scan Atlas capsule for review-room replay.
""")
            elif _path == "Explore reactor concepts (Forge)":
                st.info("Recommended: Reactor Design Forge â†’ Casebook â†’ Candidate Archive â†’ Machine Dossier.", icon="âš’ï¸")
                st.markdown("""
- Use **Forge Cockpit** with the **Helm Console**
- Keep **Margins-first** framing
- Save capsules for deterministic replay.
""")
            elif _path == "Review a finished case (Review Room)":
                st.info("Recommended: Reactor Design Forge â†’ Review Mode â†’ Review Trinity â†’ Doâ€‘Notâ€‘Build Brief.", icon="ðŸ§¾")
                st.markdown("""
- Turn on **Review Mode**
- Use **Review Trinity** and **Conflict Atlas**
- Generate a **Reviewer Packet**.
""")
            else:
                st.info("Recommended: Compare â†’ upload two artifacts â†’ inspect deltas.")
                st.markdown("""
- Use **Compare artifacts** to check reproducibility
- Prefer capsule replay over manual edits.
""")

        with o_vocab:
            st.subheader("Vocabulary Ledger")
            st.caption("Fusion-native terminology mapping (SHAMS â†” common literature â†” PROCESS-style language).")
            try:
                _vocab = (Path(__file__).resolve().parent.parent / "docs" / "VOCABULARY_LEDGER.md").read_text(encoding="utf-8")
            except Exception:
                _vocab = "(missing docs/VOCABULARY_LEDGER.md)"
            st.markdown(_vocab)

        with o_gallery:
            st.subheader("Reference Study Gallery")
            st.caption("Recognizable anchors for the community. These are reference contexts, not targets.")
            _gallery = [
                ("ITER-like", "Large, conservative, physics-demonstration anchor; often stress and divertor constraints dominate."),
                ("SPARC-like", "Compact high-field concept; often HTS margin and structural stress dominate."),
                ("ARC-like", "HTS reactor class; often net-electric closure and blanket/TBR proxies dominate."),
                ("DEMO-like", "Plant realism anchor; often recirculating power and availability assumptions dominate."),
            ]
            for name, note in _gallery:
                with st.expander(name, expanded=False):
                    st.write(note)
            st.info("Tip: use these as *discussion anchors* when presenting SHAMS outputs to reviewers.", icon="ðŸ’¡")

        with o_scope:
            st.subheader("Model Scope Card")
            st.caption("Always-visible scope declaration for review rooms.")
            try:
                _scope = (Path(__file__).resolve().parent.parent / "docs" / "MODEL_SCOPE_CARD.md").read_text(encoding="utf-8")
            except Exception:
                _scope = "(missing docs/MODEL_SCOPE_CARD.md)"
            st.markdown(_scope)

    with deck_const:
        st.subheader("Constitution")
        st.caption("Frozen truth boundary, constraint constitution, and assumption ledger (read-only).")
        c_model, c_pcm, c_assump, c_docs, c_cc, c_ci, c_cp = st.tabs([
            "Model Ledger",
            "Capability Matrix",
            "Assumptions",
            "Docs",
            "Constraint Cockpit",
            "Constraint Inspector",
            "Constraint Provenance",
        ])
        tab_model = c_model
        tab_pcm = c_pcm
        tab_assumptions = c_assump
        tab_docs = c_docs
        tab_constraints = c_cc
        tab_constraint_inspector = c_ci
        tab_cprov = c_cp

    with deck_prov:
        st.subheader("Provenance")
        st.caption("Study protocol, repro lock, regression visibility, and replay tools.")
        p_studies, p_deck, p_auth, p_dec, p_dom, p_epoch, p_delta, p_regress, p_dash = st.tabs([
            "Studies",
            "Case Deck Runner",
            "Authority & Confidence",
            "Decision Consequences",
            "Authority Dominance",
            "Epoch Feasibility",
            "Scenario Delta",
            "Regression Viewer",
            "Study Dashboard",
        ])
        tab_studies = p_studies
        tab_deck = p_deck
        tab_authority_conf = p_auth
        tab_decision_conseq = p_dec
        tab_authority_dominance = p_dom
        tab_epoch_feas = p_epoch
        tab_delta = p_delta
        tab_regress = p_regress
        tab_study_dash = p_dash

    with deck_art:
        st.subheader("Artifacts")
        st.caption("Exports, evidence packs, and benchmark bundles (deterministic).")
        a_art, a_lib, a_export, a_bench = st.tabs([
            "Artifacts Explorer",
            "Run Library",
            "Export / Share",
            "Benchmarks",
        ])
        tab_artifacts = a_art
        tab_library = a_lib
        tab_export = a_export
        tab_bench = a_bench

    with deck_diag:
        st.subheader("ðŸ©º Diagnostics")
        st.caption("Deep tools for debugging and reviewer verification (kept off the main workflow by default).")
        d_pam, d_val, d_comp, d_gate, d_nonfeas, d_solver, d_decision, d_session = st.tabs([
            "Panel Map",
            "Validation",
            "Compliance",
            "Gatechecks",
            "Non-Feasibility Guide",
            "Solver Introspection",
            "Decision Builder",
            "Session",
        ])
        tab_pam = d_pam
        tab_validation = d_val
        tab_compliance = d_comp
        tab_gatechecks = d_gate
        tab_nonfeas = d_nonfeas
        tab_solver = d_solver
        tab_decision = d_decision

        with d_gate:
            st.subheader("Gatechecks")
            st.caption("Local build integrity checks. UI-only; does not modify truth.")
            st.markdown("""
Run these from a terminal at the repo root:

- `python -m compileall -q .`
- `pytest -q`
- `streamlit run ui/app.py`

This panel also performs a lightweight hygiene scan of the working tree.
""")

            from pathlib import Path as _Path
            _root = (_Path(__file__).resolve().parent.parent)
            _forbidden = [
                '__pycache__',
                '.pytest_cache',
                'gspulse_ui',
            ]
            _hits = []
            for name in _forbidden:
                for h in _root.rglob(name):
                    _hits.append(str(h))
            # Also flag stray run_st* launchers
            for h in _root.glob('run_st*'):
                _hits.append(str(h))
            if _hits:
                st.error("Hygiene violations detected (should be removed before packaging):")
                with st.expander("Show paths", expanded=False):
                    for h in sorted(set(_hits)):
                        st.write(h)
            else:
                st.success("No hygiene violations detected in this tree.")

            st.divider()
            st.subheader("Interoperability self-check")
            st.caption("Verifies that main panels can exchange the canonical design state (no truth modifications).")

            st.divider()
            st.subheader("Interoperability contract validator (v326)")
            st.caption("Static + runtime wiring audit: declared panel contracts vs discoverable subpanels in ui/app.py.")

            def _run_contract_validator() -> dict:
                """Deterministic contract validator.

                Does not run physics or mutate truth.
                """
                from pathlib import Path as _Path
                from ui.panel_contracts import get_panel_contracts
                from tools.interoperability.contract_validator import validate_ui_contracts

                _root = (_Path(__file__).resolve().parent.parent)
                _contracts = get_panel_contracts()
                return validate_ui_contracts(_root, _contracts, session_state=dict(st.session_state))

            if st.button('Run contract validator', use_container_width=True, key='v326_contract_validator_btn'):
                st.session_state['v326_last_contract_validator_report'] = _run_contract_validator()

            _cr = st.session_state.get('v326_last_contract_validator_report')
            if isinstance(_cr, dict):
                if bool(_cr.get('ok')):
                    st.success('Contract validator: OK')
                else:
                    st.warning('Contract validator: issues detected')
                with st.expander('Contract validator report', expanded=False):
                    st.json(_cr)

            def _interop_check() -> dict:
                """Lightweight, deterministic UI-state interoperability audit.

                This is intentionally conservative: it only checks for existence and
                basic schema/type sanity of the canonical promotion keys. It does not
                run physics, solvers, or optimization.
                """
                rep = {'ok': True, 'checks': []}
                def _add(name: str, ok: bool, detail: str = ''):
                    rep['checks'].append({'name': name, 'ok': bool(ok), 'detail': str(detail)})
                    if not ok:
                        rep['ok'] = False

                # Core canonical artifacts used across modes
                for k in ['workspace_candidate', 'last_point_result', 'compare_left', 'compare_right']:
                    _add(f'session_key:{k}', k in st.session_state, 'present' if k in st.session_state else 'missing')

                # Systems mode canonical keys
                _t = st.session_state.get('systems_targets')
                _v = st.session_state.get('systems_variables')
                _add('systems_targets_type', isinstance(_t, dict) and len(_t) > 0, f"type={type(_t).__name__} len={len(_t) if isinstance(_t, dict) else 'n/a'}")
                _add('systems_variables_type', isinstance(_v, dict) and len(_v) > 0, f"type={type(_v).__name__} len={len(_v) if isinstance(_v, dict) else 'n/a'}")

                # Evidence / provenance hooks
                for k in ['last_precheck_report', 'last_systems_solution', 'last_evidence_pack_path', 'provenance_global']:
                    _add(f'provenance_key:{k}', k in st.session_state, 'present' if k in st.session_state else 'missing')

                return rep

            if st.button('Run interoperability check', use_container_width=True, key='v323_interop_check_btn'):
                st.session_state['v323_last_interop_report'] = _interop_check()

            _ir = st.session_state.get('v323_last_interop_report')
            if isinstance(_ir, dict):
                if bool(_ir.get('ok')):
                    st.success('Interoperability check: OK')
                else:
                    st.warning('Interoperability check: issues detected')
                with st.expander('Interoperability report', expanded=False):
                    st.json(_ir)

        with d_session:
            with st.expander("Session state (debug)", expanded=False):
                st.write({k: type(v).__name__ for k, v in st.session_state.items()})
            with st.expander("Version", expanded=False):
                try:
                    st.code((BASE_DIR / "VERSION").read_text().strip())
                except Exception:
                    st.code("unknown")

    with deck_chron:
        st.subheader("Chronicle")
        st.caption("Expert instruments and exploration aids (read-only; never modifies truth).")
        ch_reg, ch_sens, ch_knobs, ch_fmap, ch_mat, ch_maint, ch_prof, ch_imp, ch_disr, ch_stab, ch_solve, ch_repair, ch_refine, ch_narrow, ch_surr, ch_al = st.tabs([
            "Variable Registry",
            "Sensitivity Explorer",
            "Knob Trade-Space",
            "Feasibility Map",
            "Maturity Heatmap",
            "Maintenance & Availability",
            "Profile Authority",
            "Impurity & Radiation",
            "Disruption Risk",
            "Stability Risk",
            "Certified Search",
            "Repair Suggestions",
            "Interval Refinement",
            "Interval Narrowing",
            "Surrogate Overlay",
            "Active Learning",
        ])
        tab_registry = ch_reg
        tab_sensitivity = ch_sens
        tab_knobs = ch_knobs
        tab_feasmap = ch_fmap
        tab_maturity = ch_mat
        tab_maintenance = ch_maint
        tab_profile_auth = ch_prof
        tab_impurity = ch_imp
        tab_disruption = ch_disr
        tab_stability = ch_stab
        tab_cert_search = ch_solve
        tab_repair = ch_repair
        tab_refine = ch_refine
        tab_narrowing = ch_narrow
        tab_surrogate = ch_surr
        tab_active_learning = ch_al

    # Populate Control Room sections (ensure they are never empty)
    with tab_pam:
        _v175_panel_availability_map_panel()
    
    with tab_studies:
        st.markdown("### Study authority & publishability")
        st.write("Generate protocol â†’ lock/replay â†’ authority pack â†’ citation â†’ export.")
        try:
            _render_with_contract("_v165_study_protocol_panel", _v165_study_protocol_panel)
        except Exception:
            st.info('Panel unavailable in this build.')
        st.divider()
        try:
            _render_with_contract("_v166_repro_lock_panel", _v166_repro_lock_panel)
        except Exception:
            st.info('Panel unavailable in this build.')
        st.divider()
        try:
            _render_with_contract("_v167_authority_pack_panel", _v167_authority_pack_panel)
        except Exception:
            st.info('Panel unavailable in this build.')
        st.divider()
        try:
            _render_with_contract("_v168_citation_panel", _v168_citation_panel)
        except Exception:
            st.info('Panel unavailable in this build.')
        st.divider()
        try:
            _render_with_contract("_v170_process_export_panel", _v170_process_export_panel)
        except Exception:
            st.info('Panel unavailable in this build.')
    st.divider()
    with st.expander("ðŸ—ƒï¸ Studies manager", expanded=False):
        st.header("Studies manager")
        st.write("Save, load, and organize study configurations (scan/pareto) as JSON. This keeps studies reproducible across sessions.")
        if "studies" not in st.session_state:
            st.session_state.studies = []

        c1, c2, c3 = st.columns(3)
        with c1:
            if st.button("Save current PointInputs as study", use_container_width=True):
                if st.session_state.get("last_point_inp") is not None:
                    try:
                        inp_obj = st.session_state.last_point_inp
                        # dataclass -> dict
                        d = {k: getattr(inp_obj, k) for k in inp_obj.__dataclass_fields__.keys()}  # type: ignore
                        st.session_state.studies.append({"type": "point", "created": datetime.datetime.now().isoformat(), "inputs": d})
                        st.success("Saved.")
                    except Exception as e:
                        st.error(f"Could not save: {e}")
                else:
                    st.warning("Run a point first so `last_point_inp` exists.")

        with c2:
            up = st.file_uploader("Import studies JSON", type=["json"], key="studies_import")
            if up is not None:
                try:
                    imported = json.loads(up.getvalue().decode("utf-8"))
                    if isinstance(imported, list):
                        st.session_state.studies.extend(imported)
                    elif isinstance(imported, dict):
                        st.session_state.studies.append(imported)
                    st.success("Imported.")
                except Exception as e:
                    st.error(f"Import failed: {e}")

        with c3:
            if st.session_state.studies:
                st.download_button(
                    "Download studies JSON",
                    data=json.dumps(st.session_state.studies, indent=2, sort_keys=True),
                    file_name="shams_studies.json",
                    mime="application/json",
                    use_container_width=True,
                )

        st.markdown("### Saved studies")
        if st.session_state.studies:
            df = pd.DataFrame([{"i": i, "type": s.get("type","?"), "created": s.get("created",""), "notes": s.get("notes","")} for i,s in enumerate(st.session_state.studies)])
            st.dataframe(df, use_container_width=True)
            idx = st.number_input("Select index to view", min_value=0, max_value=max(0, len(st.session_state.studies)-1), value=0, step=1)
            st.json(st.session_state.studies[int(idx)])
            if st.button("Delete selected", use_container_width=True):
                try:
                    st.session_state.studies.pop(int(idx))
                    st.experimental_rerun()
                except Exception:
                    pass
        else:
            st.info("No studies saved yet.")


    
    with tab_model:
        # Render with proper scientific notation (MathJax) to avoid â€œASCII-lookingâ€ formulas.
        st.markdown(
            """
    This section documents the **0â€‘D (global) physics + engineering surrogate** used by SHAMS for rapid point design.
    It is intentionally transparent: the goal is to show the **model structure**, **assumptions**, and **where each number comes from**.
    
    #### Symbol key (as used below)
    - $R_0$ major radius, $a$ minor radius, $\\kappa$ elongation, $\\delta$ triangularity
    - $B_t$ toroidal field on axis, $I_p$ plasma current
    - $n$ density, $T$ temperature, $V$ plasma volume, $W$ stored energy
    - $P_{fus}$ fusion power, $P_{\\alpha}$ alpha power, $P_{aux}$ auxiliary power, $P_{SOL}$ power crossing the separatrix
    - $\\tau_E$ energy confinement time, $H$ confinement multiplier (Hâ€‘factor)
    """
        )
    
        st.markdown("#### Highâ€‘level flow (per point evaluation)")
        st.markdown(
            """
    1. **Geometry:** $(R_0, a, \\kappa, \\delta) \\rightarrow$ volumes/areas.
    2. **Plasma state:** choose targets/intent $\\rightarrow$ infer a consistent $(T, n, B_t, I_p)$ under constraints.
    3. **Power balance:** $P_{fus}, P_{\\alpha}, P_{aux}$ and losses $\\rightarrow$ steadyâ€‘state balance.
    4. **Confinement:** $\\tau_E$ from selected scaling (ITER98y2 / others) with $H$; enforce $Q$ consistency.
    5. **Current & stability:** $q_{95}$, $\\beta_N$, Greenwald fraction $f_G$.
    6. **Engineering proxies:** TF peak field / hoop stress, HTS margin.
    7. **Blanket/shield/TBR proxy:** thickness & coverage $\\rightarrow$ TBR screening.
    8. **Divertor proxy:** heatâ€‘flux screening from $P_{SOL}$ and geometry.
    9. **Radial build closure:** inboard stack fits (gap + FW + blanket + shield + VV + TF).
    """
        )
    
        st.markdown("#### Core relationships (representative)")
        st.latex(
            r"""
    \begin{aligned}
    P_{fus} &\propto n^2\,\langle\sigma v\rangle(T)\,V \\
    \tau_E &= H\,\tau_{\mathrm{ITER98y2}}(I_p, B_t, n, P, R_0, a, \kappa, \ldots) \\
    P_{heat} &= P_{\alpha} + P_{aux} \\
    P_{loss} &\approx \frac{W}{\tau_E}\;\; (\text{plus radiation terms where enabled}) \\
    q_{95} &\approx \frac{5\,a^2\,B_t}{R_0\,I_p}\,f(\kappa,\delta) \\
    \beta_N &\approx \beta\,\frac{a\,B_t}{I_p} \\
    q_{div} &\approx \frac{P_{SOL}}{2\pi R_0\,\lambda_q}\,g_{exh}(\text{geometry}) \\
    \sigma_{TF} &\propto \frac{B_{peak}^2\,R_{coil}}{\mu_0}
    \end{aligned}
    """
        )
        st.caption(
            "These are screening/closure relationships to support feasibility-first iteration. Exact authoritative pass/fail logic lives in SHAMS constraints and margins."
        )
    
    
    with tab_pcm:
        st.markdown("### ðŸ“Ž Physics Capability Matrix")
        st.caption(
            "Read-only audit map: subsystems â†’ equations/closures â†’ authority tier (proxy/parametric/external) â†’ intended validity domain."
        )
        try:
            # v228+: prefer generator-derived snapshot if present (still read-only).
            p_gen = (BASE_DIR / "docs" / "PHYSICS_CAPABILITY_MATRIX_GENERATED.md")
            p_src = (BASE_DIR / "docs" / "PHYSICS_CAPABILITY_MATRIX.md")
            if p_gen.exists():
                _pcm = p_gen.read_text(encoding="utf-8", errors="ignore")
            else:
                _pcm = p_src.read_text(encoding="utf-8", errors="ignore")
        except Exception:
            _pcm = "(missing docs/PHYSICS_CAPABILITY_MATRIX*.md)"
        st.markdown(_pcm)
        st.info(
            "Bluemira-inspired lessons are adopted for provenance and capability clarity - without introducing optimization loops or CAD-level coupling.",
            icon="ðŸ§­",
        )
    with tab_bench:
    
        st.markdown("### Benchmarks")
        st.write("Benchmark runners (validation/regression) are available via the advanced panels once you have run artifacts.")
    
        st.markdown("#### Reference superconducting tokamaks (quick lookup)")
        st.markdown(
            """
    | Tokamak | Country / Org | Status | SC type | Major R (m) | Minor a (m) | Bâ‚€ on axis (T) | Ip (MA) | Primary role |
    |---|---|---|---|---:|---:|---:|---:|---|
    | **ITER** | Intl (EU/JP/US/etc.) | Under construction | **Nbâ‚ƒSn / NbTi (LTS)** | 6.2 | 2.0 | 5.3 | 15 | Burning plasma, Qâ‰ˆ10 |
    | **JT-60SA** | Japanâ€“EU | Commissioning | **NbTi (LTS)** | 2.96 | 1.18 | 2.25 | 5.5 | Advanced plasma physics |
    | **WEST** | France | Operating | **NbTi (LTS)** | 2.5 | 0.5 | 3.7 | â‰¤1 | Long-pulse, PFC/divertor |
    | **EAST** | China | Operating | **NbTi (LTS)** | 1.8â€“1.9 | 0.4â€“0.45 | â‰¤3.5 | â‰¤1 | Long-pulse operation |
    | **KSTAR** | Korea | Operating | **NbTi-based (LTS)** | ~1.8 | ~0.5 | 3.5 | â‰¤2 | Advanced tokamak scenarios |
    | **SST-1** | India | Operating | **NbTi (LTS)** | ~1.1 | ~0.2 | â‰¤3 | â‰¤0.1 | SC tokamak development |
    | **TRIAM-1M** | Japan | Historical | **Nbâ‚ƒSn (LTS)** | ~0.8 | ~0.12â€“0.18 | 8 | - | High-field SC operation |
    | **SPARC** | USA (MIT/CFS) | Under construction | **REBCO (HTS)** | 1.85 | 0.57 | 12.2 | 8.7 | Q>1, high-field compact |
    """
        )
        st.caption(
            "Values are typical/design-point numbers collected from public summaries. For rigorous comparison, cite primary machine parameter sheets."
        )
        st.write("Below is a quick reference table of major superconducting tokamaks used as comparison anchors.")
    
        try:
            import pandas as _pd
            _bench_rows = [
                {"Tokamak":"ITER","Country / Org":"Intl (EU/JP/US/etc.)","Status":"Under construction","SC type":"Nbâ‚ƒSn / NbTi (LTS)","Major R (m)":6.2,"Minor a (m)":2.0,"Bâ‚€ on axis (T)":5.3,"Ip (MA)":15.0,"Primary role":"Burning plasma, Qâ‰ˆ10"},
                {"Tokamak":"JT-60SA","Country / Org":"Japanâ€“EU","Status":"Commissioning","SC type":"NbTi (LTS)","Major R (m)":3.0,"Minor a (m)":1.0,"Bâ‚€ on axis (T)":2.3,"Ip (MA)":5.5,"Primary role":"Advanced plasma physics"},
                {"Tokamak":"WEST","Country / Org":"France","Status":"Operating","SC type":"NbTi (LTS)","Major R (m)":2.5,"Minor a (m)":0.5,"Bâ‚€ on axis (T)":3.7,"Ip (MA)":1.0,"Primary role":"Long-pulse, PFC/divertor"},
                {"Tokamak":"EAST","Country / Org":"China","Status":"Operating","SC type":"NbTi (LTS)","Major R (m)":1.9,"Minor a (m)":0.5,"Bâ‚€ on axis (T)":3.5,"Ip (MA)":1.0,"Primary role":"Long-pulse operation"},
                {"Tokamak":"KSTAR","Country / Org":"Korea","Status":"Operating","SC type":"NbTi-based (LTS)","Major R (m)":1.8,"Minor a (m)":0.5,"Bâ‚€ on axis (T)":3.5,"Ip (MA)":2.0,"Primary role":"Advanced tokamak scenarios"},
                {"Tokamak":"SST-1","Country / Org":"India","Status":"Operating","SC type":"NbTi (LTS)","Major R (m)":1.1,"Minor a (m)":0.2,"Bâ‚€ on axis (T)":3.0,"Ip (MA)":0.1,"Primary role":"SC tokamak development"},
                {"Tokamak":"TRIAM-1M","Country / Org":"Japan","Status":"Historical","SC type":"Nbâ‚ƒSn (LTS)","Major R (m)":0.8,"Minor a (m)":0.15,"Bâ‚€ on axis (T)":8.0,"Ip (MA)":None,"Primary role":"High-field SC operation"},
                {"Tokamak":"HT-7","Country / Org":"China","Status":"Historical","SC type":"LTS","Major R (m)":1.22,"Minor a (m)":0.27,"Bâ‚€ on axis (T)":2.0,"Ip (MA)":0.2,"Primary role":"Precursor to EAST"},
                {"Tokamak":"SPARC","Country / Org":"USA (MIT/CFS)","Status":"Under construction","SC type":"REBCO (HTS)","Major R (m)":1.85,"Minor a (m)":0.57,"Bâ‚€ on axis (T)":12.2,"Ip (MA)":8.7,"Primary role":"Q>1, high-field compact"},
                {"Tokamak":"HH70","Country / Org":"China (Energy Singularity)","Status":"Operating","SC type":"REBCO (HTS)","Major R (m)":0.7,"Minor a (m)":0.28,"Bâ‚€ on axis (T)":0.6,"Ip (MA)":None,"Primary role":"Full-HTS integration demo"},
                {"Tokamak":"HH170","Country / Org":"China (Energy Singularity)","Status":"Planned","SC type":"REBCO (HTS)","Major R (m)":None,"Minor a (m)":None,"Bâ‚€ on axis (T)":None,"Ip (MA)":None,"Primary role":"Reactor-relevant HTS tokamak"},
            ]
            _df = _pd.DataFrame(_bench_rows)
            st.dataframe(_df, use_container_width=True, hide_index=True)
            st.caption("Notes: Some entries are approximate screening values (as shown). Replace with cited values if you enable web-backed references.")
        except Exception:
            st.info("Benchmark reference table unavailable in this environment.")
    
    with tab_docs:
        st.markdown("### Documentation")
        st.write("Offline docs are included in the package. Review-room and exposure guardrails are included as dedicated docs pages. Key references live in the `docs/` folder when present.")
        st.caption("Note: The **Model Ledger (0â€‘D Physics)** panel renders equations using LaTeX/MathJax for scientific typography.")
    
        try:
            from pathlib import Path as _P
    
            _readme = _P("README.md")
            if _readme.exists():
                with st.expander("README (excerpt)", expanded=False):
                    st.code(_readme.read_text(encoding="utf-8")[:3000])
    
            _docs_dir = _P("docs")
            _mds = []
            if _docs_dir.exists():
                _mds = sorted([pp for pp in _docs_dir.rglob("*.md") if pp.is_file()])
    
            if _mds:
                st.markdown("#### Docs library")
                _labels = [str(pp.relative_to(_docs_dir)) for pp in _mds]
                _sel = st.selectbox("Open a doc (readâ€‘only)", _labels, index=0)
                _path = _docs_dir / _sel
                with st.expander(f"docs/{_sel}", expanded=False):
                    st.markdown(_path.read_text(encoding="utf-8"))
            else:
                st.info("No `docs/` folder was found in this build.")
        except Exception:
            pass
    
    with tab_artifacts:
        st.markdown("### Artifacts")
        st.write("Artifacts appear after you run Point Designer / Systems Mode.")
        st.write("Use Run Library / Export tools to download bundles.")
    
    # For remaining expanders, ensure a minimal non-empty body
    for _exp in [tab_registry, tab_validation, tab_compliance, tab_deck, tab_delta, tab_library, tab_constraints,
                tab_constraint_inspector, tab_sensitivity, tab_feasmap, tab_decision, tab_nonfeas, tab_cprov,
                tab_knobs, tab_regress, tab_study_dash, tab_maturity, tab_assumptions, tab_export, tab_solver]:
        with _exp:
            st.write("This tool becomes active when required upstream artifacts exist (run history, packs, or reports).")
            st.write("If you need something here, run a study first, then return to More.")
    
# Shared state
if "last_point_out" not in st.session_state:
    st.session_state["last_point_out"] = None
if "last_point_inp" not in st.session_state:
    st.session_state["last_point_inp"] = None
if "scan_df" not in st.session_state:
    st.session_state.scan_df = pd.DataFrame()
if "scan_meta" not in st.session_state:
    st.session_state.scan_meta = {}
if "studies" not in st.session_state:
    st.session_state.studies = []  # list of study config dicts
if "compare_artifacts" not in st.session_state:
    st.session_state.compare_artifacts = {"A": None, "B": None}

# -----------------------------
# Point Designer
# -----------------------------
if _deck == "ðŸ§­ Point Designer":
    st.info(
        " **Point Designer is frozen** - It evaluates a single operating point in a constraint-authoritative, assumption-explicit 0â€‘D framework. "
        "No optimization, relaxation, or exploration occurs here. Exploration is performed in **Systems Mode**, which calls Point Designer as a fixed evaluator.",
    )

    # Point Designer deck selector (v280+): Truth Console vs outer-loop envelopes/contracts.
    _pd_deck = st.radio(
        "Point Designer deck",
        ["ðŸ§­ Truth Console", "ðŸ—ºï¸ Phase Envelopes", "ðŸ›¡ï¸ Uncertainty Contracts"],
        index=0,
        horizontal=True,
        help="Truth Console runs the frozen single-point evaluator. Phase Envelopes and Uncertainty Contracts are outer-loop diagnostics only (no solver, no dynamics).",
    )
    if _pd_deck != "ðŸ§­ Truth Console":
        _pd_art = st.session_state.get("pd_last_artifact", None)
        if not isinstance(_pd_art, dict):
            _pd_art = st.session_state.get("last_point_artifact", None)
        if _pd_deck == "ðŸ—ºï¸ Phase Envelopes":
            try:
                from ui.phase_envelopes import render_phase_envelopes_panel
                render_phase_envelopes_panel(
                    REPO_ROOT,
                    point_artifact=_pd_art,
                    ui_key_prefix="truth_phase_env",
                )
            except Exception as _e:
                st.error(f"Phase Envelopes panel import failed: {_e}")
        else:
            try:
                from ui.uncertainty_contracts import render_uncertainty_contracts_panel
                render_uncertainty_contracts_panel(
                    REPO_ROOT,
                    point_artifact=_pd_art,
                    ui_key_prefix="truth_uq_contracts",
                )
            except Exception as _e:
                st.error(f"Uncertainty Contracts panel import failed: {_e}")
        st.stop()


    st.markdown("### ðŸ§­ Truth Console - mode contract (is / is not)")
    cA, cB = st.columns(2)
    with cA:
        st.markdown("""**What this mode does**
- Evaluates a single operating point using the frozen 0â€‘D evaluator
- Reports pass/fail margins and transparent intermediate outputs
- Produces reproducible, audit-ready artifacts""")
    with cB:
        st.markdown("""**What this mode does not do**
- Optimize, relax, or search for feasibility
- Suggest parameter changes or apply designs
- Modify physics, constraints, or policy""")

    with st.expander("Provenance (optional)", expanded=False):
        st.caption("Read-only metadata for audits and screenshots.")
        st.write({"software": APP_NAME, "version": str(st.session_state.get("shams_version","unknown")), "author": APP_AUTHOR})

    tab_cfg, tab_tel, tab_con = st.tabs(["ðŸ§­ Configure", "ðŸ“¡ Telemetry", "ðŸ§¾ Constraints"])

    # IMPORTANT: Streamlit tabs are lazily executed; variables defined inside one tab
    # are NOT guaranteed to exist when another tab is selected. Keep shared flags
    # defined here at the parent scope.
    run_btn = False  # (global PD run button state)

    # Use the latest loaded preset / last point as the UI default for Point Designer.
    # This makes preset loads robust even if widget state keys change or are newly created.
    _base_pd = st.session_state.get("last_point_inp")
    # UI stability hardening: session state may contain a raw dict (e.g., legacy
    # cache formats or template imports). Normalize to PointInputs deterministically.
    if isinstance(_base_pd, dict):
        try:
            _pi_fields = {f.name for f in fields(PointInputs)}
            _base_pd = PointInputs(**{k: v for k, v in _base_pd.items() if k in _pi_fields})
            st.session_state["last_point_inp"] = _base_pd
        except Exception:
            _base_pd = None
    if _base_pd is None:
        _base_pd = PointInputs(R0_m=1.81, a_m=0.62, kappa=1.8, Bt_T=10.0, Ip_MA=8.0, Ti_keV=10.0, fG=0.8, Paux_MW=50.0)
        st.session_state["last_point_inp"] = _base_pd

    with tab_cfg:
        st.subheader("Control Deck")
        if st.button("ðŸ§¹ New machine (clear Point Designer)", use_container_width=True, help="Clear Point Designer outputs/tables/plots so you can start a new machine."):
            for k in [
                "pd_last_artifact","pd_last_outputs","pd_last_radial_png_bytes","pd_last_log_lines","pd_last_run_ts","pd_last_inputs_hash",
                "last_point_out","last_point_inp","last_solver_log",
            ]:
                st.session_state.pop(k, None)
            st.rerun()

        with st.expander("ðŸ­ Scenario Templates (Industrial, v354)", expanded=False):
            st.caption("Deterministic intent templates that set *PointInputs defaults* (no optimization, no solvers).")
            try:
                from tools.industrial_scenario_templates_v354 import template_names, get_template_payload, get_template
                _tmpl_names = template_names()
                _sel_tmpl = st.selectbox("Industrial scenario template", ["(select)"] + _tmpl_names, index=0, key="pd_industrial_template_v354")
                if _sel_tmpl != "(select)":
                    _payload = get_template_payload(_sel_tmpl)
                    st.code(json.dumps(_payload, indent=2, sort_keys=True), language="json")
                    if st.button("ðŸ“¥ Load template into Point Designer", use_container_width=True, key="pd_load_industrial_template_btn_v354"):
                        # Clear prior outputs and set a new base point for widget defaults.
                        for k in [
                            "pd_last_artifact","pd_last_outputs","pd_last_radial_png_bytes","pd_last_log_lines","pd_last_run_ts","pd_last_inputs_hash",
                            "last_point_out","last_point_inp","last_solver_log",
                        ]:
                            st.session_state.pop(k, None)
                        _ov = get_template(_sel_tmpl)
                        try:
                            _base = asdict(_base_pd) if _base_pd is not None else {}
                            _base.update({k: v for k, v in _ov.items() if k in _base})
                            st.session_state["last_point_inp"] = PointInputs(**_base)
                        except Exception:
                            # Fallback: apply only required keys
                            st.session_state["last_point_inp"] = PointInputs(**{**{
                                "R0_m": float(_ov.get("R0_m", 1.81)),
                                "a_m": float(_ov.get("a_m", 0.62)),
                                "kappa": float(_ov.get("kappa", 1.8)),
                                "Bt_T": float(_ov.get("Bt_T", 10.0)),
                                "Ip_MA": float(_ov.get("Ip_MA", 8.0)),
                                "Ti_keV": float(_ov.get("Ti_keV", 10.0)),
                                "fG": float(_ov.get("fG", 0.8)),
                                "Paux_MW": float(_ov.get("Paux_MW", 50.0)),
                            }, **{k:v for k,v in _ov.items() if k not in ("R0_m","a_m","kappa","Bt_T","Ip_MA","Ti_keV","fG","Paux_MW")}})
                        st.session_state["pd_loaded_template_name"] = str(_sel_tmpl)
                        st.rerun()
            except Exception as _e:
                st.warning(f"Scenario template library unavailable: {_e}")

        with st.expander("Plasma & geometry", expanded=False):
            R0 = _num("Major radius Râ‚€ (m)", float(_base_pd.R0_m), 0.01, help="Distance from tokamak centerline to plasma magnetic axis (major radius).", key=PD_KEYS["R0_m"])
            a = _num("Minor radius a (m)", float(_base_pd.a_m), 0.01, min_value=0.1, help="Plasma minor radius (a). Together with Râ‚€ sets aspect ratio.", key=PD_KEYS["a_m"])
            kappa = _num("Elongation Îº (â€“)", float(_base_pd.kappa), 0.05, min_value=1.0, max_value=3.2, help="Plasma elongation Îº. Used in volume/area and stability proxies.", key=PD_KEYS["kappa"])
            delta = _num("Triangularity Î´ (â€“)", float(getattr(_base_pd, "delta", 0.0) or 0.0), 0.02, min_value=0.0, max_value=0.8, help="Triangularity Î´. Used only in the transparent inboard radial-build clearance proxy (stack closure). Default 0.0 preserves legacy behavior.", key=PD_KEYS["delta"])
            B0 = _num("Toroidal field on axis Bâ‚€ (T)", float(_base_pd.Bt_T), 0.1, min_value=0.5, max_value=25.0, help="Toroidal field at plasma axis (Bâ‚€). Drives confinement and magnet sizing.", key=PD_KEYS["Bt_T"])
            Ti = _num("Ion temperature Táµ¢ (keV)", float(_base_pd.Ti_keV), 0.25, min_value=1.0, max_value=40.0, help="Core ion temperature proxy. Drives fusion reactivity and stored energy.", key=PD_KEYS["Ti_keV"])
            Ti_over_Te = _num("Ion-to-electron temperature ratio Táµ¢/Tâ‚‘ (â€“)", float(getattr(_base_pd, "Ti_over_Te", 1.0)), 0.1, min_value=0.5, help="Assumed ratio Táµ¢/Tâ‚‘; sets electron temperature for radiation estimate.", key=PD_KEYS["Ti_over_Te"])

        with st.expander("TF magnets & technology", expanded=False):
                tech_opts = [
                    "HTS_REBCO",
                    "LTS_NB3SN",
                    "LTS_NBTI",
                    "COPPER",
                ]
                _base_tech = str(getattr(_base_pd, "magnet_technology", "HTS_REBCO") or "HTS_REBCO").strip().upper()
                if _base_tech not in tech_opts:
                    _base_tech = "HTS_REBCO"
                tech = st.selectbox(
                    "TF technology (tech-axis)",
                    options=tech_opts,
                    index=tech_opts.index(_base_tech),
                    key=PD_KEYS["magnet_technology"],
                    help=(
                        "Select the TF magnet technology. This controls the superconducting critical-surface margin proxy "
                        "(or disables it for copper) and is recorded in artifacts for reviewer traceability."
                    ),
                )
                Tcoil = _num(
                    "TF coil temperature T_coil (K)",
                    float(getattr(_base_pd, "Tcoil_K", 20.0)),
                    0.5,
                    min_value=3.5,
                    max_value=350.0,
                    help=(
                        "Operating temperature for the TF conductor. Typical anchors: ~4.2â€“4.5 K for NbTi/Nb3Sn, "
                        "~20 K for REBCO screening, ~300 K for copper (resistive)."
                    ),
                    key=PD_KEYS["Tcoil_K"],
                )
        with st.expander("Model options (transparent (systems-code-inspired))", expanded=False):
                confinement_scaling_label = st.selectbox(
                    "H-factor reference scaling (for H_scaling)",
                    options=[
                        "IPB98(y,2) (H98 basis)",
                        "ITER89-P (L-mode)",
                        "Kayeâ€“Goldston (L-mode)",
                        "Neo-Alcator (ohmic/L)",
                        "Mirnov (ohmic)",
                        "Shimomura (L-mode)",
                    ],
                    index=0,
                    help=(
                        "Controls the reference scaling used for the reported H_scaling = tauE_eff / tauScaling. "
                        "H98 remains defined relative to IPB98(y,2)."
                    ),
                )
                confinement_scaling_map = {
                    "IPB98(y,2) (H98 basis)": "IPB98y2",
                    "ITER89-P (L-mode)": "ITER89P",
                    "Kayeâ€“Goldston (L-mode)": "KG",
                    "Neo-Alcator (ohmic/L)": "NEOALC",
                    "Mirnov (ohmic)": "MIRNOV",
                    "Shimomura (L-mode)": "SHIMOMURA",
                }
                confinement_scaling = confinement_scaling_map.get(confinement_scaling_label, "IPB98y2")

                # -----------------------------------------------------------------
                # v371.0: Transport contract library (governance-only)
                # -----------------------------------------------------------------
                with st.expander("ðŸš¦ Transport contract library (v371)", expanded=False):
                    include_transport_contracts_v371 = st.checkbox(
                        "Enable transport contract diagnostics",
                        value=bool(getattr(_base_pd, "include_transport_contracts_v371", False)),
                        key=PD_KEYS["include_transport_contracts_v371"],
                        help=(
                            "Regime-conditioned confinement-scaling envelope + explicit optimistic/robust caps on required confinement (H_required). "
                            "Governance-only: does not change frozen truth unless you set caps as constraints."
                        ),
                    )
                    _hopt_base = getattr(_base_pd, "H_required_max_optimistic", float("nan"))
                    _hrob_base = getattr(_base_pd, "H_required_max_robust", float("nan"))
                    cH1, cH2 = st.columns(2)
                    with cH1:
                        H_required_max_optimistic = st.number_input(
                            "H_required max (optimistic)",
                            min_value=0.5,
                            max_value=5.0,
                            value=float(_hopt_base) if (float(_hopt_base)==float(_hopt_base) and float(_hopt_base)>0) else 2.0,
                            step=0.05,
                            key=PD_KEYS["H_required_max_optimistic"],
                            disabled=not include_transport_contracts_v371,
                            help="If enabled, enforces H_required â‰¤ this cap (optimistic).",
                        )
                    with cH2:
                        H_required_max_robust = st.number_input(
                            "H_required max (robust)",
                            min_value=0.5,
                            max_value=5.0,
                            value=float(_hrob_base) if (float(_hrob_base)==float(_hrob_base) and float(_hrob_base)>0) else 1.5,
                            step=0.05,
                            key=PD_KEYS["H_required_max_robust"],
                            disabled=not include_transport_contracts_v371,
                            help="If enabled, enforces H_required â‰¤ this tighter cap (robust).",
                        )
                    st.caption("These caps are explicit constraints (no smoothing): if set, infeasible points are reported as transport-limited.")

                # -----------------------------------------------------------------
                # v372.0: Neutronicsâ€“Materials coupling (governance-only)
                # -----------------------------------------------------------------
                with st.expander("ðŸ§¬ Neutronicsâ€“Materials coupling (v372)", expanded=False):
                    include_neutronics_materials_coupling_v372 = st.checkbox(
                        "Enable neutronicsâ€“materials coupling diagnostics",
                        value=bool(getattr(_base_pd, "include_neutronics_materials_coupling_v372", False)),
                        key=PD_KEYS["include_neutronics_materials_coupling_v372"],
                        help=(
                            "Governance-only: material/spectrum-conditioned DPA-rate proxy, component damage partitions, and optional explicit DPA caps. "
                            "Does not modify frozen truth."
                        ),
                    )
                    _mat0 = str(getattr(_base_pd, "nm_material_class_v372", "RAFM"))
                    _spec0 = str(getattr(_base_pd, "nm_spectrum_class_v372", "nominal"))
                    cNM1, cNM2 = st.columns(2)
                    with cNM1:
                        nm_material_class_v372 = st.selectbox(
                            "Material class (governance)",
                            ["RAFM", "W", "SiC", "ODS"],
                            index=max(0, ["RAFM","W","SiC","ODS"].index(_mat0) if _mat0 in ["RAFM","W","SiC","ODS"] else 0),
                            disabled=not include_neutronics_materials_coupling_v372,
                            key=PD_KEYS["nm_material_class_v372"],
                        )
                    with cNM2:
                        nm_spectrum_class_v372 = st.selectbox(
                            "Spectrum class (governance)",
                            ["soft", "nominal", "hard"],
                            index=max(0, ["soft","nominal","hard"].index(_spec0) if _spec0 in ["soft","nominal","hard"] else 1),
                            disabled=not include_neutronics_materials_coupling_v372,
                            key=PD_KEYS["nm_spectrum_class_v372"],
                        )
                    _T0 = getattr(_base_pd, "nm_T_oper_C_v372", float('nan'))
                    use_T = st.checkbox(
                        "Use operating temperature window check",
                        value=bool(np.isfinite(_T0)),
                        disabled=not include_neutronics_materials_coupling_v372,
                        key=PD_KEYS["nm_T_oper_C_v372"] + "_use",
                    )
                    nm_T_oper_C_v372 = float('nan')
                    if use_T:
                        nm_T_oper_C_v372 = st.number_input(
                            "Operating temperature (Â°C)",
                            value=float(_T0) if np.isfinite(_T0) else 500.0,
                            min_value=0.0,
                            step=10.0,
                            disabled=not include_neutronics_materials_coupling_v372,
                            key=PD_KEYS["nm_T_oper_C_v372"],
                        )
                    _dpa0 = getattr(_base_pd, "dpa_rate_eff_max_v372", float('nan'))
                    use_dpa_cap = st.checkbox(
                        "Enable explicit DPA-rate cap constraint",
                        value=bool(np.isfinite(_dpa0)),
                        disabled=not include_neutronics_materials_coupling_v372,
                        key=PD_KEYS["dpa_rate_eff_max_v372"] + "_use",
                    )
                    dpa_rate_eff_max_v372 = float('nan')
                    if use_dpa_cap:
                        dpa_rate_eff_max_v372 = st.number_input(
                            "DPA-rate cap (DPA/FPY)",
                            value=float(_dpa0) if np.isfinite(_dpa0) else 20.0,
                            min_value=0.0,
                            step=1.0,
                            disabled=not include_neutronics_materials_coupling_v372,
                            key=PD_KEYS["dpa_rate_eff_max_v372"],
                        )
                    _m0 = getattr(_base_pd, "damage_margin_min_v372", float('nan'))
                    use_margin = st.checkbox(
                        "Enable minimum damage margin constraint",
                        value=bool(np.isfinite(_m0)),
                        disabled=not include_neutronics_materials_coupling_v372 or (not use_dpa_cap),
                        key=PD_KEYS["damage_margin_min_v372"] + "_use",
                    )
                    damage_margin_min_v372 = float('nan')
                    if use_margin:
                        damage_margin_min_v372 = st.number_input(
                            "Minimum damage margin (fraction)",
                            value=float(_m0) if np.isfinite(_m0) else 0.0,
                            step=0.05,
                            disabled=not include_neutronics_materials_coupling_v372 or (not use_dpa_cap),
                            key=PD_KEYS["damage_margin_min_v372"],
                        )

                st.caption("Tip: Use this for sensitivity studies (external systems codes-style). It does not change the solved operating point unless you also constrain power balance residuals.")
                profile_model = st.selectbox(
                    "Analytic profiles (Â½-D scaffold)",
                    options=["none", "parabolic", "pedestal"],
                    index=0,
                    help="If enabled, SHAMS computes simple analytic profiles and adds profile-integrated fusion diagnostics.",
                )
                profile_peaking_ne = _num("nâ‚‘ peaking (alpha)", 1.0, 0.1, min_value=0.0, help="Parabolic/pedestal core peaking control for density.")
                profile_peaking_T = _num("T peaking (alpha)", 1.5, 0.1, min_value=0.0, help="Parabolic/pedestal core peaking control for temperature.")

                # v318.0: 1.5D profile authority knobs (deterministic; no solvers)
                # v318.0: 1.5D profile authority knobs (deterministic; no solvers)
                profile_mode = st.checkbox(
                    "Enable 1.5D profile authority diagnostics",
                    value=bool(getattr(_base_pd, "profile_mode", False)),
                    key=PD_KEYS["profile_mode"],
                    help=(
                        "Enables analytic profile diagnostics + the algebraic 1.5D profile bundle. "
                        "This does NOT run transport, does NOT iterate, and does NOT modify the frozen operating point. "
                        "It only produces additional diagnostics and (bounded) bootstrap sensitivity when explicitly selected."
                    ),
                )

                # -----------------------------------------------------------------
                # v358.0: Profile Family Library Authority (transport proxy)
                # -----------------------------------------------------------------
                with st.expander("ðŸ§¬ Profile family library (v358)", expanded=False):
                    include_profile_family_v358 = st.checkbox(
                        "Enable profile family transport proxy",
                        value=bool(getattr(_base_pd, "include_profile_family_v358", False)),
                        help="Deterministic profile-family tags and shape multipliers. No solvers, no iteration.",
                    )
                    _pf_opts = ["CORE_FLAT","CORE_PEAKED","PEDESTAL_MODERATE","PEDESTAL_STRONG","HYBRID_CORE_PEAKED_PED"]
                    _pf_base = str(getattr(_base_pd, "profile_family_v358", "CORE_FLAT")).upper().replace(" ", "_")
                    _pf_idx = _pf_opts.index(_pf_base) if _pf_base in _pf_opts else 0
                    profile_family_v358 = st.selectbox(
                        "Profile family",
                        options=_pf_opts,
                        index=_pf_idx,
                        help="Certified profile narratives used to derive bounded shape factors.",
                    )
                    profile_family_pedestal_frac = st.slider(
                        "Pedestal fraction (proxy)",
                        min_value=0.0, max_value=0.40, value=float(getattr(_base_pd, "profile_family_pedestal_frac", 0.0)), step=0.01,
                    )
                    profile_family_peaking_p = st.slider(
                        "Pressure peaking factor",
                        min_value=0.70, max_value=2.00, value=float(getattr(_base_pd, "profile_family_peaking_p", 1.0)), step=0.01,
                    )
                    profile_family_peaking_j = st.slider(
                        "Current peaking factor",
                        min_value=0.70, max_value=2.00, value=float(getattr(_base_pd, "profile_family_peaking_j", 1.0)), step=0.01,
                    )
                    profile_family_shear_shape = st.slider(
                        "Shear shape (0â€“1)",
                        min_value=0.0, max_value=1.0, value=float(getattr(_base_pd, "profile_family_shear_shape", 0.5)), step=0.01,
                    )
                    profile_family_confinement_mult = st.slider(
                        "Confinement multiplier (bounded)",
                        min_value=0.50, max_value=1.80, value=float(getattr(_base_pd, "profile_family_confinement_mult", 1.0)), step=0.01,
                    )
                    profile_family_bootstrap_mult = st.slider(
                        "Bootstrap multiplier (bounded)",
                        min_value=0.50, max_value=1.80, value=float(getattr(_base_pd, "profile_family_bootstrap_mult", 1.0)), step=0.01,
                    )
                    st.caption("Outputs: profile_family_* keys, tauE_profile_s, H98_profile, profile_f_bootstrap_profile")

                c1, c2, c3 = st.columns(3)
                with c1:
                    profile_alpha_T = _num(
                        "Core T exponent Î±_T",
                        float(getattr(_base_pd, "profile_alpha_T", 1.5)),
                        0.1,
                        min_value=0.0,
                        key=PD_KEYS["profile_alpha_T"],
                        help="Parabolic exponent for diagnostic T(r) used when profile diagnostics are enabled.",
                    )
                with c2:
                    profile_alpha_n = _num(
                        "Core n exponent Î±_n",
                        float(getattr(_base_pd, "profile_alpha_n", 1.0)),
                        0.1,
                        min_value=0.0,
                        key=PD_KEYS["profile_alpha_n"],
                        help="Parabolic exponent for diagnostic n(r) used when profile diagnostics are enabled.",
                    )
                with c3:
                    profile_shear_shape = st.slider(
                        "Shear shape (0..1)",
                        min_value=0.0,
                        max_value=1.0,
                        value=float(getattr(_base_pd, "profile_shear_shape", 0.5)),
                        step=0.05,
                        key=PD_KEYS["profile_shear_shape"],
                        help="Algebraic 1.5D bundle knob: higher values increase qmin_proxy (stabilizing) in the diagnostic profile bundle.",
                    )

                pedestal_enabled = st.checkbox(
                    "Enable pedestal shaping (diagnostic scaffold)",
                    value=bool(getattr(_base_pd, "pedestal_enabled", False)),
                    key=PD_KEYS["pedestal_enabled"],
                    help="If enabled, the analytic profile scaffold applies a simple pedestal edge transition for diagnostics.",
                )
                pedestal_width_a = _num(
                    "Pedestal width (a-units)",
                    float(getattr(_base_pd, "pedestal_width_a", 0.05)),
                    0.005,
                    min_value=0.01,
                    max_value=0.25,
                    key=PD_KEYS["pedestal_width_a"],
                    help="Pedestal width used by the analytic profile scaffold (diagnostic only).",
                )
                bootstrap_model = st.selectbox(
                    "Bootstrap proxy model",
                    options=["proxy", "improved"],
                    index=0,
                    help="Select bootstrap fraction proxy used for reporting and (if enabled) steady-state current fractions.",
                )

                include_bootstrap_pressure_selfconsistency = st.checkbox(
                    "Enable Bootstrapâ€“Pressure Self-Consistency Authority (v349)",
                    value=bool(getattr(_base_pd, "include_bootstrap_pressure_selfconsistency", False)),
                    key=PD_KEYS["include_bootstrap_pressure_selfconsistency"],
                    help="Deterministic check: compares f_bs proxy from the profile bundle vs a pressure-derived expectation under the selected bootstrap proxy model. No iteration.",
                )
                f_bootstrap_consistency_abs_max = float("nan")
                if include_bootstrap_pressure_selfconsistency:
                    f_bootstrap_consistency_abs_max = _num(
                        "Max |Î”f_bs| (â€“)",
                        float(getattr(_base_pd, "f_bootstrap_consistency_abs_max", 0.08) or 0.08),
                        0.01,
                        min_value=0.0,
                        max_value=0.5,
                        key=PD_KEYS["f_bootstrap_consistency_abs_max"],
                        help="Hard cap for |f_bs(reported)-f_bs(expected)|. Enforced as a constraint when enabled.",
                    )



        with st.expander("Power & composition", expanded=False):
                Paux = _num("Auxiliary heating power P_aux (MW)", float(_base_pd.Paux_MW), 1.0, min_value=0.0, max_value=500.0, help="Auxiliary heating power delivered to the plasma (MW).", key=PD_KEYS["Paux_MW"])
                Paux_for_Q = _num("Aux power used in Q definition (MW)", float(getattr(_base_pd, "Paux_MW", 0.0)), 1.0, min_value=0.0, help="Denominator power for Q = P_fus,DT(adj)/P_aux_for_Q (MW).", key=PD_KEYS["Paux_for_Q"])

                with st.expander("Physics include/exclude", expanded=False):
                    st.caption("Disable a block to SKIP its related physics *and* its checks.")
                    include_radiation = st.checkbox("Include core radiation + impurities/dilution model", value=False, help="OFF by default (reviewer-safe). Enable explicitly for Research intent studies.")
                    include_alpha_loss = st.checkbox("Include alpha-loss fraction model", value=True)
                    include_hmode_physics = st.checkbox("Include H-mode access physics (P_LH / LH_ok)", value=True)
                    use_lambda_q = st.checkbox("Include SOL width (Î»q) proxy", value=True)

                # Defaults (used even when radiation is disabled, for deterministic artifacts)
                Zeff = 1.5
                dilution_fuel = 0.85
                f_rad_core = 0.20
                radiation_model = "fractional"
                radiation_db = "proxy_v1"
                impurity_species = "C"
                impurity_frac = 0.0
                include_synchrotron = True
                zeff_mode = "fixed"
                impurity_mix = ""

                # v320 impurity + detachment authority contract defaults
                impurity_contract_species = "Ne"
                impurity_contract_f_z = 3e-4
                impurity_partition_core = 0.50
                impurity_partition_edge = 0.20
                impurity_partition_sol = 0.20
                impurity_partition_div = 0.10
                include_sol_radiation_control = False
                q_div_target_MW_m2 = float('nan')
                T_sol_keV = 0.08
                f_V_sol_div = 0.12
                detachment_fz_max = float('nan')
                include_edge_core_coupled_exhaust = False
                edge_core_coupling_chi_core = 0.25
                f_rad_core_edge_core_max = float('nan')

                if include_radiation:
                    Zeff = _num("Effective charge Z_eff (â€“)", 1.5, 0.1, min_value=1.0, help="Effective ion charge Z_eff; used for brems proxy (diagnostic) and radiation screens when enabled.")
                    dilution_fuel = _num("Fuel dilution fraction (DT-equivalent) (â€“)", 0.85, 0.01, min_value=0.0, max_value=1.0, help="Multiplicative penalty on DT-equivalent fusion power due to dilution/impurities.")
                    f_rad_core = _num("Core radiation fraction f_rad,core (â€“)", 0.20, 0.01, min_value=0.0, max_value=0.95, help="If enabled, Prad_core = f_rad_core * Pin (simple screening model).")
            
                    radiation_model = st.selectbox(
                        "Radiation model",
                        options=["fractional", "impurity_mix"],
                        index=0,
                        help="fractional: Prad_core = f_rad_core * Pin (legacy proxy). impurity_mix: brem + (optional) synchrotron + impurity line radiation using Lz(Te) tables."
                    )
                    radiation_db = st.selectbox(
                        "Lz(Te) database",
                        options=["proxy_v1","radas_openadas_v1","file:<path>"],
                        index=0,
                        help="Repo-local Lz tables with hash recorded in artifacts. Replace proxy_v1 with validated tables for publication claims.",
                    )

                    # One-line reviewer-safe warning: if the selected DB cannot be resolved,
                    # the frozen evaluator will fall back to builtin_proxy (no crash). We
                    # surface this *before* a run so the user is not misled.
                    try:
                        _db_raw = str(radiation_db or "").strip()
                        _db_ok = True
                        if _db_raw.lower().startswith("file:"):
                            _p = _db_raw[5:].strip()
                            _db_ok = bool(_p) and Path(_p).expanduser().exists()
                        else:
                            _fname = f"lz_tables_{_db_raw.lower()}.json"
                            _db_ok = (SRC / "data" / "radiation" / _fname).exists()
                        if not _db_ok:
                            st.warning("Selected Lz(Te) DB not found â†’ will use builtin_proxy (no crash; provenance recorded).")
                    except Exception:
                        pass
                    if str(radiation_db).startswith('file:'):
                        radiation_db = st.text_input(
                            "Radiation DB file (JSON)",
                            value=radiation_db,
                            help="Provide as file:<path>. The JSON must contain {'species': {<SYM>: {'Te_keV': [...], 'Lz_W_m3': [...]}, ...}}. SHA256 will be recorded in artifacts.",
                        )
                    impurity_species = st.selectbox("Impurity species (for line radiation)", options=["C","N","Ne","Ar","W"], index=0)
                    impurity_frac = _num("Impurity fraction (rough)", 0.0, 0.001, min_value=0.0, help="Rough number fraction for line radiation placeholder model.")
                    include_synchrotron = st.checkbox("Include synchrotron radiation (rough)", value=True)

                    zeff_mode = st.selectbox(
                        "Z_eff handling",
                        options=["fixed", "from_impurity", "from_mix"],
                        index=0,
                        help="fixed: use Z_eff input directly. from_impurity: estimate Z_eff from (species, frac). from_mix: estimate Z_eff from impurity_mix dict.",
                    )
                    impurity_mix = st.text_input(
                        "Impurity mix (optional JSON dict)",
                        value="",
                        help="Optional multi-impurity number fractions, e.g. {\"C\":0.01, \"Ne\":0.002}. Used by the physics radiation model and (if selected) to estimate Z_eff.",
                    )

                    with st.expander("Impurity radiation & detachment authority (v320)", expanded=False):
                        st.caption(
                            "Algebraic contracts: (i) impurity radiation partitions using a bounded Lz envelope, "
                            "and (ii) detachment budget inversion from q_div target â†’ required SOL+div radiation â†’ implied f_z. "
                            "No time-domain modelling; no feedback into core power balance unless you add explicit constraints."
                        )
                        c1,c2 = st.columns(2)
                        with c1:
                            impurity_contract_species = st.selectbox(
                                "Contract species",
                                options=["C","N","Ne","Ar","W"],
                                index=2,
                                help="Species used by the contract envelope (separate from line-radiation mix model).",
                            )
                            impurity_contract_f_z = _num(
                                "Contract seeding fraction f_z = nZ/ne (â€“)",
                                float(impurity_contract_f_z),
                                1e-4,
                                min_value=0.0,
                                max_value=1e-2,
                                fmt="%.1e",
                                help="Declared seeding fraction for partition estimates (clamped to â‰¤1e-2 in truth).",
                            )
                        with c2:
                            detachment_fz_max = _num(
                                "Max allowed implied f_z (optional constraint)",
                                float(detachment_fz_max),
                                1e-4,
                                min_value=0.0,
                                fmt="%.1e",
                                help="If set (finite), SHAMS adds a soft feasibility cap: implied f_z_required â‰¤ this value.",
                            )

                        st.markdown("**Radiation partitions (fractions; sum â‰¤ 1; remainder â†’ core)**")
                        p1,p2,p3,p4 = st.columns(4)
                        with p1:
                            impurity_partition_core = st.slider("core", 0.0, 1.0, float(impurity_partition_core), 0.01)
                        with p2:
                            impurity_partition_edge = st.slider("edge", 0.0, 1.0, float(impurity_partition_edge), 0.01)
                        with p3:
                            impurity_partition_sol = st.slider("SOL", 0.0, 1.0, float(impurity_partition_sol), 0.01)
                        with p4:
                            impurity_partition_div = st.slider("divertor", 0.0, 1.0, float(impurity_partition_div), 0.01)

                        st.markdown("**Detachment target (diagnostic transparency)**")
                        include_sol_radiation_control = st.checkbox(
                            "Enable q_div target inversion",
                            value=bool(include_sol_radiation_control),
                            help="Uses q_div_target to compute required SOL+div radiated fraction and implied impurity f_z.",
                        )
                        q_div_target_MW_m2 = _num(
                            "Requested q_div target (MW/mÂ²)",
                            float(q_div_target_MW_m2) if q_div_target_MW_m2==q_div_target_MW_m2 else 10.0,
                            0.5,
                            min_value=0.1,
                            help="Technology goal. This does not change the operating point; it produces a required SOL+div radiation budget.",
                        ) if include_sol_radiation_control else float('nan')
                        c3,c4 = st.columns(2)
                        with c3:
                            T_sol_keV = _num("T_SOL proxy (keV)", float(T_sol_keV), 0.01, min_value=0.03, max_value=1.0)
                        with c4:
                            f_V_sol_div = _num("Effective radiating volume fraction V_SOL+div / V", float(f_V_sol_div), 0.01, min_value=0.005, max_value=0.5)

                        st.markdown('**Edgeâ€“core coupled exhaust (v348)**')
                        include_edge_core_coupled_exhaust = st.checkbox(
                            'Enable edgeâ€“core coupled exhaust re-evaluation',
                            value=bool(include_edge_core_coupled_exhaust),
                            help='One-pass: uses P_SOL,eff = P_SOL - chi_coreÂ·P_rad,req(SOL+div) to re-evaluate q_div. lambda_q is held fixed. Does not iterate.',
                        )
                        edge_core_coupling_chi_core = st.slider(
                            'Coupling coefficient chi_core (â€“)',
                            min_value=0.0, max_value=1.0, value=float(edge_core_coupling_chi_core), step=0.05,
                            help='Fraction of SOL+div radiation requirement mapped to additional core radiation penalty for exhaust budgeting.',
                        )
                        f_rad_core_edge_core_max = _num(
                            'Max allowed coupled core radiative fraction (optional)',
                            float(f_rad_core_edge_core_max),
                            0.05, min_value=0.0, max_value=2.0,
                            help='If set, enforces f_rad_core_edge_core â‰¤ max when edge-core coupling is enabled.',
                        )

                    st.markdown("**Power-channel bookkeeping (transparent; totals unchanged)**")
                    f_alpha_to_ion = st.slider("Alpha deposition to ions f_Î±â†’i", min_value=0.0, max_value=1.0, value=0.85, step=0.01)
                    f_aux_to_ion = st.slider("Aux deposition to ions f_auxâ†’i", min_value=0.0, max_value=1.0, value=0.50, step=0.01)
                    include_P_ie = st.checkbox("Include ionâ†”electron equilibration P_ie (diagnostic)", value=True)

                    st.markdown("**Particle sustainability (optional diagnostic closure)**")
                    include_particle_balance = st.checkbox("Enable particle balance closure (diagnostic)", value=False)
                    tau_p_over_tauE = _num("Ï„_p / Ï„_E,eff (â€“)", 3.0, 0.2, min_value=0.0, help="Proxy: particle confinement time Ï„_p = (Ï„_p/Ï„_E,eff)Â·Ï„_E,eff.")
                    S_fuel_max_1e22_per_s = _num("Max fueling source S_max (1e22/s) (optional)", float('nan'), 0.1, min_value=0.0, help="If set, SHAMS enforces S_required â‰¤ S_max as a feasibility constraint (only when particle closure enabled).")

                    st.markdown("**Non-inductive & risk screens (optional; system-code)**")
                    cd_enable = st.checkbox("Enable current-drive closure (proxy)", value=False)
                    cd_method = st.selectbox("CD method", options=["NBI","EC","LH"], index=0)
                    cd_fraction_of_Paux = st.slider("Fraction of Paux allocated to CD", min_value=0.0, max_value=1.0, value=0.5, step=0.05)
                    f_NI_min = _num("Min non-inductive fraction f_NI,min (optional)", float("nan"), 0.05, min_value=0.0, max_value=1.0, help="If set, enforces (I_bootstrap+I_cd)/Ip â‰¥ f_NI,min.")
                    disruption_risk_max = _num("Max disruption risk proxy (optional)", float("nan"), 0.1, min_value=0.0, help="If set, enforces disruption_risk_proxy â‰¤ max.")
                    f_rad_core_max = _num("Max core radiative fraction (optional)", float("nan"), 0.05, min_value=0.0, max_value=2.0, help="If set, enforces Prad_core/Ploss â‰¤ max.")

                else:
                    Zeff = 1.0
                    dilution_fuel = 1.0
                    f_rad_core = 0.0
                    zeff_mode = "fixed"
                    impurity_species = "C"
                    impurity_frac = 0.0
                    impurity_mix = ""
                    include_synchrotron = False
                    f_alpha_to_ion = 0.85
                    f_aux_to_ion = 0.50
                    include_P_ie = True
                    include_particle_balance = False
                    tau_p_over_tauE = 3.0
                    S_fuel_max_1e22_per_s = float("nan")
                    cd_enable = False
                    cd_method = "NBI"
                    cd_fraction_of_Paux = 0.5
                    f_NI_min = float("nan")
                    disruption_risk_max = float("nan")
                    f_rad_core_max = float("nan")

                    impurity_contract_species = "Ne"
                    impurity_contract_f_z = 3e-4
                    impurity_partition_core = 0.50
                    impurity_partition_edge = 0.20
                    impurity_partition_sol = 0.20
                    impurity_partition_div = 0.10
                    include_sol_radiation_control = False
                    q_div_target_MW_m2 = float('nan')
                    T_sol_keV = 0.08
                    f_V_sol_div = 0.12
                    detachment_fz_max = float('nan')

                if include_alpha_loss:
                    alpha_loss_frac = _num("Alpha heating loss fraction (â€“)", 0.05, 0.01, min_value=0.0, max_value=1.0, help="If enabled, fraction of alpha heating assumed lost (not deposited in core).")
                else:
                    alpha_loss_frac = 0.0

                # Optional fast-particle / ash closures (transparent (systems-code-inspired); defaults preserve legacy behavior)
                with st.expander("Advanced fast-particle / ash closures (optional)", expanded=False):
                    st.caption("All options here are **opt-in**; defaults preserve current SHAMS behavior.")
                    alpha_loss_model = st.selectbox(
                        "Alpha prompt-loss model",
                        options=["fixed", "rho_star"],
                        index=0,
                        help="fixed: use alpha_loss_frac directly. rho_star: alpha_loss_frac_eff = alpha_loss_frac + kÂ·rho* (transparent proxy).",
                    )
                    alpha_prompt_loss_k = _num(
                        "Prompt-loss slope k (â€“)",
                        0.0,
                        0.01,
                        min_value=0.0,
                        max_value=1.0,
                        help="Used only if alpha_loss_model='rho_star'. Effective alpha loss is clipped to [0,0.9].",
                    )
                    alpha_partition_model = st.selectbox(
                        "Alpha ion/electron partition proxy",
                        options=["fixed", "Te_ratio"],
                        index=0,
                        help="Bookkeeping only: affects Palpha_i/Palpha_e reporting (Pin unchanged).",
                    )
                    alpha_partition_k = _num(
                        "Partition slope k (â€“)",
                        0.0,
                        0.01,
                        min_value=0.0,
                        max_value=2.0,
                        help="Used only if alpha_partition_model='Te_ratio'.",
                    )

                    ash_dilution_mode = st.selectbox(
                        "Helium-ash dilution penalty",
                        options=["off", "fixed_fraction"],
                        index=0,
                        help="off: no additional penalty. fixed_fraction: Pfus_for_Q *= (1-f_He_ash)^2 (transparent proxy).",
                    )
                    f_He_ash = _num(
                        "Helium-ash fraction f_He_ash (â€“)",
                        0.0,
                        0.01,
                        min_value=0.0,
                        max_value=0.9,
                        help="Used only if ash_dilution_mode='fixed_fraction'.",
                    )
                if include_hmode_physics:
                    require_Hmode = st.checkbox("Require H-mode access (enforce P_aux â‰¥ (1+margin)Â·P_LH)", value=False)
                    PLH_margin = _num("P_LH margin (â€“)", 0.0, 0.05, min_value=0.0, max_value=5.0, help="If Require H-mode is enabled: require P_aux â‰¥ (1+margin)Â·P_LH.")
                else:
                    require_Hmode = False
                    PLH_margin = 0.0
        with st.expander("Operating targets (solver)", expanded=False):
                fuel_mode_label = st.radio(
                    "Fuel / design mode",
                    ["DT performance (targets Q & net electric)", "DD feasibility (includes secondary DT from DD-produced T)"],
                    index=0,
                )
                fuel_mode = "DT" if fuel_mode_label.startswith("DT") else "DD"
                if fuel_mode == "DD":
                    include_secondary_DT = st.checkbox("Include secondary DT from DD-produced tritium", value=True)
                    if include_secondary_DT:
                        tritium_retention = _num("Tritium retention fraction f_ret (â€“)", 0.5, 0.05, min_value=0.0, max_value=1.0,
                                                 help="Fraction of DD-produced tritium retained/available to burn in secondary DT.")
                        tau_T_loss_s = _num("Effective tritium loss time Ï„_T (s)", 5.0, 0.5, min_value=0.1,
                                            help="Effective confinement/retention time for produced tritium before loss/removal.")
                    else:
                        tritium_retention = 0.0
                        tau_T_loss_s = 1.0
                else:
                    include_secondary_DT = False
                    tritium_retention = 0.0
                    tau_T_loss_s = 1.0

                # Mode-specific safe defaults (DD mode prioritizes feasibility screens over performance)
                default_Q = 2.0 if fuel_mode == "DT" else 0.05
                default_H98 = 1.15 if fuel_mode == "DT" else 1.0
                Q_target = _num("Target Q (fusion gain proxy) [-]", default_Q, 0.05, min_value=0.0)
                H98_target = _num("Target H98 [-]", default_H98, 0.05, min_value=0.1, help="Required confinement factor H98. Solver adjusts Ip and f_G to meet this target.")
                use_envelope = st.checkbox("Design envelope solve (SPARC-like)", value=False, help="Use transparent (systems-code-inspired) bounded vector solve to hit targets by varying Ip, fG, and optionally Paux.")
                Pfus_target = None
                Pnet_target = None
                if use_envelope:
                    Pfus_target = _num("Target fusion power P_fus (MW)", 140.0, 10.0, min_value=0.0)
                    Pnet_target = _num("Target net electric power P_net (MW) (optional)", -1.0, 10.0, help="Set to <0 to ignore. If >0, solver will try to meet it by varying Paux as needed.", min_value=-1e6)

                # -------------------------------------------------------------
                # Optimization (transparent (systems-code-inspired)): search within bounds for a better design
                # -------------------------------------------------------------
                st.markdown("**Optimization (experimental)**")
                do_opt = st.checkbox("Run constrained optimization (random search)", value=False,
                                     help="Searches over (Ip, fG, Paux) within bounds to improve an objective while satisfying constraints.")
                opt_objective = st.selectbox("Objective", ["min_R0", "min_Bpeak", "max_Pnet", "min_recirc"], index=1)
                opt_iters = int(_num("Optimization iterations", 200, 10, min_value=20.0))
                opt_seed = int(_num("Optimization seed", 1, 1, min_value=0.0))
                st.divider()

                # Defaults track the currently loaded base point so preset loads immediately feel consistent.
                _ip0 = float(getattr(_base_pd, "Ip_MA", 8.0) or 8.0)
                _fg0 = float(getattr(_base_pd, "fG", 0.8) or 0.8)
                Ip_min = _num("Plasma current lower bound I_p,min (MA)", max(0.1, 0.80 * _ip0), 1.0, min_value=0.1, key=PD_KEYS["Ip_lo"])
                Ip_max = _num("Plasma current upper bound I_p,max (MA)", max(0.2, 1.20 * _ip0), 0.5, min_value=0.1, key=PD_KEYS["Ip_hi"])
                fG_min = _num("Greenwald fraction lower bound f_G,min (â€“)", max(0.0, _fg0 - 0.20), 0.01, min_value=0.0, max_value=2.0, key=PD_KEYS["fG_lo"])
                fG_max = _num("Greenwald fraction upper bound f_G,max (â€“)", min(2.0, _fg0 + 0.20), 0.01, min_value=0.0, max_value=2.0, key=PD_KEYS["fG_hi"])
                tol = _num("solver tol [-]", 1e-3, 1e-4, min_value=1e-6, fmt="%.1e")
                show_solver_live = st.checkbox(
                    "Show solver physics live (step-by-step)",
                    value=True,
                    help=(
                        "Visualize how the nested solver converges: outer bisection on Ip to hit the target H98, "
                        "with an inner solve on fG to match the target Q at each Ip evaluation."
                    ),
                )
        with st.expander("Engineering & plant feasibility (optional)", expanded=False):
                # These names are passed through via PointInputs **kwargs, so they must exist in your src version.
                # We keep them optional. If missing, they are simply ignored by PointInputs.
                tshield = _num("Neutron shield thickness (m)", 0.8, 0.01, min_value=0.0, help="Effective neutron shield thickness used for neutronics/HTS lifetime proxies.")
                # A small representative set; add more once you confirm exact fields in src/phase1_systems.py
                # We still allow user to run without them.

                # --- Engineering & plant feasibility (optional): per-subsystem toggles + confidence presets ---
                st.markdown("#### Engineering & plant feasibility (optional)")
                confidence = st.radio(
                    "Confidence level",
                    ["Conservative", "Nominal", "Aggressive"],
                    index=1,
                    horizontal=True,
                    help="Controls default assumptions and warning bands (WARN vs FAIL). Conservative is stricter; aggressive is more permissive."
                )
                warn_fracs = {
                    "Conservative": {"max": 0.85, "min": 1.20},
                    "Nominal":      {"max": 0.90, "min": 1.10},
                    "Aggressive":   {"max": 0.95, "min": 1.05},
                }[confidence]

                c1, c2 = st.columns(2)
                with c1:
                    include_build = st.checkbox("Build & radial build", value=True)
                    include_magnets = st.checkbox("Magnets & HTS", value=True)
                    include_divertor = st.checkbox("Divertor / SOL", value=True)
                with c2:
                    include_neutronics = st.checkbox("Neutronics (TBR, lifetime)", value=True)
                    include_net_power = st.checkbox("Net power / electrical balance", value=True)
                    include_fuelcycle = st.checkbox("Fuel-cycle (tritium throughput/inventory)", value=False)
                    include_economics = st.checkbox(
                        "Economics overlay (CAPEX proxy cap)",
                        value=False,
                        help="Enable optional PROCESS-like component CAPEX proxy knobs and an optional hard cap. Diagnostic only; does not change plasma truth.",
                    )


                defaults = _base_pd  # safe local defaults source for optional authority overlays

                # --- (v359.0) Availability & replacement ledger authority (optional) ---
                with st.expander("ðŸ› ï¸ Availability & replacement ledger (v359.0)", expanded=False):
                    st.caption("Deterministic algebraic ledger: planned baseline + forced baseline (forced_outage_base) + replacement downtime + annualized replacement cost. Disabled by default.")
                    include_availability_replacement_v359 = st.checkbox(
                        "Enable availability+replacement ledger authority (v359.0)",
                        value=bool(getattr(defaults, "include_availability_replacement_v359", False)),
                        help="Adds availability_v359, replacement cost rate, and an optional LCOE cap. Does not modify plasma truth or legacy economics outputs.",
                    )
                    cA, cB = st.columns(2)
                    with cA:
                        planned_outage_base = st.number_input(
                            "Planned outage baseline (fraction)",
                            min_value=0.0,
                            max_value=0.50,
                            value=float(getattr(defaults, "planned_outage_base", 0.05) or 0.05),
                            step=0.01,
                        )
                        availability_v359_min = st.number_input(
                            "Min availability (v359) (NaN disables)",
                            value=float(getattr(defaults, "availability_v359_min", float('nan'))),
                        )
                        LCOE_max_USD_per_MWh = st.number_input(
                            "Max LCOE proxy (v359) (USD/MWh) (NaN disables)",
                            value=float(getattr(defaults, "LCOE_max_USD_per_MWh", float('nan'))),
                        )
                    with cB:
                        heating_cd_replace_interval_y = st.number_input(
                            "Heating/CD replacement interval (y)",
                            min_value=0.5,
                            max_value=50.0,
                            value=float(getattr(defaults, "heating_cd_replace_interval_y", 8.0) or 8.0),
                            step=0.5,
                        )
                        heating_cd_replace_duration_days = st.number_input(
                            "Heating/CD replacement duration (days)",
                            min_value=0.0,
                            max_value=365.0,
                            value=float(getattr(defaults, "heating_cd_replace_duration_days", 30.0) or 30.0),
                            step=1.0,
                        )
                        tritium_plant_replace_interval_y = st.number_input(
                            "Tritium plant replacement interval (y)",
                            min_value=0.5,
                            max_value=50.0,
                            value=float(getattr(defaults, "tritium_plant_replace_interval_y", 10.0) or 10.0),
                            step=0.5,
                        )
                        tritium_plant_replace_duration_days = st.number_input(
                            "Tritium plant replacement duration (days)",
                            min_value=0.0,
                            max_value=365.0,
                            value=float(getattr(defaults, "tritium_plant_replace_duration_days", 30.0) or 30.0),
                            step=1.0,
                        )

                # --- (v368.0) Maintenance Scheduling Authority 1.0 (optional) ---
                with st.expander("ðŸ—“ï¸ Maintenance scheduling authority (v368.0)", expanded=False):
                    st.caption(
                        "Deterministic outage calendar proxy: planned+forced baselines plus a bundled replacement schedule derived from cadences and durations. "
                        "No time simulation; no optimization; does not modify plasma truth."
                    )
                    include_maintenance_scheduling_v368 = st.checkbox(
                        "Enable maintenance scheduling authority (v368.0)",
                        value=bool(getattr(defaults, "include_maintenance_scheduling_v368", False)),
                        help="Adds availability_v368, outage_total_frac_v368, replacement_cost_MUSD_per_year_v368 and an explicit maintenance_events_v368 table.",
                    )
                    cM1, cM2 = st.columns(2)
                    with cM1:
                        _bp_opts = ["independent", "bundle_in_vessel", "bundle_all"]
                        _bp_def = str(getattr(defaults, "maintenance_bundle_policy", "independent"))
                        _bp_ix = _bp_opts.index(_bp_def) if _bp_def in _bp_opts else 0
                        maintenance_bundle_policy = st.selectbox(
                            "Bundling policy",
                            _bp_opts,
                            index=_bp_ix,
                            help="Bundling is a deterministic proxy: interval=min(intervals), duration=max(durations)+overhead.",
                        )
                        maintenance_bundle_overhead_days = st.number_input(
                            "Bundle overhead (days)",
                            min_value=0.0,
                            max_value=90.0,
                            value=float(getattr(defaults, "maintenance_bundle_overhead_days", 7.0) or 7.0),
                            step=1.0,
                        )
                        _fm_opts = ["max", "baseline", "trips"]
                        _fm_def = str(getattr(defaults, "forced_outage_mode_v368", "max"))
                        _fm_ix = _fm_opts.index(_fm_def) if _fm_def in _fm_opts else 0
                        forced_outage_mode_v368 = st.selectbox(
                            "Forced outage mode",
                            _fm_opts,
                            index=_fm_ix,
                            help="max = max(forced_outage_base, trips_per_year*trip_duration_days/365).",
                        )
                    with cM2:
                        availability_v368_min = st.number_input(
                            "Min availability (v368) (NaN disables)",
                            value=float(getattr(defaults, "availability_v368_min", float('nan'))),
                        )
                        outage_fraction_v368_max = st.number_input(
                            "Max total outage fraction (v368) (NaN disables)",
                            value=float(getattr(defaults, "outage_fraction_v368_max", float('nan'))),
                        )
                        maintenance_planning_horizon_yr = st.number_input(
                            "Planning horizon (yr) (NaN uses plant lifetime)",
                            min_value=1.0,
                            max_value=100.0,
                            value=float(getattr(defaults, "maintenance_planning_horizon_yr", float('nan'))),
                            step=1.0,
                        )
                # --- (v360.0) Plant Economics Authority 1.0 (optional) ---
                with st.expander("ðŸ’° Plant Economics Authority (v360.0)", expanded=False):
                    st.caption("Deterministic CAPEX+OPEX decomposition and availability-coupled LCOE proxy. Diagnostic overlay; OFF by default.")
                    include_economics_v360 = st.checkbox(
                        "Enable plant economics authority (v360.0)",
                        value=bool(getattr(defaults, "include_economics_v360", False)),
                        help="Adds OPEX component breakdown and LCOE_proxy_v360_USD_per_MWh. Does not modify plasma truth or legacy economics unless enabled.",
                    )
                    cE1, cE2 = st.columns(2)
                    with cE1:
                        opex_fixed_MUSD_per_y = st.number_input(
                            "Fixed OPEX (MUSD/y)",
                            min_value=0.0,
                            value=float(getattr(defaults, "opex_fixed_MUSD_per_y", 0.0) or 0.0),
                            step=1.0,
                        )
                        tritium_processing_cost_USD_per_g = st.number_input(
                            "Tritium processing cost (USD/g)",
                            min_value=0.0,
                            value=float(getattr(defaults, "tritium_processing_cost_USD_per_g", 0.05) or 0.05),
                            step=0.01,
                        )
                    with cE2:
                        cryo_wallplug_multiplier = st.number_input(
                            "Cryo wall-plug multiplier (MW_e/MW@20K)",
                            min_value=0.0,
                            value=float(getattr(defaults, "cryo_wallplug_multiplier", 250.0) or 250.0),
                            step=10.0,
                        )
                        OPEX_max_MUSD_per_y = st.number_input(
                            "Max OPEX (v360) (MUSD/y) (NaN disables)",
                            value=float(getattr(defaults, "OPEX_max_MUSD_per_y", float('nan'))),
                        )

                # --- (v383.0) Plant Economics & Cost Authority 2.0 (optional) ---
                with st.expander("ðŸ’° Plant Economics & Cost Authority (v383.0)", expanded=False):
                    st.caption(
                        "Deterministic structured CAPEX+OPEX with availability-tiered capacity factor and LCOE-lite proxy. "
                        "Governance overlay only; OFF by default."
                    )
                    include_economics_v383 = st.checkbox(
                        "Enable plant economics & cost authority (v383.0)",
                        value=bool(getattr(defaults, "include_economics_v383", False)),
                        help="Adds CAPEX_structured_v383_MUSD, OPEX_structured_v383_MUSD_per_y, LCOE_lite_v383_USD_per_MWh, and tiered availability proxy.",
                        key="pd_include_economics_v383",
                    )
                    cE3, cE4 = st.columns(2)
                    with cE3:
                        CAPEX_structured_max_MUSD = st.number_input(
                            "Max structured CAPEX (v383) (MUSD) (NaN disables)",
                            value=float(getattr(defaults, "CAPEX_structured_max_MUSD", float('nan'))),
                            key="pd_CAPEX_structured_max_MUSD_v383",
                        )
                        OPEX_structured_max_MUSD_per_y = st.number_input(
                            "Max structured OPEX (v383) (MUSD/y) (NaN disables)",
                            value=float(getattr(defaults, "OPEX_structured_max_MUSD_per_y", float('nan'))),
                            key="pd_OPEX_structured_max_MUSD_per_y_v383",
                        )
                    with cE4:
                        LCOE_lite_max_USD_per_MWh = st.number_input(
                            "Max LCOE-lite (v383) (USD/MWh) (NaN disables)",
                            value=float(getattr(defaults, "LCOE_lite_max_USD_per_MWh", float('nan'))),
                            key="pd_LCOE_lite_max_USD_per_MWh_v383",
                        )



                # --- (v388.0.0) Cost Authority 3.0 â€” Industrial Depth (optional) ---
                with st.expander("ðŸ­ Cost Authority â€” Industrial Depth (v388.0.0)", expanded=False):
                    st.caption("Deterministic, engineering-driven subsystem cost scaling envelopes (industrial depth). Governance-only; OFF by default. Requires the Economics overlay toggle above so cost outputs are computed.")
                    include_cost_authority_v388 = st.checkbox(
                        "Enable cost authority 3.0 (v388.0.0)",
                        value=bool(getattr(defaults, "include_cost_authority_v388", False)),
                        key="pd_include_cost_authority_v388",
                    )
                    cC1, cC2 = st.columns(2)
                    with cC1:
                        CAPEX_industrial_max_MUSD = st.number_input(
                            "Max industrial CAPEX (v388) (MUSD) (NaN disables)",
                            value=float(getattr(defaults, "CAPEX_industrial_max_MUSD", float('nan'))),
                            key="pd_CAPEX_industrial_max_MUSD_v388",
                        )
                        OPEX_industrial_max_MUSD_per_y = st.number_input(
                            "Max industrial OPEX (v388) (MUSD/y) (NaN disables)",
                            value=float(getattr(defaults, "OPEX_industrial_max_MUSD_per_y", float('nan'))),
                            key="pd_OPEX_industrial_max_MUSD_per_y_v388",
                        )
                    with cC2:
                        LCOE_lite_v388_max_USD_per_MWh = st.number_input(
                            "Max LCOE-lite (v388) (USD/MWh) (NaN disables)",
                            value=float(getattr(defaults, "LCOE_lite_v388_max_USD_per_MWh", float('nan'))),
                            key="pd_LCOE_lite_v388_max_USD_per_MWh_v388",
                        )




                # --- (v384.0.0) Materials & Lifetime Tightening (optional) ---
                with st.expander("ðŸ§± Materials & Lifetime Tightening (v384.0.0)", expanded=False):
                    st.caption(
                        "Deterministic governance overlay: adds divertor + magnet lifetime proxies, annualized replacement cost, "
                        "and replacement-downtime coupling to a capacity factor used by economics overlays. OFF by default; truth is unchanged."
                    )
                    include_materials_lifetime_v384 = st.checkbox(
                        "Enable materials & lifetime tightening (v384.0.0)",
                        value=bool(getattr(defaults, "include_materials_lifetime_v384", False)),
                        key="pd_include_materials_lifetime_v384",
                    )
                    cML1, cML2 = st.columns(2)
                    with cML1:
                        divertor_life_ref_yr = st.number_input(
                            "Divertor life ref (yr)",
                            min_value=0.1,
                            value=float(getattr(defaults, "divertor_life_ref_yr", 3.0) or 3.0),
                            step=0.1,
                            key="pd_divertor_life_ref_yr_v384",
                        )
                        divertor_q_ref_MW_m2 = st.number_input(
                            "Divertor q_ref (MW/mÂ²)",
                            min_value=0.1,
                            value=float(getattr(defaults, "divertor_q_ref_MW_m2", 10.0) or 10.0),
                            step=0.5,
                            key="pd_divertor_q_ref_v384",
                        )
                        divertor_q_exp = st.number_input(
                            "Divertor q exponent", min_value=0.0,
                            value=float(getattr(defaults, "divertor_q_exp", 2.0) or 2.0),
                            step=0.1,
                            key="pd_divertor_q_exp_v384",
                        )
                        divertor_capex_fraction_of_total = st.number_input(
                            "Divertor CAPEX fraction of total", min_value=0.0, max_value=0.5,
                            value=float(getattr(defaults, "divertor_capex_fraction_of_total", 0.05) or 0.05),
                            step=0.01,
                            key="pd_divertor_capex_frac_v384",
                        )
                    with cML2:
                        magnet_life_ref_yr = st.number_input(
                            "Magnet life ref (yr)",
                            min_value=0.1,
                            value=float(getattr(defaults, "magnet_life_ref_yr", 30.0) or 30.0),
                            step=1.0,
                            key="pd_magnet_life_ref_yr_v384",
                        )
                        magnet_margin_ref = st.number_input(
                            "Magnet margin ref (fraction)",
                            min_value=0.001,
                            value=float(getattr(defaults, "magnet_margin_ref", 0.10) or 0.10),
                            step=0.01,
                            key="pd_magnet_margin_ref_v384",
                        )
                        magnet_margin_exp = st.number_input(
                            "Magnet margin exponent", min_value=0.0,
                            value=float(getattr(defaults, "magnet_margin_exp", 1.5) or 1.5),
                            step=0.1,
                            key="pd_magnet_margin_exp_v384",
                        )

                    st.markdown("**Downtime â†’ capacity factor coupling**")
                    cML3, cML4 = st.columns(2)
                    with cML3:
                        base_capacity_factor = st.number_input(
                            "Base capacity factor (before replacements)",
                            min_value=0.0,
                            max_value=1.0,
                            value=float(getattr(defaults, "base_capacity_factor", 0.75) or 0.75),
                            step=0.01,
                            key="pd_base_cf_v384",
                        )
                        capacity_factor_max = st.number_input(
                            "Capacity factor max (cap)",
                            min_value=0.0,
                            max_value=1.0,
                            value=float(getattr(defaults, "capacity_factor_max", 0.95) or 0.95),
                            step=0.01,
                            key="pd_cf_max_v384",
                        )
                        fw_downtime_days = st.number_input(
                            "FW replacement downtime (days)",
                            min_value=0.0,
                            value=float(getattr(defaults, "fw_downtime_days", 30.0) or 30.0),
                            step=1.0,
                            key="pd_fw_dt_days_v384",
                        )
                        blanket_downtime_days = st.number_input(
                            "Blanket replacement downtime (days)",
                            min_value=0.0,
                            value=float(getattr(defaults, "blanket_downtime_days", 60.0) or 60.0),
                            step=1.0,
                            key="pd_blanket_dt_days_v384",
                        )
                    with cML4:
                        divertor_downtime_days = st.number_input(
                            "Divertor replacement downtime (days)",
                            min_value=0.0,
                            value=float(getattr(defaults, "divertor_downtime_days", 20.0) or 20.0),
                            step=1.0,
                            key="pd_divertor_dt_days_v384",
                        )
                        magnet_downtime_days = st.number_input(
                            "Magnet replacement downtime (days)",
                            min_value=0.0,
                            value=float(getattr(defaults, "magnet_downtime_days", 120.0) or 120.0),
                            step=5.0,
                            key="pd_magnet_dt_days_v384",
                        )
                        fw_capex_fraction_of_blanket = st.number_input(
                            "FW CAPEX fraction of blanket/shield CAPEX",
                            min_value=0.0,
                            max_value=1.0,
                            value=float(getattr(defaults, "fw_capex_fraction_of_blanket", 0.20) or 0.20),
                            step=0.01,
                            key="pd_fw_capex_frac_bs_v384",
                        )
                        blanket_capex_fraction_of_blanket = st.number_input(
                            "Blanket CAPEX fraction of blanket/shield CAPEX",
                            min_value=0.0,
                            max_value=2.0,
                            value=float(getattr(defaults, "blanket_capex_fraction_of_blanket", 1.00) or 1.00),
                            step=0.05,
                            key="pd_blanket_capex_frac_bs_v384",
                        )

                    st.markdown("**Feasibility caps (NaN disables)**")
                    cML5, cML6 = st.columns(2)
                    with cML5:
                        divertor_lifetime_min_yr_v384 = st.number_input(
                            "Min divertor lifetime (yr)",
                            value=float(getattr(defaults, "divertor_lifetime_min_yr_v384", float('nan'))),
                            key="pd_div_life_min_v384",
                        )
                        magnet_lifetime_min_yr_v384 = st.number_input(
                            "Min magnet lifetime (yr)",
                            value=float(getattr(defaults, "magnet_lifetime_min_yr_v384", float('nan'))),
                            key="pd_mag_life_min_v384",
                        )
                        capacity_factor_min_v384 = st.number_input(
                            "Min capacity factor (replacement-coupled)",
                            value=float(getattr(defaults, "capacity_factor_min_v384", float('nan'))),
                            key="pd_cf_min_v384",
                        )
                    with cML6:
                        fw_lifetime_min_yr_v384 = st.number_input(
                            "Min FW lifetime (yr)",
                            value=float(getattr(defaults, "fw_lifetime_min_yr_v384", float('nan'))),
                            key="pd_fw_life_min_v384",
                        )
                        blanket_lifetime_min_yr_v384 = st.number_input(
                            "Min blanket lifetime (yr)",
                            value=float(getattr(defaults, "blanket_lifetime_min_yr_v384", float('nan'))),
                            key="pd_blanket_life_min_v384",
                        )
                        replacement_cost_max_MUSD_per_y_v384 = st.number_input(
                            "Max annualized replacement cost (MUSD/y)",
                            value=float(getattr(defaults, "replacement_cost_max_MUSD_per_y_v384", float('nan'))),
                            key="pd_repl_cost_max_v384",
                        )



                preset = {
                    "Conservative": {
                        "tblanket_m": 0.60, "t_vv_m": 0.08, "t_gap_m": 0.03, "t_tf_struct_m": 0.18, "t_tf_wind_m": 0.12,
                        "Bpeak_factor": 1.30, "sigma_allow_MPa": 800.0, "Tcoil_K": 20.0, "hts_margin_min": 0.20, "Vmax_kV": 18.0,
                        "q_div_max_MW_m2": 7.0, "TBR_min": 1.10, "hts_lifetime_min_yr": 5.0, "P_net_min_MW": 0.0,
                    },
                    "Nominal": {
                        "tblanket_m": 0.50, "t_vv_m": 0.06, "t_gap_m": 0.02, "t_tf_struct_m": 0.15, "t_tf_wind_m": 0.10,
                        "Bpeak_factor": 1.25, "sigma_allow_MPa": 850.0, "Tcoil_K": 20.0, "hts_margin_min": 0.15, "Vmax_kV": 20.0,
                        "q_div_max_MW_m2": 10.0, "TBR_min": 1.05, "hts_lifetime_min_yr": 3.0, "P_net_min_MW": 0.0,
                    },
                    "Aggressive": {
                        "tblanket_m": 0.40, "t_vv_m": 0.05, "t_gap_m": 0.015, "t_tf_struct_m": 0.12, "t_tf_wind_m": 0.08,
                        "Bpeak_factor": 1.20, "sigma_allow_MPa": 900.0, "Tcoil_K": 20.0, "hts_margin_min": 0.10, "Vmax_kV": 25.0,
                        "q_div_max_MW_m2": 15.0, "TBR_min": 1.00, "hts_lifetime_min_yr": 1.0, "P_net_min_MW": 0.0,
                    },
                }[confidence]

                def _maybe(x: float, enabled: bool) -> float:
                    return float(x) if enabled else float("nan")

                clean_knobs = {
                    # Build & radial build
                    "tblanket_m": _maybe(float(_num("Blanket thickness (inboard) (m)", preset["tblanket_m"], 0.01, min_value=0.0)), include_build),
                    "t_vv_m": _maybe(float(_num("Vacuum vessel thickness (inboard) (m)", preset["t_vv_m"], 0.005, min_value=0.0)), include_build),
                    "t_gap_m": _maybe(float(_num("Inboard gap / clearance (m)", preset["t_gap_m"], 0.005, min_value=0.0)), include_build),
                    "t_tf_struct_m": _maybe(float(_num("TF structure thickness (inboard) (m)", preset["t_tf_struct_m"], 0.01, min_value=0.0)), include_build),
                    "t_tf_wind_m": _maybe(float(_num("TF winding pack thickness (inboard) (m)", preset["t_tf_wind_m"], 0.01, min_value=0.0)), include_build),

                    # Magnets & HTS
                    "Bpeak_factor": _maybe(float(_num("Peak-field mapping factor B_peak/Bâ‚€ (â€“)", preset["Bpeak_factor"], 0.01, min_value=1.0)), include_magnets),
                    "sigma_allow_MPa": _maybe(float(_num("Allowable coil hoop stress (MPa)", preset["sigma_allow_MPa"], 10.0, min_value=10.0)), include_magnets),
                    "Tcoil_K": _maybe(float(_num("HTS operating temperature (K)", preset["Tcoil_K"], 1.0, min_value=4.0)), include_magnets),
                    "hts_margin_min": _maybe(float(_num("Minimum HTS critical-current margin (â€“)", preset["hts_margin_min"], 0.01, min_value=0.0)), include_magnets),
                    "include_hts_critical_surface": bool(st.checkbox("Use HTS critical-surface model (Jc(B,T,Îµ))", value=False, disabled=not include_magnets, help="Off by default (legacy behavior). When enabled, computes hts_margin_cs using Jc(B,T,Îµ_tf)/Jop and applies the same hts_margin_min threshold.")),
                    "Vmax_kV": _maybe(float(_num("Max dump voltage limit (kV)", preset["Vmax_kV"], 1.0, min_value=1.0)), include_magnets),

                    # Magnet quench / protection authority (v285.0)
                    "quench_energy_density_max_MJ_m3": _maybe(float(_num("Max allowable quench energy density (MJ/mÂ³)", float('nan'), 1.0, min_value=0.0, help="Used to normalize stored-energy proxy into magnet_quench_risk_proxy. Leave NaN to disable.")), include_magnets),
                    "magnet_quench_risk_max": _maybe(float(_num("Max magnet quench risk proxy (â€“)", float('nan'), 0.05, min_value=0.0, help="Optional cap on stored-energy/allowable proxy. Leave NaN to disable.")), include_magnets),

                    # Divertor / SOL
                    "q_div_max_MW_m2": _maybe(float(_num("Max divertor heat flux limit (MW/mÂ²)", preset["q_div_max_MW_m2"], 0.5, min_value=0.1)), include_divertor),

                    # Exhaust authority (v285.0)
                    "detachment_index_min": _maybe(float(_num("Detachment index floor (proxy)", float('nan'), 0.01, help="Optional floor on P_SOL/(n_e^2 R). Leave NaN to disable.")), include_divertor),
                    "detachment_index_max": _maybe(float(_num("Detachment index cap (proxy)", float('nan'), 0.01, help="Optional cap on P_SOL/(n_e^2 R). Leave NaN to disable.")), include_divertor),
                    "f_rad_total_max": _maybe(float(_num("Max total radiated fraction f_rad,total (â€“)", float('nan'), 0.01, min_value=0.0, max_value=1.0, help="Optional cap on (f_rad_core+f_rad_div). Leave NaN to disable.")), include_divertor),
                    "fuel_ion_fraction_min": _maybe(float(_num("Min fuel ion fraction (dilution)", float('nan'), 0.01, min_value=0.0, max_value=1.0, help="Optional minimum fuel-ion fraction (proxy). Leave NaN to disable.")), include_divertor),
                    "Q_effective_min": _maybe(float(_num("Min effective Q (dilution-adjusted)", float('nan'), 0.05, min_value=0.0, help="Optional minimum on Q_eff = Q*fuel^2. Leave NaN to disable.")), include_divertor),

                    # Neutronics & Materials (v309.0) â€” fast optimistic knobs + explicit contracts
                    "TBR_min": _maybe(float(_num("Minimum tritium breeding ratio (TBR)", preset["TBR_min"], 0.01, min_value=0.0)), include_neutronics),
                    "port_fraction": _maybe(float(_num("Port/penetration fraction (coverage penalty)", float(preset.get("port_fraction", 0.08)), 0.01, min_value=0.0, max_value=0.8, help="Penalty to blanket coverage in the TBR proxy.")), include_neutronics),
                    "li6_enrichment": _maybe(float(_num("Li-6 enrichment fraction (0..1)", float(preset.get("li6_enrichment", 0.30)), 0.01, min_value=0.0, max_value=0.95)), include_neutronics),
                    "blanket_type": str(st.selectbox("Blanket archetype for TBR proxy", options=["LiPb","FLiBe"], index=0, disabled=not include_neutronics, help="Used by the TBR proxy only (transport-free).")),
                    "multiplier_material": str(st.selectbox("Neutron multiplier tag", options=["None","Be","Pb","Be2"], index=0, disabled=not include_neutronics, help="Simple multiplier factor used by the TBR proxy.")),
                    "neutronics_archetype": str(st.selectbox("Nuclear heating partition archetype", options=["standard","heavy_shield","compact"], index=0, disabled=not include_neutronics, help="Chooses a deterministic fraction table for in-vessel nuclear heating.")),
                    "neutronics_domain_enforce": bool(st.checkbox("Enforce neutronics proxy validity domain as HARD", value=False, disabled=not include_neutronics, help="If checked, out-of-range proxy usage (e.g., TBR thickness/coverage domains) becomes a HARD violation. Defaults off to preserve screening behavior.")),
                    "materials_domain_enforce": bool(st.checkbox("Enforce materials admissibility as HARD", value=False, disabled=not include_neutronics, help="If checked, materials window/stress screening constraints upgrade to HARD. Defaults off.")),
                    "hts_lifetime_min_yr": _maybe(float(_num("Minimum HTS lifetime (years)", preset["hts_lifetime_min_yr"], 0.5, min_value=0.0)), include_neutronics),

                    # Optional caps & material tags (NaN disables enforcement)
                    "neutron_wall_load_max_MW_m2": _maybe(float(_num("Max neutron wall load (MW/mÂ²) (optional)", float('nan'), 0.1, min_value=0.0, help="Leave NaN to disable enforcement.")), include_neutronics),
                    "fw_dpa_max_per_year": _maybe(float(_num("Max first-wall dpa per year (optional)", float('nan'), 0.5, min_value=0.0, help="Order-of-magnitude proxy derived from wall load. Leave NaN to disable.")), include_neutronics),
                    "fw_lifetime_min_yr": _maybe(float(_num("Min first-wall replacement lifetime (yr) (optional)", float('nan'), 0.5, min_value=0.0, help="Uses DPA/He rate proxies + material limits. Leave NaN to disable.")), include_neutronics),
                    "blanket_lifetime_min_yr": _maybe(float(_num("Min blanket replacement lifetime (yr) (optional)", float('nan'), 0.5, min_value=0.0, help="Uses DPA/He rate proxies + material limits. Leave NaN to disable.")), include_neutronics),

                    # (v367.0) Materials lifetime closure: deterministic policy/cadence knobs
                    "plant_design_lifetime_yr": _maybe(float(_num(
                        "Plant design lifetime (yr) (v367 materials policy)",
                        float(getattr(defaults, "plant_design_lifetime_yr", 30.0) or 30.0),
                        1.0,
                        min_value=1.0,
                        help="Used by v367 materials lifetime closure to compute replacement counts/costs. No time-domain simulation.",
                    )), include_neutronics),
                    "materials_life_cover_plant_enforce": bool(st.checkbox(
                        "Enforce FW/blanket lifetime â‰¥ plant lifetime (v367) (HARD)",
                        value=bool(getattr(defaults, "materials_life_cover_plant_enforce", False)),
                        disabled=not include_neutronics,
                        help="Policy constraint: requires fw_lifetime_yr and blanket_lifetime_yr to cover plant_design_lifetime_yr when enabled.",
                    )),
                    "fw_replace_interval_min_yr": _maybe(float(_num(
                        "Min FW replacement cadence (yr) (v367) (optional)",
                        float(getattr(defaults, "fw_replace_interval_min_yr", float('nan'))),
                        0.5,
                        min_value=0.0,
                        help="Optional minimum on the FW replacement interval used by the replacement ledger. Leave NaN to disable.",
                    )), include_neutronics),
                    "blanket_replace_interval_min_yr": _maybe(float(_num(
                        "Min blanket replacement cadence (yr) (v367) (optional)",
                        float(getattr(defaults, "blanket_replace_interval_min_yr", float('nan'))),
                        0.5,
                        min_value=0.0,
                        help="Optional minimum on the blanket replacement interval used by the replacement ledger. Leave NaN to disable.",
                    )), include_neutronics),
                    "fw_capex_fraction_of_blanket": _maybe(float(_num(
                        "FW CAPEX fraction of blanket+shield (0..1) (v367)",
                        float(getattr(defaults, "fw_capex_fraction_of_blanket", 0.20) or 0.20),
                        0.01,
                        min_value=0.0,
                        max_value=1.0,
                        help="Used to estimate FW replacement CAPEX from capex_blanket_shield_MUSD (or a fallback).",
                    )), include_neutronics),
                    "blanket_capex_fraction_of_blanket": _maybe(float(_num(
                        "Blanket CAPEX fraction of blanket+shield (0..1) (v367)",
                        float(getattr(defaults, "blanket_capex_fraction_of_blanket", 1.00) or 1.00),
                        0.01,
                        min_value=0.0,
                        max_value=1.0,
                        help="Used to estimate blanket replacement CAPEX from capex_blanket_shield_MUSD (or a fallback).",
                    )), include_neutronics),
                    "P_nuc_total_max_MW": _maybe(float(_num("Max total nuclear heating (MW) (optional)", float('nan'), 1.0, min_value=0.0, help="Stack-based nuclear heating bookkeeping. Leave NaN to disable.")), include_neutronics),
                    "P_nuc_tf_max_MW": _maybe(float(_num("Max TF nuclear heating (MW) (optional)", float('nan'), 0.5, min_value=0.0, help="Stack-based nuclear heating in TF regions. Leave NaN to disable.")), include_neutronics),
                    "P_nuc_pf_max_MW": _maybe(float(_num("Max PF nuclear heating (MW) (optional)", float('nan'), 0.5, min_value=0.0, help="Leakage partition proxy to PF. Leave NaN to disable.")), include_neutronics),
                    "P_nuc_cryo_max_kW": _maybe(float(_num("Max cryo nuclear load (kW) (optional)", float('nan'), 10.0, min_value=0.0, help="Leakage partition proxy to cryoplant. Leave NaN to disable.")), include_neutronics),

                    "shield_material": str(st.selectbox("Shield material tag (attenuation)", options=["WC","B4C","SS316","EUROFER"], index=0, disabled=not include_neutronics, help="Used for stack attenuation and heating partitioning.")),
                    "blanket_material": str(st.selectbox("Blanket material tag (attenuation)", options=["LiPb","FLiBe"], index=0, disabled=not include_neutronics, help="Used for stack attenuation and materials proxies.")),
                    "fw_material": str(st.selectbox("First-wall material tag (materials)", options=["EUROFER","SS316","W","SiC"], index=0, disabled=not include_neutronics, help="Used for temperature window + DPA/He proxies.")),

                    # Materials admissibility: temperature windows & stress (proxy)
                    "T_fw_oper_C": _maybe(float(_num("FW operating temperature (Â°C) (optional)", float('nan'), 10.0, help="Used only for window checks; no thermal solver.")), include_neutronics),
                    "T_blanket_oper_C": _maybe(float(_num("Blanket operating temperature (Â°C) (optional)", float('nan'), 10.0, help="Used only for window checks; no thermal solver.")), include_neutronics),
                    "fw_T_enforce": bool(st.checkbox("Enforce FW temperature window as HARD", value=False, disabled=not include_neutronics)),
                    "blanket_T_enforce": bool(st.checkbox("Enforce blanket temperature window as HARD", value=False, disabled=not include_neutronics)),
                    "sigma_fw_oper_MPa": _maybe(float(_num("FW operating stress (MPa) (optional)", float('nan'), 10.0, min_value=0.0, help="Used with irradiation-adjusted allowable stress proxy. Leave NaN to disable.")), include_neutronics),
                    "sigma_blanket_oper_MPa": _maybe(float(_num("Blanket operating stress (MPa) (optional)", float('nan'), 10.0, min_value=0.0, help="Used with irradiation-adjusted allowable stress proxy. Leave NaN to disable.")), include_neutronics),



                    # Fuel-cycle / tritium ledger (v350.0) â€” optional tight closure
                    "T_reserve_days": _maybe(float(_num("Tritium reserve (days)", 3.0, 0.5, min_value=0.0,
                        help="Reserve inventory proxy: T_inventory_reserve = T_burn * reserve_days.")), include_fuelcycle),
                    "T_processing_margin": _maybe(float(_num("Tritium processing margin factor (â€“)", 1.25, 0.05, min_value=0.1,
                        help="Multiplies burn throughput to set required processing capacity.")), include_fuelcycle),
                    "T_processing_capacity_min_g_per_day": _maybe(float(_num("Min processing capacity (g/day) (optional)", float('nan'), 10.0, min_value=0.0,
                        help="Optional minimum capacity contract. Leave NaN to disable.")), include_fuelcycle),
                    "T_inventory_min_kg": _maybe(float(_num("Min on-site inventory (kg) (optional)", float('nan'), 0.1, min_value=0.0,
                        help="Optional minimum inventory contract. Leave NaN to disable.")), include_fuelcycle),

                    "include_tritium_tight_closure": bool(st.checkbox(
                        "Enable tight tritium closure (inventory+loss+self-sufficiency)",
                        value=False,
                        disabled=not include_fuelcycle,
                        help="When enabled, SHAMS computes in-vessel and total tritium inventory proxies, applies optional loss tightening to TBR_eff, and enforces optional self-sufficiency margins (all algebraic; no iteration).",
                    )),
                    "T_processing_delay_days": _maybe(float(_num("Processing delay (days) â†’ in-vessel inventory proxy", 1.0, 0.2, min_value=0.0,
                        help="In-vessel inventory proxy: T_in_vessel = T_burn * delay_days.")), include_fuelcycle),
                    "T_in_vessel_max_kg": _maybe(float(_num("Max in-vessel tritium (kg) (optional)", float('nan'), 0.1, min_value=0.0,
                        help="Optional cap on in-vessel inventory proxy. Leave NaN to disable.")), include_fuelcycle),
                    "T_total_inventory_max_kg": _maybe(float(_num("Max total tritium inventory (kg) (optional)", float('nan'), 0.5, min_value=0.0,
                        help="Optional cap on total inventory proxy (reserve+in-vessel+startup). Leave NaN to disable.")), include_fuelcycle),
                    "T_startup_inventory_kg": _maybe(float(_num("Startup tritium inventory (kg) (optional)", float('nan'), 0.5, min_value=0.0,
                        help="Optional startup inventory proxy added to total inventory.")), include_fuelcycle),
                    "T_loss_fraction": _maybe(float(_num("Effective tritium loss fraction (0..0.2) (optional)", float('nan'), 0.01, min_value=0.0, max_value=0.2,
                        help="If set, effective TBR is reduced: TBR_eff = TBR*(1-loss).")), include_fuelcycle),
                    "TBR_self_sufficiency_margin": _maybe(float(_num("Self-sufficiency margin on TBR_eff (optional)", float('nan'), 0.01, min_value=0.0, max_value=0.5,
                        help="If set, requires TBR_eff â‰¥ 1 + margin (after declared losses).")), include_fuelcycle),

                    # Economics overlay (v356.0) â€” optional component CAPEX proxy cap (diagnostic)
                    "cost_k_heating_cd": _maybe(float(_num(
                        "Heating/CD CAPEX factor (MUSD per MW launched)",
                        25.0,
                        1.0,
                        min_value=0.0,
                        help="Used only for the v356 component CAPEX proxy: capex_heating_cd = k * P_CD_launch_MW (fallback Paux).",
                    )), include_economics),
                    "cost_k_tritium_plant": _maybe(float(_num(
                        "Tritium plant CAPEX factor (MUSD per kg/day burn)",
                        40.0,
                        1.0,
                        min_value=0.0,
                        help="Used only for the v356 component CAPEX proxy: capex_tritium_plant = k * T_burn_kg_per_day.",
                    )), include_economics),
                    "CAPEX_max_proxy_MUSD": _maybe(float(_num(
                        "Max component CAPEX proxy (MUSD) (optional)",
                        float('nan'),
                        50.0,
                        min_value=0.0,
                        help="Optional hard feasibility cap on CAPEX_component_proxy_MUSD. Leave NaN to disable.",
                    )), include_economics),

                    # Current drive + NI closure + channel caps (v357.0)
                    "include_current_drive": bool(st.checkbox(
                        "Current drive & NI closure (compute P_cd)",
                        value=False,
                        help="Enables deterministic non-inductive closure: choose actuator, CD efficiency model, and target f_NI; SHAMS computes required launched P_cd (capped by Pcd_max_MW).",
                    )),
                    "include_cd_library_v357": bool(st.checkbox(
                        "CD channel library caps (v357.0)",
                        value=False,
                        disabled=False,
                        help="Adds explicit channel feasibility diagnostics and optional hard caps for LH accessibility, ECCD launcher power density, and NBI shine-through.",
                    )),

                    "f_noninductive_target": float(_num(
                        "Target non-inductive fraction f_NI,target (â€“)",
                        1.0,
                        0.02,
                        min_value=0.0,
                        max_value=1.2,
                        help="Target f_NI = f_bs + I_cd/Ip. SHAMS computes I_cd and launched P_cd to reach this target (capped).",
                    )),
                    "Pcd_max_MW": float(_num(
                        "Max launched CD power P_cd,max (MW)",
                        200.0,
                        10.0,
                        min_value=0.0,
                        help="Hard cap on launched current-drive power used in the NI closure.",
                    )),
                    "eta_cd_wallplug": float(_num(
                        "CD wall-plug efficiency Î·_cd,wall (0..1)",
                        0.35,
                        0.02,
                        min_value=0.05,
                        max_value=0.9,
                        help="Wall-plug efficiency used in plant electric ledger.",
                    )),
                    "gamma_cd_A_per_W": float(_num(
                        "CD efficiency Î³_cd (A/W) (legacy fixed model)",
                        0.05,
                        0.005,
                        min_value=1e-4,
                        max_value=0.2,
                        help="Used only when cd_model=fixed_gamma.",
                    )),
                    "cd_actuator": str(st.selectbox(
                        "CD actuator channel",
                        options=["ECCD", "LHCD", "NBI", "ICRF"],
                        index=0,
                        help="Actuator used for CD efficiency trends and v357 channel diagnostics.",
                    )),
                    "cd_model": str(st.selectbox(
                        "CD efficiency model",
                        options=["fixed_gamma", "actuator_scaling", "channel_library_v357"],
                        index=2,
                        help="Deterministic CD efficiency proxy model.",
                    )),

                    # LHCD knobs + optional bounds (caps are disabled by default via NaN)
                    "lhcd_n_parallel": float(_num(
                        "LHCD nâˆ¥ (â€“)",
                        1.8,
                        0.05,
                        min_value=1.0,
                        max_value=4.0,
                        help="Used only when cd_actuator=LHCD and cd_model=channel_library_v357.",
                    )),
                    "lhcd_n_parallel_min": float(_num(
                        "LHCD nâˆ¥ min (optional)",
                        float('nan'),
                        0.05,
                        min_value=0.5,
                        help="Optional hard constraint lower bound on nâˆ¥. Leave NaN to disable.",
                    )),
                    "lhcd_n_parallel_max": float(_num(
                        "LHCD nâˆ¥ max (optional)",
                        float('nan'),
                        0.05,
                        min_value=0.5,
                        help="Optional hard constraint upper bound on nâˆ¥. Leave NaN to disable.",
                    )),

                    # ECCD knobs + optional launcher power-density cap
                    "eccd_launcher_area_m2": float(_num(
                        "ECCD launcher area A (mÂ²)",
                        2.0,
                        0.1,
                        min_value=0.1,
                        help="Used to compute launcher power density P_cd/A for v357 cap checks.",
                    )),
                    "eccd_launch_factor": float(_num(
                        "ECCD launch factor (â€“)",
                        1.0,
                        0.05,
                        min_value=0.2,
                        max_value=2.0,
                        help="Captures qualitative steering/optics effects as a declared multiplier on Î³_cd for the v357 model.",
                    )),
                    "eccd_launcher_power_density_max_MW_m2": float(_num(
                        "ECCD launcher power density max (MW/mÂ²) (optional)",
                        float('nan'),
                        1.0,
                        min_value=0.0,
                        help="Optional hard constraint. Leave NaN to disable.",
                    )),

                    # NBI knobs + optional shine-through cap
                    "nbi_beam_energy_keV": float(_num(
                        "NBI beam energy (keV)",
                        500.0,
                        25.0,
                        min_value=50.0,
                        max_value=5000.0,
                        help="Used only when cd_actuator=NBI in the v357 model (trend scaling + shine-through proxy).",
                    )),
                    "nbi_shinethrough_frac_max": float(_num(
                        "NBI shine-through max (fraction) (optional)",
                        float('nan'),
                        0.01,
                        min_value=0.0,
                        max_value=0.5,
                        help="Optional hard constraint on shine-through fraction proxy. Leave NaN to disable.",
                    )),

                    # Net power / electrical balance
                    "P_net_min_MW": _maybe(float(_num("Minimum net electric power (MW)", preset["P_net_min_MW"], 10.0, min_value=-1e6)), include_net_power),

                    # ---------------------------------------------------------
                    # Plant power ledger caps (v361.0 actuator authority hook)
                    # ---------------------------------------------------------
                    "f_recirc_max": float(_num(
                        "Max recirculating fraction f_recirc (optional)",
                        float('nan'),
                        0.02,
                        min_value=0.0,
                        max_value=1.0,
                        help="Optional cap on recirculating fraction Precirc/Pe_gross. Leave NaN to disable.",
                    )),
                    "P_pf_avg_max_MW": float(_num(
                        "Max average PF electric draw (MW) (optional)",
                        float('nan'),
                        10.0,
                        min_value=0.0,
                        help="Optional cap on average PF electric draw proxy (pf_E_pulse_MJ/(t_burn+t_dwell)). Leave NaN to disable.",
                    )),
                    "P_aux_max_MW": float(_num(
                        "Max aux+CD wallplug electric draw (MW) (optional)",
                        float('nan'),
                        10.0,
                        min_value=0.0,
                        help="Optional cap on auxiliary+CD wallplug electric draw proxy. Leave NaN to disable.",
                    )),
                    "P_supply_peak_max_MW": float(_num(
                        "Max peak power-supply draw (MW) (optional)",
                        float('nan'),
                        10.0,
                        min_value=0.0,
                        help="Optional cap on P_supply_peak_MW = max(PF_peak, Aux/CD_wallplug, VS_control, RWM_control). Leave NaN to disable.",
                    )),
                    "P_cryo_max_MW": float(_num(
                        "Max cryo wallplug electric draw (MW) (optional)",
                        float('nan'),
                        5.0,
                        min_value=0.0,
                        help="Optional cap on cryoplant wallplug electric draw proxy. Leave NaN to disable.",
                    )),

                    # ---------------------------------------------------------
                    # Control & stability authority (v298.0) â€” optional caps
                    # ---------------------------------------------------------
                    "include_control_contracts": bool(st.checkbox(
                        "Enable control contracts (deterministic envelopes)",
                        value=False,
                        help="When enabled, SHAMS computes VS/PF/RWM control requirements and checks optional caps (no physics mutation).",
                    )),
                    "cs_V_loop_max_V": float(_num(
                        "Max CS loop voltage during ramp V_loop,max (V) (optional)",
                        float("nan"),
                        50.0,
                        min_value=0.0,
                        help="Optional cap on CS loop voltage proxy during ramp. Leave NaN to disable.",
                    )),
                    "vs_bandwidth_max_Hz": float(_num(
                        "Max VS control bandwidth (Hz) (optional)",
                        float("nan"),
                        1.0,
                        min_value=0.0,
                        help="Optional cap on VS bandwidth requirement. Leave NaN to disable.",
                    )),
                    "vs_control_power_max_MW": float(_num(
                        "Max VS control power (MW) (optional)",
                        float("nan"),
                        1.0,
                        min_value=0.0,
                        help="Optional cap on VS control power requirement. Leave NaN to disable.",
                    )),
                    "pf_I_peak_max_MA": float(_num(
                        "Max PF peak current (MA) (optional)",
                        float("nan"),
                        0.5,
                        min_value=0.0,
                        help="Optional cap on PF peak current requirement from envelope contract. Leave NaN to disable.",
                    )),
                    "pf_dIdt_max_MA_s": float(_num(
                        "Max PF dI/dt (MA/s) (optional)",
                        float("nan"),
                        0.5,
                        min_value=0.0,
                        help="Optional cap on PF ramp-rate requirement. Leave NaN to disable.",
                    )),
                    "pf_V_peak_max_V": float(_num(
                        "Max PF peak voltage (V) (optional)",
                        float("nan"),
                        100.0,
                        min_value=0.0,
                        help="Optional cap on PF peak voltage requirement. Leave NaN to disable.",
                    )),
                    "pf_P_peak_max_MW": float(_num(
                        "Max PF peak power (MW) (optional)",
                        float("nan"),
                        5.0,
                        min_value=0.0,
                        help="Optional cap on PF peak electrical power requirement. Leave NaN to disable.",
                    )),
                    "pf_E_pulse_max_MJ": float(_num(
                        "Max PF pulse energy (MJ) (optional)",
                        float("nan"),
                        10.0,
                        min_value=0.0,
                        help="Optional cap on PF pulse energy proxy from envelope contract. Leave NaN to disable.",
                    )),
                    "include_rwm_screening": bool(st.checkbox(
                        "Enable RWM screening (optional)",
                        value=False,
                        help="If enabled, evaluates an RWM screening proxy and checks bandwidth/power against caps.",
                    )),
                    "rwm_bandwidth_max_Hz": float(_num(
                        "Max RWM bandwidth (Hz) (optional)",
                        float("nan"),
                        1.0,
                        min_value=0.0,
                        help="Optional cap on RWM required bandwidth; defaults to VS cap if NaN in the evaluator. Leave NaN to disable.",
                    )),
                    "rwm_control_power_max_MW": float(_num(
                        "Max RWM control power (MW) (optional)",
                        float("nan"),
                        1.0,
                        min_value=0.0,
                        help="Optional cap on RWM required control power; defaults to VS cap if NaN in the evaluator. Leave NaN to disable.",
                    )),

                    # propagate UI choices to output for check logic
                    "_warn_frac_max": float(warn_fracs["max"]),
                    "_warn_frac_min": float(warn_fracs["min"]),
                    "_subsystem_enabled": {
                        "build": bool(include_build),
                        "magnets": bool(include_magnets),
                        "divertor": bool(include_divertor),
                        "neutronics": bool(include_neutronics),
                        "net_power": bool(include_net_power),
                    },
                }

                # (Button moved outside this expander.)

            
        # Evaluate button is intentionally *outside* the optional engineering section so
        # users don't have to expand engineering knobs just to run Point Designer.
        run_btn = st.button("Evaluate Point", type="primary", use_container_width=True)
        # Quick bridge: trigger Systems Mode precheck using the current Point inputs.
        if st.button("Run Systems Precheck (in Systems Mode)", use_container_width=True, key="pd_to_systems_precheck"):
            # Schedule the action for Systems Mode and suggest the user switch tabs.
            st.session_state["_sys_action"] = "precheck"
            st.session_state["_pending_workflow_step"] = "Diagnose"
            st.success("Scheduled Systems Precheck. Switch to the **Systems Mode** tab to view the report.")
        
        
        # --- Execute: Point Designer evaluation (frozen truth) ---
        if run_btn:
            import time as _time
            # Acquire global run lock (UX/gov only; does not affect truth).
            _owner_tok = str(st.session_state.get("_shams_owner_token") or "PointDesigner")
            _task_label = "Point Designer: Evaluate Point"
            _ok_lock = bool(_shams_runlock.acquire(_task_label, _owner_tok, app_start_ts=st.session_state.get("_shams_app_start_ts")))
            if not _ok_lock:
                _locked, _task, _started, _is_owner = _shams_runlock.status(_owner_tok, app_start_ts=st.session_state.get("_shams_app_start_ts"))
                if _locked:
                    st.warning(f"Run lock busy: {_task or 'unknown task'} (another run is in progress).")
                else:
                    st.warning("Run lock busy (another run is in progress).")
            else:
                try:
                    with st.spinner("Evaluating frozen 0-D pointâ€¦"):
                        # Defensive de-dup of fields that are passed explicitly below.
                        _ck = dict(clean_knobs) if isinstance(clean_knobs, dict) else {}
                        for _k in ("Tcoil_K", "magnet_technology", "Bt_T", "R0_m", "a_m", "kappa", "delta", "Ip_MA", "fG", "Paux_MW", "Ti_keV"):
                            _ck.pop(_k, None)
        
                        base = make_point_inputs(
                            R0_m=float(R0), a_m=float(a), kappa=float(kappa), delta=float(delta), Bt_T=float(B0),
                            magnet_technology=str(tech),
                            Tcoil_K=float(Tcoil),
                            Ip_MA=float(0.5*(Ip_min+Ip_max)),
                            Ti_keV=float(Ti),
                            fG=float(0.5*(fG_min+fG_max)),
                            t_shield_m=float(tshield),
                            Paux_MW=float(Paux),
                            Ti_over_Te=float(Ti_over_Te),
                            q95_enforcement=str(st.session_state.get("q95_enforcement","hard")),
                            greenwald_enforcement=str(st.session_state.get("greenwald_enforcement","hard")),
                            tech_tier=str(st.session_state.get("tech_tier","TRL7")),
                            confinement_scaling=confinement_scaling,
                            zeff=float(Zeff),
                            dilution_fuel=float(dilution_fuel),
                            f_rad_core=float(f_rad_core),
                            include_radiation=bool(include_radiation),
                            radiation_model=radiation_model,
                            radiation_db=radiation_db,
                            impurity_species=impurity_species,
                            impurity_frac=float(impurity_frac),
                            include_synchrotron=bool(include_synchrotron),
                            impurity_contract_species=impurity_contract_species,
                            impurity_contract_f_z=float(impurity_contract_f_z),
                            impurity_partition_core=float(impurity_partition_core),
                            impurity_partition_edge=float(impurity_partition_edge),
                            impurity_partition_sol=float(impurity_partition_sol),
                            impurity_partition_div=float(impurity_partition_div),
                            include_sol_radiation_control=bool(include_sol_radiation_control),
                            q_div_target_MW_m2=float(q_div_target_MW_m2),
                            T_sol_keV=float(T_sol_keV),
                            f_V_sol_div=float(f_V_sol_div),
                            detachment_fz_max=float(detachment_fz_max),
                            include_edge_core_coupled_exhaust=bool(include_edge_core_coupled_exhaust and include_sol_radiation_control),
                            edge_core_coupling_chi_core=float(edge_core_coupling_chi_core),
                            f_rad_core_edge_core_max=float(f_rad_core_edge_core_max),
                            confinement_model=str(confinement_scaling).lower(),  # back-compat
                            include_transport_contracts_v371=bool(include_transport_contracts_v371),
                            H_required_max_optimistic=float(H_required_max_optimistic) if bool(include_transport_contracts_v371) else float("nan"),
                            H_required_max_robust=float(H_required_max_robust) if bool(include_transport_contracts_v371) else float("nan"),
                            include_neutronics_materials_coupling_v372=bool(include_neutronics_materials_coupling_v372),
                            nm_material_class_v372=str(nm_material_class_v372) if bool(include_neutronics_materials_coupling_v372) else str(getattr(_base_pd, "nm_material_class_v372", "RAFM")),
                            nm_spectrum_class_v372=str(nm_spectrum_class_v372) if bool(include_neutronics_materials_coupling_v372) else str(getattr(_base_pd, "nm_spectrum_class_v372", "nominal")),
                            nm_T_oper_C_v372=float(nm_T_oper_C_v372) if bool(include_neutronics_materials_coupling_v372) else float("nan"),
                            dpa_rate_eff_max_v372=float(dpa_rate_eff_max_v372) if bool(include_neutronics_materials_coupling_v372) else float("nan"),
                            damage_margin_min_v372=float(damage_margin_min_v372) if bool(include_neutronics_materials_coupling_v372) else float("nan"),
                            profile_model=profile_model,
                            profile_peaking_ne=float(profile_peaking_ne),
                            profile_peaking_T=float(profile_peaking_T),
                            profile_mode=bool(profile_mode),
                            profile_alpha_T=float(profile_alpha_T),
                            profile_alpha_n=float(profile_alpha_n),
                            profile_shear_shape=float(profile_shear_shape),
                            pedestal_enabled=bool(pedestal_enabled),
                            pedestal_width_a=float(pedestal_width_a),
                            bootstrap_model=bootstrap_model,
                            include_bootstrap_pressure_selfconsistency=bool(include_bootstrap_pressure_selfconsistency),
                            f_bootstrap_consistency_abs_max=float(f_bootstrap_consistency_abs_max),
                            fuel_mode=fuel_mode,
                            include_secondary_DT=bool(include_secondary_DT),
                            tritium_retention=tritium_retention,
                            tau_T_loss_s=float(tau_T_loss_s),
                            alpha_loss_frac=float(alpha_loss_frac),
                            alpha_loss_model=alpha_loss_model,
                            alpha_prompt_loss_k=float(alpha_prompt_loss_k),
                            alpha_partition_model=alpha_partition_model,
                            alpha_partition_k=float(alpha_partition_k),
                            ash_dilution_mode=ash_dilution_mode,
                            f_He_ash=float(f_He_ash),
                            include_alpha_loss=bool(include_alpha_loss),
                            include_hmode_physics=bool(include_hmode_physics),
                            require_Hmode=bool(require_Hmode),
                            PLH_margin=float(PLH_margin),
                            use_lambda_q=bool(use_lambda_q),
                            cd_enable=bool(cd_enable),
                            cd_method=str(cd_method),
                            cd_fraction_of_Paux=float(cd_fraction_of_Paux),
                            f_NI_min=float(f_NI_min),
                            disruption_risk_max=float(disruption_risk_max),
                            include_availability_replacement_v359=bool(locals().get("include_availability_replacement_v359", False)),
                            planned_outage_base=float(locals().get("planned_outage_base", 0.05)),
                            unplanned_outage_base=float(locals().get("unplanned_outage_base", 0.05)),
                            replacement_rate_per_year=float(locals().get("replacement_rate_per_year", 0.0)),
                            include_maintenance_scheduling_v368=bool(locals().get("include_maintenance_scheduling_v368", False)),
                            maint_capacity_factor=float(locals().get("maint_capacity_factor", 1.0)),
                            include_plant_economics_v360=bool(locals().get("include_plant_economics_v360", False)),
                            discount_rate=float(locals().get("discount_rate", 0.07)),
                            wallplug_eff=float(locals().get("wallplug_eff", 0.3)),
                            **_ck,
                        )
        
                        _ev = _dsg_evaluator(origin="Point Designer", cache_enabled=True, cache_max=4096)
                        _res = _ev.evaluate(base)
                        out = _res.out if (_res is not None and getattr(_res, "ok", True) and isinstance(getattr(_res, "out", None), dict)) else {}
        
                        # Cache under canonical keys (Telemetry/Constraints are read-only views).
                        st.session_state["pd_last_outputs"] = dict(out)
                        st.session_state["last_point_out"] = dict(out)
                        st.session_state["last_point_inp"] = base.to_dict() if hasattr(base, "to_dict") else {}
                        st.session_state["pd_last_run_ts"] = float(_time.time())
                        st.session_state["pd_last_inputs_hash"] = st.session_state.get("pd_current_inputs_hash")
                        st.session_state["pd_last_artifact"] = {"inputs": st.session_state["last_point_inp"], "outputs": dict(out), "constraints": []}
                        st.session_state["last_point_artifact"] = st.session_state["pd_last_artifact"]
        
                        st.success("Point evaluation complete. Open **ðŸ§ª Telemetry** for results and ledgers.")
                except Exception as e:
                    st.error(f"Point evaluation failed: {e}")
                finally:
                    _shams_runlock.release(_owner_tok)
        
        # Point Designer usability: show cache status + last-eval timestamp + stale-input warning.
        try:
            import hashlib, json as _json
            from datetime import datetime
            import time as _time
        
            _pd_inputs_fingerprint = {
                "R0_m": float(R0), "a_m": float(a), "kappa": float(kappa), "delta": float(delta), "Bt_T": float(B0),
                "Paux_MW": float(Paux), "Ti_keV": float(Ti), "Ti_over_Te": float(Ti_over_Te),
                "fuel_mode": str(fuel_mode), "Q_target": float(Q_target), "H98_target": float(H98_target),
                "Ip_min": float(Ip_min), "Ip_max": float(Ip_max), "fG_min": float(fG_min), "fG_max": float(fG_max),
                "tshield": float(tshield),
                "magnet_technology": str(tech),
                "Tcoil_K": float(Tcoil),
                "confidence": str(confidence),
                "subsystem_enabled": dict(clean_knobs.get("_subsystem_enabled", {})),
            }
            _pd_inputs_hash = hashlib.sha1(_json.dumps(_pd_inputs_fingerprint, sort_keys=True).encode("utf-8")).hexdigest()
            _last_hash = st.session_state.get("pd_last_inputs_hash")
            _last_ts = st.session_state.get("pd_last_run_ts")
        
            if _last_ts:
                st.caption(f"Last evaluation: {datetime.fromtimestamp(float(_last_ts)).strftime('%Y-%m-%d %H:%M:%S')}")
            if (not run_btn) and ("pd_last_outputs" in st.session_state) and (_last_hash is not None) and (_pd_inputs_hash != _last_hash):
                st.warning("Inputs changed since last evaluation. Click **Evaluate Point** to refresh results.")
            # Keep current hash available to the run path below.
            st.session_state["pd_current_inputs_hash"] = _pd_inputs_hash
        except Exception:
            pass
        

        with tab_tel:
            st.subheader("Telemetry")
            # Telemetry is read-only: if no cached Point Designer results exist, guide the user.
            if "pd_last_outputs" not in st.session_state:
                st.info("No Point Designer results yet. Open **ðŸ§­ Configure** and click **Evaluate Point**, then return here.")
                st.caption("Telemetry is read-only; nothing will execute here until a cached Point evaluation exists.")
            else:
                # Verdict-first executive header (PASS/FAIL) before any tables.
                # IMPORTANT: derive from the cached *outputs* (pd_last_outputs) so that
                # Mission Snapshot / Plot Deck / Ledgers cannot disagree.
                _pd_art = st.session_state.get("pd_last_artifact") or {}
                _pd_out0 = st.session_state.get("pd_last_outputs") if isinstance(st.session_state.get("pd_last_outputs"), dict) else None
                _rs = {}
                try:
                    if _pd_out0 is not None:
                        _rs = _compute_run_summary_from_out(_pd_out0)
                        # Keep artifact in sync for exports and downstream panels.
                        if isinstance(_pd_art, dict):
                            _pd_art["run_summary"] = _rs
                            st.session_state["pd_last_artifact"] = _pd_art
                    else:
                        _rs = (_pd_art.get("run_summary") or {}) if isinstance(_pd_art, dict) else {}
                except Exception:
                    _rs = (_pd_art.get("run_summary") or {}) if isinstance(_pd_art, dict) else {}



                _tight = _rs.get("tightest_hard_constraints", []) if isinstance(_rs, dict) else []
                _tight = _tight or []

                # Policy-aware verdict: FAIL only if *blocking* constraints fail.
                _fb = []
                _fd = []
                # Detect whether we have a real evaluation payload (avoid PASS+NaN from empty dict).
                _outputs_present = bool(isinstance(_pd_out0, dict) and _pd_out0 and any(k in _pd_out0 for k in ("Pin_MW","P_fus_MW","P_net_e_MW","Ploss_MW")))
                try:
                    if isinstance(_pd_out0, dict):
                        _fb = list(_pd_out0.get("failed_blocking") or [])
                        _fd = list(_pd_out0.get("failed_diagnostic") or [])
                except Exception:
                    _fb, _fd = [], []

                def _find_entry(name: str):
                    for t in _tight:
                        if isinstance(t, dict) and str(t.get("name", "")) == str(name):
                            return t
                    return None

                if _fb:
                    _verdict = " FAIL"
                    _dom = str(_fb[0])
                    _ent = _find_entry(_dom) or {}
                    _b0 = f"Dominant constraint: {_dom}"
                    try:
                        _b1 = f"Tightest margin: {float(_ent.get('margin_frac', float('nan'))):.3g}"
                    except Exception:
                        _b1 = f"Tightest margin: {_ent.get('margin_frac','?')}"
                    _b2 = f"Power closure (MW): {_rs.get('power_closure_MW', 'n/a')}"
                elif _fd:
                    _verdict = "âš ï¸ PASS (diagnostics)"
                    _dom = str(_fd[0])
                    _ent = _find_entry(_dom) or {}
                    _b0 = f"Diagnostic exceedance: {_dom}"
                    try:
                        _b1 = f"Margin (diagnostic): {float(_ent.get('margin_frac', float('nan'))):.3g}"
                    except Exception:
                        _b1 = f"Margin (diagnostic): {_ent.get('margin_frac','?')}"
                    _b2 = f"Power closure (MW): {_rs.get('power_closure_MW', 'n/a')}"
                else:
                    if not _outputs_present:
                        _verdict = "âš ï¸ PASS + DIAG"
                        _b0 = "No evaluation outputs loaded"
                        _b1 = "Click Evaluate Point after changing intent/machine type/policy"
                        _b2 = "Net electric (MW): n/a"
                    else:
                        _verdict = " PASS"
                    if _tight and isinstance(_tight[0], dict):
                        _b0 = f"Tightest hard constraint: {_tight[0].get('name','(none)')}"
                        try:
                            _b1 = f"Margin (tightest): {float(_tight[0].get('margin_frac', float('nan'))):.3g}"
                        except Exception:
                            _b1 = f"Margin (tightest): {_tight[0].get('margin_frac','?')}"
                    else:
                        _b0, _b1 = "Tightest hard constraint: (none)", "Margin (tightest): n/a"
                    _headline = _rs.get("headline", {}) if isinstance(_rs, dict) else {}
                    _pnet = float(_headline.get("P_net_e_MW", float("nan"))) if isinstance(_headline, dict) else float("nan")
                    _b2 = f"Net electric (MW): {(_pnet if np.isfinite(_pnet) else 'n/a')}"

                _bullets = []



                for _x in (_b0, _b1, _b2):
                    if _x is None:
                        continue
                    _s = str(_x).strip()
                    if _s:
                        _bullets.append(html.escape(_s))
                _bullets_html = "\n".join([f"<li>{_s}</li>" for _s in _bullets]) or "<li>(no summary)</li>"

                st.markdown(
                    f"""<div style="padding:14px;border-radius:14px;border:1px solid #ddd;">
                    <div style="font-size:20px;font-weight:700;margin-bottom:6px;">{_verdict}</div>
                    <ul style="margin:0;padding-left:18px;line-height:1.6;">
                      {_bullets_html}
                    </ul>
                    </div>""",
                    unsafe_allow_html=True,
                )

                # v328.0: Magnet Technology Authority panel
                try:
                    _render_magnet_authority_panel(_pd_out0 or {})
                except Exception:
                    pass

                # Telemetry Deck navigation (reduces scrolling)
                _pd_tel_views = [
                    "âš¡ Mission Snapshot",
                    "ðŸ“ˆ Plot Deck",
                    "ðŸŽ¯ Dominance & Closures",
                    "ðŸ›° Control Contracts",
                    "ðŸ“š Ledgers",
                    " Sensitivity Lab",
                    "ðŸ§¾ Chronicle & Export",
                ]
                _pd_tel_view = st.radio(
                    "Telemetry deck",
                    _pd_tel_views,
                    horizontal=True,
                    label_visibility="collapsed",
                    key="pd_tel_view",
                )

                # Render live if button pressed, otherwise render cached results.
                if ("pd_last_outputs" in st.session_state):

                    # If we're just re-rendering after a Streamlit rerun (e.g., a download button),
                    # do NOT re-run the solver. Use cached outputs.
                    _use_cached = True  # Telemetry tab never re-runs the solver; always render cached outputs

                    # Activity log: user explicitly clicked Evaluate Point
                    if bool(run_btn) and (not _use_cached):
                        try:
                            _alog(
                                "ðŸ§­ Point Designer",
                                "EvaluatePoint",
                                {
                                    "inputs_hash": str(st.session_state.get("pd_current_inputs_hash", "")),
                                    "targets": {"H98": float(H98_target), "Q_DT_eqv": float(Q_target)},
                                    "bounds": {"Ip_MA": [float(Ip_min), float(Ip_max)], "fG": [float(fG_min), float(fG_max)]},
                                },
                            )
                        except Exception:
                            pass

                    if _use_cached:
                        # Cached render path (e.g., after download_button rerun)
                        try:
                            out = st.session_state.get("pd_last_outputs")
                        except Exception:
                            out = None
                        try:
                            artifact = st.session_state.get("pd_last_artifact")
                        except Exception:
                            artifact = None
                        try:
                            inputs_dict = (artifact or {}).get("inputs", {}) if isinstance(artifact, dict) else {}
                        except Exception:
                            inputs_dict = {}
                        try:
                            # mimic the original `base.__dict__` access pattern downstream
                            class _BaseObj: pass
                            base = _BaseObj()
                            for _k, _v in dict(inputs_dict).items():
                                setattr(base, _k, _v)
                        except Exception:
                            base = None
                        try:
                            log_lines = []
                        except Exception:
                            pass

                    # Solver log builder (used for both live runs and cached re-renders)
                    log_lines: List[str] = [] if not _use_cached else list(st.session_state.get("pd_last_log_lines", []) or [])

                    def _log(line: str) -> None:
                        """Append a single line to the expandable solver log."""
                        try:
                            log_lines.append(str(line))
                        except Exception:
                            pass

                    if _use_cached:
                        _log("Point Designer: cached render (solver not re-run)")
                    else:
                        _log("Point Designer solver log")

                    # NOTE: In Telemetry, the Configure tab may not have executed (Streamlit tabs are lazy).
                    # Only log what we can derive from cached artifacts.
                    try:
                        _R0 = float(getattr(base, 'R0_m')) if base is not None and hasattr(base,'R0_m') else float('nan')
                        _a  = float(getattr(base, 'a_m'))  if base is not None and hasattr(base,'a_m')  else float('nan')
                        _k  = float(getattr(base, 'kappa')) if base is not None and hasattr(base,'kappa') else float('nan')
                        _B  = float(getattr(base, 'Bt_T')) if base is not None and hasattr(base,'Bt_T') else float('nan')
                        _Ip = float(getattr(base, 'Ip_MA')) if base is not None and hasattr(base,'Ip_MA') else float('nan')
                        _Ti = float(getattr(base, 'Ti_keV')) if base is not None and hasattr(base,'Ti_keV') else float('nan')
                        _fG = float(getattr(base, 'fG')) if base is not None and hasattr(base,'fG') else float('nan')
                        _Paux = float(getattr(base, 'Paux_MW')) if base is not None and hasattr(base,'Paux_MW') else float('nan')
                        _log(f"Cached machine: R0={_R0:.6g} m, a={_a:.6g} m, kappa={_k:.6g}, Bt={_B:.6g} T; Ip={_Ip:.6g} MA; Ti={_Ti:.6g} keV; fG={_fG:.6g}; Paux={_Paux:.6g} MW")
                    except Exception:
                        pass

                    # Defensive de-dup: some UI knobs are passed explicitly below and may also live
                    # in clean_knobs depending on preset + sync pathways. Passing duplicates causes
                    # "got multiple values for keyword" errors.
                    if isinstance(clean_knobs, dict):
                        clean_knobs = dict(clean_knobs)
                        for _k in (
                            "Tcoil_K", "magnet_technology", "Bt_T", "R0_m", "a_m", "kappa", "delta",
                            # v383 economics knobs that may be injected via presets
                            "fixed_charge_rate", "capacity_factor", "capacity_factor_used",
                            "capex_structured_max_MUSD", "opex_structured_max_MUSD_per_y", "lcoe_lite_max_USD_per_MWh",
                            # v384 materials/lifetime knobs that may be injected via presets
                            "fw_capex_fraction_of_blanket", "blanket_capex_fraction_of_blanket",
                            "divertor_capex_fraction_of_total", "base_capacity_factor", "capacity_factor_max",
                            "fw_downtime_days", "blanket_downtime_days", "divertor_downtime_days", "magnet_downtime_days",
                            "divertor_life_ref_yr", "divertor_q_ref_MW_m2", "divertor_q_exp",
                            "magnet_life_ref_yr", "magnet_margin_ref", "magnet_margin_exp",
                            "fw_lifetime_min_yr_v384", "blanket_lifetime_min_yr_v384", "divertor_lifetime_min_yr_v384",
                            "magnet_lifetime_min_yr_v384", "replacement_cost_max_MUSD_per_y_v384", "capacity_factor_min_v384",
                        ):
                            if _k in clean_knobs:
                                clean_knobs.pop(_k, None)

                    base = make_point_inputs(
                        R0_m=R0, a_m=a, kappa=kappa, delta=delta, Bt_T=B0,
                        magnet_technology=str(tech),
                        Tcoil_K=float(Tcoil),
                        Ip_MA=0.5*(Ip_min+Ip_max),
                        Ti_keV=Ti, fG=0.8,
                        t_shield_m=tshield,
                        Paux_MW=Paux,
                        Ti_over_Te=Ti_over_Te,
                        q95_enforcement=str(st.session_state.get('q95_enforcement','hard')),
                        greenwald_enforcement=str(st.session_state.get('greenwald_enforcement','hard')),
                        tech_tier=str(st.session_state.get('tech_tier','TRL7')),
                        confinement_scaling=confinement_scaling,
                        zeff=Zeff,
                        dilution_fuel=dilution_fuel,
                        f_rad_core=f_rad_core,
                        include_radiation=include_radiation,
                        radiation_model=radiation_model,
                        radiation_db=radiation_db,
                        impurity_species=impurity_species,
                        impurity_frac=impurity_frac,
                        include_synchrotron=include_synchrotron,
                        impurity_contract_species=impurity_contract_species,
                        impurity_contract_f_z=float(impurity_contract_f_z),
                        impurity_partition_core=float(impurity_partition_core),
                        impurity_partition_edge=float(impurity_partition_edge),
                        impurity_partition_sol=float(impurity_partition_sol),
                        impurity_partition_div=float(impurity_partition_div),
                        include_sol_radiation_control=bool(include_sol_radiation_control),
                        q_div_target_MW_m2=float(q_div_target_MW_m2),
                        T_sol_keV=float(T_sol_keV),
                        f_V_sol_div=float(f_V_sol_div),
                        detachment_fz_max=float(detachment_fz_max),
                        include_edge_core_coupled_exhaust=bool(include_edge_core_coupled_exhaust and include_sol_radiation_control),
                        edge_core_coupling_chi_core=float(edge_core_coupling_chi_core),
                        f_rad_core_edge_core_max=float(f_rad_core_edge_core_max),
                        confinement_model=str(confinement_scaling).lower(),  # back-compat
                        include_transport_contracts_v371=bool(include_transport_contracts_v371),
                        H_required_max_optimistic=float(H_required_max_optimistic) if bool(include_transport_contracts_v371) else float('nan'),
                        H_required_max_robust=float(H_required_max_robust) if bool(include_transport_contracts_v371) else float('nan'),
                        include_neutronics_materials_coupling_v372=bool(include_neutronics_materials_coupling_v372),
                        nm_material_class_v372=str(nm_material_class_v372) if bool(include_neutronics_materials_coupling_v372) else str(getattr(_base_pd, 'nm_material_class_v372', 'RAFM')),
                        nm_spectrum_class_v372=str(nm_spectrum_class_v372) if bool(include_neutronics_materials_coupling_v372) else str(getattr(_base_pd, 'nm_spectrum_class_v372', 'nominal')),
                        nm_T_oper_C_v372=float(nm_T_oper_C_v372) if bool(include_neutronics_materials_coupling_v372) else float('nan'),
                        dpa_rate_eff_max_v372=float(dpa_rate_eff_max_v372) if bool(include_neutronics_materials_coupling_v372) else float('nan'),
                        damage_margin_min_v372=float(damage_margin_min_v372) if bool(include_neutronics_materials_coupling_v372) else float('nan'),
                        profile_model=profile_model,
                        profile_peaking_ne=profile_peaking_ne,
                        profile_peaking_T=profile_peaking_T,
                        profile_mode=bool(profile_mode),
                        profile_alpha_T=float(profile_alpha_T),
                        profile_alpha_n=float(profile_alpha_n),
                        profile_shear_shape=float(profile_shear_shape),
                        pedestal_enabled=bool(pedestal_enabled),
                        pedestal_width_a=float(pedestal_width_a),
                        bootstrap_model=bootstrap_model,
                        include_bootstrap_pressure_selfconsistency=bool(include_bootstrap_pressure_selfconsistency),
                        f_bootstrap_consistency_abs_max=float(f_bootstrap_consistency_abs_max),
                        fuel_mode=fuel_mode,
                        include_secondary_DT=include_secondary_DT,
                        tritium_retention=tritium_retention,
                        tau_T_loss_s=tau_T_loss_s,
                        alpha_loss_frac=alpha_loss_frac,
                        alpha_loss_model=alpha_loss_model,
                        alpha_prompt_loss_k=alpha_prompt_loss_k,
                        alpha_partition_model=alpha_partition_model,
                        alpha_partition_k=alpha_partition_k,
                        ash_dilution_mode=ash_dilution_mode,
                        f_He_ash=f_He_ash,
                        include_alpha_loss=include_alpha_loss,
                        include_hmode_physics=include_hmode_physics,
                        require_Hmode=require_Hmode,
                        PLH_margin=PLH_margin,
                        use_lambda_q=use_lambda_q,
                        cd_enable=bool(cd_enable),
                        cd_method=str(cd_method),
                        cd_fraction_of_Paux=float(cd_fraction_of_Paux),
                        f_NI_min=float(f_NI_min),
                        disruption_risk_max=float(disruption_risk_max),
                                        include_availability_replacement_v359=bool(locals().get('include_availability_replacement_v359', False)),
                        planned_outage_base=float(locals().get('planned_outage_base', 0.05)),
                        heating_cd_replace_interval_y=float(locals().get('heating_cd_replace_interval_y', 8.0)),
                        heating_cd_replace_duration_days=float(locals().get('heating_cd_replace_duration_days', 30.0)),
                        tritium_plant_replace_interval_y=float(locals().get('tritium_plant_replace_interval_y', 10.0)),
                        tritium_plant_replace_duration_days=float(locals().get('tritium_plant_replace_duration_days', 30.0)),
                        availability_v359_min=float(locals().get('availability_v359_min', float('nan'))),
                        LCOE_max_USD_per_MWh=float(locals().get('LCOE_max_USD_per_MWh', float('nan'))),

                        include_maintenance_scheduling_v368=bool(locals().get('include_maintenance_scheduling_v368', False)),
                        maintenance_planning_horizon_yr=float(locals().get('maintenance_planning_horizon_yr', float('nan'))),
                        maintenance_bundle_policy=str(locals().get('maintenance_bundle_policy', 'independent')),
                        maintenance_bundle_overhead_days=float(locals().get('maintenance_bundle_overhead_days', 7.0)),
                        forced_outage_mode_v368=str(locals().get('forced_outage_mode_v368', 'max')),
                        outage_fraction_v368_max=float(locals().get('outage_fraction_v368_max', float('nan'))),
                        availability_v368_min=float(locals().get('availability_v368_min', float('nan'))),

                        include_economics_v360=bool(locals().get('include_economics_v360', False)),
                        opex_fixed_MUSD_per_y=float(locals().get('opex_fixed_MUSD_per_y', 0.0)),
                        tritium_processing_cost_USD_per_g=float(locals().get('tritium_processing_cost_USD_per_g', 0.05)),
                        cryo_wallplug_multiplier=float(locals().get('cryo_wallplug_multiplier', 250.0)),
                        OPEX_max_MUSD_per_y=float(locals().get('OPEX_max_MUSD_per_y', float('nan'))),

                        include_economics_v383=bool(locals().get('include_economics_v383', False)),
                        CAPEX_structured_max_MUSD=float(locals().get('CAPEX_structured_max_MUSD', float('nan'))),
                        OPEX_structured_max_MUSD_per_y=float(locals().get('OPEX_structured_max_MUSD_per_y', float('nan'))),
                        LCOE_lite_max_USD_per_MWh=float(locals().get('LCOE_lite_max_USD_per_MWh', float('nan'))),

                        include_cost_authority_v388=bool(locals().get('include_cost_authority_v388', False)),
                        CAPEX_industrial_max_MUSD=float(locals().get('CAPEX_industrial_max_MUSD', float('nan'))),
                        OPEX_industrial_max_MUSD_per_y=float(locals().get('OPEX_industrial_max_MUSD_per_y', float('nan'))),
                        LCOE_lite_v388_max_USD_per_MWh=float(locals().get('LCOE_lite_v388_max_USD_per_MWh', float('nan'))),

                        include_materials_lifetime_v384=bool(locals().get('include_materials_lifetime_v384', False)),
                        divertor_life_ref_yr=float(locals().get('divertor_life_ref_yr', 3.0)),
                        divertor_q_ref_MW_m2=float(locals().get('divertor_q_ref_MW_m2', 10.0)),
                        divertor_q_exp=float(locals().get('divertor_q_exp', 2.0)),
                        divertor_capex_fraction_of_total=float(locals().get('divertor_capex_fraction_of_total', 0.05)),
                        magnet_life_ref_yr=float(locals().get('magnet_life_ref_yr', 30.0)),
                        magnet_margin_ref=float(locals().get('magnet_margin_ref', 0.10)),
                        magnet_margin_exp=float(locals().get('magnet_margin_exp', 1.5)),
                        base_capacity_factor=float(locals().get('base_capacity_factor', 0.75)),
                        capacity_factor_max=float(locals().get('capacity_factor_max', 0.95)),
                        fw_downtime_days=float(locals().get('fw_downtime_days', 30.0)),
                        blanket_downtime_days=float(locals().get('blanket_downtime_days', 60.0)),
                        divertor_downtime_days=float(locals().get('divertor_downtime_days', 20.0)),
                        magnet_downtime_days=float(locals().get('magnet_downtime_days', 120.0)),
                        fw_capex_fraction_of_blanket=float(locals().get('fw_capex_fraction_of_blanket', 0.20)),
                        blanket_capex_fraction_of_blanket=float(locals().get('blanket_capex_fraction_of_blanket', 1.00)),
                        fw_lifetime_min_yr_v384=float(locals().get('fw_lifetime_min_yr_v384', float('nan'))),
                        blanket_lifetime_min_yr_v384=float(locals().get('blanket_lifetime_min_yr_v384', float('nan'))),
                        divertor_lifetime_min_yr_v384=float(locals().get('divertor_lifetime_min_yr_v384', float('nan'))),
                        magnet_lifetime_min_yr_v384=float(locals().get('magnet_lifetime_min_yr_v384', float('nan'))),
                        replacement_cost_max_MUSD_per_y_v384=float(locals().get('replacement_cost_max_MUSD_per_y_v384', float('nan'))),
                        capacity_factor_min_v384=float(locals().get('capacity_factor_min_v384', float('nan'))),

                        **clean_knobs,
                    )

                    # UI-only guardrails: warn on obviously unrealistic knobs (does not block).
                    _warn_unrealistic_point_inputs(base, context="ðŸ§­ Point Designer")
                    if do_opt:
                        _log(f"Optimization enabled: objective={opt_objective}, iters={opt_iters}, seed={opt_seed}")
                        var_bounds = {"Ip_MA": (Ip_min, Ip_max), "fG": (fG_min, fG_max), "Paux_MW": (0.0, max(Paux, 1e-6)*2.0)}
                        best_inp, best_out = optimize_design(
                            base,
                            objective=opt_objective,
                            variables=var_bounds,
                            n_iter=opt_iters,
                            seed=opt_seed,
                        )
                        base = best_inp
                        _log(f"Optimization chose: Ip={best_inp.Ip_MA:.4g} MA, fG={best_inp.fG:.4g}, Paux={best_inp.Paux_MW:.4g} MW")
                        _log(f"Optimized outputs: Bpeak={best_out.get('B_peak_T', float('nan')):.4g} T, Pnet={best_out.get('P_e_net_MW', float('nan')):.4g} MW")
                    # Optional: show solver progress so the user can "see physics happening".
                    if show_solver_live:
                        status = None
                        prog = None
                        chart = None
                        table = None
                        latest = None
                        if _pd_tel_view == "ðŸ§¾ Chronicle & Export":
                            with st.expander("ðŸ›°ï¸ Live Convergence", expanded=False):
                                status = st.empty()
                                prog = st.progress(0)
                                # Live convergence diagnostics
                                chart = st.empty()
                                table = st.empty()
                                latest = st.empty()
    
                        trace_rows = []
                        sol_inp, out, ok = None, {}, False

                        # Select solver iterator (legacy stream solver vs envelope solve)
                        if use_envelope:
                            tgt = {'Q_DT_eqv': Q_target, 'H98': H98_target}
                            sol_inp_env, out_env, ok_env, msg_env = solve_sparc_envelope(
                                base, tgt, vary=['Ip_MA','fG'],
                                bounds={'Ip_MA': (Ip_min, Ip_max), 'fG': (fG_min, fG_max)},
                                tol=tol, max_iter=40,
                            )
                            def _env_events():
                                yield {'event':'iter','it':0,'Ip_MA': sol_inp_env.Ip_MA, 'fG': sol_inp_env.fG, 'H98': out_env.get('H98', float('nan')), 'Q_DT_eqv': out_env.get('Q_DT_eqv', float('nan'))}
                                yield {'event':'done','sol': sol_inp_env, 'out': out_env, 'ok': ok_env, 'message': msg_env}
                            event_iter = _env_events()
                        else:
                            event_iter = solve_Ip_for_H98_with_Q_match_stream(
                            base=base,
                            target_H98=H98_target,
                            target_Q=Q_target,
                            Ip_min=Ip_min, Ip_max=Ip_max,
                            fG_min=fG_min, fG_max=fG_max,
                            tol=tol,
                            Paux_for_Q_MW=Paux_for_Q,
                        )
                        for ev in event_iter:
                            if ev.get("event") == "bracket":
                                okb = bool(ev.get("ok"))
                                try:
                                    _log(
                                        f"BRACKET: H98(Ip_lo={ev.get('Ip_lo'):.6g})={ev.get('H98_lo'):.6g}, H98(Ip_hi={ev.get('Ip_hi'):.6g})={ev.get('H98_hi'):.6g} -> {'OK' if okb else 'NO_BRACKET'}"
                                    )
                                except Exception:
                                    _log(f"BRACKET: ok={okb}")
                                if status is not None:
                                    status.info(
                                        f"Bracketing H98 target: H98(Ip_min={ev.get('Ip_lo'):.3g})={ev.get('H98_lo'):.3g}, "
                                        f"H98(Ip_max={ev.get('Ip_hi'):.3g})={ev.get('H98_hi'):.3g}  â†’  "
                                        f"{'OK' if okb else 'NO BRACKET'}"
                                    )
                            elif ev.get("event") == "iter":
                                try:
                                    _log(
                                        f"ITER {int(ev.get('iter', 0)):>3d}: Ip={ev.get('Ip_MA'):.8g} MA, fG={ev.get('fG'):.8g}, H98={ev.get('H98'):.8g}, Q={ev.get('Q'):.8g}, residual={ev.get('residual'):.8g}"
                                    )
                                except Exception:
                                    _log(f"ITER {ev.get('iter')}: {ev}")
                                trace_rows.append({
                                    "iter": ev.get("iter"),
                                    "Ip_MA": ev.get("Ip_MA"),
                                    "fG": ev.get("fG"),
                                    "H98": ev.get("H98"),
                                    "Q": ev.get("Q"),
                                    "residual": ev.get("residual"),
                                })
                                it = int(ev.get("iter", 0))
                                if prog is not None:
                                    prog.progress(min(1.0, (it + 1) / 80.0))
                                if latest is not None:
                                    latest.metric("Current guess Ip (MA)", f"{ev.get('Ip_MA', float('nan')):.4g}")
                                if trace_rows:
                                    df = pd.DataFrame(trace_rows)
                                    # Two quick plots: residual and key state variables
                                    if chart is not None:
                                        chart.line_chart(df.set_index("iter")[["residual"]])
                                    if table is not None:
                                        table.dataframe(df.tail(10), use_container_width=True)
                            elif ev.get("event") == "done":
                                sol_inp = ev.get("sol")
                                out = ev.get("out", {})
                                ok = True
                                try:
                                    _log(
                                        f"DONE: Ip={out.get('Ip_MA', float('nan')):.8g} MA, fG={out.get('fG', float('nan')):.8g}, H98={out.get('H98', float('nan')):.8g}, Q_DT_eqv={out.get('Q_DT_eqv', float('nan')):.8g}"
                                    )
                                except Exception:
                                    _log("DONE")
                                if bool(out.get("_solver_clamped")) or bool(out.get("_solver_clamped_Q")):
                                    if status is not None:
                                        status.warning("Solver returned a point by clamping to the nearest bound (target not achievable within bounds). See log/details below.")
                                else:
                                    if status is not None:
                                        status.success("Solver converged.")
                                # Live progress UI exists only when the Chronicle deck is active.
                                if prog is not None:
                                    prog.progress(1.0)
                                break
                            elif ev.get("event") == "fail":
                                reason = ev.get("reason", "solver_failed")
                                _log("FAIL EVENT: " + json.dumps(ev, sort_keys=True))
                                it_fail = ev.get("it", None)
                                mi_fail = ev.get("max_iter", None)
                                extra = ""
                                if it_fail is not None and mi_fail is not None:
                                    extra = f" (it={it_fail}/{mi_fail})"
                                if status is not None:
                                    status.error(f"Solver failed ({reason}){extra}. Try widening Ip/fG bounds or relaxing targets.")
                                ok = False
                                break
                    else:
                        sol_inp, out, ok = solve_Ip_for_H98_with_Q_match(
                            base=base,
                            target_H98=H98_target,
                            target_Q=Q_target,
                            Ip_min=Ip_min, Ip_max=Ip_max,
                            fG_min=fG_min, fG_max=fG_max,
                            tol=tol,
                            Paux_for_Q_MW=Paux_for_Q,
                        )
                        # Minimal log summary when running in non-stream mode.
                        if ok:
                            try:
                                _log(
                                    f"DONE: Ip={out.get('Ip_MA', float('nan')):.8g} MA, fG={out.get('fG', float('nan')):.8g}, H98={out.get('H98', float('nan')):.8g}, Q_DT_eqv={out.get('Q_DT_eqv', float('nan')):.8g}"
                                )
                            except Exception:
                                _log("DONE")
                        else:
                            _log("FAIL: solver_failed")

                    # Always show expandable log for this run.
                    solver_log_text = "\n".join(log_lines).strip() + "\n"
                    st.session_state.last_solver_log = solver_log_text
                    if _pd_tel_view == "ðŸ§¾ Chronicle & Export":
                        with st.expander("ðŸ§¾ Chronicle - Solver Log", expanded=False):
                            st.download_button(
                                "Download log",
                                data=solver_log_text,
                                file_name="point_designer_solver.log",
                                mime="text/plain",
                                use_container_width=True,
                            )
                            st.code(solver_log_text)
                    if not ok:
                        # Provide best-effort diagnostics if available (e.g., H98 at bounds)
                        msg = "Solver failed to converge for (Ip, fG) at the requested (H98, Q) targets."
                        try:
                            if isinstance(out, dict) and ("H98_at_Ip_min" in out or "H98_at_Ip_max" in out):
                                msg += f"  H98(Ip_min)={out.get('H98_at_Ip_min')}, H98(Ip_max)={out.get('H98_at_Ip_max')}"
                        except Exception:
                            pass
                        st.error(msg)
                        try:
                            _alog(
                                "ðŸ§­ Point Designer",
                                "EvaluatePointResult",
                                {
                                    "ok": False,
                                    "reason": "solver_failed",
                                    "inputs_hash": str(st.session_state.get("pd_current_inputs_hash", "")),
                                },
                            )
                        except Exception:
                            pass

                        # -----------------------------------------------------------------
                        # transparent (systems-code-inspired) feasibility frontier suggestion
                        # -----------------------------------------------------------------
                        with st.expander("Try to find nearest feasible point (frontier)", expanded=False):
                            st.markdown(
                                "If the solver cannot hit the requested (H98, Q) targets inside the bounds, "
                                "SHAMS can still search for the *nearest feasible* design within your (Ip,fG) bounds. "
                                "This does **not** change your inputs automatically; it only proposes a candidate."
                            )
                            if st.button("Search nearest feasible within bounds", key="pd_frontier_btn", use_container_width=True):
                                try:
                                    fr = find_nearest_feasible(
                                        base,
                                        levers={"Ip_MA": (Ip_min, Ip_max), "fG": (fG_min, fG_max)},
                                        targets={"H98": float(H98_target), "Q_DT_eqv": float(Q_target)},
                                        n_random=80,
                                        seed=0,
                                    )
                                    st.session_state["pd_frontier_last"] = fr.report
                                except Exception as e:
                                    st.session_state["pd_frontier_last"] = {"status": "error", "message": str(e)}

                            rep = st.session_state.get("pd_frontier_last")
                            if isinstance(rep, dict) and rep:
                                if rep.get("status") == "error":
                                    st.error(rep.get("message", "frontier error"))
                                else:
                                    cols = st.columns(3)
                                    cols[0].metric("Best Ip (MA)", f"{rep.get('best_levers', {}).get('Ip_MA', float('nan')):.4g}")
                                    cols[1].metric("Best fG", f"{rep.get('best_levers', {}).get('fG', float('nan')):.4g}")
                                    cols[2].metric("Feasible?", "YES" if rep.get("best_ok") else "NO")
                                    ach = rep.get("best_achieved", {}) or {}
                                    st.write("Best achieved targets at proposed point:")
                                    st.json(ach)
                                    st.caption("Tip: widen bounds or relax targets if the frontier is still infeasible.")
                    else:
                        # Attach UI-only meta for checks (not used by physics core)
                        try:
                            out['_warn_frac_max'] = float(clean_knobs.get('_warn_frac_max', 0.90))
                            out['_warn_frac_min'] = float(clean_knobs.get('_warn_frac_min', 1.10))
                            out['_subsystem_enabled'] = clean_knobs.get('_subsystem_enabled', {})
                        except Exception:
                            pass
                        st.session_state["last_point_out"] = out
                        # Persist cached Point Designer result across Streamlit reruns (downloads)
                        st.session_state['pd_last_outputs'] = out

                        # Mark cache freshness
                        try:
                            import time as _time
                            st.session_state["pd_last_run_ts"] = float(_time.time())
                            st.session_state["pd_last_inputs_hash"] = st.session_state.get("pd_current_inputs_hash")
                        except Exception:
                            pass

                        # Activity log: successful point evaluation (constraint-first summary)
                        try:
                            _failed_hard = []
                            try:
                                _failed_hard = [str(c.name) for c in (evaluate_constraints(out) or []) if str(getattr(c,'severity','soft'))=='hard' and (not bool(getattr(c,'passed', False)))]
                            except Exception:
                                _failed_hard = []

                            _cls = _classify_failed_constraints(_failed_hard)
                            _policy = _constraint_policy_snapshot()
                            _alog(
                                "ðŸ§­ Point Designer",
                                "EvaluatePointResult",
                                {
                                    "ok": True,
                                    "inputs_hash": str(st.session_state.get("pd_current_inputs_hash", "")),
                                    "design_intent": str(st.session_state.get("design_intent", "Power Reactor (net-electric)")),
                                    "constraint_policy": _policy,
                                    "failed_hard": _failed_hard,
                                    "failed_blocking": list(_cls.get('blocking', [])),
                                    "failed_diagnostic": list(_cls.get('diagnostic', [])),
                                    "failed_ignored": list(_cls.get('ignored', [])),
                                    "headline": {"H98": float(out.get("H98", float('nan'))), "Q_DT_eqv": float(out.get("Q_DT_eqv", float('nan')))},
        },
                            )
                        except Exception:
                            pass


                        # -----------------------------------------------------------------
                        # transparent (systems-code-inspired) canonical output artifact (SHAMS-native JSON)
                        # -----------------------------------------------------------------
                        try:
                            inputs_dict = dict(base.__dict__)
                        except Exception:
                            inputs_dict = {}
                        try:
                            constraints_list = evaluate_constraints(out)
                        except Exception:
                            constraints_list = []
                        try:
                            solver_meta = None
                            try:
                                solver_meta = dict(out.get("_solver")) if isinstance(out.get("_solver"), dict) else None
                            except Exception:
                                solver_meta = None
                            if solver_meta is not None:
                                # Attach UI log if available
                                try:
                                    solver_meta.setdefault("ui_log", st.session_state.get("last_solver_log", ""))
                                except Exception:
                                    pass
                            artifact = build_run_artifact(
                                inputs=inputs_dict,
                                outputs=dict(out),
                                constraints=constraints_list,
                                meta=None,
                                baseline_inputs=inputs_dict,
                                fidelity={"assumptions": _pd_assumptions_snapshot(), "config": st.session_state.get("fidelity_config", {})}, calibration={"confinement": float(st.session_state.get("calib_confinement",1.0)), "divertor": float(st.session_state.get("calib_divertor",1.0)), "bootstrap": float(st.session_state.get("calib_bootstrap",1.0))},
                                solver=solver_meta,
                            )
                        except Exception:
                            artifact = {"inputs": inputs_dict, "outputs": dict(out), "constraints": []}

                
                        # Attach intent-aware policy metadata (UI-only; does not affect physics core)
                        try:
                            artifact = _attach_common_metadata(artifact)
                            artifact["design_intent"] = str(st.session_state.get("design_intent", "Power Reactor (net-electric)"))
                            artifact["constraint_policy"] = _constraint_policy_snapshot()
                            # Authority contracts (deterministic metadata)
                            try:
                                from provenance.authority import authority_snapshot_from_outputs
                                artifact["authority_contracts"] = authority_snapshot_from_outputs(out if isinstance(out, dict) else {})
                            except Exception:
                                pass
                            # Human-readable run summary (audit / pasteable)
                            try:
                                artifact["run_summary"] = _compute_run_summary_from_out(out if isinstance(out, dict) else {})
                            except Exception:
                                pass
                        except Exception:
                            pass


                        # Optional: attach deterministic feasibility-forensics study output (if computed)
                        try:
                            ff = st.session_state.get("pd_last_forensics")
                            ff_h = st.session_state.get("pd_last_forensics_inputs_hash")
                            if isinstance(ff, dict) and ff and (ff_h == st.session_state.get("pd_current_inputs_hash")):
                                artifact.setdefault("studies", {})
                                artifact["studies"]["feasibility_forensics"] = ff
                        except Exception:
                            pass

                        # cache the last point artifact for cross-panel use and exports
                        st.session_state['pd_last_artifact'] = artifact

                        # Provide downloadable artifacts and reports (no side effects unless user clicks)
                        if _pd_tel_view == "ðŸ§¾ Chronicle & Export":
                            with st.expander("ðŸ“¦ Export Bay - Artifacts & Downloads", expanded=False):
                                st.download_button(
                                    "Download run artifact JSON",
                                    data=_shams_json_dumps(artifact, indent=2, sort_keys=True),
                                    file_name="shams_run_artifact.json",
                                    mime="application/json",
                                    use_container_width=True,
                                )

                                # Quick interop: send this run artifact to Compare session slots (A/B)
                                st.caption("Quick interop: send the current run to Compare without downloading/uploading files.")
                                _c1, _c2, _c3 = st.columns([1, 1, 1])
                                with _c1:
                                    if st.button("ðŸ…°ï¸ Send to Compare Slot A", use_container_width=True, key="pd_send_cmp_A"):
                                        st.session_state["cmp_slot_A"] = artifact
                                        st.session_state["cmp_slot_A_meta"] = {
                                            "ts_unix": float(time.time()),
                                            "inputs_hash": str(st.session_state.get("pd_last_inputs_hash") or st.session_state.get("pd_current_inputs_hash") or ""),
                                            "label": "Point Designer (last run)",
                                        }
                                        st.success("Sent current run to Compare Slot A.")
                                with _c2:
                                    if st.button("ðŸ…±ï¸ Send to Compare Slot B", use_container_width=True, key="pd_send_cmp_B"):
                                        st.session_state["cmp_slot_B"] = artifact
                                        st.session_state["cmp_slot_B_meta"] = {
                                            "ts_unix": float(time.time()),
                                            "inputs_hash": str(st.session_state.get("pd_last_inputs_hash") or st.session_state.get("pd_current_inputs_hash") or ""),
                                            "label": "Point Designer (last run)",
                                        }
                                        st.success("Sent current run to Compare Slot B.")
                                with _c3:
                                    if st.button("ðŸ§¹ Clear Compare Slots", use_container_width=True, key="pd_clear_cmp_slots"):
                                        st.session_state.pop("cmp_slot_A", None)
                                        st.session_state.pop("cmp_slot_B", None)
                                        st.session_state.pop("cmp_slot_A_meta", None)
                                        st.session_state.pop("cmp_slot_B_meta", None)
                                        st.info("Cleared Compare slots.")

    
                                # Radial build PNG
                                try:
                                    import tempfile
                                    tmpdir = tempfile.mkdtemp(prefix="shams_export_")
                                    radial_path = os.path.join(tmpdir, "radial_build.png")
                                    plot_radial_build_from_artifact(artifact, radial_path)
                                    with open(radial_path, "rb") as f:
                                        _rb_bytes = f.read()
                                    # cache bytes for reruns / PAM / other panels
                                    try:
                                        st.session_state["pd_last_radial_png_bytes"] = _rb_bytes
                                        _s = st.session_state.get('shams_state', None)
                                        if _s is not None:
                                            setattr(_s, 'last_point_radial_png', _rb_bytes)
                                    except Exception:
                                        pass
                                    if isinstance(_rb_bytes, (bytes, bytearray)) and len(_rb_bytes) > 0:
                                        st.download_button(
                                            "Download radial build PNG",
                                            data=_rb_bytes,
                                            file_name="shams_radial_build.png",
                                            mime="image/png",
                                            use_container_width=True,
                                        )
                                except Exception as _e:
                                    st.caption("Radial-build export unavailable for this point.")
    
                                # Summary PDF
                                try:
                                    import tempfile
                                    tmpdir2 = tempfile.mkdtemp(prefix="shams_export_")
                                    pdf_path = os.path.join(tmpdir2, "summary.pdf")
                                    plot_summary_pdf(artifact, pdf_path)
                                    with open(pdf_path, "rb") as f:
                                        _pdf_bytes = f.read()
                                    try:
                                        st.session_state["pd_last_summary_pdf_bytes"] = _pdf_bytes
                                        _s = st.session_state.get('shams_state', None)
                                        if _s is not None:
                                            setattr(_s, 'last_point_summary_pdf', _pdf_bytes)
                                    except Exception:
                                        pass
                                    if isinstance(_pdf_bytes, (bytes, bytearray)) and len(_pdf_bytes) > 0:
                                        st.download_button(
                                            "Download summary PDF",
                                            data=_pdf_bytes,
                                            file_name="shams_summary.pdf",
                                            mime="application/pdf",
                                            use_container_width=True,
                                        )
                                except Exception:
                                    st.caption("PDF summary export unavailable for this point.")
    
                        if _pd_tel_view == "âš¡ Mission Snapshot":
                            with st.expander("âš¡ Mission Snapshot - Key KPIs", expanded=False):
                                # Standardized KPI set shared with PDF (see decision.kpis.KPI_SET)
                                kpis = headline_kpis(out)
                                for i in range(0, len(kpis), 4):
                                    kpi_row(kpis[i:i+4])
                                    if i + 4 < len(kpis):
                                        st.divider()

                                # --- Deep physics cards (read-only; deterministic)
                                with st.expander("ðŸ§¬ Physics Deepening - Regimes Â· Burn Â· Impurities Â· Edge Â· Neutronics", expanded=False):
                                    _deep_view = st.selectbox(
                                        "Select deck",
                                        options=[
                                            "Regime & Confinement",
                                            "Current Profile & Current Drive",
                                            "Bootstrapâ€“Pressure Self-Consistency Authority",
                                            "Current Drive Tech Authority",
                                            "Non-Inductive Closure Authority",
                                            "Burn & Alpha Power",
                                            "Impurities & Core Radiation",
                                            "Edge/Divertor & Exhaust Control",
                                            "Neutronics & Nuclear Loads",
                                            "Coupling Narratives",
                                        ],
                                        index=0,
                                        key="pd_deep_physics_view",
                                    )

                                    def _safe_num(k: str) -> float:
                                        try:
                                            return float(out.get(k, float('nan')))
                                        except Exception:
                                            return float('nan')

                                    if _deep_view == "Regime & Confinement":
                                        cA, cB, cC, cD = st.columns([1.0, 1.0, 1.0, 1.0])
                                        cA.metric("Regime label", str(out.get("confinement_regime", "unknown")))
                                        cB.metric("H98", f"{_safe_num('H98'):.2f}" if _safe_num('H98') == _safe_num('H98') else "n/a")
                                        cC.metric("H_regime", f"{_safe_num('H_regime'):.2f}" if _safe_num('H_regime') == _safe_num('H_regime') else "n/a")
                                        cD.metric("P_LH (MW)", f"{_safe_num('P_LH_MW'):.1f}" if _safe_num('P_LH_MW') == _safe_num('P_LH_MW') else "n/a")
                                        st.caption("H_regime is reported only when couple_regime_to_confinement=True; it uses IPB98 for H-regime and ITER89P for L-regime.")
                                        # v336.0: plasma regime authority
                                        if str(out.get("plasma_regime", "")):
                                            st.divider()
                                            p1, p2, p3, p4 = st.columns([1.0, 1.0, 1.0, 1.0])
                                            p1.metric("Plasma regime", str(out.get("plasma_regime", "unknown")))
                                            p2.metric("Burn regime", str(out.get("burn_regime", "-")))
                                            p2.caption("ignited / alpha_assisted / aux_dominated")
                                            p3.metric("Fragility", str(out.get("plasma_fragility_class", "UNKNOWN")))
                                            _pm = _safe_num("plasma_min_margin_frac")
                                            p4.metric("Min margin (frac)", f"{_pm:.3f}" if _pm == _pm else "n/a")
                                            st.caption("Plasma regime authority is a deterministic classifier with signed fractional margins for H-mode access, Greenwald fraction, q95, betaN, and burn (M_ign_total). No solvers, no iteration.")

                                            # v337.0: impurity species & radiation partition authority
                                            if str(out.get("impurity_regime", "")):
                                                st.divider()
                                                i1, i2, i3, i4 = st.columns([1.0, 1.0, 1.0, 1.0])
                                                i1.metric("Impurity regime", str(out.get("impurity_regime", "unknown")))
                                                i2.metric("Species", str(out.get("impurity_species", "unknown")))
                                                i3.metric("Fragility", str(out.get("impurity_fragility_class", "UNKNOWN")))
                                                _im = _safe_num("impurity_min_margin_frac")
                                                i4.metric("Min margin (frac)", f"{_im:.3f}" if _im == _im else "n/a")
                                                st.caption("Impurity & radiation authority partitions core/SOL radiation and checks conservative thresholds on Zeff and radiated power fractions. Deterministic post-processing only; no solvers, no iteration.")

                            
                                    elif _deep_view == "Current Profile & Current Drive":
                                        cA, cB, cC, cD = st.columns([1.0, 1.0, 1.0, 1.0])
                                        cA.metric("Profile regime", str(out.get("current_profile_regime", "unknown")))
                                        cB.metric("Fragility", str(out.get("current_profile_fragility_class", "UNKNOWN")))
                                        mm = _safe_num("current_profile_min_margin_frac")
                                        cC.metric("Min margin (frac)", f"{mm:.3f}" if mm == mm else "n/a")
                                        cD.metric("Top limiter", str(out.get("current_profile_top_limiter", "UNKNOWN")))

                                        c1, c2, c3, c4 = st.columns([1.0, 1.0, 1.0, 1.0])
                                        c1.metric("q95 proxy", f"{_safe_num('q95_proxy'):.2f}" if _safe_num('q95_proxy') == _safe_num('q95_proxy') else "n/a")
                                        c2.metric("qmin proxy", f"{_safe_num('profile_qmin_proxy'):.2f}" if _safe_num('profile_qmin_proxy') == _safe_num('profile_qmin_proxy') else "n/a")
                                        c3.metric("f_bootstrap proxy", f"{_safe_num('profile_f_bootstrap_proxy'):.2f}" if _safe_num('profile_f_bootstrap_proxy') == _safe_num('profile_f_bootstrap_proxy') else (f"{_safe_num('f_bs_proxy'):.2f}" if _safe_num('f_bs_proxy') == _safe_num('f_bs_proxy') else "n/a"))
                                        c4.metric("f_NI", f"{_safe_num('f_NI'):.2f}" if _safe_num('f_NI') == _safe_num('f_NI') else "n/a")

                                        c5, c6, c7, c8 = st.columns([1.0, 1.0, 1.0, 1.0])
                                        c5.metric("I_cd (MA)", f"{_safe_num('I_cd_MA'):.2f}" if _safe_num('I_cd_MA') == _safe_num('I_cd_MA') else "n/a")
                                        c6.metric("P_cd (MW)", f"{_safe_num('P_cd_MW'):.1f}" if _safe_num('P_cd_MW') == _safe_num('P_cd_MW') else "n/a")
                                        c7.metric("Î·_CD (A/W)", f"{_safe_num('cd_eta_A_per_W'):.3e}" if _safe_num('cd_eta_A_per_W') == _safe_num('cd_eta_A_per_W') else "n/a")
                                        c8.metric("Contract hash", str(out.get("current_profile_contract_sha256", ""))[:12] + ("â€¦" if str(out.get("current_profile_contract_sha256", "")) else ""))

                                        # Signed fractional margins (expandable)
                                        with st.expander("Current-profile authority margins (fractional, signed)", expanded=False):
                                            rows = []
                                            for k, v in sorted(out.items(), key=lambda kv: str(kv[0])):
                                                if str(k).startswith("current_profile_CP_"):
                                                    try:
                                                        rows.append({"check": str(k).replace("current_profile_", ""), "margin_frac": float(v)})
                                                    except Exception:
                                                        rows.append({"check": str(k).replace("current_profile_", ""), "margin_frac": float("nan")})
                                            if rows:
                                                st.dataframe(rows, use_container_width=True, hide_index=True)
                                            else:
                                                st.info("No current-profile margin fields found in this artifact.")
                                    elif _deep_view == "Bootstrapâ€“Pressure Self-Consistency Authority":
                                        cA, cB, cC, cD = st.columns([1.0, 1.0, 1.0, 1.0])
                                        cA.metric("Regime", str(out.get("bsp_regime", "unknown")))
                                        cB.metric("Fragility", str(out.get("bsp_fragility_class", "UNKNOWN")))
                                        mm = _safe_num("bsp_min_margin_frac")
                                        cC.metric("Min margin (frac)", "â€”" if mm != mm else f"{mm:+.3f}")
                                        cD.metric("Top limiter", str(out.get("bsp_top_limiter", "UNKNOWN")))

                                        c1, c2, c3, c4 = st.columns([1.0, 1.0, 1.0, 1.0])
                                        c1.metric("|Î”f_bs|", f"{_safe_num('bsp_abs_delta_f_bootstrap'):.3f}" if _safe_num('bsp_abs_delta_f_bootstrap') == _safe_num('bsp_abs_delta_f_bootstrap') else "n/a")
                                        c2.metric("Tol |Î”f_bs|", f"{_safe_num('bsp_abs_delta_max'):.3f}" if _safe_num('bsp_abs_delta_max') == _safe_num('bsp_abs_delta_max') else "n/a")
                                        c3.metric("f_bs (reported)", f"{_safe_num('bsp_f_bootstrap_reported'):.2f}" if _safe_num('bsp_f_bootstrap_reported') == _safe_num('bsp_f_bootstrap_reported') else "n/a")
                                        c4.metric("f_bs (expected)", f"{_safe_num('bsp_f_bootstrap_expected'):.2f}" if _safe_num('bsp_f_bootstrap_expected') == _safe_num('bsp_f_bootstrap_expected') else "n/a")

                                        c5, c6, c7, c8 = st.columns([1.0, 1.0, 1.0, 1.0])
                                        c5.metric("Î²_p proxy", f"{_safe_num('bsp_beta_p_proxy'):.2f}" if _safe_num('bsp_beta_p_proxy') == _safe_num('bsp_beta_p_proxy') else "n/a")
                                        c6.metric("Model", str(out.get("bsp_model", out.get("bootstrap_model", "-"))))
                                        c7.metric("q95 proxy", f"{_safe_num('q95_proxy'):.2f}" if _safe_num('q95_proxy') == _safe_num('q95_proxy') else "n/a")
                                        sha = str(out.get("bsp_contract_sha256", "") or "")
                                        c8.metric("Contract hash", sha[:12] + ("â€¦" if sha else ""))

                                        with st.expander("Bootstrapâ€“pressure authority details", expanded=False):
                                            st.caption("Deterministic check: |f_bs(reported) âˆ’ f_bs(expected)| under selected proxy model. No iteration; intended to flag pressure/bootstrap mirages.")
                                            st.json({k: out.get(k) for k in ["bsp_regime","bsp_fragility_class","bsp_min_margin_frac","bsp_top_limiter",'bsp_abs_delta_f_bootstrap','bsp_abs_delta_max','bsp_f_bootstrap_reported','bsp_f_bootstrap_expected',"bsp_delta_f_bootstrap",'bsp_beta_p_proxy',"bsp_model"] if k in out}, expanded=False)
                                    elif _deep_view == "Current Drive Tech Authority":
                                        cA, cB, cC, cD = st.columns([1.0, 1.0, 1.0, 1.0])
                                        cA.metric("CD tech regime", str(out.get("cd_tech_regime", "unknown")))
                                        cB.metric("Fragility", str(out.get("cd_fragility_class", "UNKNOWN")))
                                        mm = _safe_num("cd_min_margin_frac")
                                        cC.metric("Min margin (frac)", f"{mm:.3f}" if mm == mm else "n/a")
                                        cD.metric("Top limiter", str(out.get("cd_top_limiter", "UNKNOWN")))

                                        with st.expander("CD tech margins", expanded=False):
                                            rows = []
                                            for k, v in out.items():
                                                if isinstance(k, str) and k.startswith("cd_") and ("_margin_frac" in k):
                                                    try:
                                                        vv = float(v)
                                                    except Exception:
                                                        vv = float("nan")
                                                    rows.append({"metric": k, "value": vv})
                                            rows = sorted(rows, key=lambda r: (0 if (r["value"]==r["value"]) else 1, r["value"]))
                                            st.table(rows if rows else [{"metric": "(no margins available)", "value": float("nan")}])

                                        sha = str(out.get("cd_contract_sha256", "") or "")
                                        if sha:
                                            st.caption(f"Contract hash (SHA-256): {sha[:16]}â€¦")

                            
                                    elif _deep_view == "Non-Inductive Closure Authority":
                                        cA, cB, cC, cD = st.columns([1.0, 1.0, 1.0, 1.0])
                                        cA.metric("NI regime", str(out.get("ni_closure_regime", "unknown")))
                                        cB.metric("Fragility", str(out.get("ni_fragility_class", "UNKNOWN")))
                                        mm = _safe_num("ni_min_margin_frac")
                                        cC.metric("Min margin (frac)", "â€”" if mm != mm else f"{mm:+.3f}")
                                        cD.metric("Top limiter", str(out.get("ni_top_limiter", "UNKNOWN")))
                                        # margins table
                                        rows = []
                                        for k, v in out.items():
                                            if isinstance(k, str) and k.startswith("ni_") and k.endswith("_margin_frac"):
                                                vv = float(v) if isinstance(v, (int, float)) else float('nan')
                                                rows.append({"margin_id": k, "margin_frac": vv})
                                        if rows:
                                            import pandas as pd
                                            df = pd.DataFrame(rows).sort_values("margin_frac")
                                            with st.expander("NI closure margins (signed fractional)", expanded=False):
                                                st.dataframe(df, use_container_width=True, hide_index=True)
                                        else:
                                            st.info("No NI closure margin fields found in this artifact.")
                                    elif _deep_view == "Burn & Alpha Power":
                                        cA, cB, cC, cD = st.columns([1.0, 1.0, 1.0, 1.0])
                                        cA.metric("PÎ± (MW)", f"{_safe_num('Palpha_MW'):.1f}")
                                        cB.metric("Ploss (MW)", f"{_safe_num('Ploss_MW'):.1f}")
                                        cC.metric("M_ign = PÎ±/Ploss", f"{_safe_num('M_ign'):.2f}" if _safe_num('M_ign') == _safe_num('M_ign') else "n/a")
                                        cD.metric("M_ign_total", f"{_safe_num('M_ign_total'):.2f}" if _safe_num('M_ign_total') == _safe_num('M_ign_total') else "n/a")
                                        st.caption("M_ign_total uses Ploss+Prad_core in the denominator. Constraints can optionally enforce M_ign â‰¥ ignition_margin_min.")

                                    elif _deep_view == "Impurities & Core Radiation":
                                        cA, cB, cC, cD = st.columns([1.0, 1.0, 1.0, 1.0])
                                        cA.metric("Radiation enabled", "YES" if bool(out.get("include_radiation", False)) else "NO")
                                        cB.metric("Prad_core (MW)", f"{_safe_num('Prad_core_MW'):.1f}")
                                        cC.metric("Zeff (input)", f"{_safe_num('zeff'):.2f}" if _safe_num('zeff') == _safe_num('zeff') else "n/a")
                                        cD.metric("Radiation model", str(out.get("radiation_model", "-")))
                                        st.caption("If using physics/line radiation, the Lz DB id + SHA256 are stamped into the artifact for auditability.")

                                    elif _deep_view == "Edge/Divertor & Exhaust Control":
                                        cA, cB, cC, cD = st.columns([1.0, 1.0, 1.0, 1.0])
                                        cA.metric("q_div (MW/mÂ²)", f"{_safe_num('q_div_MW_m2'):.1f}" if _safe_num('q_div_MW_m2') == _safe_num('q_div_MW_m2') else "n/a")
                                        cB.metric("q_div limit", f"{_safe_num('q_div_max_MW_m2'):.1f}" if _safe_num('q_div_max_MW_m2') == _safe_num('q_div_max_MW_m2') else "n/a")
                                        cC.metric("f_rad_div", f"{_safe_num('f_rad_div'):.2f}" if _safe_num('f_rad_div') == _safe_num('f_rad_div') else "n/a")
                                        cD.metric("Divertor regime", str(out.get("div_regime", "unknown")))
                                        # v329.0: exhaust & radiation regime authority
                                        if str(out.get("exhaust_regime","")):
                                            st.divider()
                                            e1, e2, e3, e4 = st.columns([1.0, 1.0, 1.0, 1.0])
                                            e1.metric("Exhaust regime", str(out.get("exhaust_regime","unknown")))
                                            e2.metric("Fragility", str(out.get("exhaust_fragility_class","UNKNOWN")))
                                            _mr = _safe_num("exhaust_min_margin_frac")
                                            e3.metric("Min margin (frac)", f"{_mr:.3f}" if _mr == _mr else "n/a")
                                            _rad = _safe_num("exhaust_radiation_dominated")
                                            e4.metric("Radiation-dom", "YES" if (_rad == _rad and _rad >= 0.5) else ("NO" if _rad == _rad else "n/a"))
                                            st.caption("Exhaust regime is a deterministic classifier (attached / marginal_detach / detached / radiation_dominated / overheat) based on P_SOL/R overload, q_div margin, and (if enabled) required SOL+div radiation fraction. No solvers, no iteration.")
                                        if bool(getattr(base, "include_sol_radiation_control", False)):
                                            st.divider()
                                            c1, c2, c3, c4 = st.columns([1.0, 1.0, 1.0, 1.0])
                                            c1.metric("q_target", f"{_safe_num('q_div_target_MW_m2'):.1f}" if _safe_num('q_div_target_MW_m2') == _safe_num('q_div_target_MW_m2') else "n/a")
                                            c2.metric("f_SOL+div,req", f"{_safe_num('detachment_f_sol_div_required'):.2f}" if _safe_num('detachment_f_sol_div_required') == _safe_num('detachment_f_sol_div_required') else "n/a")
                                            c3.metric("P_rad,SOL+div req (MW)", f"{_safe_num('detachment_prad_sol_div_required_MW'):.1f}" if _safe_num('detachment_prad_sol_div_required_MW') == _safe_num('detachment_prad_sol_div_required_MW') else "n/a")
                                            c4.metric("f_z,required", f"{_safe_num('detachment_f_z_required'):.1e}" if _safe_num('detachment_f_z_required') == _safe_num('detachment_f_z_required') else "n/a")
                                            st.caption("Detachment authority is diagnostic-only unless you set a max f_z cap. It algebraically inverts q_div target â†’ required SOL+div radiation â†’ implied impurity seeding fraction.")

                                    elif _deep_view == "Neutronics & Nuclear Loads":
                                        cA, cB, cC, cD = st.columns([1.0, 1.0, 1.0, 1.0])
                                        cA.metric("n-wall load (MW/mÂ²)", f"{_safe_num('neutron_wall_load_MW_m2'):.2f}" if _safe_num('neutron_wall_load_MW_m2') == _safe_num('neutron_wall_load_MW_m2') else "n/a")
                                        cB.metric("TBR", f"{_safe_num('TBR'):.2f}" if _safe_num('TBR') == _safe_num('TBR') else "n/a")
                                        cC.metric("HTS lifetime (yr)", f"{_safe_num('hts_lifetime_yr'):.1f}" if _safe_num('hts_lifetime_yr') == _safe_num('hts_lifetime_yr') else "n/a")
                                        cD.metric("FW dpa/y", f"{_safe_num('fw_dpa_per_year'):.2f}" if _safe_num('fw_dpa_per_year') == _safe_num('fw_dpa_per_year') else "n/a")
                                        st.caption(f"**Neutronics/Materials regime:** `{out.get('neutronics_materials_regime', 'unknown')}`  |  **Fragility:** `{out.get('neutronics_materials_fragility_class', 'UNKNOWN')}`  |  **Min margin:** {out.get('neutronics_materials_min_margin_frac', float('nan')):.3f}  |  **Contract:** `{str(out.get('neutronics_materials_contract_sha256', ''))[:10]}`")
                                        st.divider()
                                        d1, d2, d3, d4 = st.columns([1.0, 1.0, 1.0, 1.0])
                                        d1.metric("Stack attenuation", f"{_safe_num('neutron_attenuation_factor'):.3g}" if _safe_num('neutron_attenuation_factor') == _safe_num('neutron_attenuation_factor') else "n/a")
                                        # v309.0: expose fast/gamma split when available
                                        _af = _safe_num('neutron_attenuation_fast')
                                        _ag = _safe_num('neutron_attenuation_gamma')
                                        if _af == _af or _ag == _ag:
                                            st.caption(f"Attenuation (fast, gamma): {(_af if _af==_af else float('nan')):.3g} / {(_ag if _ag==_ag else float('nan')):.3g}")
                                        d2.metric("P_nuc,total (MW)", f"{_safe_num('P_nuc_total_MW'):.2f}" if _safe_num('P_nuc_total_MW') == _safe_num('P_nuc_total_MW') else "n/a")
                                        d3.metric("P_nuc,TF (MW)", f"{_safe_num('P_nuc_TF_MW'):.2f}" if _safe_num('P_nuc_TF_MW') == _safe_num('P_nuc_TF_MW') else "n/a")
                                        d4.metric("FW life (yr)", f"{_safe_num('fw_lifetime_yr'):.1f}" if _safe_num('fw_lifetime_yr') == _safe_num('fw_lifetime_yr') else "n/a")

                                        e1, e2, e3, e4 = st.columns([1.0, 1.0, 1.0, 1.0])
                                        e1.metric("Blanket life (yr)", f"{_safe_num('blanket_lifetime_yr'):.1f}" if _safe_num('blanket_lifetime_yr') == _safe_num('blanket_lifetime_yr') else "n/a")
                                        e2.metric("FW He/y (appm)", f"{_safe_num('fw_He_appm_per_year'):.0f}" if _safe_num('fw_He_appm_per_year') == _safe_num('fw_He_appm_per_year') else "n/a")
                                        e3.metric("FW T margin (Â°C)", f"{_safe_num('fw_T_margin_C'):.0f}" if _safe_num('fw_T_margin_C') == _safe_num('fw_T_margin_C') else "n/a")
                                        e4.metric("FW Ïƒ margin (MPa)", f"{_safe_num('fw_sigma_margin_MPa'):.0f}" if _safe_num('fw_sigma_margin_MPa') == _safe_num('fw_sigma_margin_MPa') else "n/a")

                                        f1, f2, f3, f4 = st.columns([1.0, 1.0, 1.0, 1.0])
                                        f1.metric("FW material", str(out.get("fw_material", "-")))
                                        f2.metric("Blanket material", str(out.get("blanket_material", "-")))
                                        f3.metric("Shield material", str(out.get("shield_material", "-")))
                                        f4.metric("TBR validity", "OK" if float(out.get("TBR_validity", 0.0)) < 0.5 else "out-of-range")

                                        st.caption("Neutronics/materials: all quantities are deterministic proxies. Fast/gamma attenuation and nuclear heating partitioning are parametric; DPA/He + temperature/stress checks are screening models. Constraints are enforced only when corresponding caps/flags are set.")

                                        # (v367.0) Materials lifetime closure (replacement cadence + cost-rate)
                                        if ("materials_lifetime_schema_version" in out) or ("fw_replace_interval_y_v367" in out) or ("replacement_cost_MUSD_per_year_v367_total" in out):
                                            st.divider()
                                            m1, m2, m3, m4 = st.columns([1.0, 1.0, 1.0, 1.0])
                                            m1.metric("Plant life (yr)", f"{_safe_num('plant_design_lifetime_yr'):.0f}" if _safe_num('plant_design_lifetime_yr') == _safe_num('plant_design_lifetime_yr') else "n/a")
                                            m2.metric("FW repl (count)", f"{int(_safe_num('fw_replacements_over_plant_life'))}" if _safe_num('fw_replacements_over_plant_life') == _safe_num('fw_replacements_over_plant_life') else "n/a")
                                            m3.metric("Blanket repl (count)", f"{int(_safe_num('blanket_replacements_over_plant_life'))}" if _safe_num('blanket_replacements_over_plant_life') == _safe_num('blanket_replacements_over_plant_life') else "n/a")
                                            m4.metric("Repl cost rate (MUSD/y)", f"{_safe_num('replacement_cost_MUSD_per_year_v367_total'):.2f}" if _safe_num('replacement_cost_MUSD_per_year_v367_total') == _safe_num('replacement_cost_MUSD_per_year_v367_total') else "n/a")

                                            n1, n2, n3, n4 = st.columns([1.0, 1.0, 1.0, 1.0])
                                            n1.metric("FW cadence (yr)", f"{_safe_num('fw_replace_interval_y_v367'):.2f}" if _safe_num('fw_replace_interval_y_v367') == _safe_num('fw_replace_interval_y_v367') else "n/a")
                                            n2.metric("Blanket cadence (yr)", f"{_safe_num('blanket_replace_interval_y_v367'):.2f}" if _safe_num('blanket_replace_interval_y_v367') == _safe_num('blanket_replace_interval_y_v367') else "n/a")
                                            n3.metric("FW cost (MUSD/y)", f"{_safe_num('fw_replacement_cost_MUSD_per_year'):.2f}" if _safe_num('fw_replacement_cost_MUSD_per_year') == _safe_num('fw_replacement_cost_MUSD_per_year') else "n/a")
                                            n4.metric("Blanket cost (MUSD/y)", f"{_safe_num('blanket_replacement_cost_MUSD_per_year'):.2f}" if _safe_num('blanket_replacement_cost_MUSD_per_year') == _safe_num('blanket_replacement_cost_MUSD_per_year') else "n/a")

                                            sha = str(out.get("materials_lifetime_contract_sha256", "") or "")
                                            if sha:
                                                st.caption(f"Materials lifetime contract hash (SHA-256): {sha[:16]}â€¦")

                        

                                    elif _deep_view == "Coupling Narratives":
                                        st.caption("Deterministic coupling narratives derived from authority dominance + regime labels. No solvers, no iteration.")
                                        csum = str(out.get("coupling_summary", "") or "")
                                        if csum:
                                            st.info(csum)
                                        sev = out.get("coupling_severity_max", 0)
                                        try:
                                            sev_i = int(sev)
                                        except Exception:
                                            sev_i = 0
                                        st.metric("Max severity", f"{sev_i}/5")

                                        cn = out.get("coupling_narratives", {})
                                        items = []
                                        if isinstance(cn, dict):
                                            items = cn.get("coupling_narratives", []) or []
                                        if not items:
                                            st.caption("No coupling flags triggered for this evaluation.")
                                        else:
                                            for i, it in enumerate(items):
                                                if not isinstance(it, dict):
                                                    continue
                                                code = str(it.get("code", ""))
                                                title = str(it.get("title", "Coupling narrative"))
                                                sev = it.get("severity", "")
                                                header = f"{title}  [{code}]  (sev={sev})"
                                                with st.expander(header, expanded=False):
                                                    st.write(str(it.get("narrative", "")))


                                # --- Authority & validity contracts (verdict-first)
                                with st.expander("ðŸ§¾ Authority & Validity - Contracts", expanded=False):
                                    try:
                                        from provenance.authority import authority_snapshot_from_outputs
                                        snap = authority_snapshot_from_outputs(out if isinstance(out, dict) else {})
                                        subs = (snap.get("subsystems", {}) or {})
                                        rows = []
                                        n_proxy = 0
                                        for k, v in subs.items():
                                            tier = str((v or {}).get("tier", ""))
                                            if tier.strip().lower() == "proxy":
                                                n_proxy += 1
                                            rows.append({
                                                "subsystem": str(k),
                                                "tier": tier,
                                                "validity": str((v or {}).get("validity_domain", "")),
                                            })
                                        if rows:
                                            _df = pd.DataFrame(rows)
                                            st.dataframe(_df, use_container_width=True, height=320, hide_index=True)
                                        if n_proxy > 0:
                                            st.warning(f"{n_proxy} subsystems are currently tagged as PROXY for this run. See table for details.")
                                        else:
                                            st.success("No subsystems flagged as pure PROXY in this run (some may still be semi-authoritative).")
                                        st.caption("Contracts are declarative metadata; they do not change physics.")
                                    except Exception as e:
                                        st.caption(f"Authority contracts unavailable: {e}")

                                # --- Fuel cycle / lifetime / availability realism
                                with st.expander(" Fuel Cycle Â· Lifetime Â· Availability", expanded=False):
                                    def _m(k: str, fmt: str = "{:.3g}", suffix: str = ""):
                                        try:
                                            v = float(out.get(k, float('nan')))
                                        except Exception:
                                            v = float('nan')
                                        return (fmt.format(v) + suffix) if (v == v) else "n/a"

                                    c1, c2, c3, c4 = st.columns([1, 1, 1, 1])
                                    c1.metric("T burn (g/day)", _m("T_burn_g_per_day", "{:.2f}"))
                                    c2.metric("T inventory proxy (g)", _m("T_inventory_proxy_g", "{:.2f}"))
                                    c3.metric("TBR", _m("TBR", "{:.2f}"))
                                    c4.metric("FW dpa/y", _m("fw_dpa_per_year", "{:.2f}"))

                                    c1, c2, c3, c4 = st.columns([1, 1, 1, 1])
                                    c1.metric("Availability", _m("availability_model", "{:.2f}"))
                                    c2.metric("Annual net (MWh/y)", _m("annual_net_MWh", "{:.3g}"))
                                    c3.metric("FW interval (y)", _m("fw_replace_interval_y", "{:.2f}"))
                                    c4.metric("DIV interval (y)", _m("div_replace_interval_y", "{:.2f}"))

                                    # (v359.0) Availability & replacement ledger overlay (optional)
                                    try:
                                        _av359 = float(out.get("availability_v359", float('nan')))
                                    except Exception:
                                        _av359 = float('nan')
                                    if _av359 == _av359:
                                        c1, c2, c3, c4 = st.columns([1, 1, 1, 1])
                                        c1.metric("Availability (v359)", _m("availability_v359", "{:.2f}"))
                                        c2.metric("Net MWh/y (v359)", _m("net_electric_MWh_per_year_v359", "{:.3g}"))
                                        c3.metric("LCOE (v359) (USD/MWh)", _m("LCOE_proxy_v359_USD_per_MWh", "{:.2f}"))
                                        c4.metric("Repl. cost (MUSD/y)", _m("replacement_cost_MUSD_per_year_v359", "{:.2f}"))
                                        st.caption("v359 ledger uses planned_outage_base + forced_outage_base + replacement downtime; it does not modify truth or legacy economics outputs.")


                                    lims = []
                                    for k in ["tritium_inventory_max_g", "fw_dpa_max_per_year", "availability_min", "annual_net_MWh_min"]:
                                        try:
                                            v = float(out.get(k, float('nan')))
                                        except Exception:
                                            v = float('nan')
                                        if v == v:
                                            lims.append(f"{k}={v:.3g}")
                                    if lims:
                                        st.caption("Active caps/requirements: " + "; ".join(lims))
                                    else:
                                        st.caption("No explicit caps/requirements set for fuel-cycle/lifetime/annual-energy in this run.")

                                # --- Inboard build & coil stress/Jmargin quicklook
                                with st.expander("ðŸ§± Build Â· Coils Â· Stress Â· Margin", expanded=False):
                                    def _m2(k: str, fmt: str = "{:.3g}"):
                                        try:
                                            v = float(out.get(k, float('nan')))
                                        except Exception:
                                            v = float('nan')
                                        return fmt.format(v) if (v == v) else "n/a"

                                    c1, c2, c3, c4 = st.columns([1, 1, 1, 1])
                                    c1.metric("Inboard margin (m)", _m2("inboard_margin_m", "{:.3f}"))
                                    c2.metric("R_coil_inner (m)", _m2("R_coil_inner_m", "{:.3f}"))
                                    c3.metric("B_peak (T)", _m2("B_peak_T", "{:.2f}"))
                                    c4.metric("Ïƒ_vm (MPa)", _m2("sigma_vm_MPa", "{:.0f}"))

                                    c1, c2, c3, c4 = st.columns([1, 1, 1, 1])
                                    c1.metric("HTS margin", _m2("hts_margin", "{:.2f}"))
                                    c2.metric("TF Jop (MA/mmÂ²)", _m2("tf_Jop_MA_per_mm2", "{:.3f}"))
                                    c3.metric("TF strain", _m2("tf_strain", "{:.4f}"))
                                    c4.metric("Cryo power (MW)", _m2("cryo_power_MW", "{:.2f}"))

                                    enforce = out.get("enforce_radial_build", 0.0)
                                    if float(enforce) >= 0.5:
                                        st.info("Radial-build closure enforcement is ON (inboard_margin_m â‰¥ 0 is a hard constraint).")
                                    else:
                                        st.caption("Radial-build closure enforcement is OFF by default; enable in inputs if desired.")

                                # --- Feasibility Forensics (deterministic, local)
                                with st.expander(" Feasibility Forensics - Local Sensitivity", expanded=False):
                                    st.caption(
                                        "Deterministic finite-difference sensitivities of constraint signed margins. "
                                        "This is *diagnostic only* (no optimization, no truth mutation)."
                                    )
                                    _intent = str(st.session_state.get("design_intent", ""))
                                    if st.button("Compute forensics", key="pd_forensics_btn", use_container_width=True):
                                        try:
                                            from src.analysis.forensics import local_sensitivity
                                            ff = local_sensitivity(base, design_intent=_intent)
                                            st.session_state["pd_last_forensics"] = ff
                                            st.session_state["pd_last_forensics_inputs_hash"] = st.session_state.get("pd_current_inputs_hash")
                                        except Exception as e:
                                            st.session_state["pd_last_forensics"] = {"status": "error", "message": str(e)}
                                            st.session_state["pd_last_forensics_inputs_hash"] = st.session_state.get("pd_current_inputs_hash")
                                    ff = st.session_state.get("pd_last_forensics")
                                    if isinstance(ff, dict) and ff:
                                        if ff.get("status") == "error":
                                            st.error(ff.get("message", "forensics error"))
                                        else:
                                            # --- Summary strip (verdict-first, compact)
                                            b = ff.get("base", {}) or {}
                                            dom = str(b.get("top_dominant", ""))
                                            frag = b.get("fragility_fraction", float("nan"))
                                            stab = str(b.get("stability_label", "unknown"))

                                            c1, c2, c3, c4 = st.columns([1.3, 0.9, 0.9, 0.9])
                                            c1.metric("Dominant blocker", dom if dom else "(none)")
                                            c2.metric("Stability", stab)
                                            c3.metric(
                                                "Fragility",
                                                f"{float(frag):.2f}" if frag == frag else "n/a",
                                                help="Fraction of Â±1-step perturbations that change the dominant blocker. <=0.20 stable; >0.20 fragile.",
                                            )

                                            # Lever-confidence badge (deterministic; derived from fragility + slope consistency)
                                            lc = ff.get("lever_confidence", {}) or {}
                                            lc_score = lc.get("score", float("nan"))
                                            lc_label = str(lc.get("label", "unknown"))
                                            c4.metric(
                                                "Lever confidence",
                                                f"{lc_label}" if lc_label else "unknown",
                                                help="Heuristic quality indicator for the lever recipe: combines dominant-switch fragility with one-sided derivative consistency. 0..1 score stored in artifact.",
                                            )
                                            # --- Deterministic explanation (derived from computed sensitivities)
                                            notes = ff.get("notes", []) or []
                                            if notes:
                                                st.markdown("**Why this point is (un)stable**")
                                                for n in notes[:6]:
                                                    st.write(f"â€¢ {n}")

                                            # --- Tornado-style ranked table (no plots; expert-candy table)
                                            tornado = ff.get("tornado", {}) or {}
                                            focus = ff.get("focus_constraints", []) or []
                                            if tornado and focus:
                                                pick = st.selectbox(
                                                    "Constraint to inspect",
                                                    options=list(focus),
                                                    index=0,
                                                    key="pd_forensics_constraint_pick",
                                                )
                                                rows = list(tornado.get(pick, []) or [])
                                                if rows:
                                                    # Add a direction hint column: +dx increases margin? sign of dmargin/dx
                                                    for r in rows:
                                                        sgn = str(r.get("sign", "0"))
                                                        r["+dx effect"] = "margin â†‘" if sgn == "+" else ("margin â†“" if sgn == "-" else "flat")
                                                    st.dataframe(
                                                        rows,
                                                        use_container_width=True,
                                                        hide_index=True,
                                                        column_config={
                                                            "knob": st.column_config.TextColumn("Knob"),
                                                            "dmargin_per_unit": st.column_config.NumberColumn("âˆ‚margin/âˆ‚x", format="%.4g"),
                                                            "step": st.column_config.NumberColumn("Î”x (probe)", format="%.4g"),
                                                            "impact_abs": st.column_config.NumberColumn("|Î”margin| @ Î”x", format="%.4g"),
                                                            "+dx effect": st.column_config.TextColumn("Local lever"),
                                                        },
                                                    )
                                                    st.caption(
                                                        "Table is sorted by |Î”margin| at the deterministic probe step Î”x. "
                                                        "Use the sign (+/-) to see whether increasing the knob locally increases or decreases the margin."
                                                    )

                                            # --- Lever recipe (local-linear, no optimization)
                                            # Translate the dominant constraint's local sensitivities into directional levers.
                                            # This is intentionally conservative: read-only, no auto-application to inputs.
                                            try:
                                                dom_adv = ff.get("dominant_advice", {}) or {}
                                                dom_c = str(dom_adv.get("dominant_constraint", ""))
                                                if dom_c and isinstance(tornado, dict) and tornado.get(dom_c):
                                                    st.markdown("**Lever recipe (local-linear) - for the dominant blocker only**")
                                                    st.caption(
                                                        "Directional suggestions are derived from the local linearization at the deterministic probe step Î”x. "
                                                        "They are *not* an optimizer and may not hold far from this point."
                                                    )

                                                    dom_rows = list(tornado.get(dom_c, []) or [])
                                                    help_rows = [r for r in dom_rows if float(r.get("dmargin_per_unit", 0.0)) > 0]
                                                    hurt_rows = [r for r in dom_rows if float(r.get("dmargin_per_unit", 0.0)) < 0]

                                                    # Build two compact tables: actions that increase margin (increase knob if dmdx>0; decrease knob if dmdx<0)
                                                    def _mk_actions(rows, *, action_when_positive: str) -> list:
                                                        out_rows = []
                                                        for r in rows[:5]:
                                                            dmdx = float(r.get("dmargin_per_unit", float("nan")))
                                                            dx = float(r.get("step", float("nan")))
                                                            if not (dmdx == dmdx and dx == dx):
                                                                continue
                                                            delta = dmdx * dx
                                                            out_rows.append(
                                                                {
                                                                    "knob": str(r.get("knob", "")),
                                                                    "action": action_when_positive,
                                                                    "Î”x": dx,
                                                                    "Î”margin @ Î”x": delta,
                                                                    "|Î”margin|": abs(delta),
                                                                }
                                                            )
                                                        out_rows.sort(key=lambda rr: (float("inf") if (rr["|Î”margin|"] != rr["|Î”margin|"]) else -rr["|Î”margin|"]))
                                                        return out_rows

                                                    actions_help = _mk_actions(help_rows, action_when_positive="increase")
                                                    actions_hurt = []
                                                    # For hurting knobs, the helpful direction is to decrease the knob by Î”x.
                                                    for r in hurt_rows[:5]:
                                                        dmdx = float(r.get("dmargin_per_unit", float("nan")))
                                                        dx = float(r.get("step", float("nan")))
                                                        if not (dmdx == dmdx and dx == dx):
                                                            continue
                                                        delta_if_decrease = (-dmdx) * dx  # decreasing knob increases margin when dmdx<0
                                                        actions_hurt.append(
                                                            {
                                                                "knob": str(r.get("knob", "")),
                                                                "action": "decrease",
                                                                "Î”x": dx,
                                                                "Î”margin @ Î”x": delta_if_decrease,
                                                                "|Î”margin|": abs(delta_if_decrease),
                                                            }
                                                        )
                                                    actions_hurt.sort(key=lambda rr: (float("inf") if (rr["|Î”margin|"] != rr["|Î”margin|"]) else -rr["|Î”margin|"]))

                                                    cA, cB = st.columns([1, 1])
                                                    with cA:
                                                        st.markdown("**Increase-margin levers**")
                                                        if actions_help:
                                                            st.dataframe(
                                                                actions_help,
                                                                use_container_width=True,
                                                                hide_index=True,
                                                                column_config={
                                                                    "knob": st.column_config.TextColumn("Knob"),
                                                                    "action": st.column_config.TextColumn("Direction"),
                                                                    "Î”x": st.column_config.NumberColumn("Î”x", format="%.4g"),
                                                                    "Î”margin @ Î”x": st.column_config.NumberColumn("Î”margin", format="%.4g"),
                                                                    "|Î”margin|": st.column_config.NumberColumn("|Î”margin|", format="%.4g"),
                                                                },
                                                            )
                                                        else:
                                                            st.info("No positive-slope levers found for this dominant blocker at the current probe set.")
                                                    with cB:
                                                        st.markdown("**Avoid/regression levers**")
                                                        st.caption("These knobs locally *decrease* the dominant margin when increased; decreasing them helps.")
                                                        if actions_hurt:
                                                            st.dataframe(
                                                                actions_hurt,
                                                                use_container_width=True,
                                                                hide_index=True,
                                                                column_config={
                                                                    "knob": st.column_config.TextColumn("Knob"),
                                                                    "action": st.column_config.TextColumn("Direction"),
                                                                    "Î”x": st.column_config.NumberColumn("Î”x", format="%.4g"),
                                                                    "Î”margin @ Î”x": st.column_config.NumberColumn("Î”margin", format="%.4g"),
                                                                    "|Î”margin|": st.column_config.NumberColumn("|Î”margin|", format="%.4g"),
                                                                },
                                                            )
                                                        else:
                                                            st.info("No negative-slope (regression) levers found for this dominant blocker at the current probe set.")
                                            except Exception:
                                                # Forensics UI must never crash the Point Designer.
                                                pass

                                            # --- Optional raw dump (for audit)
                                            if st.toggle("Show raw forensics JSON", value=False, key="pd_forensics_raw_toggle"):
                                                st.json(ff)

                                st.divider()

                                # --- Capability badge (compact, non-expanding strip) ---
                                # Read-only, reviewer-friendly summary of which physics blocks are actually active.
                                try:
                                    _prof_model = str(out.get("profile_model", "none"))
                                    _prof_on = bool(out.get("profile_mode", False))
                                    _bs = str(out.get("bootstrap_model", "proxy"))
                                    _rad_on = bool(out.get("include_radiation", False))
                                    _rad_model = str(out.get("radiation_model", "off"))
                                    _rad_db_used = str(out.get("radiation_db_id_used", out.get("radiation_db", "")))
                                    _mag_tech = str(out.get("magnet_technology", "unknown"))
                                    _plant_on = bool(np.isfinite(float(out.get("P_net_e_MW", float("nan")))))

                                    _badge = (
                                        f"**Capability badge** - "
                                        f"Profiles: **{_prof_model}** ({'ON' if _prof_on else 'OFF'}) Â· "
                                        f"Bootstrap: **{_bs}** Â· "
                                        f"Radiation: **{'ON' if _rad_on else 'OFF'}**"
                                    )
                                    if _rad_on:
                                        _badge += f" ({_rad_model}{' Â· '+_rad_db_used if _rad_db_used else ''})"
                                    _badge += f" Â· Magnets: **{_mag_tech}** Â· Plant closure: **{'ON' if _plant_on else 'OFF'}**"
                                    st.caption(_badge + "  ")
                                    st.caption("See: More â†’ Assumptions Ledger â†’ Physics Capability Matrix.")
                                except Exception:
                                    pass

                                # --- Magnet Card (compact, reviewer-safe) ---
                                tech = str(out.get("magnet_technology", "unknown"))
                                tf_sc = float(out.get("tf_sc_flag", float("nan")))
                                sc_margin = out.get("sc_margin", out.get("hts_margin", float("nan")))
                                p_tf_ohm = out.get("P_tf_ohmic_MW", float("nan"))

                                # Policy note: whether TF_SC is treated as blocking/diagnostic under the active intent
                                try:
                                    _policy = out.get("constraint_policy", {}) or {}
                                    _hb = set(_policy.get("hard_blocking", []) or [])
                                    _diag = set(_policy.get("diagnostic_only", []) or [])
                                    if "TF_SC" in _hb:
                                        tf_note = "Blocking (reactor covenant)"
                                    elif "TF_SC" in _diag:
                                        tf_note = "Diagnostic (research)"
                                    else:
                                        tf_note = "(not used)"
                                except Exception:
                                    tf_note = ""

                                # Render as a small, no-scroll card
                                st.markdown("#### ðŸ§² Magnet Card")
                                c1, c2, c3, c4 = st.columns([1, 1, 1, 1])
                                c1.metric("TF technology", tech)
                                c2.metric("TF superconducting", "YES" if (tf_sc == 1.0) else ("NO" if (tf_sc == 0.0) else "n/a"))
                                if tf_sc == 1.0:
                                    c3.metric("SC margin", f"{float(sc_margin):.3f}" if sc_margin == sc_margin else "n/a")
                                    c4.metric("Tcoil [K]", f"{float(out.get('Tcoil_K', float('nan'))):.1f}" if out.get('Tcoil_K', float('nan')) == out.get('Tcoil_K', float('nan')) else "n/a")
                                else:
                                    c3.metric("TF ohmic [MW]", f"{float(p_tf_ohm):.2f}" if p_tf_ohm == p_tf_ohm else "n/a")
                                    c4.metric("Tcoil [K]", f"{float(out.get('Tcoil_K', float('nan'))):.1f}" if out.get('Tcoil_K', float('nan')) == out.get('Tcoil_K', float('nan')) else "n/a")
                                if tf_note:
                                    st.caption(f"TF_SC policy: {tf_note}")

                        
                                # --- Model activation note (prevents 'I changed a model but nothing changed' confusion) ---
                                bs_mode = str(out.get("bootstrap_model", "proxy"))
                                prof_model = str(out.get("profile_model", "none"))
                                prof_on = bool(out.get("profile_mode", False))
                                rad_on = bool(out.get("include_radiation", False))
                                rad_mode = str(out.get("radiation_model", "off"))
                                rad_db = str(out.get("radiation_db_id_used", out.get("radiation_db", "")))

                                notes = []
                                notes.append(f"Bootstrap: **{bs_mode}**")
                                notes.append(f"Profiles: **{prof_model}** ({'ON' if prof_on else 'OFF'})")
                                notes.append(f"Radiation: **{'ON' if rad_on else 'OFF'}**" + (f" ({rad_mode}{' Â· '+rad_db if rad_on and rad_db else ''})" if rad_on else ""))

                                # Sauter proxy becomes profile-sensitive; if profiles are OFF, changes may be minimal.
                                if bs_mode.lower() in {"sauter", "sauter_proxy"} and not prof_on:
                                    st.caption("Model activation: Sauter bootstrap is enabled, but analytic profiles are OFF - the result reduces to a bounded global proxy, so the operating point may not change materially.")
                                if rad_on and (rad_db in {"builtin_proxy", "", "proxy_v1"}):
                                    st.caption("Radiation provenance: using built-in proxy Lz tables. For authoritative radiation, provide a RADAS/OpenADAS-derived database (see 'Radiation DB' model card).")

                                st.caption(" Â· ".join(notes))

                                st.divider()
                                # Keep legacy KPI row, but make it tech-aware (avoid misleading 'HTS' wording)
                                m_lbl = "SC margin" if tf_sc == 1.0 else "TF ohmic [MW]"
                                m_val = (f"{float(sc_margin):.3f}" if (tf_sc == 1.0 and sc_margin == sc_margin) else (f"{float(p_tf_ohm):.2f}" if p_tf_ohm == p_tf_ohm else "n/a"))
                                kpi_row([
                                    (m_lbl, m_val),
                                    ("Lifetime [yr]", f"{out.get('hts_lifetime_yr', float('nan')):.2f}"),
                                    ("Vdump [kV]", f"{out.get('V_dump_kV', float('nan')):.1f}"),
                                    ("P_net_e [MW]", f"{out.get('P_net_e_MW', float('nan')):.1f}"),
                                ])
    
                        # -----------------------------------------------------------------
                        # Expert transparency: assumptions, power ledger, sanity dashboard
                        # -----------------------------------------------------------------
                        def _pd_assumptions_snapshot() -> Dict[str, Any]:
                            """UI-level model/assumption snapshot for expert auditability (does not affect physics)."""
                            try:
                                return {
                                    "design_intent": str(st.session_state.get("design_intent", "Power Reactor (net-electric)")),
                                    "confinement_scaling_ref": str(confinement_scaling),
                                    "profile_model": str(profile_model),
                                    "profile_peaking_ne": float(profile_peaking_ne),
                                    "profile_peaking_T": float(profile_peaking_T),
                                    "bootstrap_model": str(bootstrap_model),
                                    "include_radiation": bool(include_radiation),
                                    "radiation_model": str(radiation_model) if include_radiation else "disabled",
                                    "radiation_db": str(radiation_db) if include_radiation else "proxy_v1",
                                    "include_synchrotron": bool(include_synchrotron) if include_radiation else False,
                                    "Zeff_mode": str(zeff_mode) if include_radiation else "disabled",
                                    "Zeff": float(Zeff) if include_radiation else float("nan"),
                                    "dilution_fuel": float(dilution_fuel) if include_radiation else float("nan"),
                                    "f_rad_core": float(f_rad_core) if include_radiation else float("nan"),
                                    "impurity_species": str(impurity_species) if include_radiation else "disabled",
                                    "impurity_frac": float(impurity_frac) if include_radiation else float("nan"),
                                    "include_alpha_loss": bool(include_alpha_loss),
                                    "alpha_loss_model": str(alpha_loss_model) if include_alpha_loss else "disabled",
                                    "include_hmode_physics": bool(include_hmode_physics),
                                    "require_Hmode": bool(require_Hmode) if include_hmode_physics else False,
                                    "PLH_margin": float(PLH_margin) if include_hmode_physics else float("nan"),
                                    "use_lambda_q": bool(use_lambda_q),
                                    "particle_balance_enabled": bool(include_particle_balance) if 'include_particle_balance' in locals() else False,
                                    "ash_dilution_mode": str(ash_dilution_mode) if 'ash_dilution_mode' in locals() else "default",
                                    "fuel_mode": str(fuel_mode),
                                }
                            except Exception:
                                return {"design_intent": str(st.session_state.get("design_intent", "Power Reactor (net-electric)"))}

                        if _pd_tel_view == "âš¡ Mission Snapshot":
                            with st.expander("ðŸ”Ž Model Scope & Assumptions", expanded=False):
                                st.caption("Integrated view of what is *authoritative* vs *proxy* in this 0â€‘D point. Nothing here changes physics; it documents it.")
                                st.markdown("""**Badges:**  
        - **Authoritative** = used directly in feasibility/constraints  
        - **Proxy** = approximate model (informative unless explicitly constrained)  
        - **Diagnostic** = nonâ€‘blocking checks and tags""")

                                # User-request: keep these collapsed by default.
                                st.markdown("**Assumptions snapshot (UI-level):**")
                                st.json(_pd_assumptions_snapshot(), expanded=False)

                                try:
                                    mc = out.get("model_cards", {})
                                    if isinstance(mc, dict) and mc:
                                        st.markdown("**Model cards (provenance):**")
                                        st.json(mc, expanded=False)
                                except Exception:
                                    pass

                        if _pd_tel_view == "ðŸ“š Ledgers":
                            with st.expander("âš™ï¸ Power Ledger - Closure Table", expanded=False):
                                st.caption("Transparent Pin/Pout bookkeeping at this point (0â€‘D proxies).")
                                try:
                                    rows = []
                                    def _add(lbl, key, badge):
                                        val = out.get(key, float("nan"))
                                        rows.append({"Item": lbl, "Key": key, "MW": val, "Type": badge})
                                    _add("Input power Pin", "Pin_MW", "Authoritative")
                                    _add("Aux heating", "Paux_MW", "Authoritative")
                                    _add("Ohmic", "Pohm_MW", "Proxy")
                                    _add("Fusion alpha (generated)", "Palpha_MW", "Authoritative")
                                    _add("Core radiation", "Prad_core_MW", "Proxy" if bool(include_radiation) else "Diagnostic")
                                    _add("SOL/Separatrix power", "P_SOL_MW", "Authoritative")
                                    _add("Total loss Ploss", "Ploss_MW", "Authoritative")
                                    _add("Net electric", "P_net_e_MW", "Proxy")
                                    dfp = pd.DataFrame(rows)
                                    st.dataframe(dfp, hide_index=True, use_container_width=True)
                                    try:
                                        pin = float(out.get("Pin_MW", float("nan")))
                                        ploss = float(out.get("Ploss_MW", float("nan")))
                                        if np.isfinite(pin) and np.isfinite(ploss):
                                            st.metric("Closure check: Pin âˆ’ Ploss (MW)", f"{(pin - ploss):.3g}")
                                    except Exception:
                                        pass
                                except Exception:
                                    st.info("Power ledger unavailable (missing keys).")

                        if _pd_tel_view == "ðŸŽ¯ Dominance & Closures":
                            st.subheader("ðŸŽ¯ Dominance & Closures")
                            st.caption("Read-only decision telemetry: what limits this point, and how the closure converged. No physics is modified.")

                            art0 = st.session_state.get("pd_last_artifact") or {}
                            cons0 = (art0.get("constraints") or []) if isinstance(art0, dict) else []
                            led0 = (art0.get("constraint_ledger") or {}) if isinstance(art0, dict) else {}
                            solver0 = (art0.get("solver") or {}) if isinstance(art0, dict) else {}

                            t_dom, t_closure = st.tabs(["ðŸŽ¯ Dominance Compass", "ðŸ§® Closure Trace"])

                            with t_dom:
                                # Dominant violated constraints (intent-agnostic; policy lens is applied elsewhere)
                                top = (led0.get("top_blockers") or []) if isinstance(led0, dict) else []
                                if isinstance(top, list) and top:
                                    st.markdown("**Dominant violated constraints (hard-weighted)**")
                                    rows = []
                                    for e in top[:12]:
                                        if not isinstance(e, dict):
                                            continue
                                        rows.append({
                                            "rank": int(e.get("dominance_rank") or 0) if e.get("dominance_rank") is not None else None,
                                            "name": e.get("name"),
                                            "group": e.get("group"),
                                            "severity": e.get("severity"),
                                            "margin_frac": e.get("margin_frac"),
                                            "value": e.get("value"),
                                            "limit": e.get("limit"),
                                            "units": e.get("units"),
                                            "violation_score": e.get("violation_score"),
                                        })
                                    try:
                                        import pandas as _pd
                                        st.dataframe(_pd.DataFrame(rows), use_container_width=True, hide_index=True, height=320)
                                    except Exception:
                                        st.json(rows, expanded=False)
                                else:
                                    st.info("No violated hard constraints detected â†’ this point is hard-feasible under the frozen evaluator.")

                                # Tightest (active) hard constraints, even if PASS
                                try:
                                    hard = [c for c in (cons0 or []) if isinstance(c, dict) and str(c.get("severity","hard")).lower()=="hard"]
                                    def _mf(c):
                                        try:
                                            return float(c.get("margin_frac"))
                                        except Exception:
                                            return float("nan")
                                    hard2 = [c for c in hard if np.isfinite(_mf(c))]
                                    hard2.sort(key=lambda c: float(_mf(c)))
                                    if hard2:
                                        st.markdown("**Tightest hard constraints (active set, worst-first)**")
                                        rows2 = []
                                        for c in hard2[:12]:
                                            rows2.append({
                                                "name": c.get("name"),
                                                "passed": bool(c.get("passed", True)),
                                                "margin_frac": float(_mf(c)),
                                                "value": c.get("value"),
                                                "limit": c.get("limit"),
                                                "units": c.get("units",""),
                                                "group": c.get("group",""),
                                            })
                                        import pandas as _pd
                                        st.dataframe(_pd.DataFrame(rows2), use_container_width=True, hide_index=True, height=320)
                                except Exception:
                                    pass

                            with t_closure:
                                st.markdown("**Closure ledger (solver trace)**")
                                if isinstance(solver0, dict) and solver0.get("backend") and isinstance(solver0.get("trace"), list):
                                    tr = solver0.get("trace") or []
                                    rows = []
                                    for k, step in enumerate(tr[:200]):  # hard cap for UI stability
                                        if not isinstance(step, dict):
                                            continue
                                        row = {"iter": k}
                                        # common fields used by constraint_solver trace
                                        for kk in ["x","vars","residual_norm","target_errors","status","note","clamped","alpha","damping","trust_delta"]:
                                            if kk in step:
                                                row[kk] = step.get(kk)
                                        rows.append(row)
                                    try:
                                        import pandas as _pd
                                        st.dataframe(_pd.DataFrame(rows), use_container_width=True, hide_index=True, height=420)
                                    except Exception:
                                        st.json(rows[:50], expanded=False)
                                    st.caption(f"backend={solver0.get('backend')} â€¢ ok={solver0.get('ok')} â€¢ iters={solver0.get('iters')} â€¢ message={solver0.get('message')}")
                                else:
                                    st.info("No solver trace available for this run (e.g., direct evaluation without target solve, or legacy fallback path).")

                        if _pd_tel_view == "ðŸ›° Control Contracts":
                            st.subheader("ðŸ›° Control Contracts")
                            st.caption("Envelope-based, deterministic control feasibility. Computes requirements only; does not modify physics. Disabled by default.")

                            # Enabled?
                            try:
                                _enabled = bool(inputs_dict.get("include_control_contracts", False)) if isinstance(inputs_dict, dict) else False
                            except Exception:
                                _enabled = False

                            if not _enabled:
                                st.info("Control contracts are OFF for this run. Enable 'include_control_contracts' in inputs to compute envelopes and optional caps.")
                            else:


                                # v227.0: authority tags + control budget ledger (read-only)
                                auth = out.get("control_contracts_authority")
                                budg = out.get("control_budget_ledger")
                                cA, cB = st.columns([1,2])
                                with cA:
                                    st.markdown("**Authority tags**")
                                    if isinstance(auth, dict) and auth:
                                        st.json(auth, expanded=False)
                                    else:
                                        st.caption("Authority tags not available.")
                                with cB:
                                    st.markdown("**Control budget ledger**")
                                    if isinstance(budg, dict) and budg:
                                        try:
                                            import pandas as pd
                                            rows = [{"key": k, "value": v} for k, v in budg.items()]
                                            st.dataframe(pd.DataFrame(rows), use_container_width=True, hide_index=True, height=220)
                                        except Exception:
                                            st.json(budg, expanded=False)
                                    else:
                                        st.caption("No budget ledger available.")

                                t_vs, t_pf, t_sol, t_rwm = st.tabs(["ðŸ§² VS Control", "ðŸ“‰ PF Envelope", "ðŸŒ« SOL Control", "ðŸŒ€ RWM (MHD)"])

                                with t_vs:
                                    c1, c2, c3 = st.columns(3)
                                    c1.metric("Ï„_VS (s)", _fmt(out.get("tau_VS_s")))
                                    c2.metric("Î³_VS (1/s)", _fmt(out.get("gamma_VS_s_inv")))
                                    c3.metric("BW required (Hz)", _fmt(out.get("vs_bandwidth_req_Hz")))
                                    st.caption("Proxy mapping: vs_margin â†’ Ï„_VS via vs_tau_nominal_s; BW â‰ˆ vs_bw_factorÂ·Î³/(2Ï€).")
                                    st.markdown("**Caps (optional)**")
                                    d = {
                                        "bw_req_Hz": out.get("vs_bandwidth_req_Hz"),
                                        "bw_max_Hz": out.get("vs_bandwidth_max_Hz"),
                                        "P_req_MW": out.get("vs_control_power_req_MW"),
                                        "P_max_MW": out.get("vs_control_power_max_MW"),
                                        "ok": out.get("vs_control_ok"),
                                    }
                                    st.dataframe(pd.DataFrame([d]), use_container_width=True, hide_index=True)
                                    m = out.get("control_contract_margins")
                                    if isinstance(m, dict) and m:
                                        st.markdown("**Signed margins (cap - required)**")
                                        try:
                                            mm = {"bw_margin_Hz": m.get("vs_bandwidth_margin_Hz"), "P_margin_MW": m.get("vs_control_power_margin_MW")}
                                            st.dataframe(pd.DataFrame([mm]), use_container_width=True, hide_index=True)
                                        except Exception:
                                            st.json(m, expanded=False)

                                with t_pf:
                                    c1, c2, c3, c4 = st.columns(4)
                                    c1.metric("I_peak (MA)", _fmt(out.get("pf_I_peak_MA")))
                                    c2.metric("dI/dt_peak (MA/s)", _fmt(out.get("pf_dIdt_peak_MA_s")))
                                    c3.metric("V_peak (V)", _fmt(out.get("pf_V_peak_V")))
                                    c4.metric("P_peak (MW)", _fmt(out.get("pf_P_peak_MW")))
                                    st.metric("Pulse energy proxy (MJ)", _fmt(out.get("pf_E_pulse_MJ")))
                                    # CS/Volt-seconds bookkeeping (v298.0)
                                    st.markdown("**CS / Volt-seconds (pulsed) bookkeeping**")
                                    _cs_row = {
                                        "cs_flux_required_Wb": out.get("cs_flux_required_Wb"),
                                        "cs_flux_available_Wb": out.get("cs_flux_available_Wb"),
                                        "cs_flux_margin": out.get("cs_flux_margin"),
                                        "V_loop_ramp_V": out.get("cs_V_loop_ramp_V"),
                                        "V_loop_max_V": out.get("cs_V_loop_max_V"),
                                    }
                                    st.dataframe(pd.DataFrame([_cs_row]), use_container_width=True, hide_index=True)
                                    st.caption("Canonical rampâ€“flatâ€“ramp waveform; V â‰ˆ L_effÂ·dI/dt + R_effÂ·I. L_eff is inferred from CS flux requirement if not provided.")
                                    st.markdown("**Caps (optional)**")
                                    d2 = {
                                        "I_peak": out.get("pf_I_peak_MA"),
                                        "I_max": out.get("pf_I_peak_max_MA"),
                                        "V_peak": out.get("pf_V_peak_V"),
                                        "V_max": out.get("pf_V_peak_max_V"),
                                        "P_peak": out.get("pf_P_peak_MW"),
                                        "P_max": out.get("pf_P_peak_max_MW"),
                                        "dIdt": out.get("pf_dIdt_peak_MA_s"),
                                        "dIdt_max": out.get("pf_dIdt_max_MA_s"),
                                        "E_pulse": out.get("pf_E_pulse_MJ"),
                                        "E_max": out.get("pf_E_pulse_max_MJ"),
                                        "ok": out.get("pf_envelope_ok"),
                                    }
                                    st.dataframe(pd.DataFrame([d2]), use_container_width=True, hide_index=True)
                                    m = out.get("control_contract_margins")
                                    if isinstance(m, dict) and m:
                                        st.markdown("**Signed margins (cap - required)**")
                                        try:
                                            mm = {
                                                "I_margin_MA": m.get("pf_I_peak_margin_MA"),
                                                "dIdt_margin_MA_s": m.get("pf_dIdt_margin_MA_s"),
                                                "V_margin_V": m.get("pf_V_peak_margin_V"),
                                                "P_margin_MW": m.get("pf_P_peak_margin_MW"),
                                                "E_margin_MJ": m.get("pf_E_pulse_margin_MJ"),
                                            }
                                            st.dataframe(pd.DataFrame([mm]), use_container_width=True, hide_index=True)
                                        except Exception:
                                            st.json(m, expanded=False)
                                    wf = out.get("pf_waveform_decimated")
                                    if isinstance(wf, list) and wf:
                                        st.markdown("**Decimated waveform (t, I)**")
                                        st.dataframe(pd.DataFrame(wf), use_container_width=True, hide_index=True, height=220)

                                with t_sol:
                                    c1, c2, c3, c4 = st.columns(4)
                                    c1.metric("q_target", _fmt(out.get("q_div_target_MW_m2")))
                                    c2.metric("f_SOL+div required", _fmt(out.get("detachment_f_sol_div_required")))
                                    c3.metric("Prad_SOL+div required (MW)", _fmt(out.get("detachment_prad_sol_div_required_MW")))
                                    c4.metric("f_z required", _fmt(out.get("detachment_f_z_required")))
                                    st.caption(
                                        "Detachment authority is algebraic: q_div_target â†’ required SOL+div radiation â†’ implied impurity fraction using an Lz(T_SOL) envelope. "
                                        "It does not change the operating point unless you enforce caps."
                                    )
                                    m = out.get("control_contract_margins")
                                    if isinstance(m, dict) and m and (m.get("f_rad_SOL_margin") is not None):
                                        st.markdown("**Signed margin (cap - required)**")
                                        st.write({"f_rad_SOL_margin": m.get("f_rad_SOL_margin")})

                                with t_rwm:
                                    _rwm_enabled = bool(inputs_dict.get("include_rwm_screening", False)) if isinstance(inputs_dict, dict) else False
                                    if not _rwm_enabled:
                                        st.info("RWM screening is OFF for this run. Enable 'include_rwm_screening' to compute PROCESS-class RWM control requirements.")
                                    else:
                                        c1, c2, c3, c4 = st.columns(4)
                                        c1.metric("Regime", str(out.get("rwm_regime", "")))
                                        c2.metric("Î²N_NW", _fmt(out.get("rwm_betaN_no_wall")))
                                        c3.metric("Î²N_IW", _fmt(out.get("rwm_betaN_ideal_wall")))
                                        c4.metric("Ï‡", _fmt(out.get("rwm_chi")))
                                        c5, c6, c7 = st.columns(3)
                                        c5.metric("Ï„_w (s)", _fmt(out.get("rwm_tau_w_s")))
                                        c6.metric("BW required (Hz)", _fmt(out.get("rwm_bandwidth_req_Hz")))
                                        c7.metric("P required (MW)", _fmt(out.get("rwm_control_power_req_MW")))
                                        st.caption("Screening: Î²N between no-wall and ideal-wall limits implies an active RWM requiring feedback. Exceeding Î²N_IW is flagged as non-operable.")

                                        st.markdown("**Caps (optional; default to VS caps if not provided)**")
                                        d3 = {
                                            "bw_req_Hz": out.get("rwm_bandwidth_req_Hz"),
                                            "bw_max_Hz": out.get("rwm_bandwidth_max_Hz"),
                                            "P_req_MW": out.get("rwm_control_power_req_MW"),
                                            "P_max_MW": out.get("rwm_control_power_max_MW"),
                                            "ok": out.get("rwm_control_ok"),
                                        }
                                        st.dataframe(pd.DataFrame([d3]), use_container_width=True, hide_index=True)
                                        m = out.get("control_contract_margins")
                                        if isinstance(m, dict) and m:
                                            st.markdown("**Signed margins (cap - required)**")
                                            st.dataframe(pd.DataFrame([{
                                                "bw_margin_Hz": m.get("rwm_bandwidth_margin_Hz"),
                                                "P_margin_MW": m.get("rwm_control_power_margin_MW"),
                                            }]), use_container_width=True, hide_index=True)
    
                        if _pd_tel_view == "âš¡ Mission Snapshot":
                            with st.expander("ðŸ§­ Regime Compass - Sanity Dashboard", expanded=False):
                                st.caption("Expert quick-check panel. Values are diagnostic unless explicitly constrained.")
                                try:
                                    # Optional uncertainty bands for proxy quantities (nice-to-have; UI-only)
                                    show_unc = st.checkbox("Show proxy uncertainty bands (diagnostic)", value=False, key="pd_show_proxy_unc")
                                    unc_proxy = float(st.session_state.get("pd_unc_proxy_frac", 0.15))
                                    unc_neut = float(st.session_state.get("pd_unc_neut_frac", 0.20))
                                    if show_unc:
                                        c1, c2 = st.columns(2)
                                        unc_proxy = c1.slider("Proxy Â±%", min_value=0.0, max_value=0.50, value=float(unc_proxy), step=0.01, key="pd_unc_proxy_frac")
                                        unc_neut = c2.slider("Neutronics/SOL proxy Â±%", min_value=0.0, max_value=0.50, value=float(unc_neut), step=0.01, key="pd_unc_neut_frac")
    
                                    # Typical-range flags (non-blocking)
                                    typical = {
                                        "rho_star": (1e-4, 3e-2),
                                        "H98": (0.7, 1.5),
                                        "fG": (0.2, 1.2),
                                        "nGW": (0.1, 2.0),
                                        "betaN_proxy": (0.5, 4.0),
                                        'q95_proxy': (2.5, 6.0),
                                        "P_SOL_over_R_MW_m": (0.0, 50.0),
                                        "f_bs_proxy": (0.0, 1.0),
                                        "ne20": (0.0, 3.0),
                                        "Zeff": (1.0, 3.0),
                                        "lambda_q_mm": (0.1, 10.0),
                                        "q_div_MW_m2": (0.0, 50.0),
                                        "P_CD_MW": (0.0, 300.0),
                                        "eta_CD_A_W": (0.0, 5e-6),
                                        "TBR": (0.7, 1.4),
                                        "B_peak_T": (0.0, 30.0),
                                    }
    
                                    rows_s = [
                                        ("Ï*", "rho_star", "â€“", "Diagnostic"),
                                        ("H98", "H98", "â€“", "Authoritative"),
                                        ("fG", "fG", "â€“", "Authoritative"),
                                        ("nGW", "nGW", "Ã—1e20 mâ»Â³", "Diagnostic"),
                                        ("Î²N", "betaN_proxy", "â€“", "Proxy"),
                                        ("q95", 'q95_proxy', "â€“", "Authoritative"),
                                        ("P_SOL/R", "P_SOL_over_R_MW_m", "MW/m", "Authoritative"),
                                        ("Bootstrap f_bs", "f_bs_proxy", "â€“", "Proxy"),
                                        ("nÌ„e", "ne20", "Ã—1e20 mâ»Â³", "Authoritative"),
                                        ("Z_eff", "Zeff", "â€“", "Proxy" if bool(include_radiation) else "Diagnostic"),
                                        ("Î»q", "lambda_q_mm", "mm", "Proxy" if bool(use_lambda_q) else "Diagnostic"),
                                        ("q_div", "q_div_MW_m2", "MW/mÂ²", "Authoritative"),
                                        ("P_CD", "P_CD_MW", "MW", "Proxy"),
                                        ("Î·_CD", "eta_CD_A_W", "A/W", "Proxy"),
                                        ("TBR", "TBR", "â€“", "Proxy"),
                                        ("B_peak", "B_peak_T", "T", "Authoritative"),
                                    ]
    
                                    data = []
                                    # NOTE: do not name the loop variable `badge` here.
                                    # This file also defines a `badge()` helper function.
                                    # Using `badge` as a top-level loop variable would shadow the
                                    # function and later calls like `badge(c)` would crash with:
                                    #   TypeError: 'str' object is not callable
                                    for label, key, unit, badge_type in rows_s:
                                        try:
                                            v = out.get(key, float("nan"))
                                        except Exception:
                                            v = float("nan")
                                        lo, hi = typical.get(key, (float("nan"), float("nan")))
                                        flag = ""
                                        try:
                                            if np.isfinite(v) and np.isfinite(lo) and np.isfinite(hi):
                                                if v < lo:
                                                    flag = "LOW"
                                                elif v > hi:
                                                    flag = "HIGH"
                                        except Exception:
                                            pass
                                        unc = ""
                                        if show_unc and (badge_type == "Proxy"):
                                            frac = unc_proxy
                                            if key in ("TBR", "lambda_q_mm"):
                                                frac = unc_neut
                                            try:
                                                if np.isfinite(v):
                                                    unc = f"Â±{(100*frac):.0f}%"
                                            except Exception:
                                                pass
                                        data.append({"Metric": label, "Key": key, "Value": v, "Units": unit, "Type": badge_type, "Typical": f"{lo:g}â€“{hi:g}" if np.isfinite(lo) and np.isfinite(hi) else "", "Flag": flag, "Unc": unc})
    
                                    dfs = pd.DataFrame(data)
                                    st.dataframe(dfs, hide_index=True, use_container_width=True)
                                    st.caption("Flags are non-blocking and intended as expert context. Typical ranges are heuristic defaults.")
                                except Exception:
                                    st.info("Sanity dashboard unavailable (missing keys).")
    
                        if _pd_tel_view == " Sensitivity Lab":
                            with st.expander(" Perturbation Probe (Â±10%)", expanded=False):
                                st.caption("Perturb key inputs by Â±10% and report which hard constraints flip. This is local intuition, not optimization.")
                                if st.button("Run Â±10% perturbation scan", use_container_width=True, key="pd_run_pert_scan"):
                                    try:
                                        # Use the solved point if available; fall back to base
                                        try:
                                            pi0 = sol_inp if sol_inp is not None else base
                                        except Exception:
                                            pi0 = base
                                        base_out = hot_ion_point(pi0, Paux_for_Q_MW=Paux_for_Q)
                                        base_failed = [c.name for c in (evaluate_constraints(base_out) or []) if (getattr(c, 'severity', 'hard') == 'hard') and (not bool(getattr(c,'passed', False)))]
                                        keys = ["R0_m","a_m","kappa","Bt_T","Ip_MA","fG","Ti_keV","Paux_MW"]
                                        rows = []
                                        for k in keys:
                                            if not hasattr(pi0, k):
                                                continue
                                            x0 = float(getattr(pi0, k))
                                            if not np.isfinite(x0) or x0 == 0.0:
                                                continue
                                            for fac in (0.9, 1.1):
                                                # PointInputs is a frozen dataclass; use dataclasses.replace
                                                # instead of setattr.
                                                try:
                                                    from dataclasses import replace as _dc_replace
                                                    pi = _dc_replace(pi0, **{k: x0 * fac})
                                                except Exception:
                                                    # Fallback for older input types
                                                    pi = make_point_inputs(**dict(getattr(pi0, '__dict__', {})))
                                                    try:
                                                        setattr(pi, k, x0 * fac)
                                                    except Exception:
                                                        pass
                                                y = hot_ion_point(pi, Paux_for_Q_MW=Paux_for_Q)
                                                failed = [c.name for c in (evaluate_constraints(y) or []) if (getattr(c, 'severity', 'hard') == 'hard') and (not bool(getattr(c,'passed', False)))]
                                                rows.append({
                                                    "param": k,
                                                    "factor": fac,
                                                    "value": x0 * fac,
                                                    "hard_failed": ", ".join(failed),
                                                    "new_failures": ", ".join(sorted(set(failed) - set(base_failed))),
                                                    "resolved": ", ".join(sorted(set(base_failed) - set(failed))),
                                                })
                                        st.session_state["pd_pert_scan_rows"] = rows
                                    except Exception as e:
                                        st.warning(f"Perturbation scan failed: {e}")
                                rows = st.session_state.get("pd_pert_scan_rows", [])
                                if rows:
                                    st.dataframe(pd.DataFrame(rows), use_container_width=True, hide_index=True)
                                else:
                                    st.caption("No scan results yet.")
    
    
    
    
                        # external systems codes-style local sensitivities (finite differences)

                        # -----------------------------------------------------------------
                        # Constraint dashboard (transparent (systems-code-inspired)) with margins + suggestions
                        # -----------------------------------------------------------------
                        if _pd_tel_view == "âš¡ Mission Snapshot":
                            with st.expander("ðŸ§± Constraint Radar - Pass/Fail & Margins", expanded=False):
                                if not constraints_list:
                                    st.info("No constraints evaluated (missing keys).")
                                else:
                                    rows_c = []
                                    for c in constraints_list:
                                        try:
                                            margin = float(getattr(c, "margin"))
                                        except Exception:
                                            margin = float("nan")
                                        rows_c.append({
                                            "constraint": c.name,
                                            "sense": c.sense,
                                            "value": c.value,
                                            "limit": c.limit,
                                            "units": c.units,
                                            "passed": bool(c.passed),
                                            "margin_frac": margin,
                                            "severity": getattr(c, "severity", "hard"),
                                            "note": c.note,
                                        })
                                    dfc = pd.DataFrame(rows_c)
                                    # sort: hard fails first, then smallest margin
                                    try:
                                        dfc = dfc.sort_values(by=["passed", "severity", "margin_frac"], ascending=[True, True, True])
                                    except Exception:
                                        pass
                                    st.dataframe(dfc, use_container_width=True)
    
                                    # --- Constraint provenance (expert trust) ---
                                    # Streamlit dataframes do not support per-row tooltips well; expose a focused detail view.
                                    _prov = {
                                        "q95": {"def": "Proxy q95 computed from geometry/Bt/Ip assumptions.", "drivers": "Ip, Bt, R0, a, Îº", "sense": ">=", "notes": "Always hard in both intents."},
                                        "q_div": {"def": "Divertor peak heat flux proxy from P_SOL and wetted area / Î»q model.", "drivers": "P_SOL, R0, Î»q, f_rad", "sense": "<=", "notes": "Definition depends on SOL-width toggle."},
                                        "P_SOL/R": {"def": "Separatrix power normalized by major radius.", "drivers": "P_SOL, R0", "sense": "<=", "notes": "Often used as a heat-exhaust severity proxy."},
                                        "sigma_vm": {"def": "Von Mises stress proxy in TF structure from peak field + build.", "drivers": "B_peak, coil build, R0", "sense": "<=", "notes": "Engineering screening, not a full FEA."},
                                        "HTS margin": {"def": "HTS current-density/temperature margin proxy.", "drivers": "B_peak, Top, Jop, conductor assumption", "sense": ">=", "notes": "Screening margin; label as proxy if conductor model simplified."},
                                        "TBR": {"def": "Tritium breeding ratio proxy from blanket/shield thickness + coverage assumptions.", "drivers": "t_blanket, t_shield, coverage", "sense": ">=", "notes": "Proxy unless driven by external neutronics."},
                                        "NWL": {"def": "Neutron wall loading proxy from fusion power and surface area.", "drivers": "Pfus, R0, a, Îº", "sense": "<=", "notes": "Screening metric."},
                                        "beta": {"def": "Beta or normalized beta proxy guardrail.", "drivers": "pressure, Bt, Ip", "sense": "<=", "notes": "Proxy stability screen."},
                                    }
    
                                    with st.expander("Constraint details (definitions + drivers)", expanded=False):
                                        names = [r["constraint"] for r in rows_c]
                                        pick = st.selectbox("Select a constraint", options=names, index=0, key="pd_pick_constraint")
                                        rec = next((r for r in rows_c if r["constraint"] == pick), None) or {}
                                        st.markdown(f"**{pick}**")
                                        st.write({"sense": rec.get("sense"), "value": rec.get("value"), "limit": rec.get("limit"), "units": rec.get("units"), "passed": rec.get("passed"), "margin_frac": rec.get("margin_frac"), "severity": rec.get("severity"), "note": rec.get("note")})
                                        # Best-effort provenance lookup by substring match
                                        key_l = str(pick).lower()
                                        prov = None
                                        for k, v in _prov.items():
                                            if k.lower() in key_l:
                                                prov = v
                                                break
                                        if prov:
                                            st.write({"definition": prov.get("def"), "drivers": prov.get("drivers"), "sense": prov.get("sense"), "notes": prov.get("notes")})
                                        else:
                                            st.caption("No additional provenance notes registered for this constraint yet.")
    
                                    failed = [r for r in rows_c if not r["passed"] and r.get("severity","hard") == "hard"]
                                    if failed:
                                        st.error(f"{len(failed)} hard constraint(s) failed. See suggestions below.")
                                    soft_failed = [r for r in rows_c if not r["passed"] and r.get("severity") == "soft"]
                                    if soft_failed:
                                        st.warning(f"{len(soft_failed)} soft constraint(s) failed (screening only).")
    
                                    # Dominant limiter (tightest hard margin) - plain language
                                    try:
                                        hard_rows = [r for r in rows_c if str(r.get("severity","hard")) == "hard"]
                                        hard_rows = [r for r in hard_rows if r.get("margin_frac") == r.get("margin_frac")]
                                        hard_rows_sorted = sorted(hard_rows, key=lambda r: float(r.get("margin_frac", float('inf'))))
                                        if hard_rows_sorted:
                                            dom = hard_rows_sorted[0]
                                            st.info(
                                                f"**Dominant limiter:** {dom.get('constraint')} (margin {float(dom.get('margin_frac')):.3g}). "
                                                f"This is the tightest hard constraint at this point."
                                            )
                                    except Exception:
                                        pass
    
                                    def _suggest(name: str) -> str:
                                        n = name.lower()
                                        if "q_div" in n or "p_sol" in n:
                                            return "Reduce P_SOL (increase radiation, reduce aux), increase R0, or increase lambda_q (design/multiplier)."
                                        if "hts" in n or "b_peak" in n or "sigma" in n:
                                            return "Reduce B_peak (increase coil build/R0, reduce Bt), reduce stress (increase thickness, reduce B_peak), or raise HTS margin (lower Top or improve conductor)."
                                        if "tbr" in n:
                                            return "Increase blanket/shield thickness or improve breeding/coverage assumptions."
                                        if "nwl" in n:
                                            return "Reduce fusion power density (increase size R0 or reduce performance targets) or improve shielding."
                                        if "beta" in n:
                                            return "Increase size R0 or reduce Ip/pressure (lower Ti or fG) to bring beta below limit."
                                        if "q95" in n:
                                            return "Increase q95 (reduce Ip or increase Bt/R0) for stability margin."
                                        if "fg" in n:
                                            return "Reduce density target (lower fG) or increase Ip to raise Greenwald limit."
                                        if "p_net" in n:
                                            return "Increase Pfus (within constraints), increase thermal efficiency, or reduce recirculating loads."
                                        if "t_flat" in n:
                                            return "Increase available flux swing (CS design), reduce loop voltage (improve resistivity/current profile), or allow lower Ip."
                                        return "Adjust major radius / field / current / aux power to recover feasibility."
    
                                    if failed or soft_failed:
                                        st.markdown("**Actionable suggestions (rule-of-thumb):**")
                                        for r in failed + soft_failed:
                                            st.write("- **{}**: {}".format(r["constraint"], _suggest(r["constraint"])))
    
                        # --- Compare to baseline (delta view) ---
                        if _pd_tel_view == " Sensitivity Lab":
                            with st.expander("ðŸ†š Delta View - Compare to Baseline", expanded=False):
                                st.caption("Set a baseline (e.g., preset or previous run) and view deltas for key KPIs and tightest constraints.")
                                if st.button("Set baseline = current point", key="pd_set_baseline", use_container_width=True):
                                    try:
                                        st.session_state["pd_baseline_artifact"] = st.session_state.get("pd_last_artifact")
                                    except Exception:
                                        pass
                                base_art = st.session_state.get("pd_baseline_artifact")
                                cur_art = st.session_state.get("pd_last_artifact")
                                if isinstance(base_art, dict) and isinstance(cur_art, dict):
                                    bo = base_art.get("outputs", {}) or {}
                                    co = cur_art.get("outputs", {}) or {}
                                    kpis = [
                                        ("Q_DT_eqv", "Q_DT_eqv", "â€“"),
                                        ("H98", "H98", "â€“"),
                                        ("P_net_e", "P_net_e_MW", "MW(e)"),
                                        ("q95", 'q95_proxy', "â€“"),
                                        ("betaN", "betaN_proxy", "â€“"),
                                        ("q_div", "q_div_MW_m2", "MW/mÂ²"),
                                        ("P_SOL", "P_SOL_MW", "MW"),
                                        ("TBR", "TBR", "â€“"),
                                    ]
                                    rows = []
                                    for label, key, unit in kpis:
                                        vb = bo.get(key, float("nan"))
                                        vc = co.get(key, float("nan"))
                                        dlt = float("nan")
                                        try:
                                            if np.isfinite(vb) and np.isfinite(vc):
                                                dlt = float(vc) - float(vb)
                                        except Exception:
                                            pass
                                        rows.append({"KPI": label, "baseline": vb, "current": vc, "delta": dlt, "unit": unit})
                                    st.dataframe(pd.DataFrame(rows), hide_index=True, use_container_width=True)
                                else:
                                    st.caption("No baseline set yet.")
                        if _pd_tel_view == " Sensitivity Lab":
                            with st.expander("ðŸ“ Local Sensitivities - Finite Difference", expanded=False):
                                st.caption("Local derivatives around the current point. Useful for design intuition; not a global optimization result.")
                                try:
                                    params = ["R0_m","a_m","kappa","B0_T","Ip_MA","fG","H98","eta_CD","n_neu_frac","Zeff"]
                                    outs = ["Q_DT_eqv","P_net_e_MW","betaN","q_div_MW_m2","B_peak_T"]
                                    def _eval(pi):
                                        return hot_ion_point(pi)
                                    sens = finite_difference_sensitivities(base, _eval, params=params, outputs=outs, rel_step=1e-3)
                                    # Show a compact table: normalized sensitivities (per 1% change), where possible
                                    rows = []
                                    for o in outs:
                                        base_y = float(sens.get("_base", {}).get(o, float("nan")))
                                        for p in params:
                                            if p not in sens.get(o, {}):
                                                continue
                                            dydx = float(sens[o][p])
                                            x0 = float(getattr(base, p)) if hasattr(base, p) and getattr(base, p) is not None else float("nan")
                                            # normalized: (dY/Y) / (dX/X)  = (dY/dX) * (X/Y)
                                            norm = float("nan")
                                            if x0 == x0 and base_y == base_y and x0 != 0.0 and base_y != 0.0:
                                                norm = dydx * (x0 / base_y)
                                            rows.append({"output": o, "param": p, "dY/dX": dydx, "elasticity": norm})
                                    if rows:
                                        df_s = pd.DataFrame(rows)
                                        st.dataframe(df_s.sort_values(["output","param"]), use_container_width=True)
                                    else:
                                        st.info("Sensitivities unavailable for this point (missing keys or non-finite outputs).")
    
                                except Exception as e:
                                    st.warning(f"Sensitivity calculation failed: {e}")
    
                        # --- Point summary (compact table) ---
                        st.markdown("### Point summary")
                        try:
                            _sum_rows = [
                                {"KPI":"Q_DT_eqv","value": out.get("Q_DT_eqv"), "unit":"-"},
                                {"KPI":"H98","value": out.get("H98"), "unit":"-"},
                                {"KPI":"H_scaling","value": out.get("H_scaling"), "unit":"-"},
                                {"KPI":"tauE_eff","value": out.get("tauE_eff_s"), "unit":"s"},
                                {"KPI":"Pfus_DT_adj","value": out.get("Pfus_DT_adj_MW"), "unit":"MW"},
                                {"KPI":"P_net_e","value": out.get("P_net_e_MW"), "unit":"MW(e)"},
                                {"KPI":"q95","value": out.get('q95_proxy'), "unit":"-"},
                                {"KPI":"betaN","value": out.get("betaN_proxy"), "unit":"-"},
                                {"KPI":"f_bs","value": out.get("f_bs_proxy"), "unit":"-"},
                                {"KPI":"q_div","value": out.get("q_div_MW_m2"), "unit":"MW/mÂ²"},
                                {"KPI":"sigma_vm","value": out.get("sigma_vm_MPa"), "unit":"MPa"},
                                {"KPI":"HTS margin","value": out.get("hts_margin"), "unit":"-"},
                                {"KPI":"TBR","value": out.get("TBR"), "unit":"-"},
                            ]
                            _df_sum = pd.DataFrame(_sum_rows)
                            st.dataframe(_df_sum, hide_index=True, use_container_width=True)
                        except Exception:
                            pass

                        # --- Run summary (copy/paste; publication-friendly) ---
                        with st.expander("Run summary (copy/paste)", expanded=False):
                            try:
                                rs = (st.session_state.get('pd_last_artifact', {}) or {}).get('run_summary', {}) or {}
                                tight = rs.get('tightest_hard_constraints', []) or []
                                lines = []
                                lines.append(f"Design intent: {str(st.session_state.get('design_intent',''))}")
                                try:
                                    ver = (REPO_ROOT / 'VERSION').read_text(encoding='utf-8').strip().splitlines()[0]
                                except Exception:
                                    ver = 'unknown'
                                lines.append(f"SHAMS version: {ver}")
                                h = rs.get('headline', {}) or {}
                                _q = float(h.get('Q_DT_eqv', float('nan')))
                                _h98 = float(h.get('H98', float('nan')))
                                _pnet = float(h.get('P_net_e_MW', float('nan')))
                                lines.append(f"Headline: Q={_q:.3g} | H98={_h98:.3g} | P_net_e={_pnet:.3g} MW(e)")
                                if rs.get('power_closure_MW') == rs.get('power_closure_MW'):
                                    lines.append(f"Power closure: Pinâˆ’Ploss = {rs.get('power_closure_MW'):.3g} MW")
                                if tight:
                                    lines.append("Tightest hard constraints:")
                                    for c in tight:
                                        lines.append(f"- {c.get('name')}: passed={c.get('passed')} margin={c.get('margin_frac'):.3g} ({c.get('sense')} {c.get('limit')} {c.get('units')})")
                                st.code("\n".join(lines), language="text")
                            except Exception as e:
                                st.caption(f"Run summary unavailable: {e}")

                        if _pd_tel_view == "ðŸ“ˆ Plot Deck":
                            with st.expander("ðŸ“ˆ Plot Deck", expanded=False):
                                st.markdown("#### Plot Deck - quick-look engineering visuals")
                                st.caption("Visuals are screening-level (0â€‘D proxies). No ranking; just visibility.")
                                try:
                                    import matplotlib.pyplot as _plt
                                except Exception:
                                    _plt = None

                                def _sf(x):
                                    try:
                                        return float(x)
                                    except Exception:
                                        return float('nan')

                                o = out if isinstance(locals().get('out'), dict) else (st.session_state.get('pd_last_outputs') or {})
                                a = st.session_state.get('pd_last_artifact') or {}

                                c1, c2 = st.columns(2)

                                with c1:
                                    if _plt is not None:
                                        Pfus = _sf(o.get('P_fus_MW', o.get('Pfus_MW', float('nan'))))
                                        Paux = _sf(o.get('Paux_MW', float('nan')))
                                        Prec = _sf(o.get('P_recirc_MW', o.get('P_e_recirc_MW', float('nan'))))
                                        Pnet = _sf(o.get('P_e_net_MW', o.get('P_net_e_MW', float('nan'))))
                                        vals, labels = [], []
                                        for lab, v in [('Fusion', Pfus), ('Aux', Paux), ('Recirc', Prec), ('Net', Pnet)]:
                                            if v == v:
                                                labels.append(lab)
                                                vals.append(v)
                                        if len(vals) >= 2:
                                            fig = _plt.figure()
                                            _plt.bar(labels, vals)
                                            _plt.ylabel('MW')
                                            _plt.title('Power Stack (screening)')
                                            st.pyplot(fig, use_container_width=True)
                                        else:
                                            st.caption('Power stack unavailable for this point.')
                                    else:
                                        st.caption('Matplotlib unavailable.')

                                with c2:
                                    tight = (a.get('run_summary') or {}).get('tightest_hard_constraints', []) if isinstance(a, dict) else []
                                    tight = [t for t in (tight or []) if isinstance(t, dict)]
                                    tight = sorted(tight, key=lambda t: _sf(t.get('margin_frac', float('inf'))))[:10]
                                    if _plt is not None and tight:
                                        names = [t.get('name','?') for t in tight][::-1]
                                        mfs = [_sf(t.get('margin_frac', float('nan'))) for t in tight][::-1]
                                        fig = _plt.figure()
                                        _plt.barh(names, mfs)
                                        _plt.axvline(1.0, linestyle='--')
                                        _plt.xlabel('Margin fraction (>=1 pass)')
                                        _plt.title('Tightest Hard Constraints')
                                        st.pyplot(fig, use_container_width=True)
                                    else:
                                        st.caption('Constraint margin plot unavailable (no run summary yet).')

                                c3, c4 = st.columns(2)
                                with c3:
                                    if _plt is not None:
                                        q95 = _sf(o.get('q95_proxy', o.get('q95', float('nan'))))
                                        betaN = _sf(o.get('betaN_proxy', o.get('betaN', float('nan'))))
                                        fG = _sf(o.get('fG', float('nan')))
                                        labs, ys = [], []
                                        for lab, v in [('q95', q95), ('Î²N', betaN), ('fG', fG)]:
                                            if v == v:
                                                labs.append(lab)
                                                ys.append(v)
                                        if ys:
                                            fig = _plt.figure()
                                            _plt.bar(labs, ys)
                                            _plt.title('Regime Dials (dimensionless)')
                                            st.pyplot(fig, use_container_width=True)
                                        else:
                                            st.caption('Regime dials unavailable.')

                                with c4:
                                    if _plt is not None:
                                        Bpk = _sf(o.get('B_peak_T', float('nan')))
                                        qdiv = _sf(o.get('q_div_MW_m2', float('nan')))
                                        nwl = _sf(o.get('NWL_MW_m2', o.get('nwl_MW_m2', float('nan'))))
                                        labs, vals = [], []
                                        for lab, v in [('Bpeak (T)', Bpk), ('qdiv (MW/mÂ²)', qdiv), ('NWL (MW/mÂ²)', nwl)]:
                                            if v == v:
                                                labs.append(lab)
                                                vals.append(v)
                                        if vals:
                                            fig = _plt.figure()
                                            _plt.bar(labs, vals)
                                            _plt.title('Engineering Severity (screening)')
                                            st.pyplot(fig, use_container_width=True)
                                        else:
                                            st.caption('Engineering severity plot unavailable.')
                                st.markdown("### Plot dashboard")
                                ptab1, ptab2, ptab3, ptab4 = st.tabs(["Power balance", "Stability & limits", "Geometry / build", "ðŸŒ€ Confinement"])
    
                                with ptab1:
                                    st.caption(
                                        "Quick visual breakdown of where power is going in this 0â€‘D point (all Phaseâ€‘1 proxies)."
                                    )
                                    power_vals = {
                                        "Paux [MW]": out.get("Paux_MW"),
                                        "Pfus (DT-eqv) [MW]": out.get("Pfus_DT_adj_MW"),
                                        "PÎ± dep [MW]": out.get("Palpha_dep_MW"),
                                        "Prad_core [MW]": out.get("Prad_core_MW"),
                                        "P_SOL [MW]": out.get("P_SOL_MW"),
                                        "P_net_e [MW]": out.get("P_net_e_MW"),
                                    }
                                    plot_bars(power_vals, "Power balance (MW)")
                                    with st.expander("Physical meaning (with literature)", expanded=False):
                                        st.markdown(
                                            """
            **Q (fusion gain proxy)** is defined as fusion power divided by auxiliary heating power (here the UI uses *Paux_for_Q* as the denominator).  
            **H98** is a confinement multiplier relative to the empirical **IPB98(y,2)** ELMy Hâ€‘mode scaling used as an ITER physics-basis reference. îˆ€citeîˆ‚turn1view0îˆ‚turn0search16îˆ

            **P_LH / Hâ€‘mode access** comparisons in this app follow the multiâ€‘machine ITPA threshold scaling (often referred to as â€œMartinâ€‘2008 / PLHâ€‘08â€). îˆ€citeîˆ‚turn3search18îˆ

            If you enable SOL-width physics, the appâ€™s Î»q proxy is motivated by the multiâ€‘machine Hâ€‘mode powerâ€‘falloff width scaling (Eichâ€‘2013). îˆ€citeîˆ‚turn2search3îˆ
                                        """
                                    )

                            with ptab2:
                                st.caption("Screening metrics vs common operational â€˜guardrailsâ€™ (Phaseâ€‘1 proxies).")
                                stab_vals = {
                                    "q95": out.get('q95_proxy'),
                                    "Î²N": out.get("betaN_proxy"),
                                    "f_bs": out.get("f_bs_proxy"),
                                }
                                plot_bars(stab_vals, "Stability / operational metrics")
                                with st.expander("Physical meaning (with literature)", expanded=False):
                                    st.markdown(
                                        """
            **q95** (safety factor near 95% flux) is a standard operational metric used as a proxy for MHD margin; lower q tends to reduce kink/tearing stability margin.

            **Normalized beta Î²N** is a widely used performance/stability figure of merit that scales pressure relative to magnetic field and current (often discussed in terms of the â€œTroyonâ€ aB/I scaling). îˆ€citeîˆ‚turn0search19îˆ

            **Bootstrap fraction f_bs** indicates how much of the plasma current is selfâ€‘driven by pressure gradients (important for steadyâ€‘state operation). This UI uses a simple proxy coefficient (C_bs) rather than a full neoclassical calculation.
                                        """
                                    )

                            with ptab3:
                                st.caption("A few geometry/build proxies that drive magnet and shield feasibility checks.")
                                geom_vals = {
                                    "R0 [m]": out.get("R0_m"),
                                    "a [m]": out.get("a_m"),
                                    "B0 [T]": out.get("Bt_T"),
                                    "Bpeak [T]": out.get("Bpeak_T"),
                                    "Ïƒ_hoop [MPa]": out.get("sigma_hoop_MPa"),
                                    "t_shield [m]": out.get("t_shield_m"),
                                }
                                plot_bars(geom_vals, "Key geometry/build scalars")
                                with st.expander("Physical meaning (with literature)", expanded=False):
                                    st.markdown(
                                        """
                **Greenwald fraction fG** (used internally by the solver) expresses density as a fraction of the empirical tokamak density limit scaling with I_p and minor radius (often called the Greenwald limit).

            The *radial build* and **Bpeak/B0** mapping are engineering proxies; theyâ€™re not meant to replace detailed coil/stress finiteâ€‘element analysis.
                                        """
                                    )

                            with ptab4:
                                st.caption("Energy confinement and empirical H-factor comparators.")
                                conf_vals = {
                                    "tauE_eff [s]": out.get("tauE_eff_s"),
                                    "tauE_scaling [s]": out.get("tauScaling_s") if "tauScaling_s" in out else out.get("tauIPB_s"),
                                    "H98": out.get("H98"),
                                    "H_scaling": out.get("H_scaling"),
                                    "H_required": out.get("H_required"),
                                    "tauE_env_min [s]": out.get("tauE_envelope_min_s"),
                                    "tauE_env_max [s]": out.get("tauE_envelope_max_s"),
                                    "transport_pass_opt": out.get("transport_pass_optimistic"),
                                    "transport_pass_rob": out.get("transport_pass_robust"),
                                    "power_balance_residual [MW]": out.get("power_balance_residual_MW"),
                                }
                                plot_bars(conf_vals, "Confinement / H metrics")
                                with st.expander("Notes", expanded=False):
                                    st.markdown("H98 is defined as tauE_eff / tauE_IPB98(y,2). H_scaling compares against the selected reference scaling. See also the IPB98(y,2) and ITER89-P scaling references.")

                            st.markdown("### Raw telemetry")
                            st.dataframe(pd.DataFrame([out]).T.rename(columns={0: "value"}), use_container_width=True)

        with tab_con:
            st.subheader("Constraint Briefing")
            st.markdown(f"**Design intent:** {st.session_state.get('design_intent', 'Power Reactor (net-electric)')}")
            _pol = _constraint_policy_snapshot()
            st.caption(
                "Policy: " + ("Reactor hard constraints enforced." if _pol.get("intent_key")=="reactor" else "Research intent: only q95 is blocking; engineering limits are diagnostic; TBR ignored.")
            )
            with st.expander("Constraint notebook", expanded=False):
                out = st.session_state.last_point_out
                if out is None:
                    st.info("Run **Evaluate Point** to see constraint checks.")
                else:
                    try:
                        _failed_hard = [str(c.name) for c in (evaluate_constraints(out) or []) if str(getattr(c,'severity','soft'))=='hard' and (not bool(getattr(c,'passed', False)))]
                    except Exception:
                        _failed_hard = []
                    _cls = _classify_failed_constraints(_failed_hard)
                    if _cls.get('blocking') or _cls.get('diagnostic') or _cls.get('ignored'):
                        st.markdown('**Intent-aware constraint summary**')
                        if _cls.get('blocking'):
                            st.markdown('**Blocking (per intent):** ' + ', '.join([f'`{x}`' for x in _cls.get('blocking')]))
                        if _cls.get('diagnostic'):
                            st.markdown('**Diagnostics:** ' + ', '.join([f'`{x}`' for x in _cls.get('diagnostic')]))
                        if _cls.get('ignored'):
                            st.markdown('**Ignored:** ' + ', '.join([f'`{x}`' for x in _cls.get('ignored')]))
                    checks = compute_checks(out)
                    for c in _dedupe_checks(checks):
                        with st.expander(f"{c.get('name', 'Check')}", expanded=False):
                            st.write(f"**{c['name']}** - {badge(c)}")
                            v = c.get("value")
                            lim = c.get("limit")
                            wl = c.get("warn_limit")
                            if isinstance(v, (int, float)) and isinstance(lim, (int, float)) and math.isfinite(v) and math.isfinite(lim):
                                if isinstance(wl, (int, float)) and math.isfinite(wl):
                                    st.caption(f"value={v:.4g}  warn={wl:.4g}  limit={lim:.4g}  ({c.get('sense','')})")
                                else:
                                    st.caption(f"value={v:.4g}  limit={lim:.4g}  ({c.get('sense','')})")
                            if c.get("notes"):
                                st.caption(c["notes"])
                            st.divider()

                    with st.expander("Check summary", expanded=False):
                        bad = top_violations(checks, 3)
                        if bad:
                            st.markdown("### Top violations")
                            for c in bad:
                                st.write(f"- **{c['name']}**: value={c.get('value')} vs limit={c.get('limit')}")
                        else:
                            st.success("All enabled checks passed for this point (per Phaseâ€‘1 proxy models).")


    # -----------------------------
    # Scan Lab
    # -----------------------------
    # -----------------------------
    # Systems Mode (transparent (systems-code-inspired) coupled targeting)
    # -----------------------------

    # --- v92: Stateful Results ---
    if _deck == "ðŸ§­ Point Designer":
        try:
            st.subheader("Stateful Results")
            s = _v92_state_get()
            c1, c2 = st.columns([1,3])
            with c1:
                if st.button("Clear point state", key="v92_clear_point_state"):
                    _v92_state_clear_point()
                    st.success("Cleared point state.")
                    st.stop()
            with c2:
                st.caption("Results persist across reruns/downloads. This panel renders from session state.")

            if s.has_point():
                if isinstance(s.last_point_radial_png, (bytes, bytearray)) and len(s.last_point_radial_png) > 0:
                    st.image(s.last_point_radial_png, caption="Radial build (stateful preview)", use_container_width=True)

                import json as _json
                st.download_button(
                    "Download run artifact JSON (stateful)",
                    data=_json.dumps(s.last_point_artifact, indent=2, sort_keys=True),
                    file_name="shams_run_artifact.json",
                    mime="application/json",
                    use_container_width=True,
                    key="v92_dl_artifact_stateful",
                )
                if isinstance(s.last_point_radial_png, (bytes, bytearray)) and len(s.last_point_radial_png) > 0:
                    st.download_button(
                        "Download radial build PNG (stateful)",
                        data=s.last_point_radial_png,
                        file_name="shams_radial_build.png",
                        mime="image/png",
                        use_container_width=True,
                        key="v92_dl_png_stateful",
                    )
            else:
                st.info("No stateful Point result yet. Click 'Evaluate Point' to compute one.")
        except Exception:
            pass

if _deck == "ðŸ§  Systems Mode":
    # DSG: auto edge-kind tagging by active panel (exploration only)
    if bool(st.session_state.get("dsg_edge_kind_auto", True)):
        st.session_state["dsg_context_edge_kind"] = "systems_eval"

    # Ensure core solver knobs exist in session state even before the user opens any controls.
    # This prevents unbound-name failures on first entry/reruns when downstream blocks execute.
    st.session_state.setdefault("systems_max_iter", 35)

    st.header("ðŸ§  Systems Mode")
    st.caption("Feasibility-first system explanation around the frozen Point Designer truth. Deterministic, audit-safe, no hidden solvers.")
    render_mode_scope("systems")

    # --- Systems Mode solver parameter discipline (UI stabilization Phase 1) ---
    # Never define solver knobs conditionally inside tabs/expanders; always read from session_state safely.
    tol: float = float(st.session_state.get("systems_tol", 1e-3))
    damping: float = float(st.session_state.get("systems_damping", 0.6))
    max_iter: int = int(st.session_state.get("systems_max_iter", 35))
    # Optional trust-region cap (scaled space); None means disabled.
    trust_delta = st.session_state.get("systems_trust_delta", None)
    if trust_delta is not None:
        try:
            trust_delta = float(trust_delta)
        except Exception:
            trust_delta = None


    st.info(
        "This is an **explanation layer** around the frozen evaluator: it may propose explicit, user-controlled *what would need to change* narratives, "
        "but it **never modifies** physics truth, constraints, or evaluator behavior.",
        icon="ðŸ§­",
    )

    st.caption("Apply actions (when used) are reversible (undo/redo).")
    st.session_state.setdefault("systems_run_cards", [])

    # Compact Cockpit moved to post-run diagnostics (v374.1)

    def _sys_get_in(d: Any, path: List[str]) -> Any:
        cur: Any = d
        for k in path:
            if not isinstance(cur, dict):
                return None
            cur = cur.get(k)
        return cur

    def _sys_pick_first(d: Any, paths: List[List[str]]) -> Any:
        for p in paths:
            v = _sys_get_in(d, p)
            if v is not None:
                return v
        return None

    def _sys_fmt(v: Any, *, digits: int = 3) -> str:
        try:
            if v is None:
                return "-"
            if isinstance(v, bool):
                return "true" if v else "false"
            if isinstance(v, (int, float)):
                if not math.isfinite(float(v)):
                    return "-"
                return f"{float(v):.{digits}g}"
            s = str(v)
            return s if s.strip() else "-"
        except Exception:
            return "-"

    def _sys_compact_next_action(verdict: str, dom: str, step: str) -> str:
        v = str(verdict or '').upper()
        stp = str(step or '')
        if v.startswith('PASS'):
            if 'Explore' in stp:
                return "Explore feasible neighborhood (Scan/Feasible Search)"
            return "Apply â†’ Compare â†’ Export dossier"
        # FAIL
        if 'Recover' in stp:
            return "Run Seeded Recovery (increase budget if needed)"
        if 'Diagnose' in stp:
            return f"Inspect dominant limiter: {dom or 'unknown'}"
        return "Run Diagnose â†’ then Recover"

    def _sys_render_compact_cockpit() -> None:
        """Render an above-the-fold summary bar for expert operators.

        Rules:
        - No auto-expanding panels.
        - Never throws.
        - Uses cached artifacts when available.
        """
        try:
            s = _v92_state_get()
            art = st.session_state.get('systems_last_solve_artifact')
            if not isinstance(art, dict):
                art = getattr(s, 'last_systems_result', None)
            if not isinstance(art, dict):
                art = getattr(s, 'last_point_artifact', None)
            if not isinstance(art, dict):
                return

            # Resolve top-level summary keys (schema-tolerant).
            verdict = _sys_pick_first(art, [
                ['verdict'], ['summary','verdict'], ['point','verdict'], ['result','verdict']
            ])
            dom = _sys_pick_first(art, [
                ['dominant_constraint'], ['summary','dominant_constraint'], ['ledger','dominant_hard'], ['decision','dominant_hard']
            ])
            mech = _sys_pick_first(art, [
                ['dominant_mechanism'], ['summary','dominant_mechanism'], ['ledger','dominant_mechanism']
            ])
            step = st.session_state.get('_pending_workflow_step') or _sys_pick_first(art, [['ui_state','workflow_step'], ['workflow_step']]) or 'Diagnose'

            # Key physics/plant KPIs (optional; shown if available).
            Pfus = _sys_pick_first(art, [['P_fus_MW'], ['physics','P_fus_MW'], ['point','P_fus_MW'], ['plasma','P_fus_MW']])
            Pnet = _sys_pick_first(art, [['P_net_MW'], ['plant','P_net_MW'], ['point','P_net_MW'], ['summary','P_net_MW']])
            Qpl = _sys_pick_first(art, [['Q_plasma'], ['physics','Q_plasma'], ['point','Q_plasma'], ['summary','Q_plasma'], ['Q']])
            betaN = _sys_pick_first(art, [['beta_N'], ['physics','beta_N'], ['plasma','beta_N'], ['summary','beta_N']])
            q95 = _sys_pick_first(art, [['q95'], ['physics','q95'], ['plasma','q95'], ['summary','q95']])
            nGW = _sys_pick_first(art, [['n_over_nGW'], ['physics','n_over_nGW'], ['plasma','n_over_nGW'], ['summary','n_over_nGW']])

            # v256.0: design confidence class (trust ledger)
            try:
                ac = (art.get("authority_confidence") or {}) if isinstance(art, dict) else {}
                dc = str((ac.get("design") or {}).get("design_confidence_class", "UNKNOWN"))
            except Exception:
                dc = "UNKNOWN"

            # v257.0: decision consequences (advisory; post-processing only)
            try:
                _dec = (art.get("decision_consequences") or {}) if isinstance(art, dict) else {}
                decision_posture = str(_dec.get("decision_posture", "UNKNOWN"))
                primary_risk = str(_dec.get("primary_risk_driver", "") or "")
            except Exception:
                decision_posture = "UNKNOWN"
                primary_risk = ""

            # v258.0: epoch feasibility (Startup / Nominal / End-of-Life)
            try:
                _ef = (art.get("epoch_feasibility") or {}) if isinstance(art, dict) else {}
                epoch_overall = str(_ef.get("overall", "UNKNOWN"))
            except Exception:
                epoch_overall = "UNKNOWN"


            # Build a copyable markdown summary (deterministic, audit-friendly).
            md_lines = [
                "# Systems Compact Cockpit",
                "",
                f"- Verdict: {str(verdict)}",
                f"- Design confidence: {str(dc)}",
                f"- Decision posture: {str(decision_posture)}",
                f"- Epoch feasibility (overall): {str(epoch_overall)}",
                f"- Primary risk driver: {primary_risk if primary_risk else '-'}",
                f"- Workflow step: {str(step)}",
                f"- Dominant constraint: {str(dom)}",
                f"- Dominant mechanism: {str(mech)}",
                "",
                "## Key KPIs",
                f"- P_fus [MW]: {Pfus if Pfus is not None else '-'}",
                f"- P_net [MW]: {Pnet if Pnet is not None else '-'}",
                f"- Q_plasma [-]: {Qpl if Qpl is not None else '-'}",
                f"- beta_N [-]: {betaN if betaN is not None else '-'}",
                f"- q95 [-]: {q95 if q95 is not None else '-'}",
                f"- n/nGW [-]: {nGW if nGW is not None else '-'}",
                "",
                "## Next action (diagnostic only)",
                f"- {_sys_compact_next_action(str(verdict), str(dom), str(step))}",
            ]
            cockpit_md = "\n".join(md_lines)

            # Optional: show copy-ready markdown and a one-click download.
            if bool(st.session_state.get("systems_cockpit_show_md", False)):
                with st.expander("ðŸ“‹ Copy-ready cockpit summary (markdown)", expanded=False):
                    st.code(cockpit_md, language="markdown")
                    st.download_button(
                        "Download cockpit summary (MD)",
                        data=cockpit_md.encode("utf-8"),
                        file_name="systems_compact_cockpit.md",
                        mime="text/markdown",
                        use_container_width=True,
                        key="systems_cockpit_md_download",
                    )

            # Optional: pin a mini-cockpit in the sidebar (Streamlit-safe 'sticky').
            if bool(st.session_state.get("systems_cockpit_pin", False)):
                with st.sidebar:
                    st.markdown("### ðŸ§­ Systems Cockpit (pinned)")
                    st.caption("Diagnostic only - external to truth.")
                    st.write(f"**Verdict:** {str(verdict)}")
                    st.write(f"**Confidence:** {str(dc)}")
                    st.write(f"**Posture:** {str(decision_posture)}")
                    if primary_risk:
                        st.write(f"**Risk driver:** {str(primary_risk)}")
                    st.write(f"**Step:** {str(step)}")
                    if dom:
                        st.write(f"**Dominant:** {str(dom)}")
                    if mech:
                        st.write(f"**Mechanism:** {str(mech)}")
                    st.write(f"**P_net:** {(_sys_fmt(Pnet) + ' MW') if Pnet is not None else '-'}")
                    st.write(f"**Q:** {_sys_fmt(Qpl) if Qpl is not None else '-'}")

            # Compact header
            c0, c1, c2, c3, c4, c5 = st.columns([1.1, 1.0, 1.0, 1.0, 1.0, 1.2])
            with c0:
                st.markdown("#### Compact Cockpit")
                st.caption("Above-the-fold operator summary")
            with c5:
                st.caption("Next action")
                st.write(_sys_compact_next_action(str(verdict), str(dom), str(step)))

            # Metrics row
            m1, m2, m3, m4, m5, m6 = st.columns(6)
            with m1:
                st.metric("Verdict", _sys_fmt(verdict, digits=12))
            with m2:
                st.metric("Workflow", _sys_fmt(step, digits=12))
            with m3:
                st.metric("Dominant", _sys_fmt(dom, digits=12))
            with m4:
                st.metric("Mechanism", _sys_fmt(mech, digits=12))
            with m5:
                st.metric("P_fus", f"{_sys_fmt(Pfus)} MW" if Pfus is not None else "-")
            with m6:
                st.metric("P_net", f"{_sys_fmt(Pnet)} MW" if Pnet is not None else "-")

            _pr = f" â€¢ risk: **{primary_risk}**" if primary_risk else ""
            st.caption(
                f"Trust layer (post-processing): confidence = **{dc}**, posture = **{decision_posture}**{_pr}. Truth unchanged."
            )

            # Secondary metrics row (only if at least one is present)
            if any(v is not None for v in [Qpl, betaN, q95, nGW]):
                s1, s2, s3, s4 = st.columns(4)
                with s1:
                    st.metric("Q_plasma", _sys_fmt(Qpl))
                with s2:
                    st.metric("Î²_N", _sys_fmt(betaN))
                with s3:
                    st.metric("q95", _sys_fmt(q95))
                with s4:
                    st.metric("n/nGW", _sys_fmt(nGW))

            st.divider()
        except Exception:
            return

    # Render the compact cockpit as early as possible.
    # _sys_render_compact_cockpit()  # moved to post-run diagnostics (v374.1)
    # -----------------------------
    # Systems Console (v231 bundle): Verdict Bar + Mechanism Filter + Constraint Cards + Causal Chain + Expert Toggle
    #
    # This is UI-only: it never changes physics, constraints, or truth. It renders from cached artifacts.
    def _sys_fetch_latest_systems_artifact() -> dict | None:
        try:
            s = _v92_state_get()
        except Exception:
            s = None
        cand = None
        try:
            if isinstance(st.session_state.get("systems_last_solve_artifact"), dict):
                cand = st.session_state.get("systems_last_solve_artifact")
        except Exception:
            cand = None
        if cand is None and s is not None:
            try:
                if isinstance(getattr(s, "last_systems_result", None), dict):
                    cand = getattr(s, "last_systems_result")
            except Exception:
                pass
        if cand is None and s is not None:
            try:
                if isinstance(getattr(s, "last_point_artifact", None), dict):
                    cand = getattr(s, "last_point_artifact")
            except Exception:
                pass
        return cand if isinstance(cand, dict) else None

    def _sys_extract_constraints(art: dict) -> list[dict]:
        # Try multiple schema paths (graceful across versions)
        paths = [
            ["ledger", "constraints"],
            ["constraint_ledger", "constraints"],
            ["constraints"],
            ["ledger_entries"],
        ]
        for p in paths:
            v = _sys_get_in(art, p)
            if isinstance(v, list) and v and all(isinstance(x, dict) for x in v):
                return v
        return []

    def _sys_constraint_kind(c: dict) -> str:
        # Prefer explicit kind/status flags, fall back to common fields
        for k in ["kind", "tier", "severity", "class"]:
            v = c.get(k)
            if isinstance(v, str) and v:
                vv = v.lower()
                if "block" in vv or vv == "hard":
                    return "blocking"
                if "diag" in vv or "soft" in vv:
                    return "diagnostic"
                if "ignore" in vv or "off" in vv:
                    return "ignored"
        status = str(c.get("status") or c.get("verdict") or "").lower()
        if status in ("fail", "pass", "pass+diag"):
            # kind not inferable from status
            return str(c.get("kind") or "blocking" if status == "fail" else "diagnostic")
        return "diagnostic"

    def _sys_constraint_name(c: dict) -> str:
        return str(c.get("name") or c.get("constraint") or c.get("id") or c.get("key") or "constraint")

    def _sys_constraint_margin(c: dict) -> float | None:
        for k in ["signed_margin", "margin", "m", "delta"]:
            v = c.get(k)
            if isinstance(v, (int, float)):
                return float(v)
        # some ledgers store margin under 'metrics'
        mv = c.get("metrics") if isinstance(c.get("metrics"), dict) else None
        if isinstance(mv, dict):
            for k in ["signed_margin", "margin"]:
                v = mv.get(k)
                if isinstance(v, (int, float)):
                    return float(v)
        return None

    def _sys_constraint_status(c: dict) -> str:
        v = c.get("status") or c.get("verdict") or c.get("result")
        if isinstance(v, str) and v:
            return v.upper()
        # infer from margin if possible
        m = _sys_constraint_margin(c)
        if isinstance(m, (int, float)):
            return "FAIL" if m < 0 else "PASS"
        return "-"

    def _sys_constraint_mechanism(c: dict) -> str:
        for k in ["mechanism_group", "mechanism", "group"]:
            v = c.get(k)
            if isinstance(v, str) and v:
                return v.upper()
        return "OTHER"

    def _sys_constraint_authority(c: dict) -> str:
        for k in ["authority_tier", "authority", "tier_authority"]:
            v = c.get(k)
            if isinstance(v, str) and v:
                return v
        return "-"

    def _sys_constraint_validity(c: dict) -> str:
        for k in ["validity_domain", "validity", "domain"]:
            v = c.get(k)
            if isinstance(v, str) and v:
                return v
        return "-"

    def _sys_constraint_inputs(c: dict) -> list[str]:
        for k in ["inputs", "dominant_inputs", "drivers"]:
            v = c.get(k)
            if isinstance(v, list) and v:
                return [str(x if not isinstance(x, dict) else x.get("name") or x.get("input") or x.get("var") or x) for x in v][:6]
        return []

    def _sys_render_verdict_bar(art: dict, *, constraints: list[dict]) -> None:
        verdict = _sys_pick_first(art, [[ "verdict" ], [ "summary", "verdict" ], [ "ledger", "verdict" ]]) or "-"
        dom = _sys_pick_first(art, [[ "dominant_constraint" ], [ "summary", "dominant_constraint" ], [ "ledger", "dominant_constraint" ]])
        mech = _sys_pick_first(art, [[ "dominant_mechanism" ], [ "summary", "dominant_mechanism" ], [ "ledger", "dominant_mechanism" ]])
        # If mechanism missing, infer from dominant constraint entry
        if (not mech) and dom:
            for c in constraints:
                if _sys_constraint_name(c) == str(dom):
                    mech = _sys_constraint_mechanism(c)
                    break
        # Signed margin for dominant (if present)
        dom_margin = None
        if dom:
            for c in constraints:
                if _sys_constraint_name(c) == str(dom):
                    dom_margin = _sys_constraint_margin(c)
                    break

        cols = st.columns([1.1, 1.2, 2.2, 1.2, 1.1, 1.2])
        with cols[0]:
            st.markdown("#### Systems Verdict Bar")
        with cols[1]:
            st.metric("Verdict", _sys_fmt(verdict, digits=16))
        with cols[2]:
            st.metric("Dominant constraint", _sys_fmt(dom, digits=48) if dom else "-")
        with cols[3]:
            st.metric("Mechanism", _sys_fmt(mech, digits=16) if mech else "-")
        with cols[4]:
            st.metric("Signed margin", _sys_fmt(dom_margin) if dom_margin is not None else "-")
        with cols[5]:
            # show authority for dominant if known
            auth = "-"
            if dom:
                for c in constraints:
                    if _sys_constraint_name(c) == str(dom):
                        auth = _sys_constraint_authority(c)
                        break
            st.metric("Authority", _sys_fmt(auth, digits=16))
            # v256.0: design confidence class (trust ledger)
            try:
                ac = (art.get("authority_confidence") or {}) if isinstance(art, dict) else {}
                dc = str((ac.get("design") or {}).get("design_confidence_class", "UNKNOWN"))
            except Exception:
                dc = "UNKNOWN"
            st.metric("Confidence", dc)

        # Policy contract summary (explicit enforcement tiering)
        try:
            pol = out.get("_policy_contract") or {}
            q95_pol = str(pol.get("q95_enforcement", "hard")).strip().lower()
            fg_pol = str(pol.get("greenwald_enforcement", "hard")).strip().lower()
            if (q95_pol != "hard") or (fg_pol != "hard"):
                st.caption(f"Policy contract: q95={q95_pol.upper()} Â· Greenwald(fG)={fg_pol.upper()} (tiering only; physics unchanged)")
        except Exception:
            pass
        st.caption("Verdict Bar is diagnostic only; it does not modify physics, constraints, or truth.")
        st.divider()

    def _sys_render_causal_chain(art: dict, *, constraints: list[dict], expert: bool) -> None:
        verdict = _sys_pick_first(art, [[ "verdict" ], [ "summary", "verdict" ], [ "ledger", "verdict" ]]) or "-"
        dom = _sys_pick_first(art, [[ "dominant_constraint" ], [ "summary", "dominant_constraint" ], [ "ledger", "dominant_constraint" ]])
        mech = _sys_pick_first(art, [[ "dominant_mechanism" ], [ "summary", "dominant_mechanism" ], [ "ledger", "dominant_mechanism" ]])
        dom_entry = None
        if dom:
            for c in constraints:
                if _sys_constraint_name(c) == str(dom):
                    dom_entry = c
                    break
        if dom_entry and (not mech):
            mech = _sys_constraint_mechanism(dom_entry)

        drivers = []
        if dom_entry:
            di = dom_entry.get("dominant_inputs")
            if isinstance(di, list) and di:
                for x in di[:4]:
                    if isinstance(x, dict):
                        name = x.get("name") or x.get("input") or x.get("var") or "x"
                        sens = x.get("dmargin_dx") or x.get("sensitivity") or None
                        if isinstance(sens, (int, float)):
                            drivers.append(f"{name} (âˆ‚m/âˆ‚x={sens:+.3g})")
                        else:
                            drivers.append(str(name))
                    else:
                        drivers.append(str(x))
        chain = []
        chain.append(f"**{str(verdict)}**")
        if mech:
            chain.append(f"â†³ Mechanism: **{str(mech)}**")
        if dom:
            chain.append(f"â†³ Dominant constraint: **{str(dom)}**")
        if drivers:
            chain.append("â†³ Dominant drivers: " + ", ".join(drivers))
        # Add a minimal RWM hint when relevant (no additional physics, purely explanatory)
        if dom and "RWM" in str(dom).upper():
            chain.append("â†³ RWM screening: required bandwidth/power must fit within CONTROL caps (see Control Contracts).")
        with st.expander("Why-chain (dominant cause)", expanded=False):
            for line in chain:
                st.markdown(line)
            if expert and isinstance(dom_entry, dict):
                st.markdown("**Raw dominant entry (expert):**")
                st.json(dom_entry)

    def _sys_render_constraint_cards(art: dict, *, constraints: list[dict]) -> None:
        # Expert density toggle
        ex = st.toggle("Expert view", value=st.session_state.get("systems_expert_view", False), key="systems_expert_view")
        # Mechanism filter
        mechs = sorted({ _sys_constraint_mechanism(c) for c in constraints } | {"ALL"})
        mech_sel = st.selectbox("View by mechanism", options=mechs, index=0, key="systems_mech_filter")
        # Kind tabs
        t_block, t_diag, t_all = st.tabs(["Blocking", "Diagnostic", "All"])
        # common renderer
        def render_kind(kind: str | None):
            rows = []
            for c in constraints:
                k = _sys_constraint_kind(c)
                if kind and k != kind:
                    continue
                if mech_sel != "ALL" and _sys_constraint_mechanism(c) != mech_sel:
                    continue
                rows.append(c)
            # sort: worst margin first (None last), then FAIL first
            def keyfn(c):
                m = _sys_constraint_margin(c)
                stt = _sys_constraint_status(c)
                return (0 if stt == "FAIL" else 1, 1e9 if m is None else m)
            rows.sort(key=keyfn)
            max_cards = st.slider("Max cards", min_value=5, max_value=60, value=min(20, max(5, len(rows))), step=5, key=f"systems_max_cards_{kind or 'all'}")
            for c in rows[:max_cards]:
                name = _sys_constraint_name(c)
                status = _sys_constraint_status(c)
                margin = _sys_constraint_margin(c)
                mech = _sys_constraint_mechanism(c)
                auth = _sys_constraint_authority(c)
                sub = str(c.get("subsystem") or "-")
                val = _sys_constraint_validity(c)
                inp = _sys_constraint_inputs(c)

                header = f"{name}"
                badge = f"{status}"
                if margin is not None:
                    badge += f" | m={margin:+.4g}"
                badge += f" | {mech}"

                with st.container(border=True):
                    c1, c2, c3, c4 = st.columns([2.3, 1.0, 1.0, 1.2])
                    with c1:
                        st.markdown(f"**{header}**")
                        st.caption(sub)
                    with c2:
                        st.metric("Status", badge)
                    with c3:
                        st.metric("Authority", auth)
                    with c4:
                        st.metric("Mechanism", mech)
                    # compressed details
                    if inp:
                        st.caption("Inputs: " + ", ".join(inp))
                    if ex:
                        d1, d2 = st.columns([1,1])
                        with d1:
                            st.caption("Validity")
                            st.write(val)
                        with d2:
                            st.caption("Raw (expert)")
                            st.json(c)
        with t_block:
            render_kind("blocking")
        with t_diag:
            render_kind("diagnostic")
        with t_all:
            render_kind(None)
    # Systems Console rendering moved to post-run diagnostics (v374.1)


    # v185: additional Systems Mode freeze-grade state
    st.session_state.setdefault('systems_restore_rid', None)

    def _systems_restore_ui_from_artifact(obj2: dict, *, rerun: bool = True, source: str = "artifact") -> None:
        """Restore full Systems UI state from a Systems artifact (schema-stable).
        This must be rerun-safe and must never throw.
        """
        import streamlit as st
        try:
            st.session_state["systems_last_solve_artifact"] = obj2
            try:
                _v92_state_get().last_systems_result = obj2
            except Exception:
                pass

            ui = obj2.get("ui_state") if isinstance(obj2.get("ui_state"), dict) else obj2

            for k in ["systems_run_cards","systems_journal","v178_last_precheck","v178_last_recovery","v178_fs_last","systems_last_feasible_search"]:
                if isinstance(ui, dict) and (k in ui):
                    st.session_state[k] = ui.get(k)

            st.session_state["_pending_workflow_step"] = str((ui.get("workflow_step") if isinstance(ui, dict) else "") or "Diagnose")
            if isinstance(ui, dict) and ui.get("design_intent"):
                st.session_state["design_intent"] = ui.get("design_intent")

            # Keep a breadcrumb
            st.session_state["systems_last_restore_source"] = source
            if rerun:
                st.rerun()
        except Exception:
            pass

    # If a run-id reproduction is pending, resolve it now (before rendering panels).
    try:
        _rid = st.session_state.get('systems_restore_rid')
        if _rid:
            st.session_state['systems_restore_rid'] = None
            try:
                s = _v98_state_init_runlists()
                rec = next((x for x in (getattr(s,'run_history',[]) or []) if x.get('id') == _rid), None)
                if rec and isinstance(rec.get('payload'), dict):
                    _systems_restore_ui_from_artifact(rec.get('payload'), rerun=True, source=f"run:{_rid}")
            except Exception:
                pass
    except Exception:
        pass


    def _sys_now_iso() -> str:
        try:
            import datetime as _dt
            return _dt.datetime.utcnow().replace(microsecond=0).isoformat() + 'Z'
        except Exception:
            return ''

    def _sys_failure_taxonomy(reason: str) -> Dict[str, Any]:
        r = str(reason or '').strip()
        mapping = {
            'precheck_infeasible': {
                'title': 'Infeasible within declared bounds (precheck)',
                'next': ['Expand bounds for key variables', 'Switch intent to Research to diagnose without blocking', 'Run Seeded Recovery near your seed'],
            },
            'no_feasible_found': {
                'title': 'No feasible point found (budget exhausted)',
                'next': ['Increase budget/multi-start', 'Allow more variables to change', 'Widen bounds around the seed'],
            },
            'no_variables': {
                'title': 'No variables provided',
                'next': ['Select at least one variable with valid bounds'],
            },
            'nonfinite_range': {
                'title': 'Non-finite model outputs in sampled region',
                'next': ['Narrow bounds to avoid numerically invalid region', 'Check inputs for physical plausibility'],
            },
        }
        return dict(mapping.get(r, {'title': r or 'unknown', 'next': []}))

    def _sys_levers_from_limiters(limiters: List[str]) -> List[str]:
        # Rule-of-thumb levers (diagnostic; does not change physics)
        lever_map = {
            'q_div': ['Increase R0', 'Increase a (if allowed)', 'Increase radiation fraction / detach proxy (Research)', 'Lower power density (reduce Paux or targets)'],
            'sigma_vm': ['Lower Bt', 'Increase coil build/size (increase R0)', 'Relax stress allowables only if policy permits'],
            'HTS margin': ['Increase shield thickness', 'Reduce peak field (lower Bt or increase R0)', 'Adjust coil pack assumptions (if exposed)'],
            'TBR': ['Increase shield/blanket thickness', 'Increase R0 (more blanket volume)', 'Reduce inboard build consumption'],
            'q95': ['Increase Bt or reduce Ip (if Ip is variable)', 'Increase size (R0/a)'],
            'B_peak': ['Lower Bt', 'Increase R0 / coil radius'],
            'NWL': ['Increase R0', 'Reduce fusion power density (targets/temperature)'],
        }
        out: List[str] = []
        for lm in (limiters or []):
            for s in lever_map.get(str(lm), []):
                if s not in out:
                    out.append(s)
        return out[:8]

    def _sys_validate_bounds(bounds: Dict[str, Dict[str, float]]) -> Tuple[bool, List[str], List[str]]:
        errs: List[str] = []
        warns: List[str] = []
        # basic physical sanity envelopes (warnings only)
        phys = {
            'R0_m': (0.5, 30.0),
            'a_m': (0.1, 10.0),
            'kappa': (1.0, 3.2),
            'delta': (0.0, 0.8),
            'Bt_T': (0.5, 25.0),
            'Ti_keV': (0.5, 50.0),
            'Ti_over_Te': (0.5, 5.0),
            't_shield_m': (0.0, 3.0),
            'Paux_MW': (0.0, 2000.0),
        }
        for k, b in (bounds or {}).items():
            try:
                lo = float(b.get('lo'))
                hi = float(b.get('hi'))
            except Exception:
                errs.append(f"{k}: bounds not numeric")
                continue
            if not (math.isfinite(lo) and math.isfinite(hi)):
                errs.append(f"{k}: non-finite bounds")
                continue
            if hi <= lo:
                errs.append(f"{k}: hi must be > lo")
            if k in phys:
                plo, phi = phys[k]
                if lo < plo or hi > phi:
                    warns.append(f"{k}: bounds [{lo:g}, {hi:g}] exceed typical envelope [{plo:g}, {phi:g}] (warning only)")
        return (len(errs) == 0), errs, warns

    def _sys_append_run_card(*, kind: str, settings: Dict[str, Any], outcome: Dict[str, Any]) -> None:
        try:
            card = {
                'schema_version': SYS_RUN_CARD_SCHEMA_VERSION,
                'ts': _sys_now_iso(),
                'kind': str(kind),
                'status': 'fail' if str((outcome or {}).get('event','')).lower() == 'fail' else 'ok',
                'reason_code': str((outcome or {}).get('reason', (outcome or {}).get('reason_code',''))),
                'settings': dict(settings or {}),
                'outcome': dict(outcome or {}),
            }
            st.session_state['systems_run_cards'] = (st.session_state.get('systems_run_cards') or []) + [card]
            st.session_state['systems_run_cards'] = st.session_state['systems_run_cards'][-50:]
        except Exception:
            pass

    try:
        _v93_stateful_systems_panel()
    except Exception:
        pass

    st.subheader("Constraint Solver Cockpit")
    st.markdown(
        "Solve for a self-consistent operating point by adjusting **iteration variables** "
        "to hit **targets** (e.g., Q, H98, net electric) while reporting engineering and physics margins. "
        "This is inspired by external systems codes's constraint-driven workflow, but remains SHAMS-native and transparent."
    )

    # v182.1: Persist and re-render the latest Systems *solve* results across reruns.
    # Streamlit download_button triggers a rerun; if results are only rendered inside the
    # `if run:` block, they will appear to "disappear" after downloads.
    try:
        _cached = st.session_state.get('systems_last_solve_artifact')
        if not isinstance(_cached, dict):
            _cached = getattr(_v92_state_get(), 'last_systems_result', None)
        if isinstance(_cached, dict):
            with st.expander('Last solve snapshot (cached)', expanded=False):
                _v182_render_latest_systems_solve_results(artifact=_cached, point_artifact=getattr(_v92_state_get(), 'last_point_artifact', None), key_prefix="latest_cached")
            # v184.9: Persist and re-render the latest Feasible Search results across reruns.
            # Users often run the search inside Explore, then move workflow steps; this cached view
            # keeps the results discoverable and downloadable.
            try:
                _fs_cached = st.session_state.get('systems_last_feasible_search')
                if not isinstance(_fs_cached, dict):
                    _fs_cached = getattr(_v92_state_get(), 'last_feasible_search_artifact', None)
                if isinstance(_fs_cached, dict):
                    _t = _fs_cached.get('ts_unix')
                    try:
                        _tstr = datetime.datetime.fromtimestamp(float(_t)).strftime('%Y-%m-%d %H:%M:%S') if _t else 'unknown time'
                    except Exception:
                        _tstr = 'unknown time'
                    st.caption(f"Latest feasible-search cache: **{_fs_cached.get('reason','')}** at **{_tstr}** (see expander below)")
                    # Above-the-fold discoverability: don't auto-expand panels, but make new results obvious.
                    if bool(st.session_state.get('systems_fs_new', False)):
                        _b1, _b2 = st.columns([5,1])
                        with _b1:
                            st.info("New feasible-search candidates are ready. Click **Show now** to open them below.")
                        with _b2:
                            if st.button('Show now', key='systems_fs_show_now_btn'):
                                st.session_state['systems_show_fs_cached'] = True
                                st.session_state['systems_fs_new'] = False
                                st.rerun()
                    _exp_fs = bool(st.session_state.get('systems_show_fs_cached', False))
                    with st.expander('Latest cached Feasible Design Search results', expanded=_exp_fs):
                        _v184_render_latest_feasible_search_results(report=_fs_cached, key_prefix="fs_latest_cached")
            except Exception:
                pass
    except Exception:
        pass

    # -----------------------------
    # Decision State + Teaching Mode (SHOULD/COULD)
    # -----------------------------
    st.session_state.setdefault('systems_decision_state', 'Diagnose infeasibility')
    st.session_state.setdefault('systems_teaching_mode', False)
    _ds_opts = [
        'Diagnose infeasibility',
        'Recover feasibility near seed',
        'Choose a compromise (best-compromise)',
        'Explore trade space (scan/frontier)',
        'Apply & iterate (update Base/x0)',
    ]
    _c_ds1, _c_ds2 = st.columns([3,2])
    with _c_ds1:
        st.selectbox('Design decision state', options=_ds_opts, index=_ds_opts.index(st.session_state.get('systems_decision_state', _ds_opts[0])) if st.session_state.get('systems_decision_state') in _ds_opts else 0,
                     key='systems_decision_state',
                     help='This does not change physics. It clarifies what you are trying to do right now and tunes guidance text.')
    with _c_ds2:
        st.checkbox('Teaching / narrated mode', value=bool(st.session_state.get('systems_teaching_mode', False)),
                    key='systems_teaching_mode',
                    help='Shows extra guidance and step-by-step hints (diagnostic only).')
    try:
        if st.session_state.get('systems_teaching_mode', False):
            st.info(
                f"**Narrated mode:** You are in **{st.session_state.get('systems_decision_state')}**. "
                "Recommended flow: Precheck â†’ Recovery (if needed) â†’ Search/Compare â†’ Apply to Base/x0 â†’ Recheck."
            )
    except Exception:
        pass

    # -----------------------------

    # -----------------------------
    
    # -----------------------------
    # Negotiation Chronicle (v218) - transcript + dominance switching (read-only)
    # -----------------------------
    try:
        run_cards = st.session_state.get('systems_run_cards', []) or []
        journal = st.session_state.get('systems_journal', []) or []
        # Build a compact, reviewer-safe transcript
        transcript = []
        for k, rc in enumerate(run_cards):
            if not isinstance(rc, dict):
                continue
            art = rc.get("artifact") or rc.get("payload") or {}
            cons = None
            try:
                cons = (art.get("constraints") or []) if isinstance(art, dict) else None
            except Exception:
                cons = None
            # Dominant (first failing) hard constraint name if available
            dom = None
            try:
                if isinstance(art, dict) and isinstance(art.get("constraint_ledger"), dict):
                    tb = (art["constraint_ledger"].get("top_blockers") or [])
                    if isinstance(tb, list) and tb and isinstance(tb[0], dict):
                        dom = tb[0].get("name")
            except Exception:
                dom = None
            if dom is None and isinstance(cons, list):
                try:
                    # fallback: smallest margin_frac among failed hard constraints
                    failed = [c for c in cons if isinstance(c, dict) and (not bool(c.get("passed", True))) and str(c.get("severity","hard")).lower()=="hard"]
                    failed.sort(key=lambda c: float(c.get("margin_frac", 0.0)))
                    dom = failed[0].get("name") if failed else None
                except Exception:
                    dom = None

            transcript.append({
                "k": int(k),
                "ts": rc.get("ts") or rc.get("timestamp") or None,
                "label": rc.get("label") or rc.get("name") or f"run_{k}",
                "action": rc.get("action") or rc.get("reason") or "",
                "design_intent": (art.get("design_intent") if isinstance(art, dict) else None),
                "dominant_constraint": dom,
                "ok_hard": bool((art.get("kpis") or {}).get("feasible_hard")) if isinstance(art.get("kpis"), dict) else None,
            })

        # Dominance switching analysis
        dom_seq = [t.get("dominant_constraint") for t in transcript if t.get("dominant_constraint")]
        switches = []
        last = None
        for i, d in enumerate(dom_seq):
            if last is None:
                last = d
                continue
            if str(d) != str(last):
                switches.append({"at": int(i), "from": str(last), "to": str(d)})
                last = d

        with st.expander("ðŸ§¾ Negotiation Chronicle - audit transcript (read-only)", expanded=False):
            st.caption("A compact, reviewer-safe log of Systems Mode actions and what constraint dominated each step. This does not change any physics or state.")
            if transcript:
                try:
                    import pandas as _pd
                    st.dataframe(_pd.DataFrame(transcript), use_container_width=True, hide_index=True, height=260)
                except Exception:
                    st.json(transcript[:40], expanded=False)
                st.download_button(
                    "Download Systems transcript (JSON)",
                    data=_shams_json_dumps({"schema":"systems_transcript.v1","transcript":transcript,"switches":switches}, indent=2).encode("utf-8"),
                    file_name="shams_systems_transcript_v1.json",
                    mime="application/json",
                    use_container_width=True,
                    key="systems_transcript_dl",
                )
            else:
                st.info("No Systems run cards yet. Run a workflow step (Precheck/Recovery/Search) to generate transcript entries.")

        with st.expander("ðŸ§¨ Dominance Switchboard - regime boundary hints", expanded=False):
            st.caption("Where the dominant blocker changes, you crossed a regime boundary in the local feasibility landscape.")
            if switches:
                try:
                    import pandas as _pd
                    st.dataframe(_pd.DataFrame(switches), use_container_width=True, hide_index=True, height=220)
                except Exception:
                    st.json(switches, expanded=False)
            else:
                st.info("No dominance switches detected in the current Systems run history (or not enough runs).")
    except Exception:
        pass

# Workflow navigator (v180)
    # -----------------------------
    # Workflow navigator (v180) - session-safe (no widget/state conflict)
    # -----------------------------
    # Derive local defaults from session state without setting widget value.
    _default_step = str(st.session_state.get('systems_workflow_step', 'Diagnose'))
    # Back-compat: older builds stored emoji in the raw workflow value.
    if _default_step == "ðŸ“¤ Export":
        _default_step = "Export"
    _default_power = bool(st.session_state.get('systems_workflow_power_user', False))

    # Apply pending workflow changes BEFORE widget instantiation, without directly setting widget state.
    try:
        if '_pending_workflow_step' in st.session_state:
            _default_step = str(st.session_state.pop('_pending_workflow_step'))
        if '_pending_workflow_power_user' in st.session_state:
            _default_power = bool(st.session_state.pop('_pending_workflow_power_user'))
    except Exception:
        pass



    c_wf1, c_wf2 = st.columns([3, 1])
    with c_wf1:
        _wf_keys = ["Setup", "Diagnose", "Recover", "Explore", "Compare/Apply", "Export", "Advanced"]
        _wf_labels = {
            "Setup": "ðŸ§° Setup",
            "Diagnose": "ðŸ§ª Diagnose",
            "Recover": "ðŸ§¯ Recover",
            "Explore": "ðŸ§­ Explore",
            "Compare/Apply": "ðŸ§© Compare/Apply",
            "Export": "ðŸ“¤ Export",
            "Advanced": "ðŸ§  Advanced",
        }

        # Normalize legacy stored state if present.
        try:
            _legacy = st.session_state.get("systems_workflow_step")
            if _legacy == "ðŸ“¤ Export":
                st.session_state["systems_workflow_step"] = "Export"
        except Exception:
            pass

        st.radio(
            "Workflow",
            options=_wf_keys,
            index=_wf_keys.index(_default_step) if _default_step in _wf_keys else 1,
            horizontal=True,
            format_func=lambda k: _wf_labels.get(str(k), str(k)),
            key="systems_workflow_step",
            help="Guided workflow view. Enable Power-user to show all panels at once.",
        )
    with c_wf2:
        st.toggle(
            "Power-user",
            value=_default_power,
            key="systems_workflow_power_user",
            help="Show all Systems panels (advanced/debug).",
        )

    def _sys_show(*steps: str) -> bool:
        """Whether to render a Systems panel in the current workflow view.

        SHAMS UI law: avoid long scroll. Systems Mode is workflow-guided.
        - Power-user: show everything (debug/forensics).
        - Guided view: show only panels relevant to the selected step.

        This must never make Systems Mode "empty"; core headers and the workflow selector
        remain outside step-gated panels.
        """
        try:
            if bool(st.session_state.get("systems_workflow_power_user", False)):
                return True
            step = str(st.session_state.get("systems_workflow_step", "Diagnose") or "Diagnose")
            if not steps:
                return True
            return step in set(map(str, steps))
        except Exception:
            return True


# -----------------------------
# Systems Mode freeze-readiness helpers (v180+)
# -----------------------------
if _deck == "ðŸ§  Systems Mode":
    SYS_RUN_CARD_SCHEMA_VERSION = 1
    SYS_TRACE_SCHEMA_VERSION = 1
    SYS_ARTIFACT_SCHEMA_VERSION = 1

    _WIDGET_KEYS_PREFIXES = ("v178_", "v179_", "v180_", "systems_")
    def _is_widget_owned_key(k: str) -> bool:
        # Heuristic: any key that is used as a widget key should never be mutated after instantiation.
        # We track a registry of widget keys touched by st.* widgets in this run.
        try:
            reg = set(st.session_state.get("_widget_key_registry", set()) or set())
            return str(k) in reg
        except Exception:
            return False

    def _register_widget_key(k: str) -> None:
        try:
            reg = set(st.session_state.get("_widget_key_registry", set()) or set())
            reg.add(str(k))
            st.session_state["_widget_key_registry"] = reg
        except Exception:
            pass

    def safe_state_set(key: str, value, *, allow_widget_keys: bool = False) -> None:
        # Prevent accidental Streamlit widget-key mutation errors.
        if (not allow_widget_keys) and _is_widget_owned_key(key):
            raise RuntimeError(f"Attempted to modify widget-owned session_state key: {key}")
        st.session_state[key] = value

    def _sys_run_card(kind: str, status: str, reason_code: str, message: str, **fields) -> dict:
        rc = {
            "schema_version": SYS_RUN_CARD_SCHEMA_VERSION,
            "kind": str(kind),
            "status": str(status),
            "reason_code": str(reason_code),
            "message": str(message),
            "ts_unix": float(time.time()),
        }
        for k, v in fields.items():
            rc[k] = v
        return rc

    def _sys_state() -> dict:
        st.session_state.setdefault("systems_state", {})
        s = st.session_state["systems_state"]
        if not isinstance(s, dict):
            s = {}
            st.session_state["systems_state"] = s
        s.setdefault("schema_version", SYS_ARTIFACT_SCHEMA_VERSION)
        s.setdefault("history", [])
        s.setdefault("history_index", -1)
        s.setdefault("stories", {})
        return s

    def _sys_push_history(label: str, payload: dict) -> None:
        s = _sys_state()
        h = list(s.get("history", []) or [])
        idx = int(s.get("history_index", -1))
        # drop redo tail
        if idx < len(h) - 1:
            h = h[: idx + 1]
        h.append({"label": str(label), "payload": payload, "ts_unix": float(time.time())})
        s["history"] = h
        s["history_index"] = len(h) - 1
        st.session_state["systems_state"] = s

    def _sys_validate_invariants(require_precheck: bool = False, require_candidates: bool = False) -> tuple[bool, str]:
        # Minimal invariants: base point exists; evaluator contract is fixed; assumption lock matches if enabled.
        try:
            if st.session_state.get("base_last_eval") is None and st.session_state.get("v178_base_last_eval") is None:
                return False, "Missing Base evaluation. Run Point Designer once or evaluate Base in Systems Mode."
        except Exception:
            pass
        if require_precheck:
            if not isinstance(st.session_state.get("v178_last_precheck"), dict):
                return False, "No Precheck results found. Run Precheck first."
        if require_candidates:
            rep = st.session_state.get("v178_fs_last") or st.session_state.get("v178_last_recovery")
            cands = []
            if isinstance(rep, dict):
                cands = list(rep.get("candidates", []) or [])
            if not cands:
                return False, "No candidates available yet. Run Recovery or Feasible Search first."
        # Assumption lock (if enabled)
        try:
            if bool(st.session_state.get("systems_assumption_lock_enabled", False)):
                lock_hash = st.session_state.get("systems_assumption_lock_hash")
                cur_hash = st.session_state.get("systems_assumption_current_hash")
                if lock_hash and cur_hash and str(lock_hash) != str(cur_hash):
                    return False, "Assumptions drifted while lock is enabled. Unlock to edit or restore locked assumptions."
        except Exception:
            pass
        return True, ""

    def _sys_primary_action_header(step: str, title: str, subtitle: str, action_label: str, action_key: str) -> bool:
        st.markdown(f"### {title}")
        st.caption(subtitle)
        return st.button(action_label, use_container_width=True, key=action_key)

    # Assumption lock (SHOULD): lock key Systems settings across runs
    # -----------------------------
    st.session_state.setdefault('systems_assumption_lock', {'locked': False, 'snapshot': {}})
    if _sys_show('Setup','Advanced'):
        with st.expander('Assumption lock (Systems settings)', expanded=False):
            _lock = bool(st.session_state.get('systems_assumption_lock', {}).get('locked', False))
            _lock = st.checkbox('Lock key Systems settings across runs', value=_lock, key='systems_assumption_lock_locked',
                                help='Locks intent/objective/diagnostic relaxations to prevent accidental drift between runs.')
            snap = dict((st.session_state.get('systems_assumption_lock', {}) or {}).get('snapshot', {}) or {})
            if st.button('Capture lock snapshot = current', use_container_width=True, key='systems_assumption_lock_capture'):
                snap = {
                    'design_intent': st.session_state.get('design_intent'),
                    'fs_objective': st.session_state.get('v178_fs_obj'),
                    'diag_relax': st.session_state.get('systems_diag_relax', {}),
                }
                st.session_state['systems_assumption_lock'] = {'locked': _lock, 'snapshot': snap}
                st.success('Captured snapshot.')
            if st.button('Clear snapshot', use_container_width=True, key='systems_assumption_lock_clear'):
                st.session_state['systems_assumption_lock'] = {'locked': _lock, 'snapshot': {}}
                st.success('Cleared.')
            # persist lock flag
            st.session_state['systems_assumption_lock'] = {'locked': _lock, 'snapshot': snap}
            if snap:
                st.caption('Locked snapshot')
                st.json(snap)
    
    # v176.0: Systems Mode contract + latest summary
    s = _v92_state_get()
    last_point_art = getattr(s, 'last_point_artifact', None)
    last_sys_art = getattr(s, 'last_systems_result', None)

    def _artifact_sha(obj):
        try:
            return _v152_artifact_sha(obj)
        except Exception:
            try:
                import json, hashlib
                return hashlib.sha256(json.dumps(obj, sort_keys=True, default=str).encode('utf-8')).hexdigest()
            except Exception:
                return None

    if _sys_show('Setup','Advanced'):
        with st.expander('Systems Mode contract (inputs -> solve -> artifact)', expanded=False):
            st.markdown(
                "**Contract**: Systems Mode produces a traceable run artifact from (base inputs, targets, variable bounds, solver options). "
                "If a solve fails, SHAMS reports why (precheck infeasible, continuation step failure, nonconvergence) and shows actionable guidance."
            )
            cols = st.columns(3)
            with cols[0]:
                st.caption('Latest Point artifact')
                if isinstance(last_point_art, dict):
                    st.code(((_artifact_sha(last_point_art) or '(sha unavailable)')[:12]), language='text')
                else:
                    st.write('-')
            with cols[1]:
                st.caption('Latest Systems artifact')
                if isinstance(last_sys_art, dict):
                    st.code(((_artifact_sha(last_sys_art) or '(sha unavailable)')[:12]), language='text')
                else:
                    st.write('-')
            with cols[2]:
                st.caption('State')
                st.write(f"Point={'yes' if isinstance(last_point_art, dict) else 'no'} | Systems={'yes' if isinstance(last_sys_art, dict) else 'no'}")
    
            # Bundle export (point + systems)
            import io, zipfile, json as _json
            buf = io.BytesIO()
            with zipfile.ZipFile(buf, 'w', compression=zipfile.ZIP_DEFLATED) as z:
                if isinstance(last_point_art, dict):
                    z.writestr('run_artifact_point.json', _shams_json_dumps(last_point_art, indent=2, sort_keys=True))
                if isinstance(last_sys_art, dict):
                    z.writestr('run_artifact_systems.json', _shams_json_dumps(last_sys_art, indent=2, sort_keys=True))
            buf.seek(0)
            st.download_button(
                'Download latest Point+Systems bundle (zip)',
                data=buf.getvalue(),
                file_name='shams_latest_point_systems_bundle.zip',
                mime='application/zip',
                use_container_width=True,
                key='v176_dl_latest_bundle',
            )
    
    # -----------------------------
    
    # -----------------------------
    # Primary Action (workflow-first)
    # -----------------------------
    try:
        _step = str(_default_step)
        _power = _default_power
        if not _power:
            if _step == "Setup":
                st.info("**Setup**: Choose targets and variables, then run **Precheck** in *Diagnose*.")
            elif _step == "Diagnose":
                if _sys_primary_action_header("Diagnose", "Diagnose feasibility", "Run Precheck to see dominant limiters and whether targets/bounds can reach feasibility.", "Run Precheck", "sys_primary_precheck"):
                    safe_state_set("_sys_action", "precheck", allow_widget_keys=True)
                    st.rerun()
            elif _step == "Recover":
                ok, msg = _sys_validate_invariants(require_precheck=True)
                if not ok:
                    st.warning(msg)
                if _sys_primary_action_header("Recover", "Recover feasibility", "Try seeded recovery to find the nearest feasible (or best compromise for Research intent).", "Run Recovery", "sys_primary_recovery"):
                    safe_state_set("_sys_action", "recovery", allow_widget_keys=True)
                    st.rerun()
            elif _step == "Explore":
                ok, msg = _sys_validate_invariants(require_precheck=True)
                if not ok:
                    st.warning(msg)
                if _sys_primary_action_header("Explore", "Explore feasible designs", "Run feasible-only search to generate top-K candidates and a frontier/trace.", "Run Feasible Search", "sys_primary_search"):
                    safe_state_set("_sys_action", "search", allow_widget_keys=True)
                    st.rerun()
            elif _step == "Compare/Apply":
                ok, msg = _sys_validate_invariants(require_candidates=True)
                if not ok:
                    st.warning(msg)
                else:
                    st.info("Compare candidates and apply the selected one to **Base** or **x0**, then re-run Precheck.")
            elif _step == "ðŸ“¤ Export":
                st.info("Export the full Systems bundle (run cards, traces, logs, and point artifacts if present).")
            elif _step == "Advanced":
                st.info("Advanced view shows all tools. Use with care.")
    except Exception:
        pass


# -------------------------------------------------------------------------
# World-class Systems Mode decision support (v183)
# -------------------------------------------------------------------------
    # Persistent decision journal (Design Stories 2.0)
    st.session_state.setdefault("systems_journal", [])
    def _sys_journal_append(kind: str, payload: dict | None = None):
        try:
            entry = {
                "ts_unix": float(time.time()),
                "kind": str(kind),
                "workflow_step": str(st.session_state.get("systems_workflow_step", "")),
                "design_intent": str(st.session_state.get("design_intent", "")),
                "payload": payload or {},
            }
            st.session_state["systems_journal"].append(entry)
        except Exception:
            pass

    # Limiter graph: curated causal map (0-D explainability)
    _LIMITER_GRAPH = {
        "q95": ["Ip_MA (-)", "Bt_T (+)", "R0_m (+)", "kappa (+)", "li (proxy)"],
        "q_div": ["P_SOL_MW (+)", "lambda_q_mm (-)", "R0_m (+) via wetted area", "kappa (+)"],
        "sigma_vm": ["B_peak_T (+)", "R0_m (-) (structure leverage)", "tf_wp geometry"],
        "B_peak": ["Bt_T (+)", "R0_m (-)", "coil build"],
        "HTS margin": ["B_peak_T (+)", "T_op (fixed)", "Jop (proxy)", "wp_fill_factor"],
        "TBR": ["R0_m (+)", "blanket fraction (+)", "A (proxy)", "shield thickness (-)"],
        "P_SOL/R": ["P_SOL_MW (+)", "R0_m (-)"],
        "NWL": ["P_fus (+)", "wall area (-)", "R0_m (+)"],
    }

    # Determine the latest available Systems artifact (rerun-safe)
    try:
        _sys_latest_art = st.session_state.get("systems_last_solve_artifact") or getattr(s, "last_systems_result", None)
    except Exception:
        _sys_latest_art = None



    # Always render the latest cached Systems solve results (rerun-safe).
    # Streamlit download/export triggers a rerun; results must persist from session_state cache.
    if isinstance(_sys_latest_art, dict):
        st.markdown("### Latest Systems results (cached)")
        try:
            _outs = (_sys_latest_art.get("headline") or _sys_latest_art.get("outputs") or {})
            _kc = st.columns(4)
            def _m(_col, _k):
                try:
                    v = float(_outs.get(_k, float("nan")))
                except Exception:
                    v = float("nan")
                with _col:
                    st.metric(_k, f"{v:.3g}" if v == v else "NaN")
            _m(_kc[0], "Q_DT_eqv")
            _m(_kc[1], "H98")
            _m(_kc[2], "P_e_net_MW")
            _m(_kc[3], "q_div_MW_m2")
        except Exception:
            pass
        with st.expander("Downloads, export bundle, and full details", expanded=False):
            _v182_render_latest_systems_solve_results(
                artifact=_sys_latest_art,
                point_artifact=getattr(_v92_state_get(), 'last_point_artifact', None),
                key_prefix="downloads",
            )

    # Candidate sources (recovery/search)
    def _sys_get_candidates():
        cands = []
        try:
            rec = st.session_state.get("v178_last_recovery")
            if isinstance(rec, dict) and isinstance(rec.get("candidates"), list):
                cands += list(rec["candidates"])
        except Exception:
            pass
        try:
            fs = st.session_state.get("v178_fs_last")
            if isinstance(fs, dict) and isinstance(fs.get("candidates"), list):
                cands += list(fs["candidates"])
        except Exception:
            pass
        # de-dup by hash if present
        seen=set()
        out=[]
        for c in cands:
            try:
                hid = c.get("inputs_hash") or c.get("hash") or json.dumps(c.get("x",{}), sort_keys=True, default=str)
            except Exception:
                hid = str(id(c))
            if hid in seen:
                continue
            seen.add(hid)
            out.append(c)
        return out

    # Ranking profiles (transparent + deterministic)
    _RANKING_PROFILES = {
        "Balanced": {"feasible_only": False, "w_margin": 1.0, "w_perf": 1.0, "w_compact": 0.3},
        "Margin-first": {"feasible_only": True, "w_margin": 2.0, "w_perf": 0.5, "w_compact": 0.2},
        "Performance-first": {"feasible_only": False, "w_margin": 0.6, "w_perf": 2.0, "w_compact": 0.1},
        "Compactness-first": {"feasible_only": False, "w_margin": 0.8, "w_perf": 0.8, "w_compact": 2.0},
    }
    st.session_state.setdefault("systems_ranking_profile", "Balanced")

    def _cand_margin_score(c: dict) -> float:
        # Prefer explicit margin dict; otherwise use best_margins
        try:
            m = c.get("margins") or c.get("best_margins") or {}
            if isinstance(m, dict) and m:
                return float(min(m.values()))
        except Exception:
            pass
        return float("nan")

    def _cand_perf_score(c: dict) -> float:
        # Use any headline metric if present
        for k in ("Q_DT_eqv", "H98", "P_e_net_MW", "Pfus_MW"):
            try:
                v = (c.get("headline") or {}).get(k)
                if v is not None:
                    return float(v)
            except Exception:
                pass
            try:
                v = c.get(k)
                if v is not None:
                    return float(v)
            except Exception:
                pass
        return float("nan")

    def _cand_compact_score(c: dict) -> float:
        # smaller R0 is "more compact"; return negative R0 so higher is better
        for k in ("R0_m", "R0"):
            try:
                v = (c.get("x") or {}).get(k)
                if v is not None:
                    return -float(v)
            except Exception:
                pass
        return 0.0

    def _rank_candidates(cands: list[dict], profile_name: str) -> list[dict]:
        prof = _RANKING_PROFILES.get(profile_name, _RANKING_PROFILES["Balanced"])
        def key(c):
            ms = _cand_margin_score(c)
            ps = _cand_perf_score(c)
            cs = _cand_compact_score(c)
            # NaNs sort last deterministically
            ms2 = ms if (ms == ms) else -1e9
            ps2 = ps if (ps == ps) else -1e9
            score = prof["w_margin"]*ms2 + prof["w_perf"]*ps2 + prof["w_compact"]*cs
            # deterministic tie-breaker: inputs_hash string
            t = str(c.get("inputs_hash") or c.get("hash") or "")
            return (score, ms2, ps2, t)
        return sorted(cands, key=key, reverse=True)

    # Families: simple archetype grouping for cognitive load reduction
    def _candidate_family(c: dict) -> str:
        """Cluster candidates into families using constraint-signature first (beyond heuristics).

        Signature clustering is robust across different objective profiles and aligns with
        how systems engineers think: "these designs fail the same way".
        """
        # 1) Constraint-signature clustering (preferred)
        try:
            sig_parts = []
            for k in ["failed_blocking", "failed_hard", "failed_diagnostic", "failed_ignored", "failed"]:
                v = c.get(k)
                if isinstance(v, list) and v:
                    sig_parts.extend([str(x) for x in v if x is not None])
            sig_parts = sorted(set(sig_parts))
            if sig_parts:
                # Keep family names readable: first few constraints + count
                head = ", ".join(sig_parts[:3])
                more = (len(sig_parts) - 3)
                return f"Signature: {head}" + (f" (+{more})" if more > 0 else "")
        except Exception:
            pass

        # 2) Dominant limiter grouping (fallback)
        try:
            lim = str(c.get("dominant_limiter") or c.get("limiter") or "")
            if lim:
                return f"Limiter: {lim}"
        except Exception:
            pass

        # 3) Compactness heuristic (last fallback)
        try:
            r0 = (c.get("x") or {}).get("R0_m", None)
            if r0 is not None:
                return "Compact" if float(r0) < 3.0 else "Large"
        except Exception:
            pass
        return "Other"

    # Validity / confidence panel: aggregate typical-range flags if present
    def _validity_summary(art: dict) -> dict:
        out = {"validity_score": None, "flags": []}
        try:
            flags = art.get("typical_range_flags") or art.get("flags") or []
            if isinstance(flags, dict):
                flags = [f"{k}:{v}" for k, v in flags.items() if v]
            if isinstance(flags, list):
                out["flags"] = list(flags)
                # Simple score: fewer flags => higher score
                out["validity_score"] = max(0.0, 1.0 - 0.1*len(out["flags"]))
        except Exception:
            pass
        return out

    with st.expander("World-class Systems Mode (Decision support & audit)", expanded=False):
        st.caption("Explainable feasibility, transparent ranking, families/frontier, decision journal, artifact viewer, and model validity - all 0-D friendly and audit-ready.")

        # Ranking profile selector
        st.selectbox("Ranking profile", options=list(_RANKING_PROFILES.keys()), key="systems_ranking_profile")

        # v185: Freeze-grade provenance badge (always visible in this audit panel)
        try:
            import hashlib as _hashlib, json as _json
            _art = _sys_latest_art if isinstance(_sys_latest_art, dict) else {}
            _ih = str(_art.get("inputs_hash") or "")
            _schema = str(_art.get("schema_version") or "1")
            _intent = str(st.session_state.get("design_intent") or "")
            _rankp = str(st.session_state.get("systems_ranking_profile") or "")
            _seed = str(st.session_state.get("v178_fs_seed", ""))

            _mini = {"inputs_hash": _ih, "schema_version": _schema, "intent": _intent, "ranking": _rankp, "seed": _seed}
            _mini_hash = _hashlib.sha256(_json.dumps(_mini, sort_keys=True, default=str).encode("utf-8")).hexdigest()[:12]
            st.caption(f"**Provenance** - schema={_schema} â€¢ inputs_hash={_ih[:12]} â€¢ intent={_intent} â€¢ ranking={_rankp} â€¢ seed={_seed} â€¢ badge={_mini_hash}")
        except Exception:
            pass

        # v185: Micro-onboarding (workflow guidance)
        with st.expander("What to do next (30-second workflow)", expanded=False):
            st.markdown("- **Diagnose**: Run *Precheck* to see dominant limiters and unreachable targets.\n- **Recover**: Run *Seeded Recovery* near your seed, then Apply to Base/x0.\n- **Explore**: Run *Feasible Design Search* to generate Topâ€‘K candidates and families/frontier.\n- **Compare/Apply**: Pick a candidate, apply, and re-run precheck for confirmation.\n- **Export**: Download artifact + Decision Report PDF for audit-grade sharing.")

        # v185: Reproduce + Diff + Regression guardrails (no hidden state)
        with st.expander("Reproduce / Diff / Regression (freeze-grade)", expanded=False):
            try:
                s = _v98_state_init_runlists()
                runs = [r for r in (getattr(s,'run_history',[]) or []) if str(r.get("kind","")) == "systems"]
                runs = list(reversed(runs))  # newest first
                if not runs:
                    st.info("No recorded Systems runs yet. Run Precheck/Recovery/Search/Solve first.")
                else:
                    labels = [f"{r.get('id')} - {r.get('ts')} - {r.get('mode','')}" for r in runs]
                    _ids = [r.get('id') for r in runs]
                    rid = st.selectbox("Pick a recorded Systems run", options=_ids, format_func=lambda x: labels[_ids.index(x)], key="systems_repro_pick")
                    c1,c2 = st.columns(2)
                    with c1:
                        if st.button("Reproduce this run (restore full UI state)", use_container_width=True, key="systems_repro_btn"):
                            st.session_state["systems_restore_rid"] = rid
                            st.success("Restoring runâ€¦")
                            st.rerun()
                    with c2:
                        st.download_button("Download run artifact JSON", data=_shams_json_dumps(next((r for r in runs if r.get('id')==rid), {}).get('payload', {}), indent=2, sort_keys=True).encode("utf-8"),
                                           file_name=f"{rid}.json", mime="application/json", use_container_width=True, key="systems_repro_dl_json")

                    # Diff two runs
                    st.markdown("**Diff two runs (structural JSON diff)**")
                    rid_a = st.selectbox("Run A", options=_ids, key="systems_diff_a")
                    rid_b = st.selectbox("Run B", options=_ids, index=min(1, len(_ids)-1), key="systems_diff_b")
                    A = next((r.get("payload") for r in runs if r.get("id")==rid_a), {})
                    B = next((r.get("payload") for r in runs if r.get("id")==rid_b), {})
                    diffs = _v98_json_diff(A, B)
                    st.write(f"Changed fields: {len(diffs)}")
                    for d in diffs[:200]:
                        st.write("- " + d)

                    # Regression guard: emit a minimal test case JSON
                    st.markdown("**Create regression test from Run A**")
                    reg = {
                        "rid": rid_a,
                        "schema_version": (A.get("schema_version") if isinstance(A, dict) else None),
                        "design_intent": (A.get("design_intent") if isinstance(A, dict) else None),
                        "inputs_hash": (A.get("inputs_hash") if isinstance(A, dict) else None),
                        "expected": {
                            "ok": (A.get("ok") if isinstance(A, dict) else None),
                            "reason": (A.get("reason") if isinstance(A, dict) else None),
                        },
                    }
                    st.download_button("Download regression JSON", data=_shams_json_dumps(reg, indent=2, sort_keys=True).encode("utf-8"),
                                       file_name=f"regression_{rid_a}.json", mime="application/json", use_container_width=True, key="systems_reg_dl")
            except Exception as _e:
                st.caption(f"Reproduce/Diff unavailable: {_e!r}")

        # v185: Preset QA harness (in-app, developer friendly)
        with st.expander("Preset QA harness (developer)", expanded=False):
            st.caption("Runs the repo-level smoke QA (scripts/run_systems_qa.py) inside the app process. PASS/FAIL only.")
            if st.button("Run Systems QA smoke check", use_container_width=True, key="systems_run_qa_btn"):
                try:
                    from scripts.run_systems_qa import main as _qa_main
                    rc = int(_qa_main())
                    if rc == 0:
                        st.success("SYSTEMS_QA: PASS")
                    else:
                        st.error(f"SYSTEMS_QA: FAIL (exit {rc})")
                except Exception as _e:
                    st.error(f"QA runner failed: {_e!r}")


        # Latest limiter explanations
        st.subheader("Limiter graph")
        st.write("Curated causal map: **what usually drives each constraint** (qualitative).")
        st.json(_LIMITER_GRAPH)

        # Local sensitivities (fast finite-difference around current Systems base)
        st.subheader("Local sensitivities (finite difference)")
        st.caption("Computes local sensitivities around the current Systems base point using the same evaluator as the point model (0-D).")
        _sens_knobs = st.multiselect("Knobs", ["R0_m","a_m","kappa","delta","Bt_T","Ip_MA","fG","Paux_MW"], default=["Ip_MA","fG","Paux_MW"])
        _sens_outputs = st.multiselect("Outputs", ["Q_DT_eqv","H98","Pfus_MW","Palpha_MW","beta_N","nbar20","Tbr","q95","q_div","B_peak_T","sigma_vm_MPa"], default=["Q_DT_eqv","H98","q95"])
        _h = st.number_input("Step size (absolute)", value=0.05, min_value=1e-6, format="%.6f", key="systems_sens_h")
        if st.button("Compute sensitivities", use_container_width=True, key="systems_sens_btn"):
            try:
                from solvers.sensitivity import local_sensitivities
                from phase1_hot_ion_point import hot_ion_point
                def _ev(x: PointInputs):
                    out = hot_ion_point(x)
                    return out if isinstance(out, dict) else {}
                sens = local_sensitivities(base, params=_sens_knobs, outputs=_sens_outputs, evaluator=_ev, h=float(_h))
                # render as table
                rows=[]
                for outk, dd in (sens or {}).items():
                    for pk, dv in (dd or {}).items():
                        rows.append({"output": outk, "param": pk, "d(output)/d(param)": float(dv)})
                if rows:
                    st.dataframe(rows, use_container_width=True, hide_index=True)
                else:
                    st.info("No sensitivity data returned.")
                _sys_journal_append("sensitivities", {"knobs": _sens_knobs, "outputs": _sens_outputs, "h": float(_h)})
            except Exception as e:
                st.error(f"Sensitivity failed: {e}")

        # Candidates + frontier (Pareto) view
        st.subheader("Candidates, families, and frontier (Pareto)")
        cands = _sys_get_candidates()
        if not cands:
            st.info("No candidates available yet. Run Recovery or Feasible Search first.")
        else:
            ranked = _rank_candidates(cands, st.session_state.get("systems_ranking_profile","Balanced"))
            # v185: Explain why the top candidate wins (score breakdown)
            try:
                if ranked:
                    top = ranked[0]
                    st.subheader("Why the top candidate wins (explainable ranking)")
                    bd = top.get("score_breakdown") if isinstance(top, dict) else None
                    if not isinstance(bd, dict):
                        # Fallback: reconstruct minimal breakdown
                        bd = {
                            "perf_score": _cand_perf_score(top),
                            "margin_score": _cand_margin_score(top),
                            "distance_score": float(top.get("distance_score", 0.0)) if isinstance(top, dict) else 0.0,
                            "total_score": float(top.get("score", _cand_perf_score(top)+_cand_margin_score(top))) if isinstance(top, dict) else (_cand_perf_score(top)+_cand_margin_score(top)),
                            "tie_breakers": top.get("tie_breakers", []) if isinstance(top, dict) else [],
                        }
                    st.json(bd)

                    # v185: Research sanity rails - warn if best-compromise is outside validity regime
                    try:
                        if _design_intent_key() == "research":
                            vf = _validity_summary(_sys_latest_art if isinstance(_sys_latest_art, dict) else {})
                            flags = list(vf.get("flags") or [])
                            if len(flags) >= 4 or float(vf.get("validity_score", 1.0)) < 0.6:
                                st.warning("Sanity rail: candidate appears outside typical validated regime (many validity flags). Treat as exploratory only.")
                    except Exception:
                        pass
            except Exception:
                pass

            # Families
            fam={}
            for c in ranked:
                fam.setdefault(_candidate_family(c), []).append(c)
            st.write({k: len(v) for k,v in fam.items()})
            # Frontier axes
            xk = st.selectbox("Frontier X", ["perf_score","margin_score"], index=0, key="systems_frontier_x")
            yk = st.selectbox("Frontier Y", ["margin_score","perf_score"], index=1, key="systems_frontier_y")
            # Build points
            pts=[]
            for c in ranked:
                pts.append({
                    "id": str(c.get("inputs_hash") or c.get("hash") or ""),
                    "perf_score": _cand_perf_score(c),
                    "margin_score": _cand_margin_score(c),
                    "family": _candidate_family(c),
                })
            try:
                import pandas as _pd
                df=_pd.DataFrame(pts)
                st.dataframe(df, use_container_width=True, hide_index=True)
                # simple plotly scatter if available
                try:
                    import plotly.express as px
                    fig=px.scatter(df, x=xk, y=yk, hover_name="id", color="family")
                    st.plotly_chart(fig, use_container_width=True)
                except Exception:
                    pass
            except Exception:
                pass

        # Decision journal
        st.subheader("Decision journal (Design Stories 2.0)")
        j = st.session_state.get("systems_journal", [])
        st.write(f"Entries: {len(j)}")
        if j:
            st.dataframe(list(reversed(j[-50:])), use_container_width=True, hide_index=True)
            # export as markdown
            try:
                import json as _json
                md = "# SHAMS Systems Mode - Decision Journal\n\n"
                for e in j:
                    md += f"- **{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(e.get('ts_unix',0)))}** [{e.get('kind','')}] step={e.get('workflow_step','')} intent={e.get('design_intent','')}\n"
                st.download_button("Download decision journal (md)", data=md.encode("utf-8"), file_name="systems_decision_journal.md", mime="text/markdown", use_container_width=True, key="systems_dl_journal_md")
                st.download_button("Download decision journal (json)", data=_json.dumps(j, indent=2, sort_keys=True, default=str).encode("utf-8"), file_name="systems_decision_journal.json", mime="application/json", use_container_width=True, key="systems_dl_journal_json")
                # Decision Report PDF (journal + top candidates + artifact hashes)
                try:
                    from tools.reports.decision_report import build_decision_report_pdf_bytes
                    _cand = _sys_get_candidates()
                    _ranked = _rank_candidates(_cand, st.session_state.get("systems_ranking_profile","Balanced")) if _cand else []
                    pdf_bytes = build_decision_report_pdf_bytes(
                        systems_artifact=_sys_latest_art if isinstance(_sys_latest_art, dict) else None,
                        point_artifact=getattr(_v92_state_get(), 'last_point_artifact', None),
                        journal=j,
                        top_candidates=_ranked[:10],
                    )
                    st.download_button("Download Decision Report (PDF)", data=pdf_bytes, file_name="shams_systems_decision_report.pdf", mime="application/pdf", use_container_width=True, key="systems_dl_decision_report_pdf")

                    # v185: Executive Summary (1-page PDF)
                    try:
                        from tools.reports.executive_summary import build_executive_summary_pdf_bytes
                        _cand = _sys_get_candidates()
                        _ranked = _rank_candidates(_cand, st.session_state.get("systems_ranking_profile","Balanced")) if _cand else []
                        pdf2 = build_executive_summary_pdf_bytes(
                            systems_artifact=_sys_latest_art if isinstance(_sys_latest_art, dict) else None,
                            point_artifact=getattr(_v92_state_get(), 'last_point_artifact', None),
                            top_candidate=_ranked[0] if _ranked else None,
                        )
                        st.download_button("Download Executive Summary (PDF, 1 page)", data=pdf2, file_name="shams_systems_executive_summary.pdf", mime="application/pdf", use_container_width=True, key="systems_dl_exec_summary_pdf")
                    except Exception as _e2:
                        st.caption(f"Executive summary unavailable: {_e2}")

                except Exception as _e:
                    st.caption(f"PDF report unavailable: {_e}")

            except Exception:
                pass
        else:
            st.caption("Journal entries are created when you run sensitivities or systems actions (precheck/recovery/search/apply/export).")

        # Artifact viewer (load JSON, upgrade schema, display)
        st.subheader("Artifact viewer (schema-stable)")
        up = st.file_uploader("Upload a Systems artifact JSON", type=["json"], key="systems_artifact_uploader")
        if up is not None:
            try:
                raw = up.read()
                obj = json.loads(raw.decode("utf-8"))
                from src.systems.schema import upgrade_systems_artifact
                obj2 = upgrade_systems_artifact(obj)
                st.success(f"Loaded artifact. schema_version={obj2.get('schema_version','?')}")
                if st.button("Restore full Systems UI state from this artifact", use_container_width=True, key="systems_restore_from_artifact_btn"):
                    try:
                        st.session_state["systems_last_solve_artifact"] = obj2
                        try:
                            _v92_state_get().last_systems_result = obj2
                        except Exception:
                            pass

                        ui = obj2.get("ui_state") if isinstance(obj2.get("ui_state"), dict) else obj2

                        # Restore common cached panels if present
                        for k in ["systems_run_cards","systems_journal","v178_last_precheck","v178_last_recovery","v178_fs_last"]:
                            if isinstance(ui, dict) and (k in ui):
                                st.session_state[k] = ui.get(k)

                        # Restore workflow/intention hints
                        st.session_state["_pending_workflow_step"] = str((ui.get("workflow_step") if isinstance(ui, dict) else "") or "Diagnose")
                        if isinstance(ui, dict) and ui.get("design_intent"):
                            st.session_state["design_intent"] = ui.get("design_intent")

                        st.success("Systems UI state restored from artifact.")
                        st.rerun()
                    except Exception as e:
                        st.error(f"Restore failed: {e}")

                st.json(obj2)
                _sys_journal_append("artifact_view", {"file": up.name, "schema_version": obj2.get("schema_version", None)})
            except Exception as e:
                st.error(f"Failed to load artifact: {e}")

        # Validity / confidence
        st.subheader("Model validity & confidence")
        if isinstance(_sys_latest_art, dict):
            vs = _validity_summary(_sys_latest_art)
            st.write(vs)
            if vs.get("flags"):
                st.warning("Some typical-range / validity flags are present. Treat results as assumption-dependent.")
        else:
            st.caption("No Systems artifact cached yet.")
# Run Cards (MUST): standardized summaries for every Systems action
    # -----------------------------
    with st.expander('Run cards (history)', expanded=False):
        cards = st.session_state.get('systems_run_cards', []) or []
        if not cards:
            st.info('No Systems run cards yet. Run precheck / recovery / search / solve to populate.')
        else:
            import pandas as _pd
            rows = []
            for c in list(cards)[::-1]:
                o = c.get('outcome', {}) or {}
                rows.append({
                    'ts': c.get('ts'),
                    'kind': c.get('kind'),
                    'status': o.get('status'),
                    'reason': o.get('reason'),
                    'dominant_limiter': o.get('dominant_limiter'),
                })
            st.dataframe(_pd.DataFrame(rows), use_container_width=True, hide_index=True)
            # Detail view
            idx = st.number_input('Detail index (0 = most recent)', min_value=0, max_value=max(0, len(cards)-1), value=0, step=1)
            try:
                cc = list(cards)[-(int(idx)+1)]
                st.markdown('**Details**')
                st.json(cc)
            except Exception:
                pass


    # -----------------------------
    # Design stories (COULD): save/load decision-grade snapshots
    # -----------------------------
    st.session_state.setdefault('systems_design_stories', [])
    with st.expander('Design stories (save / load / export)', expanded=False):
        stories = st.session_state.get('systems_design_stories', []) or []
        name = st.text_input('Story name', value=f"Story {len(stories)+1}", key='systems_story_name')
        notes = st.text_area('Notes (optional)', value='', key='systems_story_notes', height=80)
        if st.button('Save current Systems story', use_container_width=True, key='systems_story_save'):
            try:
                story = {
                    'ts': _sys_now_iso(),
                    'name': str(name).strip() or f"Story {len(stories)+1}",
                    'notes': str(notes or ''),
                    'design_intent': st.session_state.get('design_intent'),
                    'base_overrides': dict(st.session_state.get('systems_base_overrides', {}) or {}),
                    'bounds_overrides': dict(st.session_state.get('systems_bounds_overrides', {}) or {}),
                    'inputs_overrides': dict(st.session_state.get('systems_inputs_overrides', {}) or {}),
                    'last_precheck': getattr(st.session_state.get('last_precheck_report', None), '__dict__', None),
                    'last_recovery': dict(st.session_state.get('v178_last_recovery', {}) or {}),
                    'last_feasible_search': dict(st.session_state.get('v178_fs_last', {}) or {}),
                    'last_run_card': (st.session_state.get('systems_run_cards', []) or [])[-1] if (st.session_state.get('systems_run_cards') or []) else None,
                }
                st.session_state['systems_design_stories'] = (stories + [story])[-50:]
                st.success('Saved.')
            except Exception as _e:
                st.error(f'Failed to save: {_e}')

        if stories:
            st.markdown('**Saved stories**')
            labels = [f"{i}: {s.get('name','(unnamed)')} ({s.get('ts','')})" for i, s in enumerate(stories)]
            sel = st.selectbox('Select', options=list(range(len(stories))), format_func=lambda i: labels[i], key='systems_story_sel')
            s = stories[int(sel)]
            c1, c2 = st.columns(2)
            with c1:
                if st.button('Load Base + x0 from story', use_container_width=True, key='systems_story_load'):
                    try:
                        # Load Base overrides (staged safely)
                        bo = dict(s.get('base_overrides', {}) or {})
                        st.session_state['systems_base_overrides'] = bo
                        st.session_state['systems_pending_base_apply'] = dict(bo)
                        st.session_state['systems_pending_base_apply_source'] = 'StoryLoadBase'
                        # Load x0 into bounds overrides if available
                        bdo = dict(s.get('bounds_overrides', {}) or {})
                        if bdo:
                            st.session_state['systems_bounds_overrides'] = bdo
                        # Load inputs overrides (diagnostic limits etc.)
                        io = dict(s.get('inputs_overrides', {}) or {})
                        if io:
                            st.session_state['systems_inputs_overrides'] = io
                        st.success('Loaded. Re-running precheckâ€¦')
                        st.session_state['systems_run_precheck_now'] = True
                        st.rerun()
                    except Exception as _e:
                        st.error(f'Load failed: {_e}')
            with c2:
                if st.button('Delete story', use_container_width=True, key='systems_story_delete'):
                    try:
                        st.session_state['systems_design_stories'] = [ss for j, ss in enumerate(stories) if j != int(sel)]
                        st.success('Deleted.')
                        st.rerun()
                    except Exception:
                        pass
            with st.expander('Story details', expanded=False):
                st.json(s)
            try:
                import json as _json
                st.download_button('Export stories JSON', data=_json.dumps(stories, indent=2, sort_keys=True, default=str),
                                   file_name='shams_systems_design_stories.json', mime='application/json', use_container_width=True)
            except Exception:
                pass
        else:
            st.caption('No stories yet. Save a story after a Recovery/Search run.')

    # -----------------------------
    # Constraint activity timeline (SHOULD): summarize what dominated during a run
    # -----------------------------
    with st.expander('Constraint activity timeline (dominant limiter over steps)', expanded=False):
        src_opts2 = ['Seeded Recovery (last)', 'Feasible Search (last)']
        src2 = st.radio('Source', src_opts2, index=0, horizontal=True, key='systems_timeline_src')
        rows_tl = []
        try:
            if src2 == 'Seeded Recovery (last)':
                rep = st.session_state.get('v178_last_recovery', {}) or {}
                tr = rep.get('trace') or []
                for i, t in enumerate(tr):
                    rows_tl.append({
                        'i': i,
                        'feasible': bool(t.get('feasible')),
                        'V': t.get('V'),
                        'dominant': t.get('dominant'),
                    })
            else:
                rep = st.session_state.get('v178_fs_last', {}) or {}
                tr = rep.get('trace') or []
                for i, t in enumerate(tr):
                    hf = list(t.get('hard_failed', []) or [])
                    dom = None
                    if hf:
                        dom = hf[0]
                    rows_tl.append({
                        'i': i,
                        'feasible': bool(t.get('feasible')),
                        'V': t.get('V'),
                        'dominant': dom,
                        'min_margin': t.get('min_margin'),
                    })
        except Exception:
            rows_tl = []

        if not rows_tl:
            st.info('No trace available yet. Run Seeded Recovery or Feasible Search with trace enabled.')
        else:
            import pandas as _pd
            df = _pd.DataFrame(rows_tl)
            st.dataframe(df, use_container_width=True, hide_index=True)
            # Aggregate view
            try:
                doms = [r.get('dominant') for r in rows_tl if r.get('dominant')]
                if doms:
                    from collections import Counter
                    cnt = Counter(doms)
                    top = cnt.most_common(5)
                    st.markdown('**Most frequent dominant limiters**')
                    st.write('\n'.join([f"- {k}: {v} steps" for k, v in top]))
            except Exception:
                pass


    # -----------------------------
    # Stability & control margin certification (v374.0)
    # -----------------------------
    with st.expander('Stability & control margin certification (vertical / RWM / volt-seconds)', expanded=False):
        st.caption(
            "Deterministic, algebraic certification derived from the last Systems artifact (no solves, no iteration). "
            "Reports vertical stability proxy margin, RWM proximity, and CS flux-swing (volt-seconds) headroom."
        )

        # Safe defaults (UI law): never conditionally define keys used later.
        st.session_state.setdefault('systems_stability_cert', None)
        eps_active = float(st.session_state.get('systems_stability_eps_active', 0.01))
        eps_tight = float(st.session_state.get('systems_stability_eps_tight', 0.10))
        probe_frac = float(st.session_state.get('systems_stability_probe_frac', 0.01))

        cA, cB, cC = st.columns([1, 1, 1])
        with cA:
            st.session_state['systems_stability_eps_active'] = st.number_input('Îµ_active', min_value=0.0, max_value=0.5, value=eps_active, step=0.005, format='%.3f')
        with cB:
            st.session_state['systems_stability_eps_tight'] = st.number_input('Îµ_tight', min_value=0.0, max_value=1.0, value=eps_tight, step=0.01, format='%.2f')
        with cC:
            _stability_probe_frac = st.number_input(
                'Probe fraction', min_value=0.0, max_value=0.1,
                value=float(st.session_state.get('systems_stability_probe_frac', probe_frac)),
                step=0.005, format='%.3f', key='systems_stability_probe_frac_input'
            )
            st.session_state['systems_stability_probe_frac'] = float(_stability_probe_frac)

        can_compute = isinstance(last_sys_art, dict) and isinstance(last_sys_art.get('outputs'), dict)
        if not can_compute:
            st.info('No Systems artifact available yet. Run a Systems solve first.')
        else:
            if st.button('Compute certification (cache)', use_container_width=True, key='systems_compute_stability_cert_btn'):
                try:
                    from src.certification.stability_control_certification_v374 import (
                        certify_stability_control_margins,
                        certification_table_rows,
                    )

                    outs = dict(last_sys_art.get('outputs') or {})
                    ins = dict(last_sys_art.get('inputs') or {})
                    run_id = str(last_sys_art.get('run_id') or (last_sys_art.get('run') or {}).get('run_id') or '')
                    ih = str(last_sys_art.get('inputs_hash') or '')

                    cert = certify_stability_control_margins(
                        outputs=outs,
                        inputs=ins,
                        run_id=(run_id or None),
                        inputs_hash=(ih or None),
                        eps_active=float(st.session_state['systems_stability_eps_active']),
                        eps_tight=float(st.session_state['systems_stability_eps_tight']),
                        probe_frac=float(st.session_state['systems_stability_probe_frac']),
                    )
                    st.session_state['systems_stability_cert'] = cert
                    st.success('Certification computed and cached (systems_stability_cert).')
                except Exception as _e:
                    st.error(f'Certification failed: {_e}')

            cert = st.session_state.get('systems_stability_cert', None)
            if isinstance(cert, dict):
                try:
                    import pandas as _pd
                    from src.certification.stability_control_certification_v374 import certification_table_rows
                    rows, cols = certification_table_rows(cert)
                    st.dataframe(_pd.DataFrame(rows, columns=cols), use_container_width=True, hide_index=True)
                except Exception:
                    st.json(cert)

                # Download JSON
                try:
                    st.download_button(
                        'Download certification JSON',
                        data=json.dumps(cert, indent=2, sort_keys=True, default=str),
                        file_name='systems_stability_control_margin_certification_v374.json',
                        mime='application/json',
                        use_container_width=True,
                        key='systems_dl_stability_cert_json',
                    )
                except Exception:
                    pass

                with st.expander('Certification details (JSON)', expanded=False):
                    st.json(cert)

    # -----------------------------
    # Control & actuation authority (v378.0)
    # -----------------------------
    with st.expander('Control & actuation authority (PF/RWM, certified) â€” actuator margins', expanded=False):
        st.caption(
            "Deterministic governance-only certification derived from the last Systems artifact (no solves, no iteration). "
            "Computes actuator margin proxies: VS bandwidth/power caps, PF power cap, RWM power proxy vs cap, and CS/V-loop headroom."
        )

        st.session_state.setdefault('systems_control_actuation_cert', None)
        # Governance caps (UI inputs; do not mutate truth).
        st.session_state.setdefault('systems_v378_vs_bw_cap_Hz', 300.0)
        st.session_state.setdefault('systems_v378_vs_P_cap_MW', 50.0)
        st.session_state.setdefault('systems_v378_pf_P_cap_MW', 200.0)
        st.session_state.setdefault('systems_v378_rwm_P_cap_MW', 20.0)
        st.session_state.setdefault('systems_v378_rwm_P_ref_MW', 10.0)

        cap_cols = st.columns(5)
        with cap_cols[0]:
            st.session_state['systems_v378_vs_bw_cap_Hz'] = st.number_input(
                'VS BW cap (Hz)', min_value=1.0, max_value=5000.0,
                value=float(st.session_state.get('systems_v378_vs_bw_cap_Hz', 300.0)),
                step=10.0, format='%.1f'
            )
        with cap_cols[1]:
            st.session_state['systems_v378_vs_P_cap_MW'] = st.number_input(
                'VS P cap (MW)', min_value=0.1, max_value=2000.0,
                value=float(st.session_state.get('systems_v378_vs_P_cap_MW', 50.0)),
                step=5.0, format='%.2f'
            )
        with cap_cols[2]:
            st.session_state['systems_v378_pf_P_cap_MW'] = st.number_input(
                'PF P cap (MW)', min_value=0.1, max_value=5000.0,
                value=float(st.session_state.get('systems_v378_pf_P_cap_MW', 200.0)),
                step=10.0, format='%.2f'
            )
        with cap_cols[3]:
            st.session_state['systems_v378_rwm_P_cap_MW'] = st.number_input(
                'RWM P cap (MW)', min_value=0.1, max_value=2000.0,
                value=float(st.session_state.get('systems_v378_rwm_P_cap_MW', 20.0)),
                step=1.0, format='%.2f'
            )
        with cap_cols[4]:
            st.session_state['systems_v378_rwm_P_ref_MW'] = st.number_input(
                'RWM P ref (MW)', min_value=0.1, max_value=2000.0,
                value=float(st.session_state.get('systems_v378_rwm_P_ref_MW', 10.0)),
                step=1.0, format='%.2f'
            )

        can_compute = isinstance(last_sys_art, dict) and isinstance(last_sys_art.get('outputs'), dict)
        if not can_compute:
            st.info('No Systems artifact available yet. Run a Systems solve first.')
        else:
            if st.button('Compute certification (cache)', use_container_width=True, key='systems_compute_control_actuation_cert_btn'):
                try:
                    from src.certification.control_actuation_certification_v378 import (
                        certify_control_actuation,
                        ActuationCaps,
                    )

                    outs = dict(last_sys_art.get('outputs') or {})
                    ins = dict(last_sys_art.get('inputs') or {})
                    run_id = str(last_sys_art.get('run_id') or (last_sys_art.get('run') or {}).get('run_id') or '')
                    ih = str(last_sys_art.get('inputs_hash') or '')

                    caps = ActuationCaps(
                        vs_bandwidth_cap_Hz=float(st.session_state.get('systems_v378_vs_bw_cap_Hz', 300.0)),
                        vs_power_cap_MW=float(st.session_state.get('systems_v378_vs_P_cap_MW', 50.0)),
                        pf_power_cap_MW=float(st.session_state.get('systems_v378_pf_P_cap_MW', 200.0)),
                        rwm_power_cap_MW=float(st.session_state.get('systems_v378_rwm_P_cap_MW', 20.0)),
                        rwm_power_ref_MW=float(st.session_state.get('systems_v378_rwm_P_ref_MW', 10.0)),
                    )

                    cert = certify_control_actuation(
                        outputs=outs,
                        inputs=ins,
                        run_id=(run_id or None),
                        inputs_hash=(ih or None),
                        caps=caps,
                    )
                    st.session_state['systems_control_actuation_cert'] = cert
                    st.success('Certification computed and cached (systems_control_actuation_cert).')
                except Exception as _e:
                    st.error(f'Certification failed: {_e}')

            cert = st.session_state.get('systems_control_actuation_cert', None)
            if isinstance(cert, dict):
                try:
                    import pandas as _pd
                    from src.certification.control_actuation_certification_v378 import certification_table_rows
                    rows, cols = certification_table_rows(cert)
                    st.dataframe(_pd.DataFrame(rows, columns=cols), use_container_width=True, hide_index=True)
                    tier = (((cert.get('tiers') or {}).get('overall')) if isinstance(cert.get('tiers'), dict) else None)
                    if tier in ('BLOCK', 'TIGHT'):
                        st.warning('Actuation authority is tight or blocking (proxy). Treat as governance risk; truth is unchanged.')
                except Exception:
                    st.json(cert)

                try:
                    st.download_button(
                        'Download certification JSON',
                        data=json.dumps(cert, indent=2, sort_keys=True, default=str),
                        file_name='systems_control_actuation_certification_v378.json',
                        mime='application/json',
                        use_container_width=True,
                        key='systems_dl_control_actuation_cert_json',
                    )
                except Exception:
                    pass

                with st.expander('Certification details (JSON)', expanded=False):
                    st.json(cert)

    # -----------------------------
    # Confinement & transport certification (v376.0)
    # -----------------------------
    with st.expander('Confinement & transport authority (certified) â€” H98 credibility', expanded=False):
        st.caption(
            "Deterministic certification derived from the last Systems artifact (no solves, no iteration). "
            "Reports H98 vs a conservative credibility envelope (intent-aware) and optional Ï„E terms if available."
        )

        # UI law: safe defaults; no conditional variable definitions.
        cA, cB = st.columns([1, 2])
        with cA:
            _transport_probe_frac = st.number_input(
                'Probe fraction', min_value=0.0, max_value=0.1,
                value=float(st.session_state.get('systems_transport_probe_frac', probe_frac)),
                step=0.005, format='%.3f', key='systems_transport_probe_frac_input'
            )
            st.session_state['systems_transport_probe_frac'] = float(_transport_probe_frac)
        with cB:
            st.caption('Envelope is intent-aware (reactor tighter than research). This is a credibility contract only; truth is unchanged.')

        can_compute = isinstance(last_sys_art, dict) and isinstance(last_sys_art.get('outputs'), dict)
        if not can_compute:
            st.info('No Systems artifact available yet. Run a Systems solve first.')
        else:
            if st.button('Compute certification (cache)', use_container_width=True, key='systems_compute_transport_cert_btn'):
                try:
                    from src.certification.transport_confinement_certification_v376 import (
                        certify_transport_confinement,
                    )

                    outs = dict(last_sys_art.get('outputs') or {})
                    ins = dict(last_sys_art.get('inputs') or {})
                    run_id = str(last_sys_art.get('run_id') or (last_sys_art.get('run') or {}).get('run_id') or '')
                    ih = str(last_sys_art.get('inputs_hash') or '')

                    cert = certify_transport_confinement(
                        outputs=outs,
                        inputs=ins,
                        run_id=(run_id or None),
                        inputs_hash=(ih or None),
                        probe_frac=float(st.session_state['systems_transport_probe_frac']),
                    )
                    st.session_state['systems_transport_cert'] = cert
                    st.success('Certification computed and cached (systems_transport_cert).')
                except Exception as _e:
                    st.error(f'Certification failed: {_e}')

            cert = st.session_state.get('systems_transport_cert', None)
            if isinstance(cert, dict):
                try:
                    import pandas as _pd
                    from src.certification.transport_confinement_certification_v376 import certification_table_rows
                    rows, cols = certification_table_rows(cert)
                    st.dataframe(_pd.DataFrame(rows, columns=cols), use_container_width=True, hide_index=True)
                    cls = ((cert.get('classification') or {}).get('H98_class') if isinstance(cert.get('classification'), dict) else None)
                    if cls == 'super-credible-viol':
                        st.warning('H98 exceeds the credibility envelope for this intent. This is a certification flag; truth is unchanged.')
                except Exception:
                    st.json(cert)

                try:
                    st.download_button(
                        'Download certification JSON',
                        data=json.dumps(cert, indent=2, sort_keys=True, default=str),
                        file_name='systems_transport_confinement_certification_v376.json',
                        mime='application/json',
                        use_container_width=True,
                        key='systems_dl_transport_cert_json',
                    )
                except Exception:
                    pass

                with st.expander('Certification details (JSON)', expanded=False):
                    st.json(cert)




    # -----------------------------
    # Transport profile authority (v382.0)
    # -----------------------------
    with st.expander('ðŸ§© Transport profile authority (certified) â€” 1.5D-lite proxies', expanded=False):
        st.caption(
            "Deterministic governance-only certification derived from the last Systems artifact (no solves, no iteration). "
            "Best-effort 1.5D-lite proxies: peaking-factor plausibility (central/avg) and internal inductance (li) bounds by intent tier. "
            "This is a credibility contract only; truth is unchanged."
        )

        st.session_state.setdefault('systems_transport_profile_cert', None)
        can_compute = isinstance(last_sys_art, dict) and isinstance(last_sys_art.get('outputs'), dict)
        if not can_compute:
            st.info('No Systems artifact available yet. Run a Systems solve first.')
        else:
            if st.button('Compute certification (cache)', use_container_width=True, key='systems_compute_transport_profile_cert_btn'):
                try:
                    from src.certification.transport_profile_certification_v382 import (
                        certify_transport_profile,
                    )

                    outs = dict(last_sys_art.get('outputs') or {})
                    ins = dict(last_sys_art.get('inputs') or {})
                    run_id = str(last_sys_art.get('run_id') or (last_sys_art.get('run') or {}).get('run_id') or '')
                    ih = str(last_sys_art.get('inputs_hash') or '')

                    cert = certify_transport_profile(
                        outputs=outs,
                        inputs=ins,
                        run_id=(run_id or None),
                        inputs_hash=(ih or None),
                    ).to_dict()

                    st.session_state['systems_transport_profile_cert'] = cert
                    st.success('Certification computed and cached (systems_transport_profile_cert).')
                except Exception as _e:
                    st.error(f'Certification failed: {_e}')

            cert = st.session_state.get('systems_transport_profile_cert', None)
            if isinstance(cert, dict):
                try:
                    import pandas as _pd
                    from src.certification.transport_profile_certification_v382 import certification_table_rows
                    rows, cols = certification_table_rows(cert)
                    st.dataframe(_pd.DataFrame(rows, columns=cols), use_container_width=True, hide_index=True)
                    tier = str(cert.get('tier') or '')
                    if tier in ('TIGHT', 'BLOCK'):
                        st.warning('Transport profile authority is tight/blocking (proxy). Treat as governance risk; truth is unchanged.')
                except Exception:
                    st.json(cert)

                try:
                    st.download_button(
                        'Download certification JSON',
                        data=json.dumps(cert, indent=2, sort_keys=True, default=str),
                        file_name='systems_transport_profile_certification_v382.json',
                        mime='application/json',
                        use_container_width=True,
                        key='systems_dl_transport_profile_cert_json',
                    )
                except Exception:
                    pass

                with st.expander('Certification details (JSON)', expanded=False):
                    st.json(cert)
    # -----------------------------
    # Current drive authority (v381.0)
    # -----------------------------
    # -----------------------------
    # Materials & lifetime tightening authority (v384.0.0)
    # -----------------------------
    with st.expander('ðŸ§± Materials & lifetime tightening (certified) â€” divertor+magnet + downtimeâ†’CF', expanded=False):
        st.caption(
            "Deterministic governance-only certification derived from the last Systems artifact (no solves, no iteration). "
            "Summarizes the v384 lifetime proxies and the replacement-coupled capacity factor/cost proxy."
        )

        st.session_state.setdefault('systems_materials_lifetime_v384_cert', None)
        can_compute = isinstance(last_sys_art, dict) and isinstance(last_sys_art.get('outputs'), dict)
        if not can_compute:
            st.info('No Systems artifact available yet. Run a Systems solve first.')
        else:
            if st.button('Compute certification (cache)', use_container_width=True, key='systems_compute_materials_life_v384_cert_btn'):
                try:
                    from src.certification.materials_lifetime_certification_v384 import (
                        certify_materials_lifetime_v384,
                    )

                    outs = dict(last_sys_art.get('outputs') or {})
                    ins = dict(last_sys_art.get('inputs') or {})
                    run_id = str(last_sys_art.get('run_id') or (last_sys_art.get('run') or {}).get('run_id') or '')
                    ih = str(last_sys_art.get('inputs_hash') or '')

                    cert = certify_materials_lifetime_v384(
                        outputs=outs,
                        inputs=ins,
                        run_id=(run_id or None),
                        inputs_hash=(ih or None),
                    ).to_dict()

                    st.session_state['systems_materials_lifetime_v384_cert'] = cert
                    st.success('Certification computed and cached (systems_materials_lifetime_v384_cert).')
                except Exception as _e:
                    st.error(f'Certification failed: {_e}')

            cert = st.session_state.get('systems_materials_lifetime_v384_cert', None)
            if isinstance(cert, dict):
                try:
                    import pandas as _pd
                    from src.certification.materials_lifetime_certification_v384 import certification_table_rows
                    rows, cols = certification_table_rows(cert)
                    st.dataframe(_pd.DataFrame(rows, columns=cols), use_container_width=True, hide_index=True)
                    tier = str(cert.get('tier') or '')
                    if tier in ('BLOCK', 'TIGHT'):
                        st.warning('Materials/lifetime authority is tight/blocking (proxy). Treat as governance risk; truth is unchanged.')
                except Exception:
                    st.json(cert)

                try:
                    st.download_button(
                        'Download certification JSON',
                        data=json.dumps(cert, indent=2, sort_keys=True, default=str),
                        file_name='systems_materials_lifetime_certification_v384.json',
                        mime='application/json',
                        use_container_width=True,
                        key='systems_dl_materials_life_v384_cert_json',
                    )
                except Exception:
                    pass

                with st.expander('Certification details (JSON)', expanded=False):
                    st.json(cert)

    with st.expander('âš¡ Current drive authority (certified) â€” regime-aware credibility', expanded=False):
        st.caption(
            "Deterministic governance-only certification derived from the last Systems artifact (no solves, no iteration). "
            "Certifies current-drive and non-inductive fraction claims with conservative efficiency bounds and density-based regime flags."
        )

        st.session_state.setdefault('systems_current_drive_cert', None)
        can_compute = isinstance(last_sys_art, dict) and isinstance(last_sys_art.get('outputs'), dict)
        if not can_compute:
            st.info('No Systems artifact available yet. Run a Systems solve first.')
        else:
            if st.button('Compute certification (cache)', use_container_width=True, key='systems_compute_current_drive_cert_btn'):
                try:
                    from src.certification.current_drive_certification_v381 import evaluate_current_drive_authority

                    outs = dict(last_sys_art.get('outputs') or {})
                    _cert = evaluate_current_drive_authority(outs).to_dict()
                    st.session_state['systems_current_drive_cert'] = _cert
                    st.success('Certification computed and cached (systems_current_drive_cert).')
                except Exception as _e:
                    st.error(f'Certification failed: {_e}')

            cert = st.session_state.get('systems_current_drive_cert', None)
            if isinstance(cert, dict):
                try:
                    import pandas as _pd
                    from src.certification.current_drive_certification_v381 import certification_table_rows
                    st.dataframe(_pd.DataFrame([certification_table_rows(cert)]), use_container_width=True, hide_index=True)
                    tier = str(cert.get('tier') or '')
                    if tier in ('BLOCK', 'TIGHT'):
                        st.warning('Current-drive authority is tight or blocking (proxy). Treat as governance risk; truth is unchanged.')
                except Exception:
                    st.json(cert)

                try:
                    st.download_button(
                        'Download certification JSON',
                        data=json.dumps(cert, indent=2, sort_keys=True, default=str),
                        file_name='systems_current_drive_certification_v381.json',
                        mime='application/json',
                        use_container_width=True,
                        key='systems_dl_current_drive_cert_json',
                    )
                except Exception:
                    pass

                with st.expander('Certification details (JSON)', expanded=False):
                    st.json(cert)

    # -----------------------------
    # Disruption severity & quench proxy authority (v377.0)
    # -----------------------------
    with st.expander('Disruption & quench authority (certified) â€” severity proxies', expanded=False):
        st.caption(
            "Deterministic governance-only certification derived from the last Systems artifact (no solves, no iteration). "
            "Adds consequence-severity proxies: disruption proximity index, thermal quench severity W/A, and halo force proxy."
        )

        st.session_state.setdefault('systems_disruption_quench_cert', None)
        can_compute = isinstance(last_sys_art, dict) and isinstance(last_sys_art.get('outputs'), dict)
        if not can_compute:
            st.info('No Systems artifact available yet. Run a Systems solve first.')
        else:
            if st.button('Compute certification (cache)', use_container_width=True, key='systems_compute_disruption_quench_cert_btn'):
                try:
                    from src.certification.disruption_quench_certification_v377 import certify_disruption_quench

                    outs = dict(last_sys_art.get('outputs') or {})
                    ins = dict(last_sys_art.get('inputs') or {})
                    run_id = str(last_sys_art.get('run_id') or (last_sys_art.get('run') or {}).get('run_id') or '')
                    ih = str(last_sys_art.get('inputs_hash') or '')

                    cert = certify_disruption_quench(
                        outputs=outs,
                        inputs=ins,
                        run_id=(run_id or None),
                        inputs_hash=(ih or None),
                    )
                    st.session_state['systems_disruption_quench_cert'] = cert
                    st.success('Certification computed and cached (systems_disruption_quench_cert).')
                except Exception as _e:
                    st.error(f'Certification failed: {_e}')

            cert = st.session_state.get('systems_disruption_quench_cert', None)
            if isinstance(cert, dict):
                try:
                    import pandas as _pd
                    from src.certification.disruption_quench_certification_v377 import certification_table_rows
                    rows, cols = certification_table_rows(cert)
                    st.dataframe(_pd.DataFrame(rows, columns=cols), use_container_width=True, hide_index=True)
                    tier = (((cert.get('metrics') or {}).get('disruption_proximity_tier')) if isinstance(cert.get('metrics'), dict) else None)
                    if tier == 'HIGH':
                        st.warning('High disruption proximity index (proxy). Treat as governance risk; truth is unchanged.')
                except Exception:
                    st.json(cert)

                try:
                    st.download_button(
                        'Download certification JSON',
                        data=json.dumps(cert, indent=2, sort_keys=True, default=str),
                        file_name='systems_disruption_quench_certification_v377.json',
                        mime='application/json',
                        use_container_width=True,
                        key='systems_dl_disruption_quench_cert_json',
                    )
                except Exception:
                    pass

                with st.expander('Certification details (JSON)', expanded=False):
                    st.json(cert)

    # -----------------------------
    # Frontier visualization (SHOULD)
    # -----------------------------
    with st.expander('Frontier visualization (samples / trace)', expanded=False):
        src_opts = ['Precheck samples', 'Seeded Recovery trace (last)', 'Feasible Search trace (last)']
        src_sel = st.radio('Source', src_opts, index=0, horizontal=True, key='sys_frontier_src')
        x_key = st.selectbox('X-axis variable', ['R0_m','a_m','Bt_T','Ti_keV','Paux_MW','kappa','t_shield_m'], index=0, key='sys_frontier_x')
        y_opts = ['q_div_MW_m2','sigma_vm','sigma_hoop_MPa','TBR','H98','Q_DT_eqv','Pfus_DT_adj_MW']
        # Recovery trace doesn't store full outputs; use V as the y-axis.
        if src_sel == 'Seeded Recovery trace (last)':
            y_opts = ['V (hard violation)']
        y_key = st.selectbox('Y-axis metric', y_opts, index=0, key='sys_frontier_y')
        pts = []
        try:
            if src_sel == 'Precheck samples':
                _pre = st.session_state.get('last_precheck_report', None)
                if _pre is not None:
                    _samp = getattr(_pre, 'samples', None) or []
                    for sr in _samp:
                        try:
                            sp = getattr(sr, 'sample', None)
                            xv = float(getattr(sp, 'values', {}).get(x_key, float('nan')))
                            yv = float(getattr(sr, 'outputs', {}).get(y_key, float('nan')))
                            feas = (len(getattr(sr, 'hard_failed', []) or []) == 0)
                            if math.isfinite(xv) and math.isfinite(yv):
                                pts.append((xv, yv, feas))
                        except Exception:
                            continue
            elif src_sel == 'Seeded Recovery trace (last)':
                rep = st.session_state.get('v178_last_recovery', {}) or {}
                tr = rep.get('trace') or []
                for t in tr:
                    try:
                        x = t.get('x', {}) or {}
                        xv = float(x.get(x_key, float('nan')))
                        yv = float(t.get('V', float('nan')))
                        feas = bool(t.get('feasible'))
                        if math.isfinite(xv) and math.isfinite(yv):
                            pts.append((xv, yv, feas))
                    except Exception:
                        continue
            else:
                rep = st.session_state.get('v178_fs_last', {}) or {}
                tr = rep.get('trace') or []
                for t in tr:
                    try:
                        x = t.get('x', {}) or {}
                        xv = float(x.get(x_key, float('nan')))
                        if y_key == 'V (hard violation)':
                            yv = float(t.get('V', float('nan')))
                        else:
                            met = t.get('metrics', {}) or {}
                            yv = float(met.get(y_key, float('nan')))
                        feas = bool(t.get('feasible'))
                        if math.isfinite(xv) and math.isfinite(yv):
                            pts.append((xv, yv, feas))
                    except Exception:
                        continue
        except Exception:
            pts = []

        if not pts:
            st.info('No points available yet. Run a precheck, seeded recovery, or feasible search first.')
        else:
            try:
                import matplotlib.pyplot as _plt
                xs = [p[0] for p in pts]
                ys = [p[1] for p in pts]
                cs = [p[2] for p in pts]
                fig = _plt.figure()
                ax = fig.add_subplot(111)
                # Plot feasible vs infeasible without hardcoding colors (matplotlib chooses default cycle)
                xf = [x for x, c in zip(xs, cs) if c]
                yf = [y for y, c in zip(ys, cs) if c]
                xi = [x for x, c in zip(xs, cs) if not c]
                yi = [y for y, c in zip(ys, cs) if not c]
                if xf:
                    ax.scatter(xf, yf, label='feasible')
                if xi:
                    ax.scatter(xi, yi, label='infeasible')
                ax.set_xlabel(x_key)
                ax.set_ylabel(y_key)
                ax.legend()
                st.pyplot(fig, use_container_width=True)
            except Exception as _e:
                st.warning(f'Plot failed: {_e}')
            try:
                import json as _json
                st.download_button('Download plotted points JSON', data=_json.dumps([{'x':p[0],'y':p[1],'feasible':p[2]} for p in pts], indent=2, sort_keys=True), file_name='systems_frontier_points.json', mime='application/json', use_container_width=True)
            except Exception:
                pass

    with st.expander('Latest Systems summary (stateful)', expanded=False):
        if isinstance(last_sys_art, dict):
            outs = (last_sys_art.get('outputs') or {})
            st.write('Key outputs (from last Systems run):')
            st.json({
                'Q_DT_eqv': outs.get('Q_DT_eqv'),
                'H98': outs.get('H98'),
                'Pfus_DT_adj_MW': outs.get('Pfus_DT_adj_MW'),
                'P_net_e_MW': outs.get('P_net_e_MW'),
                'q_div_MW_m2': outs.get('q_div_MW_m2'),
                'sigma_hoop_MPa': outs.get('sigma_hoop_MPa'),
                'TBR': outs.get('TBR'),
            })
        else:
            st.info('No Systems artifact yet. Run a Systems solve to populate this summary.')



    # v176.1: always-visible solve controls (so users don't miss them)
    st.markdown('### Solve controls')
    csc1, csc2, csc3 = st.columns([2,2,3])
    with csc1:
        preset = st.selectbox(
            'Solve preset',
            ['Robust (recommended)', 'Fast'],
            index=0,
            key='v176_solve_preset',
            help='Robust uses smaller steps and more iterations; Fast uses more aggressive steps for quicker convergence when well-behaved.',
        )
    with csc2:
        warm_start = st.checkbox(
            'Warm-start (use last Systems solution)',
            value=True,
            key='v176_warm_start',
            help='Uses the last Systems solution values as the initial guess for solved variables (within bounds).',
        )
    with csc3:
        st.caption('Tip: targets/variables live in the expander below. Preset + warm-start affect solver behavior.')

    base0 = st.session_state.get("last_point_inp")
    if base0 is None:
        base0 = PointInputs(R0_m=1.85, a_m=0.57, kappa=1.8, Bt_T=12.2, Ip_MA=8.0, Ti_keV=15.0, fG=0.8, Paux_MW=20.0)

    # v178.11: Robust Base-design apply
    #
    # Streamlit forbids mutating session_state entries that are bound to widgets
    # *after* those widgets have been instantiated in a run. Buttons in later
    # sections (Feasible Search / Seeded Recovery) previously attempted to
    # directly set widget keys like `sys_base_R0_m`, which can raise a
    # StreamlitAPIException and surface as "Failed to apply Base vars.".
    #
    # Fix: stage updates in `systems_pending_base_apply` and apply them here,
    # before the Base-design widgets are created.
    st.session_state.setdefault('systems_pending_base_apply', None)
    # Optional source tag for Undo/history attribution.
    st.session_state.setdefault('systems_pending_base_apply_source', None)
    st.session_state.setdefault('systems_base_history', [])
    _pending_base = st.session_state.get('systems_pending_base_apply')
    if isinstance(_pending_base, dict) and _pending_base:
        try:
            _keymap = {
                'R0_m': 'sys_base_R0_m',
                'a_m': 'sys_base_a_m',
                'kappa': 'sys_base_kappa',
                'delta': 'sys_base_delta',
                'Bt_T': 'sys_base_Bt_T',
                'Ti_keV': 'sys_base_Ti_keV',
                'Ti_over_Te': 'sys_base_Ti_over_Te',
                't_shield_m': 'sys_base_t_shield_m',
            }
            # Save current Base overrides for Undo.
            try:
                _bo_prev = dict(st.session_state.get('systems_base_overrides', {}) or {})
                st.session_state['systems_base_history'].append({
                    'ts_unix': float(time.time()),
                    'base_overrides': _bo_prev,
                    'source': str(st.session_state.get('systems_pending_base_apply_source') or 'pending_apply'),
                })
                # keep history bounded
                st.session_state['systems_base_history'] = st.session_state['systems_base_history'][-50:]
            except Exception:
                pass
            _bo_apply = st.session_state.get('systems_base_overrides', {}) or {}
            for _k, _v in list(_pending_base.items()):
                try:
                    _fv = float(_v)
                    _bo_apply[_k] = _fv
                    _wk = _keymap.get(_k)
                    if _wk:
                        st.session_state[_wk] = _fv
                except Exception:
                    pass
            st.session_state['systems_base_overrides'] = _bo_apply
        except Exception:
            pass
        # Always clear so a bad value doesn't loop forever.
        st.session_state['systems_pending_base_apply'] = None
        st.session_state['systems_pending_base_apply_source'] = None

    with st.expander("Base design (starting point)", expanded=False):
        # v178.1: allow Systems Mode tools (like Seeded Feasibility Recovery) to
        # apply recovered base-design values deterministically.
        # We do this by using explicit widget keys and a small session_state
        # override dict. This does NOT change the physics/solver; it is UI state.
        st.session_state.setdefault('systems_base_overrides', {})
        _bo = st.session_state.get('systems_base_overrides', {}) or {}

        # Safety controls: undo/restore Base design overrides
        _hist = st.session_state.get('systems_base_history', []) or []
        u1, u2, u3 = st.columns([1.2, 1.2, 2.6])
        with u1:
            if st.button('Undo last Base apply', use_container_width=True, disabled=(len(_hist) == 0), key='sys_base_undo_btn'):
                try:
                    last = (st.session_state.get('systems_base_history') or [])[-1]
                    prev = dict(last.get('base_overrides') or {})
                    st.session_state['systems_base_overrides'] = prev
                    # Stage widget-key updates.
                    st.session_state['systems_pending_base_apply'] = dict(prev)
                    st.session_state['systems_run_precheck_now'] = True
                    try:
                        _alog('Systems', 'UndoBaseApply', {'source': str(last.get('source','')), 'n_keys': int(len(prev))})
                    except Exception:
                        pass
                    # Pop after staging
                    st.session_state['systems_base_history'] = (st.session_state.get('systems_base_history') or [])[:-1]
                    st.rerun()
                except Exception:
                    st.warning('Undo failed (unexpected).')
        with u2:
            if st.button('Clear Base history', use_container_width=True, disabled=(len(_hist) == 0), key='sys_base_clear_hist_btn'):
                st.session_state['systems_base_history'] = []
                st.rerun()
        with u3:
            if _hist:
                st.caption(f"Base history: {len(_hist)} step(s). Undo is local to this session.")
        colA, colB, colC = st.columns(3)
        with colA:
            R0_m = _num("R0 [m]", float(_bo.get('R0_m', _safe_get(base0, 'R0_m'))), 0.01, help="Major radius.", key='sys_base_R0_m')
            a_m = _num("a [m]", float(_bo.get('a_m', _safe_get(base0, 'a_m'))), 0.01, help="Minor radius.", key='sys_base_a_m')
            kappa = _num("Îº [-]", float(_bo.get('kappa', _safe_get(base0, 'kappa'))), 0.01, help="Elongation.", key='sys_base_kappa')
            delta = _num("Î´ [-]", float(_bo.get('delta', getattr(base0, "delta", 0.0) or 0.0)), 0.02, min_value=0.0, max_value=0.8, help="Triangularity Î´ used only in the inboard radial-build clearance proxy.", key='sys_base_delta')
        with colB:
            Bt_T = _num("Bt [T]", float(_bo.get('Bt_T', _safe_get(base0, 'Bt_T'))), 0.1, help="On-axis toroidal field.", key='sys_base_Bt_T')
            Ti_keV = _num("Ti [keV]", float(_bo.get('Ti_keV', _safe_get(base0, 'Ti_keV'))), 0.5, help="Ion temperature (volume-average input in 0-D mode).", key='sys_base_Ti_keV')
            Ti_over_Te = _num("Ti/Te [-]", float(_bo.get('Ti_over_Te', getattr(base0, "Ti_over_Te", 2.0))), 0.05, help="Temperature ratio; sets Te.", key='sys_base_Ti_over_Te')
        with colC:
            t_shield_m = _num("Shield thickness [m]", float(_bo.get('t_shield_m', getattr(base0, "t_shield_m", 0.70))), 0.01, help="Inboard shielding thickness proxy (affects neutronics/HTS lifetime).", key='sys_base_t_shield_m')
            steady_state = st.checkbox("Steady-state (no CS pulse constraint)", value=bool(getattr(base0, "steady_state", True)))
            P_net_min_MW = _num("Minimum net electric [MW(e)]", float(getattr(base0, "P_net_min_MW", 0.0)), 5.0, help="Optional requirement; 0 disables hard requirement.")
        # model options
        st.markdown("**Model options**")
        m1, m2, m3 = st.columns(3)
        with m1:
            _c_label = st.selectbox("H-factor reference scaling (for H_scaling)", [
                'IPB98(y,2) (H98 basis)',
                'ITER89-P (L-mode)',
                'Kayeâ€“Goldston (L-mode)',
                'Neo-Alcator (ohmic/L)',
                'Mirnov (ohmic)',
                'Shimomura (L-mode)',
            ], index=0)
            _c_map = {
                'IPB98(y,2) (H98 basis)': 'IPB98y2',
                'ITER89-P (L-mode)': 'ITER89P',
                'Kayeâ€“Goldston (L-mode)': 'KG',
                'Neo-Alcator (ohmic/L)': 'NEOALC',
                'Mirnov (ohmic)': 'MIRNOV',
                'Shimomura (L-mode)': 'SHIMOMURA',
            }
            confinement_scaling = _c_map.get(_c_label, 'IPB98y2')
            confinement_model = str(confinement_scaling).lower()  # back-compat
        with m2:
            profile_model = st.selectbox("Profiles (Â½-D)", ["none","parabolic","pedestal"], index=0)
        with m3:
            zeff_mode = st.selectbox("Zeff mode", ["fixed","from_impurity"], index=0)
        profile_peaking_ne = _num("n peaking (alpha)", float(getattr(base0, "profile_peaking_ne", 1.0)), 0.1, help="Profile peaking control (if profiles enabled).")
        profile_peaking_T  = _num("T peaking (alpha)", float(getattr(base0, "profile_peaking_T", 1.5)), 0.1, help="Profile peaking control (if profiles enabled).")
        bootstrap_model = st.selectbox("Bootstrap proxy model", ["proxy", "improved"], index=0, help="Select bootstrap fraction proxy used for f_bs_proxy.")

        # Optional: compute TF Jop from winding-pack geometry (screening proxy)
        with st.expander("TF winding-pack Jop (optional)", expanded=False):
            tf_Jop_from_wp_geometry = st.checkbox(
                "Compute TF Jop from required ampere-turns and winding-pack area",
                value=bool(getattr(base0, "tf_Jop_from_wp_geometry", False)),
                help="If enabled, SHAMS derives an engineering current density from Bt,R0 and an explicit winding-pack area proxy (no detailed magnet model).",
            )
            tf_wp_width_m = _num("TF winding-pack width [m]", float(getattr(base0, "tf_wp_width_m", 0.25)), 0.01, min_value=0.05, help="Radial width of the winding pack used for Jop-from-geometry proxy.")
            tf_wp_height_factor = _num("TF winding-pack height factor [-]", float(getattr(base0, "tf_wp_height_factor", 2.4)), 0.05, min_value=0.5, help="Height proxy: H_wp = factor * (a*Îº).")
            tf_wp_fill_factor = _num("TF winding-pack fill factor [-]", float(getattr(base0, "tf_wp_fill_factor", 1.0)), 0.05, min_value=0.05, max_value=1.0, help="Fraction of winding-pack area treated as conducting cross-section in the Jop-from-geometry proxy.")

        base = PointInputs(
            R0_m=R0_m, a_m=a_m, kappa=kappa, delta=delta, Bt_T=Bt_T,
            tf_Jop_from_wp_geometry=tf_Jop_from_wp_geometry,
            tf_wp_width_m=tf_wp_width_m,
            tf_wp_height_factor=tf_wp_height_factor,
            tf_wp_fill_factor=tf_wp_fill_factor,
            Ip_MA=float(getattr(base0, "Ip_MA", 8.0)),
            Ti_keV=Ti_keV,
            fG=float(getattr(base0, "fG", 0.8)),
            Paux_MW=float(getattr(base0, "Paux_MW", 20.0)),
            t_shield_m=t_shield_m,
            Ti_over_Te=Ti_over_Te,
            confinement_model=str(confinement_scaling).lower(),  # back-compat
            bootstrap_model=bootstrap_model,
            include_bootstrap_pressure_selfconsistency=False,
            f_bootstrap_consistency_abs_max=float("nan"),
            profile_model=profile_model,
            profile_peaking_ne=profile_peaking_ne,
            profile_peaking_T=profile_peaking_T,
            zeff_mode=zeff_mode,
            steady_state=steady_state,
            P_net_min_MW=P_net_min_MW,
            calib_confinement=float(st.session_state.get('calib_confinement', 1.0)),
            calib_divertor=float(st.session_state.get('calib_divertor', 1.0)),
            calib_bootstrap=float(st.session_state.get('calib_bootstrap', 1.0)),
        )

        # Persist the current Base-design fields so other Systems-mode tools can
        # reference them (e.g., Seeded Feasibility Recovery). This is UI state.
        st.session_state['systems_base_overrides'] = {
            'R0_m': float(R0_m),
            'a_m': float(a_m),
            'kappa': float(kappa),
            'delta': float(delta),
            'Bt_T': float(Bt_T),
            'Ti_keV': float(Ti_keV),
            'Ti_over_Te': float(Ti_over_Te),
            't_shield_m': float(t_shield_m),
        }

    if _sys_show('Setup','Advanced'):
        with st.expander("Targets and iteration variables", expanded=False):
            col1, col2 = st.columns(2)
            with col1:
                use_Q = st.checkbox("Target Q", value=True)
                Q_t = _num("Q target [-]", 10.0, 0.5, help="Target fusion gain Q.", key=PD_KEYS["Q_tgt"])
                # Default is intentionally conservative to help first-run success:
                # a single target (Q) with a single solved variable (Paux).
                use_H = st.checkbox("Target H98", value=False)
                H_t = _num("H98 target [-]", 1.15, 0.05, help="Target confinement H-factor.", key=PD_KEYS["H98_tgt"])
            with col2:
                use_Pnet = st.checkbox("Target net electric", value=False)
                Pnet_t = _num("P_net target [MW(e)]", 50.0, 5.0, help="Target net electric power.")
                # iteration vars
                st.markdown("**Iteration variables (solved)**")
                solve_Ip = st.checkbox("Solve Ip [MA]", value=False)
                solve_fG = st.checkbox("Solve fG [-]", value=False)
                solve_Paux = st.checkbox("Solve Paux [MW]", value=True)

            targets = {}
            if use_Q:
                targets["Q_DT_eqv"] = float(Q_t)
            if use_H:
                targets["H98"] = float(H_t)
            if use_Pnet:
                targets["P_e_net_MW"] = float(Pnet_t)

            variables = {}
            if solve_Ip:
                variables["Ip_MA"] = (float(base.Ip_MA), 0.5*float(base.Ip_MA), 1.8*float(base.Ip_MA))
            if solve_fG:
                variables["fG"] = (float(base.fG), 0.2, 1.2)
            if solve_Paux:
                variables["Paux_MW"] = (float(base.Paux_MW), 0.0, max(200.0, 3.0*float(base.Paux_MW)))

            # v177.2+: Persist assistant-applied bounds/target changes across reruns (Streamlit reruns on button clicks).
            st.session_state.setdefault('systems_bounds_overrides', {})
            st.session_state.setdefault('systems_targets_overrides', {})

            # Apply persisted overrides (if any) to the currently selected targets/variables.
            _bo = st.session_state.get('systems_bounds_overrides', {})
            if isinstance(_bo, dict) and _bo:
                for _vk, _bc in _bo.items():
                    if _vk in variables and isinstance(_bc, dict):
                        _x0, _lo, _hi = variables[_vk]
                        _lo2 = float(_bc.get('lo', _lo))
                        _hi2 = float(_bc.get('hi', _hi))
                        _x02 = float(_bc.get('x0', _x0))
                        variables[_vk] = (_x02, _lo2, _hi2)

            _to = st.session_state.get('systems_targets_overrides', {})
            if isinstance(_to, dict) and _to:
                for _tk, _tv in _to.items():
                    if _tk in targets:
                        targets[_tk] = float(_tv)

            # v323.1: Always persist the currently active Systems targets/variables.
            # Rationale: These dictionaries were previously defined only inside an
            # optional "Advanced" expander. If a user never opens that expander,
            # Streamlit reruns would leave `systems_targets`/`systems_variables`
            # empty, disabling Precheck/Solve buttons and making the UI appear
            # non-functional.
            st.session_state['systems_targets'] = dict(targets)
            st.session_state['systems_variables'] = dict(variables)

            # v176.1: preset + warm-start are defined above in always-visible Solve controls
            if preset.startswith('Fast'):
                _default_tol = 2e-3
                _default_damping = 0.85
                _default_max_iter = 25.0
                _default_trust = 10.0
            else:
                _default_tol = 1e-3
                _default_damping = 0.6
                _default_max_iter = 35.0
                _default_trust = 5.0


            tol = _num("Solver tolerance", _default_tol, 1e-3, help="Absolute tolerance on each target residual.", min_value=1e-5, max_value=1e-1)
            damping = _num("Damping", _default_damping, 0.05, help="Newton step damping for robustness.", min_value=0.1, max_value=1.0)

    
            # Persisted so downstream solve blocks never see an unbound name on reruns.
            max_iter = int(
                _num(
                    "Max iterations",
                    _default_max_iter,
                    1.0,
                    help="Maximum Newton iterations for Systems solve.",
                    min_value=1.0,
                    max_value=500.0,
                    key="systems_max_iter",
                )
            )
            override_trust = st.checkbox(
                "Override trust-region Î” (scaled)",
                value=False,
                help="Optional step-size cap in scaled variable space. Lower Î” for harder/brittle solves; raise Î” for faster convergence when stable.",
            )
            trust_delta = None
            if override_trust:
                trust_delta = _num(
                    "Trust-region Î” (scaled)",
                    _default_trust,
                    0.5,
                    help="Caps max(|dx_scaled|) per iteration. Smaller = safer steps; larger = more aggressive.",
                    min_value=0.1,
                    max_value=50.0,
                )
            st.caption("Solver trace will show `trust_region` events when steps are clipped or Î” adapts.")
            # Persist across reruns: this value is referenced later in the Systems solve block,
            # which may execute even if earlier UI branches were skipped.
            block_solve = st.checkbox(
                "Block-ordered solve (density â†’ power â†’ confinement â†’ exhaust)",
                value=bool(st.session_state.get("systems_block_solve", False)),
                key="systems_block_solve",
                help="Runs a staged solve to reduce singular Jacobians. Stages are heuristic and fully traced.",
            )

            # Persist across reruns: this value is referenced later in the Systems solve block,
            # which may execute even if earlier UI branches were skipped.
            do_precheck = st.checkbox(
                "Feasibility-first precheck (explicit)",
                value=bool(st.session_state.get("systems_do_precheck", True)),
                key="systems_do_precheck",
                help="Before running Newton iterations, evaluate targets/constraints at variable bounds to detect obviously impossible target combinations. This does not change physics or solver behavior; it only exits early with an explicit reason when infeasibility is detected within the declared bounds.",
            )
            # Persist across reruns: referenced later in the Systems solve block.
            do_continuation = st.checkbox(
                "Continuation ramp to targets (path-following)",
                value=bool(st.session_state.get("systems_do_continuation", True)),
                key="systems_do_continuation",
                help="For coupled solves, ramp targets from the starting-point values toward the requested targets in small steps. Each step is solved explicitly and logged as `cont_step` / `cont_result`. This is a UI-side workflow for robustness; physics/models are unchanged.",
            )
            cont_steps = int(
                _num(
                    "Continuation steps",
                    float(st.session_state.get("systems_cont_steps", 10.0)),
                    1.0,
                    help="Number of continuation increments (only used when continuation is enabled and the solve is coupled).",
                    min_value=2.0,
                    max_value=50.0,
                    key="systems_cont_steps",
                )
            )
            st.caption("Continuation is applied only when there is more than one target or more than one solved variable.")

            # v177: feasibility-first solve (scout) and micro-atlas tools (integrated UI)
            cff1, cff2 = st.columns([2,3])
            with cff1:
                st.checkbox(
                    "Feasibility-first solve scout (find feasible start before Newton)",
                    value=False,
                    key='v177_feasibility_scout_enabled',
                    help="Runs a deterministic feasibility search within current variable bounds to find a starting point that satisfies hard constraints before targeting. This does not change physics; it only changes the initial guess.",
                )
            with cff2:
                cff2a, cff2b, cff2c = st.columns(3)
                with cff2a:
                    st.number_input('Scout samples', min_value=8, max_value=512, value=int(st.session_state.get('v177_scout_n_samples', 64)), step=8, key='v177_scout_n_samples')
                with cff2b:
                    st.number_input('Scout refine steps', min_value=0, max_value=200, value=int(st.session_state.get('v177_scout_n_refine', 20)), step=5, key='v177_scout_n_refine')
                with cff2c:
                    st.caption('Used only when scout is enabled.')

            with st.expander('Micro feasibility atlas (2D slice)', expanded=False):
                st.caption('Sweeps two solved variables over a small grid and shows feasibility / dominant hard constraint. Other solved variables are held at midpoint.')
                if len(variables) < 2:
                    st.info('Define at least two solved variables to use the atlas.')
                else:
                    try:
                        from systems.atlas import compute_micro_atlas
                    except Exception:
                        from src.systems.atlas import compute_micro_atlas  # type: ignore
                    keys = list(variables.keys())
                    ax1, ax2, ax3 = st.columns([2,2,2])
                    with ax1:
                        var_x = st.selectbox('X variable', keys, index=0, key='v177_atlas_var_x')
                    with ax2:
                        var_y = st.selectbox('Y variable', keys, index=1 if len(keys)>1 else 0, key='v177_atlas_var_y')
                    with ax3:
                        grid_n = int(st.number_input('Grid size', min_value=5, max_value=31, value=int(st.session_state.get('v177_atlas_grid_n', 15)), step=2, key='v177_atlas_grid_n'))

                    if st.button('Compute micro-atlas', key='v177_run_micro_atlas', use_container_width=True):
                        try:
                            from evaluator.core import Evaluator
                        except Exception:
                            from src.evaluator.core import Evaluator  # type: ignore
                        _ev_at = _dsg_evaluator(origin="UI", cache_enabled=True, cache_max=4096)
                        atlas = compute_micro_atlas(base, variables, var_x, var_y, nx=grid_n, ny=grid_n, evaluator=_ev_at)
                        st.session_state['v177_last_micro_atlas'] = atlas

                    atlas = st.session_state.get('v177_last_micro_atlas', None)
                    if isinstance(atlas, dict) and atlas.get('ok'):
                        # --- Cartography 2.0 (derived-only) ---
                        with st.expander('Cartography 2.0 (robust / fragile / empty)', expanded=False):
                            try:
                                try:
                                    from systems.cartography2 import classify_cells, mechanism_histogram, label_fractions, mechanism_group_histogram
                                except Exception:
                                    from src.systems.cartography2 import classify_cells, mechanism_histogram, label_fractions, mechanism_group_histogram  # type: ignore

                                thr = float(st.number_input('Robust margin threshold (fraction)', min_value=0.0, max_value=0.50, value=float(st.session_state.get('v177_cart2_thr', 0.10)), step=0.01, key='v177_cart2_thr'))
                                c2 = classify_cells(atlas, robust_margin_min=thr)
                                if isinstance(c2, dict) and c2.get('ok'):
                                    labels = c2.get('labels') or []
                                    fr = label_fractions(labels)
                                    cA, cB, cC = st.columns(3)
                                    cA.metric('Robust', f"{100.0*fr.get('robust',0.0):.1f}%")
                                    cB.metric('Fragile', f"{100.0*fr.get('fragile',0.0):.1f}%")
                                    cC.metric('Empty', f"{100.0*fr.get('empty',0.0):.1f}%")

                                    hist = mechanism_histogram(atlas)
                                    gh = mechanism_group_histogram(atlas)
                                    if gh:
                                        rows_g = []
                                        total_fail = max(1, sum(gh.values()))
                                        for k, v in sorted(gh.items(), key=lambda kv: kv[1], reverse=True):
                                            rows_g.append({"group": k, "cells": int(v), "share": float(v)/float(total_fail)})
                                            if len(rows_g) >= 10:
                                                break
                                        try:
                                            import pandas as pd
                                            st.markdown('**Failing mechanism groups (overlay)**')
                                            st.dataframe(pd.DataFrame(rows_g), use_container_width=True, height=220, hide_index=True)
                                        except Exception:
                                            st.json(rows_g, expanded=False)

                                    # Present top mechanisms (excluding 'ok')
                                    rows = []
                                    for k, v in sorted(hist.items(), key=lambda kv: kv[1], reverse=True):
                                        if k == 'ok':
                                            continue
                                        rows.append({'mechanism': k, 'cells': int(v), 'share': float(v)/max(1,sum(hist.values()))})
                                        if len(rows) >= 12:
                                            break
                                    if rows:
                                        import pandas as pd
                                        st.dataframe(pd.DataFrame(rows), use_container_width=True, height=260, hide_index=True)
                                    else:
                                        st.caption('No failing mechanisms in the current atlas slice.')
                                else:
                                    st.caption('Cartography2 classification unavailable for this atlas.')
                            except Exception as _e:
                                st.caption(f'Cartography2 unavailable: {_e}')
                        try:
                            import numpy as _np  # type: ignore
                            import matplotlib.pyplot as _plt  # type: ignore
                            dom = atlas.get('dominant') or []
                            cats = sorted({str(c) for row in dom for c in row})
                            cmap = {c:i for i,c in enumerate(cats)}
                            arr = _np.array([[cmap[str(c)] for c in row] for row in dom], dtype=float)
                            fig = _plt.figure()
                            _plt.imshow(arr, origin='lower', aspect='auto')
                            _plt.title('Dominant hard constraint (categorical)')
                            _plt.xlabel('y index')
                            _plt.ylabel('x index')
                            st.pyplot(fig, use_container_width=True)
                            st.caption('Legend (index -> category):')
                            st.json(cats)
                        except Exception as _e:
                            st.caption(f'Atlas plot unavailable: {_e}')
                        with st.expander('Atlas raw data (xs/ys)', expanded=False):
                            st.json({'var_x': atlas.get('var_x'), 'var_y': atlas.get('var_y'), 'xs': atlas.get('xs'), 'ys': atlas.get('ys'), 'dominant': atlas.get('dominant')})
                    elif isinstance(atlas, dict) and (not atlas.get('ok')):
                        st.warning(f"Atlas unavailable: {atlas.get('reason')}")

    # Show what the assistant last applied (so the user is never unsure).
    _lac = st.session_state.get('systems_last_applied_change')
    if isinstance(_lac, dict) and _lac.get('changes'):
        with st.expander('Last applied assistant change', expanded=False):
            st.write(_lac.get('changes'))
            if _lac.get('score') is not None:
                st.caption(f"proposal score: {float(_lac.get('score')):.3g}")
            st.caption('These changes are applied to the current Systems variables/targets/overrides.')

    if bool(st.session_state.pop('systems_just_applied', False)):
        st.success('Assistant change applied. Precheck will re-run automatically (or has just re-run).')


    # Guard against missing variables/targets due to earlier conditional branches.
    # Streamlit executes scripts top-level on each interaction; failures must degrade gracefully.
    try:
        targets  # type: ignore[name-defined]
    except NameError:
        targets = st.session_state.get('systems_targets', {}) or {}
    try:
        variables  # type: ignore[name-defined]
    except NameError:
        variables = st.session_state.get('systems_variables', {}) or {}
    try:
        do_precheck  # type: ignore[name-defined]
    except NameError:
        # Some UI branches define this checkbox earlier; ensure it's always defined
        # before it is used in the Systems solve/precheck block.
        do_precheck = bool(st.session_state.get('systems_do_precheck', True))
    try:
        do_continuation  # type: ignore[name-defined]
    except NameError:
        do_continuation = bool(st.session_state.get('systems_do_continuation', True))
    try:
        cont_steps  # type: ignore[name-defined]
    except NameError:
        try:
            cont_steps = int(st.session_state.get('systems_cont_steps', 10))
        except Exception:
            cont_steps = 10
    if not isinstance(targets, dict):
        targets = {}
    if not isinstance(variables, dict):
        variables = {}

    # v323.1: Ensure Systems has a functional default target/variable set even
    # when the user never opens the "Targets and iteration variables" expander.
    # This prevents disabled Run Precheck / Run Systems Solve buttons.
    if len(targets) == 0 and len(variables) == 0:
        # Conservative ...
        try:
            _Paux0 = float(getattr(base, 'Paux_MW', 20.0))
        except Exception:
            _Paux0 = 20.0
        targets = {'Q_DT_eqv': 10.0}
        variables = {'Paux_MW': (_Paux0, 0.0, max(200.0, 3.0*_Paux0))}
        st.session_state['systems_targets'] = dict(targets)
        st.session_state['systems_variables'] = dict(variables)

    # Persistent precheck controls (do not disappear on rerun)
    c_pre, c_solve = st.columns([1, 1])
    with c_pre:
        action = st.session_state.get("_sys_action")
        run_precheck_btn = st.button("Run precheck", use_container_width=True, disabled=(len(targets)==0 or len(variables)==0), key="v177_run_precheck_btn") or (action == "precheck")
        if run_precheck_btn and action == "precheck":
            st.session_state.pop("_sys_action", None)
    with c_solve:
        run = st.button("Run systems solve", type="primary", use_container_width=True, disabled=(len(targets)==0 or len(variables)==0), key="v177_run_systems_solve_btn")

    # If the user applied an assistant proposal, we auto-run precheck once so they immediately see the effect.
    if bool(st.session_state.pop('systems_run_precheck_now', False)):
        run_precheck_btn = True

    # Run (and persist) precheck even if the user did not click "Run systems solve".
    if run_precheck_btn and do_precheck and len(variables) > 0:
        try:
            import time as _time
            t_pre0 = _time.perf_counter()
            try:
                from evaluator.core import Evaluator
            except Exception:
                from src.evaluator.core import Evaluator  # type: ignore
            try:
                from systems.feasibility_completion import run_precheck
            except Exception:
                from src.systems.feasibility_completion import run_precheck  # type: ignore

            # Apply any Systems-mode constraint knob overrides (from assistant) before precheck
            base_for_pre = base
            st.session_state.setdefault('systems_inputs_overrides', {})
            ov = st.session_state.get('systems_inputs_overrides', {}) or {}
            if isinstance(ov, dict) and ov:
                try:
                    from dataclasses import replace as _dc_replace, fields as _dc_fields
                    _valid = {f.name for f in _dc_fields(base_for_pre)}
                    _kwargs = {k: float(v) for k, v in ov.items() if k in _valid}
                    if _kwargs:
                        base_for_pre = _dc_replace(base_for_pre, **_kwargs)
                except Exception:
                    pass

            _sys_ev_pre = _dsg_evaluator(origin="UI", cache_enabled=True, cache_max=4096)
            _pre = run_precheck(
                base_for_pre,
                targets,
                variables,
                include_random=True,
                n_random=int(st.session_state.get('v177_precheck_n_random', 8)),
                seed=int(st.session_state.get('v177_precheck_seed', 1337)),
                evaluator=_sys_ev_pre,
                hard_constraint_names=_hard_constraint_names_for_intent(),
            )
            st.session_state['last_precheck_report'] = _pre
            st.session_state['systems_precheck_seconds'] = float(_time.perf_counter() - t_pre0)

            # If precheck is infeasible and auto-recovery is enabled, trigger
            # Seeded Feasibility Recovery once (handled in the recovery panel
            # later in this file).
            try:
                if (not bool(getattr(_pre, 'ok', False))) and bool(st.session_state.get('v178_recovery_enabled', True)) and bool(st.session_state.get('v178_recovery_auto', True)):
                    st.session_state['v178_recovery_autotrigger'] = True
            except Exception:
                pass
            try:
                _alog(
                    'Systems',
                    'RunPrecheck',
                    {
                        'ok': bool(getattr(_pre, 'ok', False)),
                        'reason': str(getattr(_pre, 'reason', '')),
                        'samples': int(getattr(_pre, 'n_samples', 0)),
                        'confidence': str(getattr(_pre, 'unreachable_targets_confidence', '')),
                        'failed_all': list(getattr(_pre, 'hard_constraints_failed_at_all_samples', []) or []),
                        'unreachable_targets': list(getattr(_pre, 'unreachable_targets', []) or []),
                        'precheck_seconds': float(getattr(_pre, 'precheck_seconds', float('nan'))),
                    },
                )
            except Exception:
                pass
        except Exception as _e:
            st.session_state['last_precheck_report'] = {'ok': False, 'reason': 'precheck_exception', 'error': str(_e)}
            _alog_exc('Systems', 'RunPrecheckException', _e)

    # Always show the latest precheck + assistant if we have a report (so Apply never "hides" it).
    _pre_last = st.session_state.get('last_precheck_report', None)
    if _pre_last is not None:
        try:
            _ok = bool(getattr(_pre_last, 'ok', _pre_last.get('ok', False) if isinstance(_pre_last, dict) else False))
        except Exception:
            _ok = False
        if _ok:
            st.success('Precheck: feasible within declared bounds (sampled).')
        else:
            if _design_intent_key() == 'reactor':
                st.error('Precheck: infeasible within declared bounds (sampled evaluation). Use the assistant below to apply minimal changes.')
            else:
                # v178.6: intent-aware messaging (avoid claiming we're using reactor hard set in research intent).
                _hs = sorted(list(_hard_constraint_names_for_intent()))
                st.warning('Precheck: infeasible under current hard-constraint set ' + (f"({', '.join(_hs)})" if _hs else '') + '. In **Experimental Device** intent, this does not block exploration; you can still run solves to study the machine.')
        if _sys_show('Diagnose','Recover','Advanced'):
            with st.expander('Precheck report (detailed)', expanded=False):
                view_mode = st.radio("View", options=["Summary", "Detailed"], index=0, horizontal=True, key="systems_precheck_view_mode")

                st.caption(f"Report type: {type(_pre_last).__name__}")
                st.caption('If this panel ever looks empty, the raw report is shown at the bottom for debugging.')
                # Summary view (default): compact, readable.
                try:
                    n_samp = int(getattr(_pre_last, 'n_samples', _pre_last.get('n_samples', 0)))
                    conf = str(getattr(_pre_last, 'unreachable_targets_confidence', _pre_last.get('unreachable_targets_confidence', '')))
                    failed = list(getattr(_pre_last, 'hard_constraints_failed_at_all_samples', _pre_last.get('hard_constraints_failed_at_all_samples', [])))
                    st.write(f"Samples: **{n_samp}**" + (f" | Unreachable confidence: **{conf}**" if conf else ""))
                    if failed:
                        st.warning("Failed at all samples: " + ", ".join(map(str, failed)))
                    else:
                        st.success("No hard constraint failed at all samples.")
                except Exception:
                    pass

                if view_mode == "Detailed":

                    try:
                        n_samp = int(getattr(_pre_last, 'n_samples', _pre_last.get('n_samples', 0)))
                        conf = str(getattr(_pre_last, 'unreachable_targets_confidence', _pre_last.get('unreachable_targets_confidence', '')))
                        st.write(f"Samples evaluated: **{n_samp}**")
                        if conf:
                            st.write(f"Unreachable targets confidence: **{conf}**")
                    except Exception:
                        pass
                    try:
                        failed = list(getattr(_pre_last, 'hard_constraints_failed_at_all_samples', _pre_last.get('hard_constraints_failed_at_all_samples', [])))
                        bestm = getattr(_pre_last, 'hard_constraints_best_margin', _pre_last.get('hard_constraints_best_margin', {})) or {}
                        bests = getattr(_pre_last, 'hard_constraints_best_sample', _pre_last.get('hard_constraints_best_sample', {})) or {}
                        if failed:
                            st.markdown('**Hard constraints failed at all samples**')

                            # Minimal-change recommender (qualitative) (COULD but very useful)
                            try:
                                _recs = []
                                for nm in list((_pre.hard_constraints_failed_at_all_samples or []) if hasattr(_pre,'hard_constraints_failed_at_all_samples') else (failed or [])):
                                    nml = str(nm).lower()
                                    if 'q_div' in nml:
                                        _recs.append(('q_div', 'Increase R0 / major radius, increase Î»q / broaden SOL, reduce P_SOL, or relax q_div_max (Research only).'))
                                    elif 'sigma' in nml or 'stress' in nml:
                                        _recs.append(('sigma_vm', 'Increase structural allowance / coil build, reduce B_peak/currents, increase R0, or relax sigma_allow (Research only).'))
                                    elif 'hts' in nml or 'margin' in nml:
                                        _recs.append(('HTS margin', 'Reduce peak field/current density, increase coil build, increase operating margin, or relax hts_margin_min (Research only).'))
                                    elif 'tbr' in nml:
                                        _recs.append(('TBR', 'Increase blanket effectiveness (if modeled), increase R0, reduce penetrations, or relax TBR_min (Research only).'))
                                    else:
                                        _recs.append((str(nm), 'Adjust bounds on sensitive variables or relax this constraint in Research intent (diagnostic).'))
                                if _recs:
                                    st.markdown('**What usually helps (qualitative)**')
                                    for k, msg in _recs[:8]:
                                        st.write(f'- **{k}**: {msg}')
                            except Exception:
                                pass

                            ranked = sorted([(nm, float(bestm.get(nm, float('nan'))), str(bests.get(nm, ''))) for nm in failed], key=lambda t: (t[1] if t[1]==t[1] else -1e9), reverse=True)
                            for nm, bm, sn in ranked:
                                st.write(f"- **{nm}**: best margin {bm:.3g} at sample `{sn}`")
                        else:
                            # v178.9: avoid an empty detailed report when precheck is feasible.
                            st.caption('No hard constraints failed at all samples.')
                            # Still show best-margin dictionary if available (useful for "how feasible?" diagnostics).
                            if isinstance(bestm, dict) and bestm:
                                st.caption('Best hard-constraint margins across samples:')
                                # Keep it compact; Streamlit will allow expand/copy.
                                st.json(bestm)
                    except Exception:
                        pass

                    # Show unreachable targets (if any) even when feasible.
                    try:
                        unreachable = list(getattr(_pre_last, 'unreachable_targets', _pre_last.get('unreachable_targets', [])) or [])
                        if unreachable:
                            st.markdown('**Targets outside sampled reachable range**')
                            for u in unreachable:
                                tgt_name = str(u.get('target'))
                                st.write(f"- **{tgt_name}**: requested {u.get('target_value')} vs sampled range [{u.get('sample_min')}, {u.get('sample_max')}]")
                        else:
                            st.caption('No unreachable targets detected in sampled range.')
                    except Exception:
                        pass
                    # Always show the raw report (guarantees non-empty UI)
                    try:
                        if hasattr(_pre_last, '__dict__'):
                            st.markdown('**Raw precheck report (debug)**')
                            st.json(dict(_pre_last.__dict__))
                        elif isinstance(_pre_last, dict):
                            st.markdown('**Raw precheck report (debug)**')
                            st.json(_pre_last)
                    except Exception as _e:
                        st.caption(f'Raw report unavailable: {_e}')


            # Only show the assistant when infeasible
            if not _ok:
                if _sys_show('Diagnose','Recover','Advanced'):
                    with st.expander('Feasibility completion assistant (guided minimal changes)', expanded=False):
                        c1, c2, c3 = st.columns(3)
                        with c1:
                            st.number_input('Random samples (precheck)', min_value=0, max_value=64, value=int(st.session_state.get('v177_precheck_n_random', 8)), step=1, key='v177_precheck_n_random')
                        with c2:
                            st.number_input('Deterministic seed', min_value=0, max_value=999999, value=int(st.session_state.get('v177_precheck_seed', 1337)), step=1, key='v177_precheck_seed')
                        with c3:
                            st.caption('Proposals are deterministic for a fixed seed and bounds.')

                        try:
                            try:
                                from systems.feasibility_completion import propose_feasibility_completion
                            except Exception:
                                from src.systems.feasibility_completion import propose_feasibility_completion  # type: ignore
                            try:
                                from evaluator.core import Evaluator
                            except Exception:
                                from src.evaluator.core import Evaluator  # type: ignore

                            # Apply overrides before proposing
                            base_for_props = base
                            try:
                                ov = st.session_state.get('systems_inputs_overrides', {})
                                if isinstance(ov, dict) and ov:
                                    d0 = dict(base_for_props.__dict__)
                                    d0.update({k: float(v) for k, v in ov.items()})
                                    base_for_props = PointInputs(**d0)
                            except Exception:
                                pass

                            _ev_props = _dsg_evaluator(origin="UI", cache_enabled=True, cache_max=4096)
                            props = propose_feasibility_completion(
                                base_for_props,
                                targets,
                                variables,
                                evaluator=_ev_props,
                                include_random=True,
                                n_random=int(st.session_state.get('v177_precheck_n_random', 8)),
                                seed=int(st.session_state.get('v177_precheck_seed', 1337)),
                                max_k_changes=2,
                                hard_constraint_names=_hard_constraint_names_for_intent(),
                            )
                        except Exception as _e:
                            props = []
                            st.caption(f"Proposal generation failed: {_e}")

                        st.session_state.setdefault('systems_undo_stack', [])
                        st.session_state.setdefault('systems_inputs_overrides', {})
                        st.session_state.setdefault('systems_bounds_overrides', {})
                        st.session_state.setdefault('systems_targets_overrides', {})

                        # NOTE: We also persist initial-guess overrides (x0) using the same
                        # systems_bounds_overrides structure: {var: {x0, lo, hi}}.

                        def _push_undo():
                            st.session_state['systems_undo_stack'].append({
                                'variables': dict(variables),
                                'targets': dict(targets),
                                'inputs_overrides': dict(st.session_state.get('systems_inputs_overrides', {})),
                                'bounds_overrides': dict(st.session_state.get('systems_bounds_overrides', {})),
                                'targets_overrides': dict(st.session_state.get('systems_targets_overrides', {})),
                            })

                        def _apply_prop(p):
                            ch = p.get('changes') or {}
                            if 'bounds' in ch:
                                bo = st.session_state.get('systems_bounds_overrides', {}) or {}
                                for vk, bc in (ch.get('bounds') or {}).items():
                                    if not isinstance(bc, dict):
                                        continue
                                    if vk in variables:
                                        x0, lo, hi = variables[vk]
                                        lo2 = float(bc.get('lo', lo))
                                        hi2 = float(bc.get('hi', hi))
                                        x02 = float(bc.get('x0', x0))
                                        variables[vk] = (x02, lo2, hi2)
                                    bo[vk] = {k: float(v) for k, v in bc.items() if v is not None}
                                st.session_state['systems_bounds_overrides'] = bo
                            if 'targets' in ch:
                                to = st.session_state.get('systems_targets_overrides', {}) or {}
                                for tk, tv in (ch.get('targets') or {}).items():
                                    if tk in targets:
                                        targets[tk] = float(tv)
                                    to[tk] = float(tv)
                                st.session_state['systems_targets_overrides'] = to
                            if 'constraints' in ch:
                                ov = st.session_state.get('systems_inputs_overrides', {}) or {}
                                for kk, vv in (ch.get('constraints') or {}).items():
                                    ov[kk] = float(vv)
                                st.session_state['systems_inputs_overrides'] = ov

                        if st.button('Undo last assistant change', use_container_width=True, disabled=(len(st.session_state['systems_undo_stack'])==0), key='v177_undo_assistant'):
                            last_u = st.session_state['systems_undo_stack'].pop()
                            try:
                                variables.clear(); variables.update(last_u.get('variables') or {})
                                targets.clear(); targets.update(last_u.get('targets') or {})
                                st.session_state['systems_inputs_overrides'] = dict(last_u.get('inputs_overrides') or {})
                                st.session_state['systems_bounds_overrides'] = dict(last_u.get('bounds_overrides') or {})
                                st.session_state['systems_targets_overrides'] = dict(last_u.get('targets_overrides') or {})
                                st.success('Undo applied. Precheck will re-run.')
                                st.session_state['systems_run_precheck_now'] = True
                                st.rerun()
                            except Exception:
                                st.warning('Undo failed (unexpected state).')

                        st.markdown('**Suggested minimal changes**')
                        if not props:
                            st.info('No proposals available. Consider expanding bounds for R0/Bt or relaxing constraints explicitly.')
                        else:
                            for i, pr in enumerate(props, start=1):
                                cols = st.columns([4,1])
                                with cols[0]:
                                    st.write(f"{i}. **{pr.description}**  _(type: {pr.kind})_")
                                    st.caption(f"score: {float(pr.score):.3g}")
                                with cols[1]:
                                    if st.button('Apply', key=f'v177_apply_prop_persist_{i}', use_container_width=True):
                                        _push_undo()
                                        _apply_prop({'changes': pr.changes})
                                        st.session_state['systems_last_applied_change'] = {
                                            'changes': pr.changes,
                                            'score': float(pr.score) if hasattr(pr, 'score') else None,
                                        }
                                        st.session_state['systems_just_applied'] = True
                                        _alog('Systems', 'ApplyProposal', {'changes': pr.changes, 'score': float(pr.score) if hasattr(pr,'score') else None})
                                        st.session_state['systems_run_precheck_now'] = True
                                        st.success('Applied. Re-running precheckâ€¦')
                                        st.rerun()

                        with st.expander('Constraint knob overrides (Systems Mode)', expanded=False):
                            ov = st.session_state.get('systems_inputs_overrides', {})
                            if ov:
                                st.json(ov)
                                if st.button('Clear constraint overrides', key='v177_clear_overrides_persist', use_container_width=True):
                                    st.session_state['systems_inputs_overrides'] = {}
                                    st.session_state['systems_run_precheck_now'] = True
                                    st.success('Cleared. Re-running precheckâ€¦')
                                    st.rerun()
                            else:
                                st.caption('No overrides applied.')

                # -----------------------------------------------------------------
                # Seeded Feasibility Recovery
            #
            # Finds a hard-constraint-feasible point close to a user seed, and
            # can apply it as the initial guess (x0) for Systems solve.
            #
            # This is NOT an optimizer: feasibility is the primary objective,
            # closeness-to-seed is the secondary objective.
            # -----------------------------------------------------------------
        if _sys_show('Recover','Advanced'):
            with st.expander('Seeded Feasibility Recovery (find feasible machine near your seed)', expanded=False):
                st.caption('If your guessed starting point is far from reality, SHAMS can search for a **nearby feasible machine** within the declared bounds. The recovered point can be applied as the Systems initial guess (x0).')

                rec_enabled = st.toggle(
                    'Enable seeded recovery',
                    value=bool(st.session_state.get('v178_recovery_enabled', True)),
                    key='v178_recovery_enabled',
                )
                if not rec_enabled:
                    st.info('Seeded recovery is disabled.')
                else:
                    cA, cB, cC, cD = st.columns(4)
                    with cA:
                        st.number_input('Recovery eval budget', min_value=30, max_value=2000, value=int(st.session_state.get('v178_recovery_budget', 250)), step=10, key='v178_recovery_budget')
                    with cB:
                        st.number_input('Local steps', min_value=10, max_value=400, value=int(st.session_state.get('v178_recovery_local_steps', 80)), step=5, key='v178_recovery_local_steps')
                    with cC:
                        st.number_input('Multi-start samples', min_value=0, max_value=400, value=int(st.session_state.get('v178_recovery_multistart', 40)), step=5, key='v178_recovery_multistart')
                    with cD:
                        st.number_input('Deterministic seed', min_value=0, max_value=999999, value=int(st.session_state.get('v178_recovery_seed', 2026)), step=1, key='v178_recovery_seed')
                    st.number_input('Multi-seed runs (N)', min_value=1, max_value=20, value=int(st.session_state.get('v179_rec_multiseed_n', 1)), step=1, key='v179_rec_multiseed_n', help='Runs recovery multiple times with different deterministic seeds and keeps the best result.')

                    st.toggle(
                        'Auto-run recovery after infeasible precheck',
                        value=bool(st.session_state.get('v178_recovery_auto', True)),
                        key='v178_recovery_auto',
                        help='When enabled, after a precheck reports infeasible, recovery runs automatically so you immediately get a candidate starting point.',
                    )

                    seed_mode = st.radio(
                        'Seed source',
                        ['Midpoint of bounds', 'Last Point Designer result', 'Manual (edit variables)'],
                        index=int(st.session_state.get('v178_recovery_seed_mode_idx', 0)),
                        key='v178_recovery_seed_mode',
                        horizontal=True,
                    )
                    try:
                        st.session_state['v178_recovery_seed_mode_idx'] = ['Midpoint of bounds', 'Last Point Designer result', 'Manual (edit variables)'].index(seed_mode)
                    except Exception:
                        pass

                    # -----------------------------
                    # A) Recovery variables (separate from solver iteration variables)
                    # -----------------------------
                    st.session_state.setdefault('v178_recovery_basevars_enabled', False)
                    basevars_enabled = st.checkbox(
                        'Allow recovery to adjust Base design variables (explicit bounds)',
                        value=bool(st.session_state.get('v178_recovery_basevars_enabled', False)),
                        key='v178_recovery_basevars_enabled',
                        help='This does NOT change the solver iteration-variable list. It only enables the feasibility-recovery search to move Base-design fields within explicit bounds.',
                    )

                    # Candidate base vars (PointInputs field names)
                    _BASE_VAR_MAP = [
                        ('R0_m', 'R0 [m]'),
                        ('a_m', 'a [m]'),
                        ('kappa', 'Îº [-]'),
                        ('delta', 'Î´ [-]'),
                        ('Bt_T', 'Bt [T]'),
                        ('Ti_keV', 'Ti [keV]'),
                        ('Ti_over_Te', 'Ti/Te [-]'),
                        ('t_shield_m', 'Shield thickness [m]'),
                    ]
                    _base_labels_by_key = {k: lab for k, lab in _BASE_VAR_MAP}

                    selected_base_vars: List[str] = []
                    if basevars_enabled:
                        selected_base_vars = st.multiselect(
                            'Base design variables to include in recovery',
                            options=[k for k, _ in _BASE_VAR_MAP],
                            default=list(st.session_state.get('v178_recovery_basevars', []) or []),
                            format_func=lambda k: _base_labels_by_key.get(k, k),
                            key='v178_recovery_basevars',
                        )
                        st.caption('For each selected Base variable, set explicit recovery bounds. This keeps SHAMS feasibility-authoritative and auditable.')

                    # Build recovery bounds dict (solved iteration variables + optional base vars)
                    bounds_rec: Dict[str, Dict[str, float]] = {k: {'lo': float(lo), 'hi': float(hi)} for k, (_x0, lo, hi) in list(variables.items())}

                    # Base-design bounds UI (defaults: Â±20% around current base value, clamped)
                    st.session_state.setdefault('v178_recovery_base_bounds', {})
                    _bb = st.session_state.get('v178_recovery_base_bounds', {}) or {}
                    if basevars_enabled and selected_base_vars:
                        with st.expander('Base-variable recovery bounds', expanded=False):
                            for _k in selected_base_vars:
                                try:
                                    _v0 = float(getattr(base, _k))
                                except Exception:
                                    continue
                                # Near-zero variables (notably delta) need an absolute bound span.
                                # Percent-based spans collapse to ~0 and make the variable unusable.
                                if _k == 'delta' and abs(_v0) < 1e-6:
                                    _span = 0.5
                                    _dlo = 0.0
                                    _dhi = 0.5
                                else:
                                    _span = max(1e-9, abs(_v0))
                                    _dlo = max(0.0, _v0 - 0.20 * _span)
                                    _dhi = _v0 + 0.20 * _span
                                _stored = _bb.get(_k, {}) if isinstance(_bb, dict) else {}
                                _lo_key = f'v178_rec_bound_lo_{_k}'
                                _hi_key = f'v178_rec_bound_hi_{_k}'
                                c1, c2, c3 = st.columns([2, 2, 1])
                                with c1:
                                    _lo = st.number_input(
                                        f"{_base_labels_by_key.get(_k, _k)} lo",
                                        value=float(_stored.get('lo', _dlo)),
                                        step=float(max(1e-6, 0.01 * _span)),
                                        key=_lo_key,
                                    )
                                with c2:
                                    _hi = st.number_input(
                                        f"{_base_labels_by_key.get(_k, _k)} hi",
                                        value=float(_stored.get('hi', _dhi)),
                                        step=float(max(1e-6, 0.01 * _span)),
                                        key=_hi_key,
                                    )
                                with c3:
                                    _pin = st.checkbox('Pin', value=bool(_stored.get('pin', False)), key=f'v178_rec_pin_{_k}', help='If pinned, recovery keeps this variable closer to the seed (higher distance weight).')
                                _lo2 = float(min(_lo, _hi))
                                _hi2 = float(max(_lo, _hi))
                                bounds_rec[_k] = {'lo': _lo2, 'hi': _hi2}
                                _bb[_k] = {'lo': _lo2, 'hi': _hi2, 'pin': bool(_pin)}
                        st.session_state['v178_recovery_base_bounds'] = _bb

                    # -----------------------------
                    # Seed dict (for recovery variables)
                    # -----------------------------
                    seed_dict: Dict[str, float] = {}
                    if seed_mode == 'Last Point Designer result':
                        try:
                            _lp = getattr(s, 'last_point_result', None)
                            _outs = None
                            if isinstance(_lp, dict):
                                _outs = _lp.get('outputs')
                            if not isinstance(_outs, dict):
                                _outs = getattr(s, 'last_point_outputs', None)
                            if isinstance(_outs, dict):
                                for _vn in list(bounds_rec.keys()):
                                    if _vn in _outs and _outs[_vn] is not None:
                                        try:
                                            seed_dict[_vn] = float(_outs[_vn])
                                        except Exception:
                                            pass
                        except Exception:
                            pass
                        if not seed_dict:
                            st.caption('No prior Point Designer outputs found for recovery variables; defaulting to midpoint/base.')

                    # For base vars, the natural seed is the current Base design
                    for _k in selected_base_vars:
                        try:
                            seed_dict.setdefault(_k, float(getattr(base, _k)))
                        except Exception:
                            pass

                    if seed_mode == 'Manual (edit variables)':
                        st.caption('Edit the seed values for all recovery variables. Values are clamped to bounds.')
                        with st.expander('Manual seed editor', expanded=False):
                            for _vn, _b in list(bounds_rec.items()):
                                try:
                                    _lo = float(_b.get('lo'))
                                    _hi = float(_b.get('hi'))
                                    if not (math.isfinite(_lo) and math.isfinite(_hi)):
                                        continue
                                except Exception:
                                    continue
                                _key = f'v178_seed_{_vn}'
                                _default = float(st.session_state.get(_key, seed_dict.get(_vn, (_lo + _hi) / 2.0)))
                                st.number_input(
                                    f"Seed {_base_labels_by_key.get(_vn, _vn)}",
                                    min_value=float(_lo),
                                    max_value=float(_hi),
                                    value=float(_default),
                                    step=(float(_hi) - float(_lo)) / 100.0 if float(_hi) > float(_lo) else 0.1,
                                    key=_key,
                                )
                                try:
                                    seed_dict[_vn] = float(st.session_state.get(_key, _default))
                                except Exception:
                                    pass

                    # Distance weights (pins)
                    weights_rec: Dict[str, float] = {}
                    try:
                        for _k, _v in (_bb or {}).items():
                            if isinstance(_v, dict) and bool(_v.get('pin')):
                                weights_rec[_k] = 10.0
                    except Exception:
                        pass

                    # Helper to run recovery
                    def _run_seeded_recovery() -> None:
                        try:
                            from evaluator.core import Evaluator
                        except Exception:
                            from src.evaluator.core import Evaluator  # type: ignore
                        try:
                            from systems.recovery import recover_feasible_near_seed
                        except Exception:
                            from src.systems.recovery import recover_feasible_near_seed  # type: ignore

                        # Apply constraint knob overrides to base inputs
                        base_rec = base
                        try:
                            ov = st.session_state.get('systems_inputs_overrides', {}) or {}
                            if isinstance(ov, dict) and ov:
                                from dataclasses import replace as _dc_replace, fields as _dc_fields
                                _valid = {f.name for f in _dc_fields(base_rec)}
                                _kwargs = {k: float(v) for k, v in ov.items() if k in _valid}
                                if _kwargs:
                                    base_rec = _dc_replace(base_rec, **_kwargs)
                        except Exception:
                            pass

                        # Build bounds dict for recovery (solved vars + optional base vars)
                        _bounds_rec = {k: {'lo': float(v.get('lo')), 'hi': float(v.get('hi'))} for k, v in (bounds_rec or {}).items()}
                        _weights_rec = weights_rec if isinstance(weights_rec, dict) else None
                        ev = _dsg_evaluator(origin="UI", cache_enabled=True, cache_max=4096)
                        # Safety rails (MUST): validate bounds before running.
                        _okb, _errs, _warns = _sys_validate_bounds(_bounds_rec)
                        for _w in _warns:
                            try:
                                _alog('Systems', 'BoundsWarning', {'msg': _w})
                            except Exception:
                                pass
                        if not _okb:
                            st.error('Invalid recovery bounds:\n- ' + '\n- '.join(_errs))
                            return

                        rep = None
                        _nms = int(st.session_state.get('v179_rec_multiseed_n', 1))
                        _base_seed = int(st.session_state.get('v178_recovery_seed', 2026))
                        _reps = []
                        _best = None
                        _best_key = None
                        for _j in range(max(1, _nms)):
                            _seedj = _base_seed + int(_j)
                            _r = recover_feasible_near_seed(
                                base=base_rec,
                                variables=_bounds_rec,
                                evaluator=ev,
                                seed=seed_dict if seed_dict else None,
                                weights=_weights_rec,
                                rng_seed=int(_seedj),
                                budget_evals=int(st.session_state.get('v178_recovery_budget', 250)),
                                local_steps=int(st.session_state.get('v178_recovery_local_steps', 80)),
                                multi_start=int(st.session_state.get('v178_recovery_multistart', 40)),
                                hard_constraint_names=_hard_constraint_names_for_intent(),
                                return_trace=True,
                                trace_keep=2500,
                            )
                            try:
                                _r['rng_seed_used'] = int(_seedj)
                            except Exception:
                                pass
                            _reps.append(_r)
                            try:
                                _ok = bool(_r.get('ok'))
                                _d = float(_r.get('best_distance', 1e9))
                                _v = float(_r.get('best_V', 1e9))
                            except Exception:
                                _ok, _d, _v = False, 1e9, 1e9
                            _key = (0 if _ok else 1, _d if _ok else 0.0, _v)
                            if _best is None or _key < _best_key:
                                _best, _best_key = _r, _key
                        rep = _best or (_reps[0] if _reps else {'ok': False, 'reason': 'no_result'})
                        try:
                            rep['multi_seed_runs'] = int(_nms)
                            rep['all_runs'] = _reps
                        except Exception:
                            pass
                        # Merge traces with a seed tag (for timeline/frontier). Keep bounded.
                        try:
                            _mtr = []
                            for _rr in _reps:
                                for _t in (_rr.get('trace') or []):
                                    _tt = dict(_t)
                                    _tt['seed'] = _rr.get('rng_seed_used')
                                    _mtr.append(_tt)
                            rep['trace'] = _mtr[-2500:]
                        except Exception:
                            pass
                        if isinstance(rep, dict):
                            rep.setdefault('schema_version', 1)
                        st.session_state['v178_last_recovery'] = rep
                        # Run card (MUST)
                        try:
                            bm = dict(rep.get('best_margins', {}) or {})
                            dom = None
                            if bm:
                                dom = sorted([(k, float(v)) for k, v in bm.items() if isinstance(v, (int, float)) and math.isfinite(float(v))], key=lambda t: t[1])[0][0]
                            outc = {
                                'status': 'ok' if bool(rep.get('ok')) else 'fail',
                                'reason': str(rep.get('reason')),
                                'dominant_limiter': dom,
                                'limiters': list((rep.get('best_nan') or []) and ['numerics'] or ([] if bool(rep.get('ok')) else list((rep.get('best_violations') or {}).keys())[:3])),
                                'next': _sys_failure_taxonomy(str(rep.get('reason'))).get('next', []) + _sys_levers_from_limiters([dom] if dom else []),
                            }
                            _sys_append_run_card(kind='SeededRecovery', settings={'intent': st.session_state.get('design_intent',''), 'vars': list(_bounds_rec.keys()), 'budget': int(st.session_state.get('v178_recovery_budget', 250))}, outcome=outc)
                        except Exception:
                            pass
                        try:
                            _alog('Systems', 'SeededRecovery', {
                                'ok': bool(rep.get('ok')),
                                'reason': str(rep.get('reason')),
                                'evals': int(rep.get('evals', 0)),
                                'best_V': float(rep.get('best_V', float('nan'))),
                                'best_distance': float(rep.get('best_distance', float('nan'))),
                                'best_margins': dict(rep.get('best_margins', {}) or {}),
                                'best_nan': list(rep.get('best_nan', []) or []),
                                'recovery_vars': list(_bounds_rec.keys()),
                                'recovery_bounds': _bounds_rec,
                                'weights': _weights_rec or {},
                            })
                        except Exception:
                            pass

                    # Auto-run recovery when the latest precheck is infeasible
                    try:
                        _pre_last = st.session_state.get('last_precheck_report', None)
                        _pre_ok = bool(getattr(_pre_last, 'ok', _pre_last.get('ok', False) if isinstance(_pre_last, dict) else False))
                    except Exception:
                        _pre_ok = False
                    if (not _pre_ok) and bool(st.session_state.get('v178_recovery_auto', True)) and bool(st.session_state.pop('v178_recovery_autotrigger', False)):
                        _run_seeded_recovery()

                    action = st.session_state.get('_sys_action')
                    if st.button('Run seeded recovery now', use_container_width=True, key='v178_run_recovery_btn') or (action == 'recovery'):
                        st.session_state.pop('_sys_action', None) if action == 'recovery' else None
                        _run_seeded_recovery()

                    rep = st.session_state.get('v178_last_recovery')
                    if isinstance(rep, dict) and rep:
                        if bool(rep.get('ok')):
                            st.success(f"Recovered feasible point found ({rep.get('reason')}).")
                        else:
                            st.warning(f"No feasible point found within bounds (best effort). Reason: {rep.get('reason')}")

                        with st.expander('Recovery report (details)', expanded=False):
                            st.write({'evals': rep.get('evals'), 'best_V': rep.get('best_V'), 'best_distance': rep.get('best_distance')})
                            bm = rep.get('best_margins', {}) or {}
                            if bm:
                                st.caption('Hard-constraint margins at recovered best point:')
                                st.json(bm)
                            bp = rep.get('best_point', {}) or {}
                            sp = rep.get('seed_point', {}) or {}
                            if bp:
                                deltas = {k: float(bp.get(k, float('nan'))) - float(sp.get(k, float('nan'))) for k in bp.keys()}
                                st.caption('Î” from seed (best - seed) for recovery variables:')
                                st.json(deltas)

                            _nan = rep.get('best_nan', []) or []
                            if isinstance(_nan, list) and _nan:
                                st.caption('NaN diagnostics (numerically invalid hard constraints at the best recovered point):')
                                st.json(_nan)

                        # SHOULD/COULD: comparison + economics + narrative (recovered best point)
                        try:
                            _bp = rep.get('best_point', {}) or {}
                            if isinstance(_bp, dict) and _bp:
                                with st.expander('Recovered point vs Base (delta) / Economics / Narrative', expanded=False):
                                    # Base reference
                                    try:
                                        from dataclasses import asdict as _dc_asdict
                                        _base_ref = _dc_asdict(base)
                                    except Exception:
                                        _base_ref = dict(getattr(base, '__dict__', {}) or {})

                                    rows = []
                                    for k, v in sorted(_bp.items()):
                                        try:
                                            fv = float(v)
                                        except Exception:
                                            continue
                                        row = {'var': str(k), 'value': fv}
                                        if k in _base_ref and _base_ref.get(k) is not None:
                                            try:
                                                row['base'] = float(_base_ref.get(k))
                                                row['Î”'] = fv - float(_base_ref.get(k))
                                            except Exception:
                                                pass
                                        rows.append(row)
                                    if rows:
                                        st.dataframe(pd.DataFrame(rows), use_container_width=True, hide_index=True)

                                    # Economics proxy (diagnostic)
                                    try:
                                        R0v = float(_bp.get('R0_m', _base_ref.get('R0_m', float('nan'))))
                                        av = float(_bp.get('a_m', _base_ref.get('a_m', float('nan'))))
                                        Bv = float(_bp.get('Bt_T', _base_ref.get('Bt_T', float('nan'))))
                                        st.caption('Economics proxy (diagnostic only):')
                                        st.write({'cost_proxy': (R0v**2) * max(0.1, av) * (Bv**2)})
                                    except Exception:
                                        pass

                                    # Narrative summary (COULD)
                                    if st.checkbox('Generate narrative summary (auto)', value=False, key='sys_rec_narr_toggle'):
                                        try:
                                            diffs = []
                                            for k in ['R0_m','a_m','kappa','delta','Bt_T','Ti_keV','Ti_over_Te','t_shield_m','Paux_MW']:
                                                if k in _bp and k in _base_ref and _base_ref[k] is not None:
                                                    dv = float(_bp[k]) - float(_base_ref[k])
                                                    if abs(dv) > 1e-9:
                                                        diffs.append(f"{k}: {float(_base_ref[k]):.3g} â†’ {float(_bp[k]):.3g} (Î”{dv:+.3g})")
                                            bm = rep.get('best_margins', {}) or {}
                                            dom = None
                                            try:
                                                mlist = [(kk, float(vv)) for kk, vv in bm.items() if isinstance(vv, (int, float)) and math.isfinite(float(vv))]
                                                if mlist:
                                                    dom = sorted(mlist, key=lambda t: t[1])[0]
                                            except Exception:
                                                dom = None
                                            st.markdown('**Auto summary**')
                                            st.write(
                                                f"Recovered point is {'feasible' if bool(rep.get('ok')) else 'best-effort'} under intent. "
                                                + (f"Dominant limiter (tightest margin): `{dom[0]}` (margin={dom[1]:.3g}). " if dom else "")
                                            )
                                            if diffs:
                                                st.write('Key changes vs Base:')
                                                st.write('\n'.join([f"- {d}" for d in diffs]))
                                        except Exception as _e:
                                            st.warning(f'Narrative failed: {_e}')
                        except Exception:
                            pass

                        if bool(rep.get('ok')) and isinstance(rep.get('best_point'), dict):
                            _bp = rep.get('best_point') or {}

                            # Apply recovered Base-design values (explicit user action)
                            if basevars_enabled and selected_base_vars:
                                if st.button('Apply recovered Base design values', use_container_width=True, key='v178_apply_recovered_base'):
                                    try:
                                        bo2 = st.session_state.get('systems_base_overrides', {}) or {}
                                        applied = {}
                                        for _k in selected_base_vars:
                                            if _k in _bp:
                                                try:
                                                    _v = float(_bp.get(_k))
                                                    bo2[_k] = _v
                                                    applied[_k] = _v
                                                except Exception:
                                                    pass
                                        st.session_state['systems_base_overrides'] = bo2
                                        # Stage the widget-key updates to the next rerun (before widgets are created).
                                        # This avoids StreamlitAPIException.
                                        st.session_state['systems_pending_base_apply'] = dict(applied)
                                        st.session_state['systems_pending_base_apply_source'] = 'SeededRecoveryApplyBase'
                                        st.session_state['systems_last_applied_change'] = {'changes': {'base': applied}, 'score': None}
                                        st.session_state['systems_just_applied'] = True
                                        try:
                                            _alog('Systems', 'ApplyRecoveredBase', {'base': applied})
                                        except Exception:
                                            pass
                                        st.session_state['systems_run_precheck_now'] = True
                                        st.success('Applied recovered Base design values. Re-running precheckâ€¦')
                                        st.rerun()
                                    except Exception:
                                        st.error('Failed to apply recovered Base design values (unexpected error).')

                            if st.button('Apply recovered point as Systems initial guess (x0)', use_container_width=True, key='v178_apply_recovered_x0'):
                                # Persist x0 into systems_bounds_overrides
                                bo = st.session_state.get('systems_bounds_overrides', {}) or {}
                                for _vn, (_x0, _lo, _hi) in list(variables.items()):
                                    try:
                                        _x = float(_bp.get(_vn, _x0))
                                        bo[_vn] = {
                                            'x0': max(float(_lo), min(float(_hi), _x)),
                                            'lo': float(_lo),
                                            'hi': float(_hi),
                                        }
                                    except Exception:
                                        pass
                                st.session_state['systems_bounds_overrides'] = bo
                                st.session_state['systems_last_applied_change'] = {
                                    'changes': {'x0': _bp},
                                    'score': None,
                                }
                                st.session_state['systems_just_applied'] = True
                                try:
                                    _alog('Systems', 'ApplyRecoveredX0', {'x0': _bp})
                                except Exception:
                                    pass
                                st.session_state['systems_run_precheck_now'] = True
                                st.success('Applied recovered x0. Re-running precheckâ€¦')
                                st.rerun()

    # -----------------------------------------------------------------
    # Feasible Design Search (Systems + Optimization)
    # -----------------------------------------------------------------
    if _sys_show('Explore','Advanced'):
        with st.expander('Feasible Design Search (Systems + Optimization)', expanded=False):
            st.caption('Feasible-only search: hard constraints must pass; then we optimize an engineering objective among feasible machines.')

            src_opts = ['Last feasible from Seeded Recovery', 'Last Systems solution (if any)', 'Manual (current Base + midpoints)']
            src = st.radio('Starting point source', src_opts, index=int(st.session_state.get('v178_fs_src_idx', 0)), horizontal=True, key='v178_fs_src')
            try:
                st.session_state['v178_fs_src_idx'] = src_opts.index(src)
            except Exception:
                pass

            obj_opts = [
                ('q_div_MW_m2', 'Minimize divertor q_div'),
                ('P_SOL_over_R_MW_m', 'Minimize P_SOL/R'),
                ('neutron_wall_load_MW_m2', 'Minimize NWL'),
                ('sigma_vm_MPa', 'Minimize stress sigma_vm'),
                ('B_peak_T', 'Minimize B_peak'),
                ('-TBR', 'Maximize TBR'),
                ('-hts_margin', 'Maximize HTS margin'),
            ]
            obj_key = st.selectbox('Objective', options=[k for k,_ in obj_opts], index=int(st.session_state.get('v178_fs_obj_idx', 0)), format_func=lambda k: dict(obj_opts).get(k,k), key='v178_fs_obj')
            try:
                st.session_state['v178_fs_obj_idx'] = [k for k,_ in obj_opts].index(obj_key)
            except Exception:
                pass

            c1, c2, c3, c4 = st.columns(4)
            with c1:
                st.number_input('Search eval budget', min_value=50, max_value=5000, value=int(st.session_state.get('v178_fs_budget', 800)), step=50, key='v178_fs_budget')
            with c2:
                st.number_input('Top-K candidates', min_value=1, max_value=50, value=int(st.session_state.get('v178_fs_topk', 12)), step=1, key='v178_fs_topk')
            with c3:
                st.number_input('Deterministic seed', min_value=0, max_value=999999, value=int(st.session_state.get('v178_fs_seed', 2026)), step=1, key='v178_fs_seed')
            st.number_input('Multi-seed runs (N)', min_value=1, max_value=20, value=int(st.session_state.get('v179_fs_multiseed_n', 1)), step=1, key='v179_fs_multiseed_n', help='Runs feasible search multiple times with different deterministic seeds and merges the Top-K candidates.')
            with c4:
                st.number_input('Local radius (fraction of bound span)', min_value=0.01, max_value=1.0, value=float(st.session_state.get('v178_fs_radius', 0.25)), step=0.01, key='v178_fs_radius')

            # Default bounds from solver variable bounds + recovery base bounds (if any)
            bounds_default = {}
            try:
                for k, (_x0, lo, hi) in list(variables.items()):
                    bounds_default[k] = {'lo': float(lo), 'hi': float(hi)}
            except Exception:
                pass
            try:
                _bb = st.session_state.get('v178_recovery_base_bounds', {}) or {}
                for k, v in (_bb or {}).items():
                    if isinstance(v, dict) and 'lo' in v and 'hi' in v:
                        bounds_default[k] = {'lo': float(v.get('lo')), 'hi': float(v.get('hi'))}
            except Exception:
                pass

            default_vars = list(st.session_state.get('v178_fs_vars', []) or [])
            if not default_vars:
                rep0 = st.session_state.get('v178_last_recovery')
                if isinstance(rep0, dict):
                    rv = rep0.get('recovery_vars')
                    if isinstance(rv, list) and rv:
                        default_vars = [str(x) for x in rv]
            if not default_vars:
                default_vars = list(bounds_default.keys())

            search_vars = st.multiselect(
                'Variables to search (explicit bounds required)',
                options=list(bounds_default.keys()),
                default=[k for k in default_vars if k in bounds_default],
                key='v178_fs_vars',
            )

            st.session_state.setdefault('v178_fs_bounds', {})
            _fsb = st.session_state.get('v178_fs_bounds', {}) or {}
            with st.expander('Search bounds (explicit)', expanded=False):
                for k in search_vars:
                    b0 = bounds_default.get(k, {})
                    lo0 = float((_fsb.get(k, {}) or {}).get('lo', b0.get('lo', 0.0)))
                    hi0 = float((_fsb.get(k, {}) or {}).get('hi', b0.get('hi', 1.0)))
                    span = max(1e-9, abs(hi0 - lo0))
                    a,b = st.columns(2)
                    with a:
                        lo = st.number_input(f'{k} lo', value=float(lo0), step=float(span/50.0), key=f'v178_fs_blo_{k}')
                    with b:
                        hi = st.number_input(f'{k} hi', value=float(hi0), step=float(span/50.0), key=f'v178_fs_bhi_{k}')
                    lo2 = float(min(lo, hi))
                    hi2 = float(max(lo, hi))
                    _fsb[k] = {'lo': lo2, 'hi': hi2}
            st.session_state['v178_fs_bounds'] = _fsb

            def _fs_obj_value(out: dict) -> float:
                try:
                    if str(obj_key).startswith('-'):
                        k = str(obj_key)[1:]
                        v = float(out.get(k, float('nan')))
                        return -v if math.isfinite(v) else float('inf')
                    v = float(out.get(str(obj_key), float('nan')))
                    return v if math.isfinite(v) else float('inf')
                except Exception:
                    return float('inf')

            _intent_key_fs = _design_intent_key()
            _hard_set_fs = set(_hard_constraint_names_for_intent())
            _ignore_set_fs = set(_ignored_constraint_names_for_intent())
            _soft_set_fs = set(_INTENT_SOFT.get(_intent_key_fs, set()))

            def _fs_is_feasible(out: dict) -> bool:
                """Feasibility under the active Design Intent hard-constraint set."""
                try:
                    from constraints.constraints import evaluate_constraints
                except Exception:
                    from src.constraints.constraints import evaluate_constraints  # type: ignore
                try:
                    cs = evaluate_constraints(out or {})
                    for c in cs:
                        nm = str(getattr(c, 'name', ''))
                        if nm in _ignore_set_fs:
                            continue
                        if nm in _hard_set_fs and not bool(getattr(c, 'passed', False)):
                            return False
                    return True
                except Exception:
                    return False

            def _fs_violation_score(out: dict) -> float:
                """Intent-aware continuous violation score (0 is best).

                Reactor intent: primarily used only as diagnostics (we still filter feasible-only).
                Research intent: used as the main objective to find best-compromise designs.
                """
                try:
                    from constraints.constraints import evaluate_constraints
                except Exception:
                    from src.constraints.constraints import evaluate_constraints  # type: ignore
                try:
                    cs = evaluate_constraints(out or {})
                except Exception:
                    cs = []
                V = 0.0
                for c in cs:
                    nm = str(getattr(c, 'name', ''))
                    if nm in _ignore_set_fs:
                        continue
                    try:
                        m = float(getattr(c, 'margin', float('nan')))
                    except Exception:
                        m = float('nan')
                    if not math.isfinite(m):
                        v = 1e6
                    else:
                        v = max(0.0, -m)
                    if nm in _hard_set_fs:
                        V += 100.0 * v * v
                    elif nm in _soft_set_fs:
                        V += 1.0 * v * v
                return float(V)

            def _fs_margins(out: dict) -> dict:
                try:
                    from constraints.constraints import evaluate_constraints
                except Exception:
                    from src.constraints.constraints import evaluate_constraints  # type: ignore
                m = {}
                try:
                    cs = evaluate_constraints(out or {})
                    for c in cs:
                        if getattr(c, 'severity', 'hard') == 'hard':
                            try:
                                m[c.name] = float(getattr(c, 'margin'))
                            except Exception:
                                m[c.name] = float('nan')
                except Exception:
                    pass
                return m

            def _fs_build_inputs(base_inp, varvals: dict):
                try:
                    from dataclasses import replace as _dc_replace, fields as _dc_fields
                    _valid = {f.name for f in _dc_fields(base_inp)}
                    _kwargs = {k: float(v) for k, v in (varvals or {}).items() if k in _valid}
                    return _dc_replace(base_inp, **_kwargs)
                except Exception:
                    return base_inp

            # Starting values
            start_vals = {}
            if src == 'Last feasible from Seeded Recovery':
                rep0 = st.session_state.get('v178_last_recovery')
                if isinstance(rep0, dict) and bool(rep0.get('ok')) and isinstance(rep0.get('best_point'), dict):
                    start_vals = dict(rep0.get('best_point') or {})
            elif src == 'Last Systems solution (if any)':
                try:
                    s_state = _v92_state_get()
                    _ls = getattr(s_state, 'last_systems_result', None)
                    if isinstance(_ls, dict) and isinstance(_ls.get('outputs'), dict):
                        out0 = _ls.get('outputs') or {}
                        start_vals = {k: float(out0.get(k)) for k in search_vars if k in out0 and out0.get(k) is not None}
                except Exception:
                    start_vals = {}

            if not start_vals:
                for k in search_vars:
                    b = _fsb.get(k, bounds_default.get(k, {})) or {}
                    lo = float(b.get('lo', 0.0)); hi = float(b.get('hi', 1.0))
                    start_vals[k] = 0.5*(lo+hi)

            try:
                from evaluator.core import Evaluator
            except Exception:
                from src.evaluator.core import Evaluator  # type: ignore
            ev_fs = _dsg_evaluator(origin="UI", cache_enabled=True, cache_max=8192)

            base_fs = base
            try:
                ov = st.session_state.get('systems_inputs_overrides', {}) or {}
                if isinstance(ov, dict) and ov:
                    base_fs = _fs_build_inputs(base_fs, ov)
            except Exception:
                pass

            def _run_feasible_search(_seed_override=None):
                seed0 = int(_seed_override) if _seed_override is not None else int(st.session_state.get('v178_fs_seed', 2026))
                rng = random.Random(seed0)
                budget = int(st.session_state.get('v178_fs_budget', 800))
                topk = int(st.session_state.get('v178_fs_topk', 12))
                radius = float(st.session_state.get('v178_fs_radius', 0.25))

                # Explicit bounds (with safety rails).
                bounds = {k: dict(_fsb.get(k, bounds_default.get(k, {})) or {}) for k in search_vars}
                for k, b in list(bounds.items()):
                    try:
                        lo = float(b.get('lo'))
                        hi = float(b.get('hi'))
                        bounds[k] = {'lo': lo, 'hi': hi}
                    except Exception:
                        del bounds[k]
                if not bounds:
                    return {'ok': False, 'reason': 'no_bounds', 'candidates': []}

                # MUST: validate bounds before running.
                _okb, _errs, _warns = _sys_validate_bounds(bounds)
                for _w in _warns:
                    try:
                        _alog('Systems', 'BoundsWarning', {'msg': _w})
                    except Exception:
                        pass
                if not _okb:
                    return {'ok': False, 'reason': 'invalid_bounds', 'errors': _errs, 'candidates': []}

                x_start = {k: float(start_vals.get(k, 0.5*(bounds[k]['lo']+bounds[k]['hi']))) for k in bounds.keys()}
                res0 = ev_fs.evaluate(_fs_build_inputs(base_fs, x_start))
                out0 = res0.out if res0 and res0.ok else {}
                feas0 = _fs_is_feasible(out0)
                obj0 = _fs_obj_value(out0)
                V0 = _fs_violation_score(out0)

                best_x = dict(x_start)
                best_obj = float(obj0) if math.isfinite(obj0) else float('inf')
                best_V = float(V0) if math.isfinite(V0) else float('inf')

                cands = []
                # Trace (COULD): store a lightweight evaluation trace for frontier plots and reproducibility.
                trace_keep = int(st.session_state.get('v178_fs_trace_keep', 2500))
                trace = []
                _MET_KEYS = ['q_div_MW_m2','P_SOL_over_R_MW_m','neutron_wall_load_MW_m2','sigma_vm_MPa','B_peak_T','TBR','hts_margin','H98','Q_DT_eqv','Pfus_DT_adj_MW']
                # Always record the start point as candidate in research mode; in reactor mode
                # we keep backward behavior (feasible-only candidates).
                if (_intent_key_fs == 'research') or feas0:
                    cands.append({'x': dict(x_start), 'obj': float(obj0), 'V': float(V0), 'feasible': bool(feas0), 'margins': _fs_margins(out0), 'headline': {'Q': out0.get('Q_DT_eqv'), 'H98': out0.get('H98'), 'P_net': out0.get('P_e_net_MW')}, 'metrics': {k: out0.get(k) for k in _MET_KEYS}})
                try:
                    if len(trace) < trace_keep:
                        trace.append({'i': 0, 'x': dict(x_start), 'obj': float(obj0) if math.isfinite(obj0) else None, 'V': float(V0) if math.isfinite(V0) else None, 'feasible': bool(feas0), 'metrics': {k: out0.get(k) for k in _MET_KEYS}})
                except Exception:
                    pass
                for i in range(max(0, budget-1)):
                    frac = max(0.05, radius * (1.0 - i/max(1, budget-1)))
                    x = {}
                    for k, b in bounds.items():
                        lo = b['lo']; hi = b['hi']
                        span = hi - lo
                        x0 = float(best_x.get(k, x_start.get(k))) if best_x else float(x_start.get(k))
                        step = frac * span
                        xv = x0 + (rng.random()*2.0 - 1.0) * step
                        xv = max(lo, min(hi, xv))
                        x[k] = float(xv)
                    res = ev_fs.evaluate(_fs_build_inputs(base_fs, x))
                    if not (res and res.ok and isinstance(res.out, dict)):
                        continue
                    out = res.out
                    feas = _fs_is_feasible(out)
                    V = _fs_violation_score(out)
                    obj = _fs_obj_value(out)

                    try:
                        if len(trace) < trace_keep:
                            trace.append({'x': dict(x), 'obj': float(obj) if math.isfinite(obj) else None, 'V': float(V) if math.isfinite(V) else None, 'feasible': bool(feas), 'metrics': {k: out.get(k) for k in _MET_KEYS}})
                    except Exception:
                        pass

                    if _intent_key_fs == 'reactor' and not feas:
                        continue

                    cands.append({'x': dict(x), 'obj': float(obj), 'V': float(V), 'feasible': bool(feas), 'margins': _fs_margins(out), 'headline': {'Q': out.get('Q_DT_eqv'), 'H98': out.get('H98'), 'P_net': out.get('P_e_net_MW')}, 'metrics': {k: out.get(k) for k in _MET_KEYS}})

                    # Update best according to intent.
                    if _intent_key_fs == 'reactor':
                        if feas and obj < best_obj:
                            best_obj = float(obj)
                            best_x = dict(x)
                    else:
                        # Research: prioritize violation score, then objective.
                        if (V < best_V - 1e-12) or (abs(V - best_V) <= 1e-12 and obj < best_obj):
                            best_V = float(V)
                            best_obj = float(obj)
                            best_x = dict(x)

                if _intent_key_fs == 'reactor':
                    cands.sort(key=lambda c: float(c.get('obj', float('inf'))))
                else:
                    cands.sort(key=lambda c: (float(c.get('V', float('inf'))), float(c.get('obj', float('inf')))))
                top = cands[:max(1, topk)] if cands else []
                return {
                    'ok': bool(len(top) > 0),
                    'reason': (
                        'feasible_candidates' if (_intent_key_fs == 'reactor' and len(top) > 0) else
                        ('best_compromise' if (_intent_key_fs == 'research' and len(top) > 0) else
                         ('start_not_feasible' if not feas0 else 'no_feasible_found'))
                    ),
                    'objective': str(obj_key),
                    'budget': budget,
                    'topk': topk,
                    'radius': radius,
                    'seed': int(seed0),
                    'vars': list(bounds.keys()),
                    'bounds': bounds,
                    'start_feasible': bool(feas0),
                    'start_obj': float(obj0) if math.isfinite(obj0) else None,
                    'start_V': float(V0) if math.isfinite(V0) else None,
                    'best_V': float(best_V) if math.isfinite(best_V) else None,
                    'best_obj': float(best_obj) if math.isfinite(best_obj) else None,
                    'best_x': dict(best_x) if best_x else None,
                    'candidates': top,
                    'trace': trace,
                    'trace_keep': trace_keep,
                    'ts_unix': float(time.time()),
                }
            action = st.session_state.get('_sys_action')

            _fs_running = bool(st.session_state.get('systems_fs_running', False))
            # Phase-1 UI stabilization: Feasible Search running watchdog + crash-safe execution.
            _fs_started_ts = float(st.session_state.get('systems_fs_started_ts', 0.0) or 0.0)
            if _fs_running and _fs_started_ts > 0.0:
                _fs_age_s = float(time.time()) - _fs_started_ts
                if _fs_age_s > 30.0:
                    with st.expander('âš ï¸ Feasible search appears stuck (watchdog)', expanded=False):
                        st.warning('A feasible-search run is marked as running, but no completion was recorded. You can safely clear the running flag to re-enable the button.')
                        st.caption(f'running_since = {_fs_age_s:.1f} s')
                        if st.button('Clear feasible-search running flag', use_container_width=True, key='v178_fs_clear_running'):
                            st.session_state['systems_fs_running'] = False
                            st.session_state['systems_fs_started_ts'] = 0.0
                            st.session_state['systems_fs_last_error'] = None
                            st.success('Cleared. You can run feasible search again.')
                            st.rerun()

            _run_search_clicked = st.button(
                'Run feasible design search',
                use_container_width=True,
                key='v178_fs_run_btn',
                disabled=_fs_running,
                help=('Runningâ€¦ please wait.' if _fs_running else None),
            ) or (action == 'search')
            if _fs_running and (action == 'search'):
                _run_search_clicked = False

            if _run_search_clicked:
                st.session_state['systems_fs_running'] = True
                st.session_state['systems_fs_started_ts'] = float(time.time())
                st.session_state['systems_fs_last_error'] = None
                if action == 'search':
                    st.session_state.pop('_sys_action', None)
                try:
                    try:
                        _alog('Systems', 'FeasibleSearchClicked', {
                            'objective': str(obj_key),
                            'budget': int(st.session_state.get('v178_fs_budget', 0)),
                            'topk': int(st.session_state.get('v178_fs_topk', 0)),
                            'multi_seed_runs': int(st.session_state.get('v179_fs_multiseed_n', 1)),
                            'vars': list(search_vars or []),
                            'intent': str(st.session_state.get('design_intent', '')),
                        })
                    except Exception:
                        pass

                    _nms = int(st.session_state.get('v179_fs_multiseed_n', 1))
                    _base_seed = int(st.session_state.get('v178_fs_seed', 2026))
                    _all = []
                    with st.spinner(f"Running feasible searchâ€¦ (budget={int(st.session_state.get('v178_fs_budget', 800))}, runs={_nms}). This may take a while."):
                        for _j in range(max(1, _nms)):
                            _all.append(_run_feasible_search(_seed_override=int(_base_seed + _j)))
                    rep = _all[0] if _all else {'ok': False, 'reason': 'no_result', 'candidates': []}
                    try:
                        _intent = str(st.session_state.get('design_intent','')).lower()
                        _intent_key = 'research' if 'research' in _intent else 'reactor'
                        _topk = int(rep.get('topk', int(st.session_state.get('v178_fs_topk', 12))))
                        _cands = []
                        _trace = []
                        for _r in _all:
                            for _c in list(_r.get('candidates', []) or []):
                                _cc = dict(_c); _cc['seed'] = _r.get('seed'); _cands.append(_cc)
                            for _t in list(_r.get('trace', []) or []):
                                _tt = dict(_t); _tt['seed'] = _r.get('seed'); _trace.append(_tt)
                        if _intent_key == 'reactor':
                            _cands.sort(key=lambda c: float(c.get('obj', float('inf'))))
                        else:
                            _cands.sort(key=lambda c: (float(c.get('V', float('inf'))), float(c.get('obj', float('inf')))))
                        rep['candidates'] = _cands[:max(1, _topk)] if _cands else []
                        rep['trace'] = _trace[-int(rep.get('trace_keep', 2500)):] if _trace else []
                        rep['multi_seed_runs'] = int(_nms)
                        rep['all_runs'] = _all
                        rep['seed'] = int(_base_seed)
                        rep['ok'] = bool(len(rep.get('candidates', []) or []) > 0)
                        rep['reason'] = 'multi_seed_' + str(rep.get('reason'))
                    except Exception:
                        pass

                    st.session_state['v178_fs_last'] = rep
                    try:
                        _alog('Systems', 'FeasibleSearchDone', {'ok': bool(rep.get('ok', False)), 'reason': str(rep.get('reason',''))})
                    except Exception:
                        pass
                except Exception as _e:
                    st.session_state['systems_fs_last_error'] = str(_e)
                    _alog_exc('Systems', 'FeasibleSearchError', _e)
                    st.error(f'Feasible search failed: {_e}')
                finally:
                    st.session_state['systems_fs_running'] = False
                    st.session_state['systems_fs_started_ts'] = 0.0
                    st.rerun()

    _precheck_only = False  # legacy flag (kept for backward compatibility)
    # Persisted solver knobs must be defined regardless of which UI subsections were rendered on this rerun.
    # (Phase-1 rule: no conditional variable definitions.)
    block_solve = bool(st.session_state.get("systems_block_solve", False))
    if run:
        # Local flow-control exception: block the full solve without calling
        # st.stop(), which can destabilize Streamlit rerun/tab selection.
        class _SysPrecheckBlocksSolve(Exception):
            """Raised to skip the full Systems solve when precheck blocks it."""
            pass

        _warn_unrealistic_point_inputs(base, context="Systems")
        st.info("Running coupled solveâ€¦")
        try:
            _alog(
                "Systems",
                "RunSystemsSolve",
                {
                    "n_targets": int(len(targets)),
                    "n_variables": int(len(variables)),
                    "targets": {k: float(v) for k, v in (targets or {}).items()},
                    "mode": "robust" if robust_mode else "fast",
                    "warm_start": bool(warm_start),
                    "continuation": bool(do_continuation),
                    "block_solve": bool(block_solve),
                },
            )
        except Exception:
            pass
        log = st.empty()
        last = None

        coupled = (len(targets) > 1) or (len(variables) > 1)
        base_for_solve = base
        # v177.5: apply persisted constraint-threshold overrides (Inputs knobs) to Systems base inputs.
        st.session_state.setdefault('systems_inputs_overrides', {})
        _io = st.session_state.get('systems_inputs_overrides', {}) or {}
        if isinstance(_io, dict) and _io:
            try:
                from dataclasses import replace as _dc_replace
                _kwargs = {k: float(v) for k, v in _io.items() if hasattr(base_for_solve, k)}
                if _kwargs:
                    base_for_solve = _dc_replace(base_for_solve, **_kwargs)
            except Exception:
                pass


        # v176.0: warm-start initial guesses from last Systems artifact
        if warm_start:
            try:
                _ls = getattr(s, 'last_systems_result', None)
                if isinstance(_ls, dict):
                    _outs = _ls.get('outputs') or {}
                    for _vn, (_x0, _lo, _hi) in list(variables.items()):
                        if _vn in _outs and _outs[_vn] is not None:
                            try:
                                _x = float(_outs[_vn])
                                # clamp to bounds
                                _x = max(float(_lo), min(float(_hi), _x))
                                variables[_vn] = (_x, float(_lo), float(_hi))
                            except Exception:
                                pass
            except Exception:
                pass


        # -----------------------------
        # Feasibility-first precheck (explicit; UI-side only)
        # -----------------------------


        # v177: richer precheck + completion assistant (core helpers in src/systems)
        if do_precheck and len(variables) > 0:
            import time as _time
            t_pre0 = _time.perf_counter()
            try:
                from evaluator.core import Evaluator
            except Exception:
                from src.evaluator.core import Evaluator  # type: ignore
            try:
                from systems.feasibility_completion import run_precheck, propose_feasibility_completion
            except Exception:
                from src.systems.feasibility_completion import run_precheck, propose_feasibility_completion  # type: ignore

            # Share a single evaluator cache for precheck + atlas + scout within this run
            _sys_ev = _dsg_evaluator(origin="UI", cache_enabled=True, cache_max=4096)

            try:
                _pre = run_precheck(
                    base_for_solve,
                    targets,
                    variables,
                    include_random=True,
                    n_random=int(st.session_state.get('v177_precheck_n_random', 8)),
                    seed=int(st.session_state.get('v177_precheck_seed', 1337)),
                    evaluator=_sys_ev,
                    hard_constraint_names=_hard_constraint_names_for_intent(),
                )
            except Exception as _e:
                _pre = None

            try:
                precheck_s = float(_time.perf_counter() - t_pre0)
            except Exception:
                precheck_s = None
            if precheck_s is not None:
                st.session_state['systems_precheck_seconds'] = float(precheck_s)

            if _pre is not None:
                st.session_state['last_precheck_report'] = _pre

            # Run card (MUST): standardized precheck summary
            try:
                if _pre is not None:
                    bm = dict(getattr(_pre, 'hard_constraints_best_margin', {}) or {})
                    dom = None
                    try:
                        if bm:
                            dom = sorted([(k, float(v)) for k, v in bm.items() if isinstance(v, (int, float)) and math.isfinite(float(v))], key=lambda t: t[1])[0][0]
                    except Exception:
                        dom = None
                    outc = {
                        'status': 'ok' if bool(getattr(_pre, 'ok', False)) else 'fail',
                        'reason': str(getattr(_pre, 'reason', '')),
                        'dominant_limiter': dom,
                        'limiters': list(getattr(_pre, 'hard_constraints_failed_at_all_samples', []) or []),
                        'next': _sys_failure_taxonomy(str(getattr(_pre, 'reason', ''))).get('next', []) + _sys_levers_from_limiters([dom] if dom else list(getattr(_pre, 'hard_constraints_failed_at_all_samples', []) or [])),
                    }
                    _sys_append_run_card(kind='Precheck', settings={'intent': st.session_state.get('design_intent',''), 'n_samples': int(getattr(_pre, 'n_samples', 0)), 'n_vars': int(len(variables or {}))}, outcome=outc)
            except Exception:
                pass

            # If we were triggered by an assistant 'Apply' action, only run precheck (do not run the full solve).
            if _precheck_only and (_pre is not None) and bool(getattr(_pre, 'ok', False)):
                st.success('Precheck: feasible within declared bounds (sampled).')
                with st.expander('Precheck report (detailed)', expanded=False):
                    st.write(f"Samples evaluated: **{int(_pre.n_samples)}**")
                    st.write(f"Unreachable targets confidence: **{_pre.unreachable_targets_confidence}**")
                    st.caption('Full solve was not run. Click **Run systems solve** to proceed.')
                st.stop()

            if (_pre is not None) and (not bool(_pre.ok)):
                try:
                    _alog(
                        "Systems",
                        "PrecheckInfeasible",
                        {
                            "samples": int(getattr(_pre, 'n_samples', 0)),
                            "confidence": str(getattr(_pre, 'unreachable_targets_confidence', '')),
                            "failed_all": list(getattr(_pre, 'hard_constraints_failed_at_all_samples', []) or []),
                            "best_margins": dict(getattr(_pre, 'hard_constraints_best_margin', {}) or {}),
                            "unreachable_targets": list(getattr(_pre, 'unreachable_targets', []) or []),
                        },
                    )
                except Exception:
                    pass
                fail = {
                    'event': 'fail',
                    'reason': 'precheck_infeasible',
                    'unreachable_targets': _pre.unreachable_targets,
                    # keep backward-compatible key name
                    'hard_constraints_failed_at_all_corners': _pre.hard_constraints_failed_at_all_samples,
                    'precheck_seconds': precheck_s,
                }
                log.code(json.dumps(fail, indent=2, sort_keys=True))
                if _design_intent_key() == 'reactor':
                    st.error('Precheck: infeasible within declared bounds (sampled evaluation). Use the assistant below to apply minimal changes.')
                else:
                    # v178.6: intent-aware messaging.
                    _hs = sorted(list(_hard_constraint_names_for_intent()))
                    st.warning('Precheck: infeasible under current hard-constraint set ' + (f"({', '.join(_hs)})" if _hs else '') + '. In **Experimental Device** intent, this does not block exploration; you can still run solves to study the machine.')

                with st.expander('Precheck report (detailed)', expanded=False):
                    st.write(f"Samples evaluated: **{int(_pre.n_samples)}**")
                    st.write(f"Unreachable targets confidence: **{_pre.unreachable_targets_confidence}**")
                    if _pre.hard_constraints_failed_at_all_samples:
                        st.markdown('**Hard constraints failed at all samples**')
                        ranked = sorted(
                            [(nm, float(_pre.hard_constraints_best_margin.get(nm, float('nan'))), _pre.hard_constraints_best_sample.get(nm, '')) for nm in _pre.hard_constraints_failed_at_all_samples],
                            key=lambda t: (t[1] if t[1]==t[1] else -1e9),
                            reverse=True,
                        )
                        for nm, bm, sn in ranked:
                            st.write(f"- **{nm}**: best margin {bm:.3g} at sample `{sn}`")
                    if _pre.unreachable_targets:
                        st.markdown('**Targets outside sampled reachable range**')
                        for u in _pre.unreachable_targets:
                            if 'sample_min' in u:
                                tgt_name = str(u.get('target'))
                                st.write(f"- **{tgt_name}**: requested {u.get('target_value')} vs sampled range [{u.get('sample_min')}, {u.get('sample_max')}]")

                                # Quick-fix button for the most common case: Q target is outside sampled range.
                                # Users may miss the target widget (collapsed expander / scrolled UI), so offer
                                # a one-click adjustment that is fully logged.
                                try:
                                    if tgt_name in ('Q_DT_eqv', 'Q', 'Q_target'):
                                        smin = float(u.get('sample_min'))
                                        # Nudge above the sampled minimum (so it is clearly reachable).
                                        suggested = float(math.ceil(smin * 10.0) / 10.0)
                                        if st.button(f"Set Q target to {suggested:g} (make reachable)", key=f"v178_set_Q_from_pre_{suggested:g}"):
                                            try:
                                                st.session_state[PD_KEYS["Q_tgt"]] = suggested
                                            except Exception:
                                                st.session_state["v178_Q_tgt_fallback"] = suggested
                                            try:
                                                _alog('Systems', 'SetTargetFromPrecheck', {
                                                    'target': tgt_name,
                                                    'suggested': suggested,
                                                    'sample_min': smin,
                                                })
                                            except Exception:
                                                pass
                                            st.experimental_rerun()
                                except Exception:
                                    pass
                            else:
                                st.write(f"- **{u.get('target')}**: {u.get('reason')}")

                    with st.expander('Sample table (debug)', expanded=False):
                        try:
                            import pandas as _pd  # type: ignore
                            rows = []
                            for sr in _pre.samples:
                                rows.append({
                                    'sample': sr.sample.name,
                                    'hard_failed': ', '.join(sr.hard_failed),
                                    **{k: float(sr.sample.values.get(k, float('nan'))) for k in variables.keys()},
                                })
                            st.dataframe(_pd.DataFrame(rows), use_container_width=True)
                        except Exception as _e:
                            st.caption(f"Sample table unavailable: {_e}")

                # Reactor intent is feasibility-authoritative: an infeasible precheck
                # blocks the full solve. Do **not** call st.stop() here; instead raise
                # a local flow-control exception that is handled below so UI remains
                # stable and navigation is not affected.
                if _design_intent_key() == 'reactor':
                    raise _SysPrecheckBlocksSolve('precheck_infeasible_reactor')

# Assistant UI is rendered in the persistent Precheck panel above.
                st.info('Feasibility completion assistant is available in the Precheck panel. Run precheck there to generate and apply proposals.')
        # v177: optionally run a feasibility scout before target solve (deterministic)
        if st.session_state.get('v177_feasibility_scout_enabled', False):
            try:
                from systems.feasibility_completion import feasibility_scout
            except Exception:
                from src.systems.feasibility_completion import feasibility_scout  # type: ignore
            try:
                from evaluator.core import Evaluator
            except Exception:
                from src.evaluator.core import Evaluator  # type: ignore
            _ev2 = _dsg_evaluator(origin="UI", cache_enabled=True, cache_max=4096)
            scout = feasibility_scout(
                base_for_solve,
                variables,
                evaluator=_ev2,
                n_samples=int(st.session_state.get('v177_scout_n_samples', 64)),
                seed=int(st.session_state.get('v177_precheck_seed', 1337)),
                n_refine=int(st.session_state.get('v177_scout_n_refine', 20)),
                hard_constraint_names=_hard_constraint_names_for_intent(),
            )
            if scout.get('ok'):
                base_for_solve = scout.get('best_inp', base_for_solve)
                log.code(json.dumps({'event': 'feasibility_scout', 'ok': True, 'best_min_margin': scout.get('best_min_margin')}, indent=2, sort_keys=True))
                st.info('Feasibility scout found a feasible start point within bounds; using it as initial guess.')
            else:
                log.code(json.dumps({'event': 'feasibility_scout', 'ok': False, 'best_score': scout.get('best_score'), 'hard_failed': scout.get('hard_failed')}, indent=2, sort_keys=True))
                st.warning('Feasibility scout did not find a fully feasible point; proceeding with the chosen base guess.')

        # Apply Systems-mode constraint knob overrides (from assistant)
        try:
            ov = st.session_state.get('systems_inputs_overrides', {})
            if isinstance(ov, dict) and ov:
                d0 = dict(base_for_solve.__dict__)
                d0.update({k: float(v) for k, v in ov.items()})
                base_for_solve = PointInputs(**d0)
        except Exception:
            pass

        # Solver knob defaults must be available regardless of which UI branches executed on this rerun.
        # (Phase-1 rule: no conditional variable definitions.)
        block_solve = bool(st.session_state.get("systems_block_solve", False))


        # -----------------------------
        # Continuation ramp to targets (explicit; UI-side only)
        # -----------------------------
        if coupled and do_continuation and cont_steps >= 2:
            try:
                out0 = hot_ion_point(base_for_solve)
            except Exception:
                out0 = {}

            start_targets = {}
            for k in targets.keys():
                try:
                    v = float(out0.get(k, float("nan")))
                except Exception:
                    v = float("nan")
                if v == v and abs(v) != float("inf"):
                    start_targets[k] = v

            def _make_req(_base: PointInputs, _t: dict) -> SolverRequest:
                opts = {"multistart": True, "restarts": 8, "cache_enabled": True, "cache_max": 1024}
                if trust_delta is not None:
                    opts["trust_delta"] = float(trust_delta)
                if block_solve:
                    opts["block_solve"] = True
                return SolverRequest(base=_base, targets=_t, variables=variables, max_iter=max_iter, tol=float(tol), damping=float(damping), options=opts)

            base_stage = base_for_solve
            for s in range(1, int(cont_steps)):
                alpha = float(s) / float(cont_steps)
                step_targets = {}
                for k, final in targets.items():
                    if k in start_targets:
                        step_targets[k] = float(start_targets[k] + alpha * (float(final) - float(start_targets[k])))
                    else:
                        step_targets[k] = float(final)

                log.code(json.dumps({"event": "cont_step", "step": float(s), "n_steps": float(cont_steps), "alpha": alpha, "targets": step_targets}, indent=2, sort_keys=True))
                _res = solve_request(_make_req(base_stage, step_targets), backend=DefaultTargetSolverBackend())
                log.code(json.dumps({"event": "cont_result", "step": float(s), "ok": bool(_res.ok), "iters": float(_res.iters), "message": _res.message}, indent=2, sort_keys=True))
                if not _res.ok:
                    fail = {"event": "fail", "reason": "continuation_step_fail", "step": float(s), "alpha": alpha, "message": _res.message}
                    log.code(json.dumps(fail, indent=2, sort_keys=True))
                    st.error("Continuation step failed. Adjust targets/bounds or disable continuation.")
                    st.stop()
                base_stage = _res.inp

            base_for_solve = base_stage
        try:
            for step in solve_for_targets_stream(
                base_for_solve,
                targets=targets,
                variables=variables,
                max_iter=max_iter,
                tol=float(tol),
                damping=float(damping),
                trust_delta=(float(trust_delta) if trust_delta is not None else None),
            ):
                last = step
                log.code(json.dumps(step, indent=2, sort_keys=True))
            req = SolverRequest(base=base_for_solve, targets=targets, variables=variables, max_iter=max_iter, tol=float(tol), damping=float(damping), options={"multistart": True, "restarts": 8, "cache_enabled": True, "cache_max": 1024, **({"trust_delta": float(trust_delta)} if trust_delta is not None else {}), **({"block_solve": True} if block_solve else {})})
            import time as _time
            t_solve0 = _time.perf_counter()
            res = solve_request(req, backend=DefaultTargetSolverBackend())
            wall_s = float(_time.perf_counter() - t_solve0)
            inp_sol = res.inp
            out_sol = res.out
            st.success(f"Done. Converged={res.ok}, iterations={res.iters}")

            if not res.ok and ("Ip_MA" in variables and "fG" in variables) and ("H98" in targets and "Q_DT_eqv" in targets):
                with st.expander("Target feasibility at (Iâ‚š, f_G) bound corners", expanded=False):
                    try:
                        from solvers.constraint_solver import evaluate_targets_at_corners
                        lo0, hi0 = float(variables["Ip_MA"][1]), float(variables["Ip_MA"][2])
                        lo1, hi1 = float(variables["fG"][1]), float(variables["fG"][2])
                        rows = evaluate_targets_at_corners(base, {"H98": float(targets["H98"]), "Q_DT_eqv": float(targets["Q_DT_eqv"])}, ("Ip_MA", lo0, hi0), ("fG", lo1, hi1))
                        import pandas as _pd  # type: ignore
                        st.dataframe(_pd.DataFrame(rows), use_container_width=True)
                    except Exception as _e:
                        st.caption(f"Corner table unavailable: {_e}")
            st.session_state["last_point_inp"] = inp_sol
            st.session_state["last_point_out"] = out_sol

            constraints_list = evaluate_constraints(out_sol)
            solver_meta = {"message": res.message, "trace": res.trace or []}
            artifact = build_run_artifact(
                inputs=dict(inp_sol.__dict__),
                outputs=dict(out_sol),
                constraints=constraints_list,
                meta={"mode": "systems"},
                solver=solver_meta,
                baseline_inputs=dict(base.__dict__),
                subsystems={
                    "fidelity": st.session_state.get("fidelity_config", {}),
                    "calibration": {
                        "confinement": float(st.session_state.get("calib_confinement", 1.0)),
                        "divertor": float(st.session_state.get("calib_divertor", 1.0)),
                        "bootstrap": float(st.session_state.get("calib_bootstrap", 1.0)),
                    },
                },
            )

            # ---- Systems Mode freeze contract (schema_version=1) ----
            # NOTE: This is purely metadata; it does not affect physics.
            try:
                from src.systems.schema import SCHEMA_VERSION as _SYS_SCHEMA_V, freeze_contract as _sys_freeze_contract
            except Exception:  # pragma: no cover
                from systems.schema import SCHEMA_VERSION as _SYS_SCHEMA_V, freeze_contract as _sys_freeze_contract
            artifact.setdefault("schema_version", int(_SYS_SCHEMA_V))
            artifact.setdefault("freeze_contract", _sys_freeze_contract())
            artifact.setdefault("artifact_kind", "systems")

            # ---- Human-readable freeze statement (mirrors Point Designer governance banner) ----
            try:
                artifact.setdefault("freeze_statement", {
                    "systems_mode": "FROZEN",
                    "version": "v187.1",
                    "basis": "Point Designer physics/constraints are immutable; Systems Mode explores and ranks candidates using intent-aware acceptance without altering evaluator logic.",
                })
            except Exception:
                pass

            # ---- Intent-aware feasibility summary (resolves hard-feasibility vs intent ambiguity) ----
            try:
                failed_names = []
                for _c in constraints_list:
                    try:
                        if not bool(getattr(_c, "passed", True)) and str(getattr(_c, "severity", "hard")).lower() == "hard":
                            failed_names.append(str(getattr(_c, "name", "")))
                    except Exception:
                        pass
                _cls = _classify_failed_constraints(failed_names)
                artifact["intent_feasibility_summary"] = {
                    **_constraint_policy_snapshot(),
                    "blocking_feasible": (len(_cls.get("blocking", [])) == 0),
                    "failed_blocking": _cls.get("blocking", []),
                    "failed_diagnostic": _cls.get("diagnostic", []),
                    "failed_ignored": _cls.get("ignored", []),
                    "note": "Feasibility under active Design Intent; hard constraint failures may be diagnostic/ignored in Research intent.",
                }
            except Exception:
                pass

            # Frozen top-level key: run_cards (keep ui_state copy for restore)
            try:
                _cards = st.session_state.get("systems_run_cards", [])
                artifact["run_cards"] = _shams_json_sanitize(_cards)
            except Exception:
                pass

            # Embed enough UI state into the artifact so it can fully restore Systems Mode later.
            # This does NOT change physics; it is purely UX/state persistence.
            try:
                artifact.setdefault("ui_state", {})
                _ui_state = {
                    "workflow_step": str(st.session_state.get("systems_workflow_step", "")),
                    "design_intent": str(st.session_state.get("design_intent", "")),
                    "systems_run_cards": st.session_state.get("systems_run_cards", []),
                    "systems_journal": st.session_state.get("systems_journal", []),
                    "v178_last_precheck": st.session_state.get("v178_last_precheck"),
                    "v178_last_recovery": st.session_state.get("v178_last_recovery"),
                    "v178_fs_last": st.session_state.get("v178_fs_last"),
                }
                # Important: sanitize to prevent circular refs / Streamlit state leaks.
                artifact["ui_state"].update(_shams_json_sanitize(_ui_state))
            except Exception:
                pass



            # v380.0: Impurity radiation partition + detachment requirement certification (governance-only)
            try:
                from src.certification.impurity_radiation_detachment_certification_v380 import (
                    evaluate_impurity_radiation_detachment_authority,
                )
                _cert = evaluate_impurity_radiation_detachment_authority(out_sol).to_dict()
                artifact.setdefault('certifications', {})
                if isinstance(artifact.get('certifications'), dict):
                    artifact['certifications']['impurity_radiation_detachment_v380'] = _cert
            except Exception:
                pass
            # v381.0: Advanced current-drive authority (governance-only)
            try:
                from src.certification.current_drive_certification_v381 import (
                    evaluate_current_drive_authority,
                )
                _cd_cert = evaluate_current_drive_authority(out_sol).to_dict()
                artifact.setdefault('certifications', {})
                if isinstance(artifact.get('certifications'), dict):
                    artifact['certifications']['current_drive_v381'] = _cd_cert
            except Exception:
                pass

            # v383.0: Plant economics & cost authority 2.0 (governance-only)
            try:
                from src.certification.plant_economics_certification_v383 import (
                    evaluate_plant_economics_authority_v383,
                )
                _pe_cert = evaluate_plant_economics_authority_v383(
                    out_sol,
                    inputs=(artifact.get('inputs') if isinstance(artifact.get('inputs'), dict) else None),
                ).to_dict()
                artifact.setdefault('certifications', {})
                if isinstance(artifact.get('certifications'), dict):
                    artifact['certifications']['plant_economics_v383'] = _pe_cert
            except Exception:
                pass

            # v388.0.0: Cost Authority 3.0 â€” Industrial Depth (governance-only)
            try:
                from src.certification.cost_authority_certification_v388 import (
                    evaluate_cost_authority_v388,
                )
                _c388 = evaluate_cost_authority_v388(
                    out_sol,
                    inputs=(artifact.get('inputs') if isinstance(artifact.get('inputs'), dict) else None),
                ).to_dict()
                artifact.setdefault('certifications', {})
                if isinstance(artifact.get('certifications'), dict):
                    artifact['certifications']['cost_authority_v388'] = _c388
            except Exception:
                pass

            # v176.2: attach lightweight telemetry so users can verify performance changes
            try:
                precheck_s = st.session_state.get("systems_precheck_seconds", None)
                tel = {
                    "wall_s": float(wall_s),
                    "precheck_s": (float(precheck_s) if precheck_s is not None else None),
                    "backend": str(solver_backend),
                    "robust_mode": bool(robust_mode),
                    "warm_start": bool(warm_start),
                    "continuation": bool(cont_enabled),
                    "block_solve": bool(block_solve),
                    "n_targets": int(len(targets)),
                    "n_variables": int(len(variables)),
                    "max_iter": int(max_iter),
                    "tol": float(tol),
                }
                artifact.setdefault("meta", {})
                artifact["meta"]["telemetry"] = tel
                st.session_state["last_systems_telemetry"] = tel
            except Exception:
                pass

            # v98.1: cache + ledger record (systems)
            try:
                s = _v92_state_get()
                s.last_systems_result = artifact
                # v182.1: also cache into session_state for rerun-stable rendering
                st.session_state['systems_last_solve_artifact'] = artifact
                # Canonical interop cache keys (Phase-1 contract): other decks
                # (Scan/Pareto/Trade Study) consume these without triggering compute.
                st.session_state['systems_last_solution'] = artifact
                st.session_state['last_systems_solution'] = artifact
                _v98_record_run("systems", artifact, mode="systems_mode")
            except Exception:
                pass

            st.markdown("### Key results")
            kcols = st.columns(4)
            def _k(metric, key, fmt="{:.3g}"):
                v = float(out_sol.get(key, float("nan")))
                with metric:
                    st.metric(key, fmt.format(v) if v==v else "NaN")
            _k(kcols[0], "Q_DT_eqv", "{:.3g}")
            _k(kcols[1], "H98", "{:.3g}")
            _k(kcols[2], "P_e_net_MW", "{:.3g}")
            _k(kcols[3], "q_div_MW_m2", "{:.3g}")

            # constraints dashboard
            with st.expander("Constraints & margins (systems mode)", expanded=False):
                rows_c = []
                for c in constraints_list:
                    try:
                        margin = float(getattr(c, "margin"))
                    except Exception:
                        margin = float("nan")
                    rows_c.append({
                        "constraint": c.name,
                        "sense": c.sense,
                        "value": c.value,
                        "limit": c.limit,
                        "units": c.units,
                        "passed": bool(c.passed),
                        "margin_frac": margin,
                        "severity": getattr(c, "severity", "hard"),
                        "note": c.note,
                    })
                dfc = pd.DataFrame(rows_c)
                st.dataframe(dfc, use_container_width=True)

            # Sankey + radial build
            with st.expander("Plots (radial build + power balance)", expanded=False):
                try:
                    import tempfile, os
                    tmpdir = tempfile.mkdtemp(prefix="shams_systems_")
                    rb = os.path.join(tmpdir, "radial_build.png")
                    plot_radial_build_from_artifact(artifact, rb)
                    st.image(rb, caption="Radial build (proxy)", use_container_width=True)
                except Exception as e:
                    st.warning(f"Radial build plot unavailable: {e}")
                try:
                    import tempfile, os
                    from shams_io.sankey import build_power_balance_sankey
                    import plotly.graph_objects as go
                    sank = build_power_balance_sankey(artifact)
                    fig = go.Figure(data=[go.Sankey(**sank)])
                    st.plotly_chart(fig, use_container_width=True)
                except Exception as e:
                    st.warning(f"Sankey unavailable: {e}")

            # v374.2: Post-Key-results diagnostics bundle (Compact Cockpit + Systems Console)
            with st.expander("ðŸ”Ž Detailed Systems Diagnostics (post-run)", expanded=False):
                # Compact Cockpit controls
                cc1, cc2, cc3 = st.columns([1, 1, 2])
                with cc1:
                    sys_cockpit_pin = st.toggle(
                        "Pin cockpit",
                        value=st.session_state.get("systems_cockpit_pin", False),
                        key="systems_cockpit_pin",
                    )
                with cc2:
                    sys_cockpit_show_md = st.toggle(
                        "Show markdown",
                        value=st.session_state.get("systems_cockpit_show_md", False),
                        key="systems_cockpit_show_md",
                    )
                with cc3:
                    st.caption("Diagnostics only (does not change physics, constraints, or truth).")

                _ = sys_cockpit_pin  # retained for future pin behavior

                # Compact cockpit
                try:
                    _sys_render_compact_cockpit()
                except Exception:
                    st.caption("Compact cockpit unavailable (non-fatal).")

                # Systems Console: verdict bar + why-chain + constraint cards
                try:
                    _sys_art2 = st.session_state.get("systems_last_solve_artifact") or _sys_fetch_latest_systems_artifact()
                    if isinstance(_sys_art2, dict):
                        _sys_cons2 = _sys_extract_constraints(_sys_art2)
                        _sys_render_verdict_bar(_sys_art2, constraints=_sys_cons2)
                        _sys_render_causal_chain(
                            _sys_art2,
                            constraints=_sys_cons2,
                            expert=st.session_state.get("systems_expert_view", False),
                        )
                        _sys_render_constraint_cards(_sys_art2, constraints=_sys_cons2)
                    else:
                        st.info("No cached Systems artifact yet. Run Precheck / Solve to populate diagnostics.")
                except Exception:
                    st.caption("Detailed diagnostics unavailable (non-fatal).")

            # v375.0: Exhaust authority (certified bundle) â€” kept under Key results
            with st.expander("ðŸ”¥ Exhaust & Divertor Authority (certified)", expanded=False):
                try:
                    _ea = {
                        "lambda_q_mm_raw": float(out_sol.get("lambda_q_mm_raw", float("nan"))),
                        "lambda_q_mm_used": float(out_sol.get("lambda_q_mm", float("nan"))),
                        "flux_expansion_raw": float(out_sol.get("flux_expansion_raw", float("nan"))),
                        "flux_expansion_used": float(out_sol.get("flux_expansion", float("nan"))),
                        "n_strike_points_raw": int(out_sol.get("n_strike_points_raw", out_sol.get("n_strike_points", 2)) or 2),
                        "n_strike_points_used": int(out_sol.get("n_strike_points", 2) or 2),
                        "f_wet_raw": float(out_sol.get("f_wet_raw", float("nan"))),
                        "f_wet_used": float(out_sol.get("f_wet_divertor", float("nan"))),
                        "A_wet_m2": float(out_sol.get("A_wet_m2", float("nan"))),
                        "q_div_MW_m2": float(out_sol.get("q_div_MW_m2", float("nan"))),
                        "q_div_max_MW_m2": float(out_sol.get("q_div_max_MW_m2", float("nan"))),
                        "q_div_unit_suspect": float(out_sol.get("q_div_unit_suspect", 0.0)),
                        "contract_sha256": str(out_sol.get("exhaust_authority_contract_sha256", "")),
                    }
                    st.dataframe(pd.DataFrame([_ea]), use_container_width=True)
                    if float(_ea.get("q_div_unit_suspect", 0.0)) >= 0.5:
                        st.warning("q_div magnitude looks unit-suspect (>1e5 MW/mÂ²). This is a flag only; truth is unchanged.")
                except Exception as _e:
                    st.caption(f"Exhaust authority bundle unavailable (non-fatal): {_e}")

            # v380.0: Impurity radiation partition + detachment requirement (certified)
            with st.expander("ðŸ§ª Impurity radiation & detachment authority (certified)", expanded=False):
                try:
                    from src.certification.impurity_radiation_detachment_certification_v380 import (
                        evaluate_impurity_radiation_detachment_authority,
                        certification_table_rows,
                    )

                    _cert = evaluate_impurity_radiation_detachment_authority(out_sol).to_dict()
                    st.dataframe(pd.DataFrame([certification_table_rows(_cert)]), use_container_width=True)
                    with st.expander("Details", expanded=False):
                        st.json(_cert)
                except Exception as _e:
                    st.caption(f"Impurity/detachment authority unavailable (non-fatal): {_e}")


                # v381.0: Current drive authority (certified)
                with st.expander("âš¡ Current drive authority (certified)", expanded=False):
                    try:
                        _cert = None
                        _certs = _art_cached.get('certifications', {}) if isinstance(_art_cached.get('certifications', {}), dict) else {}
                        _cert = _certs.get('current_drive_v381')
                        if not isinstance(_cert, dict):
                            st.info("No cached current-drive certification yet. Re-run Systems Solve to generate it.")
                        else:
                            from src.certification.current_drive_certification_v381 import certification_table_rows
                            st.dataframe(pd.DataFrame([certification_table_rows(_cert)]), use_container_width=True)
                            with st.expander("Details", expanded=False):
                                st.json(_cert)
                    except Exception as _e:
                        st.caption(f"Current-drive authority unavailable (non-fatal): {_e}")


                # v383.0: Plant economics & cost authority 2.0 (certified)
                with st.expander("ðŸ’° Plant economics & cost authority (certified)", expanded=False):
                    try:
                        _certs = _art_cached.get('certifications', {}) if isinstance(_art_cached.get('certifications', {}), dict) else {}
                        _pec = _certs.get('plant_economics_v383')
                        if not isinstance(_pec, dict):
                            st.info("No cached plant economics certification yet. Re-run Systems Solve to generate it.")
                        else:
                            from src.certification.plant_economics_certification_v383 import certification_table_rows
                            st.dataframe(pd.DataFrame([certification_table_rows(_pec)]), use_container_width=True)
                            with st.expander("Details", expanded=False):
                                st.json(_pec)
                    except Exception as _e:
                        st.caption(f"Plant economics authority unavailable (non-fatal): {_e}")


                # v388.0.0: Cost Authority 3.0 â€” Industrial Depth (certified)
                with st.expander("ðŸ­ Cost authority â€” industrial depth (certified)", expanded=False):
                    try:
                        _certs = _art_cached.get('certifications', {}) if isinstance(_art_cached.get('certifications', {}), dict) else {}
                        _c388 = _certs.get('cost_authority_v388')
                        if not isinstance(_c388, dict):
                            st.info("No cached cost authority certification yet. Re-run Systems Solve to generate it.")
                        else:
                            from src.certification.cost_authority_certification_v388 import certification_table_rows
                            st.dataframe(pd.DataFrame([certification_table_rows(_c388)]), use_container_width=True)
                            with st.expander("Details", expanded=False):
                                st.json(_c388)
                    except Exception as _e:
                        st.caption(f"Cost authority unavailable (non-fatal): {_e}")



        except _SysPrecheckBlocksSolve:
            st.warning(
                "Full Systems solve skipped: **precheck is infeasible under Reactor intent**. "
                "Use Seeded Recovery / assistant proposals to regain feasibility, or switch Design Intent "
                "to **Experimental Device** for exploratory solves.",
                icon="â›”",
            )
            try:
                st.session_state['systems_last_solve_blocked_reason'] = 'precheck_infeasible_reactor'
            except Exception:
                pass
        except Exception as e:
            st.error(f"Systems solver error: {e}")
            try:
                _alog_exc('Systems', 'RunSystemsSolveException', e)
            except Exception:
                pass

    # ---------------------------------------------------------------------
    # v374.2+ render contract: if there is a cached Systems solution, ALWAYS
    # render Key results + post-run expanders from cache (no compute here).
    # This avoids the "results disappeared" symptom on rerun.
    # ---------------------------------------------------------------------
    if not run:
        try:
            _art_cached = st.session_state.get('systems_last_solution') or st.session_state.get('systems_last_solve_artifact')
            if isinstance(_art_cached, dict):
                _out_cached = _art_cached.get('outputs', {}) if isinstance(_art_cached.get('outputs', {}), dict) else {}
                _cons_cached = _sys_extract_constraints(_art_cached)

                st.markdown("### Key results")
                kcols = st.columns(4)
                def _k(metric, key, fmt="{:.3g}"):
                    v = float(_out_cached.get(key, float("nan")))
                    with metric:
                        st.metric(key, fmt.format(v) if v==v else "NaN")
                _k(kcols[0], "Q_DT_eqv", "{:.3g}")
                _k(kcols[1], "H98", "{:.3g}")
                _k(kcols[2], "P_e_net_MW", "{:.3g}")
                _k(kcols[3], "q_div_MW_m2", "{:.3g}")

                with st.expander("Constraints & margins (systems mode)", expanded=False):
                    rows_c = []
                    for c in _cons_cached:
                        try:
                            margin = float(getattr(c, "margin"))
                        except Exception:
                            margin = float("nan")
                        rows_c.append({
                            "constraint": c.name,
                            "sense": c.sense,
                            "value": c.value,
                            "limit": c.limit,
                            "units": c.units,
                            "passed": bool(c.passed),
                            "margin_frac": margin,
                            "severity": getattr(c, "severity", "hard"),
                            "note": c.note,
                        })
                    st.dataframe(pd.DataFrame(rows_c), use_container_width=True)

                with st.expander("Plots (radial build + power balance)", expanded=False):
                    try:
                        import tempfile, os
                        tmpdir = tempfile.mkdtemp(prefix="shams_systems_")
                        rb = os.path.join(tmpdir, "radial_build.png")
                        plot_radial_build_from_artifact(_art_cached, rb)
                        st.image(rb, caption="Radial build (proxy)", use_container_width=True)
                    except Exception as e:
                        st.warning(f"Radial build plot unavailable: {e}")
                    try:
                        from shams_io.sankey import build_power_balance_sankey
                        import plotly.graph_objects as go
                        sank = build_power_balance_sankey(_art_cached)
                        fig = go.Figure(data=[go.Sankey(**sank)])
                        st.plotly_chart(fig, use_container_width=True)
                    except Exception as e:
                        st.warning(f"Sankey unavailable: {e}")

                with st.expander("ðŸ”Ž Detailed Systems Diagnostics (post-run)", expanded=False):
                    try:
                        _sys_render_compact_cockpit()
                    except Exception:
                        st.caption("Compact cockpit unavailable (non-fatal).")
                    try:
                        _sys_render_verdict_bar(_art_cached, constraints=_cons_cached)
                        _sys_render_causal_chain(
                            _art_cached,
                            constraints=_cons_cached,
                            expert=st.session_state.get("systems_expert_view", False),
                        )
                        _sys_render_constraint_cards(_art_cached, constraints=_cons_cached)
                    except Exception:
                        st.caption("Detailed diagnostics unavailable (non-fatal).")

                with st.expander("ðŸ”¥ Exhaust & Divertor Authority (certified)", expanded=False):
                    try:
                        _ea = {
                            "lambda_q_mm_raw": float(_out_cached.get("lambda_q_mm_raw", float("nan"))),
                            "lambda_q_mm_used": float(_out_cached.get("lambda_q_mm", float("nan"))),
                            "flux_expansion_raw": float(_out_cached.get("flux_expansion_raw", float("nan"))),
                            "flux_expansion_used": float(_out_cached.get("flux_expansion", float("nan"))),
                            "n_strike_points_raw": int(_out_cached.get("n_strike_points_raw", _out_cached.get("n_strike_points", 2)) or 2),
                            "n_strike_points_used": int(_out_cached.get("n_strike_points", 2) or 2),
                            "f_wet_raw": float(_out_cached.get("f_wet_raw", float("nan"))),
                            "f_wet_used": float(_out_cached.get("f_wet_divertor", float("nan"))),
                            "A_wet_m2": float(_out_cached.get("A_wet_m2", float("nan"))),
                            "q_div_MW_m2": float(_out_cached.get("q_div_MW_m2", float("nan"))),
                            "q_div_max_MW_m2": float(_out_cached.get("q_div_max_MW_m2", float("nan"))),
                            "q_div_unit_suspect": float(_out_cached.get("q_div_unit_suspect", 0.0)),
                            "contract_sha256": str(_out_cached.get("exhaust_authority_contract_sha256", "")),
                        }
                        st.dataframe(pd.DataFrame([_ea]), use_container_width=True)
                        if float(_ea.get("q_div_unit_suspect", 0.0)) >= 0.5:
                            st.warning("q_div magnitude looks unit-suspect (>1e5 MW/mÂ²). This is a flag only; truth is unchanged.")
                    except Exception as _e:
                        st.caption(f"Exhaust authority bundle unavailable (non-fatal): {_e}")

                # v380.0: Impurity radiation partition + detachment requirement (certified)
                with st.expander("ðŸ§ª Impurity radiation & detachment authority (certified)", expanded=False):
                    try:
                        _cert = None
                        # Prefer cached certifications if present (strict render-from-cache).
                        _certs = _art_cached.get('certifications', {}) if isinstance(_art_cached.get('certifications', {}), dict) else {}
                        _cert = _certs.get('impurity_radiation_detachment_v380')
                        if not isinstance(_cert, dict):
                            st.info("No cached impurity/detachment certification yet. Re-run Systems Solve to generate it.")
                        else:
                            from src.certification.impurity_radiation_detachment_certification_v380 import certification_table_rows
                            st.dataframe(pd.DataFrame([certification_table_rows(_cert)]), use_container_width=True)
                            with st.expander("Details", expanded=False):
                                st.json(_cert)
                    except Exception as _e:
                        st.caption(f"Impurity/detachment authority unavailable (non-fatal): {_e}")

                # v383.0: Plant economics & cost authority 2.0 (certified)
                with st.expander("ðŸ’° Plant economics & cost authority (certified)", expanded=False):
                    try:
                        _certs = _art_cached.get('certifications', {}) if isinstance(_art_cached.get('certifications', {}), dict) else {}
                        _pec = _certs.get('plant_economics_v383')
                        if not isinstance(_pec, dict):
                            st.info("No cached plant economics certification yet. Re-run Systems Solve to generate it.")
                        else:
                            from src.certification.plant_economics_certification_v383 import certification_table_rows
                            st.dataframe(pd.DataFrame([certification_table_rows(_pec)]), use_container_width=True)
                            with st.expander("Details", expanded=False):
                                st.json(_pec)
                    except Exception as _e:
                        st.caption(f"Plant economics authority unavailable (non-fatal): {_e}")


                # v388.0.0: Cost Authority 3.0 â€” Industrial Depth (certified)
                with st.expander("ðŸ­ Cost authority â€” industrial depth (certified)", expanded=False):
                    try:
                        _certs = _art_cached.get('certifications', {}) if isinstance(_art_cached.get('certifications', {}), dict) else {}
                        _c388 = _certs.get('cost_authority_v388')
                        if not isinstance(_c388, dict):
                            st.info("No cached cost authority certification yet. Re-run Systems Solve to generate it.")
                        else:
                            from src.certification.cost_authority_certification_v388 import certification_table_rows
                            st.dataframe(pd.DataFrame([certification_table_rows(_c388)]), use_container_width=True)
                            with st.expander("Details", expanded=False):
                                st.json(_c388)
                    except Exception as _e:
                        st.caption(f"Cost authority unavailable (non-fatal): {_e}")

        except Exception:
            pass

if _deck == "ðŸ—ºï¸ Scan Lab":
    # DSG: auto edge-kind tagging by active panel (exploration only)
    if bool(st.session_state.get("dsg_edge_kind_auto", True)):
        st.session_state["dsg_context_edge_kind"] = "scan"

    st.header("ðŸ—ºï¸ Scan Lab")
    st.caption("Cartography over the frozen evaluator: map feasibility, emptiness, fragility, and dominant mechanisms. Deterministic; no internal optimizer.")
    render_mode_scope("scan")

    # --- World-class Scan Lab (v188) ---
    # NOTE: Scan Lab should remain usable even if optional features fail to import.
    # Import errors are captured and surfaced explicitly (freeze-readiness requirement).
    _scan_import_errors = []

    try:
        # Fix: evaluator lives under src/ in the merged repo layout.
        from src.evaluator.core import Evaluator  # type: ignore
    except Exception as _e:
        Evaluator = None  # type: ignore
        _scan_import_errors.append(f"Evaluator import failed: {_e}")

    try:
        from tools.scan_cartography import build_cartography_report
    except Exception as _e:
        build_cartography_report = None  # type: ignore
        _scan_import_errors.append(f"scan_cartography import failed: {_e}")

    try:
        from tools.golden_scans import build_golden_scan_presets
    except Exception as _e:
        build_golden_scan_presets = None  # type: ignore
        _scan_import_errors.append(f"golden_scans import failed: {_e}")

    try:
        from tools.canonical_questions import build_canonical_questions
    except Exception as _e:
        build_canonical_questions = None  # type: ignore
        _scan_import_errors.append(f"canonical_questions import failed: {_e}")

    try:
        from tools.scan_insights import (
            build_causality_trace,
            uncertainty_stress_test,
            time_to_failure_along_knob,
            null_direction_2d,
        )
    except Exception as _e:
        build_causality_trace = None  # type: ignore
        uncertainty_stress_test = None  # type: ignore
        time_to_failure_along_knob = None  # type: ignore
        null_direction_2d = None  # type: ignore
        _scan_import_errors.append(f"scan_insights import failed: {_e}")

    try:
        from tools.scan_next_tier import (
            local_powerlaw_fit,
            label_regime,
            explain_impossible_region,
            detect_irrelevant_constraints,
            projection_stability_check,
            path_follow_scan,
            assumption_stress_hotspots,
            counterfactual_lens,
            guided_steps,
            build_scan_atlas_pdf_bytes,
            surprise_detector,
        )
    except Exception as _e:
        local_powerlaw_fit = None  # type: ignore
        label_regime = None  # type: ignore
        explain_impossible_region = None  # type: ignore
        detect_irrelevant_constraints = None  # type: ignore
        projection_stability_check = None  # type: ignore
        path_follow_scan = None  # type: ignore
        assumption_stress_hotspots = None  # type: ignore
        counterfactual_lens = None  # type: ignore
        guided_steps = None  # type: ignore
        build_scan_atlas_pdf_bytes = None  # type: ignore
        surprise_detector = None  # type: ignore
        _scan_import_errors.append(f"scan_next_tier import failed: {_e}")


    try:
        from tools.scan_v1p1_worldclass import (
            build_constraint_dictionary,
            build_reproducibility_capsule,
            monotonicity_sanity_overlay,
            boundary_thickness_metric,
            explain_uncertainty_disagreement,
            to_json_bytes,
        )
    except Exception as _e:
        build_constraint_dictionary = None  # type: ignore
        build_reproducibility_capsule = None  # type: ignore
        monotonicity_sanity_overlay = None  # type: ignore
        boundary_thickness_metric = None  # type: ignore
        explain_uncertainty_disagreement = None  # type: ignore
        to_json_bytes = None  # type: ignore
        _scan_import_errors.append(f"scan_v1p1_worldclass import failed: {_e}")

    try:
        from tools.scan_expert_features import (
            SCAN_LAB_CONTRACT,
            compute_fingerprints,
            ScanClaim,
            build_claim_evidence,
            build_claim_pdf_bytes,
            falsify_claim,
        )
    except Exception as _e:
        SCAN_LAB_CONTRACT = ""  # type: ignore
        compute_fingerprints = None  # type: ignore
        ScanClaim = None  # type: ignore
        build_claim_evidence = None  # type: ignore
        build_claim_pdf_bytes = None  # type: ignore
        falsify_claim = None  # type: ignore
        _scan_import_errors.append(f"scan_expert_features import failed: {_e}")

    try:
        from tools.reports.scan_signature_atlas import build_signature_atlas_pdf_bytes
    except Exception as _e:
        build_signature_atlas_pdf_bytes = None  # type: ignore
        _scan_import_errors.append(f"scan_signature_atlas import failed: {_e}")

    try:
        from tools.scan_artifact_schema import (
            build_scan_artifact,
            upgrade_scan_artifact,
            SCAN_SCHEMA_VERSION,
            stable_hash,
        )
    except Exception as _e:
        build_scan_artifact = None  # type: ignore
        upgrade_scan_artifact = None  # type: ignore
        SCAN_SCHEMA_VERSION = 0  # type: ignore
        stable_hash = None  # type: ignore
        _scan_import_errors.append(f"scan_artifact_schema import failed: {_e}")

    # Persist import errors so the UI can show them near the run buttons.
    st.session_state["scan_import_errors"] = _scan_import_errors

    def _v188_scan_lab_panel() -> None:
        import streamlit as st
        import numpy as np
        import pandas as pd
        import io
        import matplotlib.pyplot as plt
        from matplotlib.colors import ListedColormap
        import matplotlib.patches as mpatches

        st.subheader("ðŸ—ºï¸ Cartography Deck")
        _scan_deck = st.radio(
            "Scan Lab deck",
            options=["ðŸ—ºï¸ Cartography", "ðŸ§° Orientation"],
            index=0,
            horizontal=True,
            key="scan_deck",
            help="Deck-based navigation: render one Scan workspace at a time (no scroll walls).",
        )
        if _scan_deck == "ðŸ§° Orientation":
            with st.expander("Orientation Deck (optional, read-only helpers)", expanded=False):
                st.caption("These helpers are read-only. Switch back to ðŸ—ºï¸ Cartography to run scans.")
                st.info("Scan Lab orientation only (no scan executed).", icon="ðŸ§­")
            st.stop()

        st.info("Scan Lab is **frozen**: deterministic cartography/interpretability over the frozen evaluator. No optimization, no relaxation, no recommendations.", icon="ðŸ§Š")

        st.caption("A microscope, not an engine: map feasibility structure and failure mechanisms across a parameter space.")

        # --- Pre-Cartography orientation (freeze-compliant; UI-only) ---
        with st.expander("Orientation Deck (optional, read-only helpers)", expanded=False):
            st.caption("These sections are optional. Cartography controls are below.")
            st.markdown("### Orientation Deck (read-only)")
            st.caption("These helpers are **read-only**. They do not change the evaluator, physics, or scan results.")
    
            with st.expander(" Scan Lab is frozen - freeze statement (read-only)", expanded=False):
                c1, c2 = st.columns([1.0, 1.0])
                with c1:
                    st.markdown("**Freeze statement**: `docs/SCANLAB_FREEZE.md`")
                    st.caption("Semantics are frozen; Scan Lab is cartography/insight only.")
                with c2:
                    try:
                        from pathlib import Path
                        _sf = (Path(__file__).resolve().parent.parent / "docs" / "SCANLAB_FREEZE.md").read_text(encoding="utf-8")
                    except Exception:
                        _sf = "(missing docs/SCANLAB_FREEZE.md)"
                    st.download_button(
                        "Download freeze statement",
                        data=_sf,
                        file_name="SCANLAB_FREEZE.md",
                        mime="text/markdown",
                        use_container_width=False,
                        key="scan_freeze_dl_pre",
                    )
                    with st.expander("View freeze statement", expanded=False):
                        st.markdown(_sf)
    
            # --- Legacy scan + stateful download (kept for backward compatibility) ---
            with st.expander("Legacy Grid Scan + stateful download (archived)", expanded=False):
                try:
                    _v93_stateful_scan_panel()
                except Exception:
                    pass
                st.markdown(
                    """
    This legacy scan performs a **nested solver grid** over (Ti, H98, a, Q, g_conf). It remains available for older workflows,
    but the recommended Scan Lab path is now the **Cartography** scan below.
                    """
                )
    
            with st.expander("Parameter guide (units, meaning, min/max)", expanded=False):
                st.markdown(
                    """
    Below are the **recommended** ranges used for input validation in this UI.  
    They are intentionally broad to avoid overâ€‘constraining early exploration.
    
    | Parameter | Meaning | Recommended min | Recommended max |
    |---|---|---:|---:|
    | Râ‚€ (m) | Major radius | 0.5 | 10 |
    | Bâ‚€ (T) | Toroidal field on axis | 1 | 25 |
    | Shield (m) | Neutron shield thickness | 0 | 2 |
    | P_aux (MW) | External heating power | 0 | 200 |
    | P_aux for Q (MW) | Power used in Q = P_fus / P_aux_for_Q | 0 | 200 |
    | Táµ¢/Tâ‚‘ (â€“) | Ion/electron temperature ratio | 0.5 | 5 |
    | Ti_start/stop (keV) | Ion temperature scan bounds | 1 | 40 |
    | Ti_step (keV) | Ion temperature step | 0.05 | 5 |
    | H98_start/stop (â€“) | H98y2 confinement multiplier bounds | 0.5 | 3 |
    | H98_step (â€“) | H98 step | 0.01 | 0.5 |
    | a_min/a_max (m) | Minor radius scan bounds | 0.2 | 5 |
    | a_step (m) | Minor radius step | 0.001 | 1 |
    | Q_start/stop (â€“) | Target Q scan bounds (screening target) | 0.1 | 100 |
    | Q_step (â€“) | Q step | 0.05 | 20 |
    | g_conf start/stop (â€“) | Additional confinement gain factor | 0.5 | 5 |
    | g_conf step (â€“) | g_conf step | 0.01 | 1 |
    | Iâ‚š bounds (MA) | Solver search bounds for plasma current | 1 | 50 |
    | fG bounds (â€“) | Greenwald fraction screening bounds | 0.01 | 1.5 |
    | tol (â€“) | Numerical tolerance for the solver | 1e-6 | 1e-2 |
    | Zeff (â€“) | Effective charge | 1.0 | 4.0 |
    | dilution_fuel (â€“) | Fuel dilution factor (â‰¤1) | 0.2 | 1.0 |
    | extra_rad_factor (â€“) | Extra radiation multiplier | 0 | 2 |
    | alpha_loss_frac (â€“) | Fraction of alpha power lost | 0 | 0.5 |
    | kappa (â€“) | Elongation | 1.0 | 3.0 |
    | q95_min (â€“) | Minimum q95 constraint | 1.5 | 10 |
    | betaN_max (â€“) | Maximum normalized beta constraint | 1.0 | 8 |
    | C_bs (â€“) | Bootstrap coefficient proxy | 0 | 1 |
    | f_bs_max (â€“) | Max bootstrap fraction | 0 | 1 |
    | PSOL/R max (MW/m) | SOL power per major radius limit | 0 | 200 |
    | PLH_margin (â€“) | Extra margin over PLH if Hâ€‘mode required | 0 | 1 |
                    """
                )
    
            with st.expander("Scan Lab â†’ Physics block mapping (what each parameter affects)", expanded=False):
                st.caption("UI-only helper: shows which Phaseâ€‘1 physics/systems blocks each Scan Lab parameter feeds.")
                rows = []
                # Keep the ordering aligned with the form layout.
                ordered = [
                    ("R0", "Major radius Râ‚€ (m)"),
                    ("B0", "Toroidal field on axis Bâ‚€ (T)"),
                    ("tshield", "Neutron shield thickness (m)"),
                    ("Paux", "Auxiliary heating power P_aux (MW)"),
                    ("Paux_for_Q", "Aux power used in Q definition (MW)"),
                    ("Ti_over_Te", "Ion-to-electron temperature ratio Táµ¢/Tâ‚‘ (â€“)"),
                    ("Ti", "Ti axis (Ti_start/stop/step)"),
                    ("H98", "H98 axis (H98_start/stop/step)"),
                    ("a", "a axis (a_min/a_max/a_step)"),
                    ("Q", "Q axis (Q_start/stop/Q_step)"),
                    ("g_conf", "g_conf axis (start/stop/step)"),
                    ("Ip_bounds", "Iâ‚š bounds (I_p,min / I_p,max)"),
                    ("fG_bounds", "fG bounds (fG_min / fG_max)"),
                    ("tol", "tol"),
                    ("Zeff", "Zeff"),
                    ("dilution_fuel", "dilution_fuel"),
                    ("extra_rad_factor", "extra_rad_factor"),
                    ("alpha_loss_frac", "alpha_loss_frac"),
                    ("kappa", "kappa"),
                    ("q95_min", "q95_min"),
                    ("betaN_max", "betaN_max"),
                    ("C_bs", "C_bs"),
                    ("f_bs_max", "f_bs_max"),
                    ("PSOL_over_R_max", "PSOL/R max"),
                    ("require_Hmode", "Require H-mode access"),
                    ("PLH_margin", "PLH_margin"),
                    ("tblanket_m", "Blanket thickness (inboard)"),
                    ("t_vv_m", "Vacuum vessel thickness (inboard)"),
                    ("t_gap_m", "Inboard gap / clearance"),
                    ("t_tf_struct_m", "TF structure thickness (inboard)"),
                    ("t_tf_wind_m", "TF winding pack thickness (inboard)"),
                    ("Bpeak_factor", "Bpeak_factor"),
                    ("sigma_allow_MPa", "Allowable coil hoop stress"),
                    ("Tcoil_K", "HTS operating temperature"),
                    ("hts_margin_min", "HTS margin min"),
                    ("Vmax_kV", "Max dump voltage limit"),
                    ("q_div_max_MW_m2", "Max divertor heat flux limit"),
                    ("TBR_min", "TBR_min"),
                    ("hts_lifetime_min_yr", "Minimum HTS lifetime"),
                    ("P_net_min_MW", "Minimum net electric power"),
                ]
                for k, label in ordered:
                    blocks = _scan_blocks(k)
                    rows.append(
                        {
                            "Parameter": label,
                            "Badge": _scan_badge(k),
                            "Physics blocks": ", ".join(blocks) if blocks else "(unmapped)",
                        }
                    )
                st.dataframe(pd.DataFrame(rows), use_container_width=True, hide_index=True)
    
            # Teaching Mode (UI-only). Adds gentle guidance without changing results.
            teaching_mode = st.checkbox("Teaching Mode (adds gentle guidance)", value=bool(st.session_state.get("scan_teaching_mode", False)), key="scan_teaching_mode")
            if teaching_mode:
                st.info("Teaching Mode is ON: Scan Lab will add small callouts explaining what youâ€™re seeing. No physics or results change.")
    
    
    
            st.markdown("### ðŸ—ºï¸ Cartography Deck - mode contract (is / is not)")
            cA, cB = st.columns(2)
            with cA:
                st.markdown("""**What this mode does**
    - Maps the frozen Point Designer truth across a chosen parameter plane
    - Reveals dominant constraints, cliffs, intent splits, and robustness categories
    - Exports replayable scan artifacts (schema v1) and a fixed Scan Lab Atlas""")
            with cB:
                st.markdown("""**What this mode does not do**
    - Optimize, relax constraints, or recommend a best design
    - Apply changes to your base point automatically
    - Redefine physics or hide empty regions""")
    
            # How to think with Scan Lab (philosophy)
            with st.expander("How to think with Scan Lab", expanded=False):
                st.markdown(
                    "**Scan Lab is a microscope, not an engine.**\n"
                    "- It maps the frozen Point Designer truth across a space.\n"
                    "- It does not search for 'best' designs.\n"
                    "- If a region is empty, nature (given assumptions) said *no*.\n\n"
                    "Use it to answer: *What limits me? Where are the cliffs? Which direction helps most?*"
                )
    
            # Contract (always visible, collapsible)
            with st.expander("Scan Lab Contract", expanded=False):
                st.markdown(SCAN_LAB_CONTRACT)
                try:
                    from tools.scan_visual_identity import VISUAL_IDENTITY
                    st.caption(f"Visual semantics frozen: **{VISUAL_IDENTITY.version}**")
                except Exception:
                    st.caption("Visual semantics frozen (Scan Lab v1.0)")
    
            # Restore (artifact -> full UI state)
            with st.expander("Restore Scan Artifact (JSON)", expanded=False):
                st.caption("Upload a previously exported Scan Lab artifact. SHAMS will auto-upgrade it to schema v1 and restore the Scan Lab state.")
                up = st.file_uploader("Upload scan artifact", type=["json"], key="scan_restore_upl")
                if up is not None:
                    try:
                        payload = json.loads(up.getvalue().decode("utf-8"))
                    except Exception as e:
                        payload = None
                        st.error(f"Invalid JSON: {e}")
                    if isinstance(payload, dict) and st.button("Restore this artifact", use_container_width=True, key="scan_restore_btn"):
                        try:
                            art = payload
                            if callable(upgrade_scan_artifact):
                                art = upgrade_scan_artifact(payload)
                            rep = art.get("report") if isinstance(art, dict) else None
                            settings = art.get("settings") if isinstance(art, dict) else None
    
                            if not isinstance(rep, dict):
                                raise ValueError("Artifact missing 'report'")
    
                            # Restore report + artifact
                            st.session_state["scan_cartography_report"] = rep
                            st.session_state["scan_cartography_artifact"] = art
    
                            # Restore scan settings (best effort)
                            if isinstance(settings, dict):
                                st.session_state["scan_cart_x_key"] = settings.get("x_key")
                                st.session_state["scan_cart_y_key"] = settings.get("y_key")
                                st.session_state["scan_cart_x_lo"] = settings.get("x_lo")
                                st.session_state["scan_cart_x_hi"] = settings.get("x_hi")
                                st.session_state["scan_cart_y_lo"] = settings.get("y_lo")
                                st.session_state["scan_cart_y_hi"] = settings.get("y_hi")
                                st.session_state["scan_cart_nx"] = settings.get("nx")
                                st.session_state["scan_cart_ny"] = settings.get("ny")
                                st.session_state["scan_cart_intents"] = settings.get("intents")
                                st.session_state["scan_cart_inc_out"] = settings.get("include_outputs")
    
                            st.success("Restored Scan Lab state from artifact.")
                            st.rerun()
                        except Exception as e:
                            st.error(f"Restore failed: {e}")
    
            # Citation-grade provenance / fingerprints
            with st.expander("Provenance (fingerprints)", expanded=False):
                import os
                repo_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
                fps = {}
                try:
                    if callable(compute_fingerprints):
                        fps = compute_fingerprints(repo_root)
                except Exception:
                    fps = {}
                if fps:
                    st.code(f"fingerprint: {fps.get('fingerprint','n/a')}")
                    st.json(fps, expanded=False)
                else:
                    st.info("Fingerprints unavailable.")
    
            # Freeze readiness (determinism + regression)
            with st.expander("Freeze readiness tools", expanded=False):
                st.caption("These checks do not change physics. They validate determinism and export plumbing.")
                if Evaluator is None or not callable(build_cartography_report) or not callable(stable_hash):
                    st.warning("Freeze readiness checks unavailable (imports missing).")
                else:
                    if st.button("Run quick replay determinism audit", use_container_width=True, key="scan_replay_audit_btn"):
                        try:
                            import numpy as _np
                            ev = _dsg_evaluator(origin="UI", cache_enabled=True)
                            # Small deterministic neighborhood around the current base point
                            base0 = st.session_state.get("last_point_inp")
                            if base0 is None:
                                raise ValueError("No baseline point available.")
                            xk = st.session_state.get("scan_cart_x_key", "Ip_MA")
                            yk = st.session_state.get("scan_cart_y_key", "R0_m")
                            bx = float(getattr(base0, xk, 1.0) or 1.0)
                            by = float(getattr(base0, yk, 1.0) or 1.0)
                            xv = list(_np.linspace(0.95 * bx, 1.05 * bx, 11))
                            yv = list(_np.linspace(0.95 * by, 1.05 * by, 9))
                            intents0 = list(st.session_state.get("scan_cart_intents") or ["Reactor"])
                            rep_a = build_cartography_report(evaluator=ev, base_inputs=base0, x_key=str(xk), y_key=str(yk), x_vals=xv, y_vals=yv, intents=intents0, include_outputs=False)
                            rep_b = build_cartography_report(evaluator=ev, base_inputs=base0, x_key=str(xk), y_key=str(yk), x_vals=xv, y_vals=yv, intents=intents0, include_outputs=False)
                            ha = {
                                "report": stable_hash(rep_a),
                                "dominance": stable_hash(rep_a.get("dominance", {})),
                                "intent_stats": stable_hash(rep_a.get("intent_stats", {})),
                            }
                            hb = {
                                "report": stable_hash(rep_b),
                                "dominance": stable_hash(rep_b.get("dominance", {})),
                                "intent_stats": stable_hash(rep_b.get("intent_stats", {})),
                            }
                            if ha == hb:
                                st.success("Replay determinism audit: PASS")
                            else:
                                st.error("Replay determinism audit: FAIL (hash mismatch)")
                            st.json({"runA": ha, "runB": hb}, expanded=False)
                        except Exception as e:
                            st.error(f"Replay audit failed: {e}")
    
                st.caption("For full freeze gating, run: python scripts/run_scanlab_freeze_qa.py")
    
            # Keyboard quick-jump (expert)
            with st.expander("Expert quick-jump (keyboard)", expanded=False):
                st.caption("Type a letter and press Enter: D=Dominance, F=First-failure, I=Intent split, C=Causality trace")
                cmd = st.text_input("Jump", value="", key="scan_cmd_jump").strip().upper()
                if cmd in ["D","F","I","C"]:
                    st.session_state["scan_view_mode"] = cmd
                    st.success(f"View set to {cmd}.")
    
            base = st.session_state.get("last_point_inp")
            if base is None:
                st.info("Run **Point Designer** first (Scan Lab uses the last evaluated point as the baseline).")
                return
    
            # Canonical questions (teaching / onboarding)
            with st.expander("Canonical questions (teaching)", expanded=False):
                st.caption("A small library of physics questions that Scan Lab can answer. These load scan settings or suggest a view - no designs are applied.")
                qs = []
                try:
                    if callable(build_canonical_questions):
                        qs = build_canonical_questions()
                except Exception:
                    qs = []
                if not qs:
                    st.info("Canonical questions unavailable.")
                else:
                    q_labels = [q.get('question') for q in qs]
                    qpick = st.selectbox("Pick a question", options=q_labels, index=0, key="scan_canon_pick")
                    q = qs[q_labels.index(qpick)]
                    st.write({"hint": q.get("hint"), "suggested_golden_label": q.get("suggested_golden_label")})
                    if q.get("suggested_golden_label"):
                        st.info("Tip: load the suggested golden scan below, then run Cartography.")
    
            # Golden scans (institutional memory)
            with st.expander("Golden scans (teaching + QA)", expanded=False):
                st.caption("One-click canonical landscapes. These load scan settings only; they do not apply designs.")
                presets = []
                try:
                    if callable(build_golden_scan_presets):
                        presets = build_golden_scan_presets(base_inputs=base)
                except Exception:
                    presets = []
                if not presets:
                    st.warning("Golden presets unavailable.")
                else:
                    labels = [p["label"] for p in presets]
                    pick = st.selectbox("Preset", options=labels, index=0, key="scan_golden_pick")
                    gp = presets[labels.index(pick)]
                    st.write({"note": gp.get("note"), "intents": gp.get("intents"), "x": gp.get("x_key"), "y": gp.get("y_key")})
                    if st.button("Load this golden scan", use_container_width=True, key="scan_load_golden"):
                        st.session_state["scan_cart_x_key"] = gp.get("x_key")
                        st.session_state["scan_cart_y_key"] = gp.get("y_key")
                        st.session_state["scan_cart_intents"] = gp.get("intents")
                        st.session_state["scan_cart_x_lo"] = float(gp.get("x_range")[0])
                        st.session_state["scan_cart_x_hi"] = float(gp.get("x_range")[1])
                        st.session_state["scan_cart_y_lo"] = float(gp.get("y_range")[0])
                        st.session_state["scan_cart_y_hi"] = float(gp.get("y_range")[1])
                        st.session_state["scan_cart_nx"] = int(gp.get("n_x"))
                        st.session_state["scan_cart_ny"] = int(gp.get("n_y"))
                        st.session_state["scan_cart_base_override"] = asdict(gp.get("base_inputs")) if gp.get("base_inputs") is not None else None
                        st.success("Loaded golden scan settings.")
    
        st.markdown("---")
        st.markdown("### ðŸ—ºï¸ Cartography")
        st.caption("Produces: constraint-dominance maps, first-failure order, intent-split overlays, robustness labels, and a narrative summary.")

        # Variable pickers
        # Keep list tight and meaningful; these must exist on PointInputs.
        vars2d = [
            ("R0_m", "R0 (m)"),
            ("a_m", "a (m)"),
            ("Bt_T", "B0 (T)"),
            ("Ip_MA", "Ip (MA)"),
            ("fG", "fG (-)"),
            ("Paux_MW", "Paux (MW)"),
            ("kappa", "kappa (-)"),
            ("Ti_keV", "Ti (keV)"),
        ]
        key_to_label = {k: v for k, v in vars2d}
        klist = [k for k, _ in vars2d]

        c1, c2, c3 = st.columns([1, 1, 1])
        with c1:
            x_key = st.selectbox("x-axis", options=klist, index=klist.index(st.session_state.get("scan_cart_x_key", "Ip_MA")) if st.session_state.get("scan_cart_x_key", "Ip_MA") in klist else 0, format_func=lambda k: f"{k} - {key_to_label.get(k,k)}", key="scan_cart_x")
        with c2:
            y_key = st.selectbox("y-axis", options=klist, index=klist.index(st.session_state.get("scan_cart_y_key", "R0_m")) if st.session_state.get("scan_cart_y_key", "R0_m") in klist else 1, format_func=lambda k: f"{k} - {key_to_label.get(k,k)}", key="scan_cart_y")
        with c3:
            intents = st.multiselect("Intent lenses", options=["Research", "Reactor"], default=st.session_state.get("scan_cart_intents", ["Reactor"]), key="scan_cart_intents")

            # Always-visible intent badge (clarity; no logic change)
            _sel_intents = intents or []
            st.markdown(f"**Intent badge:** {' / '.join([str(x) for x in _sel_intents]) if _sel_intents else '(none)'}")
            st.caption('Projection note: Scan Lab shows a 2D slice. Off-axis constraints may dominate outside this plane.')

        # Ranges + resolution
        def _g(attr: str, default: float) -> float:
            try:
                return float(getattr(base, attr))
            except Exception:
                return float(default)

        bx = _g(x_key, 1.0)
        by = _g(y_key, 1.0)
        c1, c2, c3, c4 = st.columns(4)
        with c1:
            x_lo = st.number_input("x min", value=float(st.session_state.get("scan_cart_x_lo", 0.7 * bx)), step=0.1, key="scan_cart_x_lo")
        with c2:
            x_hi = st.number_input("x max", value=float(st.session_state.get("scan_cart_x_hi", 1.3 * bx)), step=0.1, key="scan_cart_x_hi")
        with c3:
            y_lo = st.number_input("y min", value=float(st.session_state.get("scan_cart_y_lo", 0.7 * by)), step=0.1, key="scan_cart_y_lo")
        with c4:
            y_hi = st.number_input("y max", value=float(st.session_state.get("scan_cart_y_hi", 1.3 * by)), step=0.1, key="scan_cart_y_hi")

        c1, c2, c3 = st.columns([1, 1, 2])
        with c1:
            nx = int(st.slider("Nx", 11, 61, int(st.session_state.get("scan_cart_nx", 31)), 2, key="scan_cart_nx"))
        with c2:
            ny = int(st.slider("Ny", 11, 61, int(st.session_state.get("scan_cart_ny", 25)), 2, key="scan_cart_ny"))
        with c3:
            include_outputs = st.checkbox("Include compact outputs in report", value=False, key="scan_cart_inc_out")

        # Optional base override (loaded from golden scan)
        base_override = st.session_state.get("scan_cart_base_override")
        if isinstance(base_override, dict) and base_override:
            with st.expander("Baseline override (from golden scan)", expanded=False):
                st.json(base_override)

        # Run
        if st.button("Run cartography scan", type="primary", use_container_width=True, key="scan_cart_run"):
            if Evaluator is None or not callable(build_cartography_report):
                st.error("Scan cartography engine unavailable (import error).")
                errs = st.session_state.get("scan_import_errors") or []
                if errs:
                    st.caption("Import details")
                    st.code("\n".join([str(e) for e in errs])
                            [:2000])
                return
            if float(x_hi) <= float(x_lo) or float(y_hi) <= float(y_lo):
                st.error("Invalid bounds: max must be greater than min for both axes.")
                return
            else:
                try:
                    from dataclasses import replace
                    base2 = base
                    if isinstance(base_override, dict) and base_override:
                        try:
                            base2 = replace(base2, **{k: base_override[k] for k in base_override if k in base2.__dict__})
                        except Exception:
                            pass
                    ev = _dsg_evaluator(origin="UI", cache_enabled=True)
                    x_vals = list(np.linspace(float(x_lo), float(x_hi), int(nx)))
                    y_vals = list(np.linspace(float(y_lo), float(y_hi), int(ny)))
                    import time
                    _t0 = time.time()
                    total_pts = max(int(nx) * int(ny), 1)
                    prog = st.progress(0.0, text=f"Stage 1/2 - Evaluating {total_pts} points (frozen evaluator)â€¦")

                    def _cb(done, total):
                        try:
                            dt = max(time.time() - _t0, 1e-6)
                            rate = float(done) / dt
                            eta = (float(total) - float(done)) / max(rate, 1e-6)
                            frac = float(done) / max(float(total), 1.0)
                            prog.progress(min(1.0, max(0.0, frac)), text=f"{done}/{total} pts - {rate:.1f} pt/s - ETA {eta:.0f}s")
                        except Exception:
                            pass

                    rep = build_cartography_report(
                        evaluator=ev,
                        base_inputs=base2,
                        x_key=str(x_key),
                        y_key=str(y_key),
                        x_vals=x_vals,
                        y_vals=y_vals,
                        intents=list(intents or ["Reactor"]),
                        include_outputs=bool(include_outputs),
                        progress_cb=_cb,
                    )
                    try:
                        prog.progress(1.0, text='Stage 2/2 - Computing narrative/topology/artifactâ€¦')
                    except Exception:
                        pass
                    rep['run_seconds'] = float(time.time() - _t0)
                    # Attach common metadata and record
                    rep = _attach_common_metadata(rep)
                    st.session_state["scan_cartography_report"] = rep

                    # Freeze-grade Scan Artifact (schema v1)
                    settings = {
                        "x_key": str(x_key),
                        "y_key": str(y_key),
                        "x_lo": float(x_lo),
                        "x_hi": float(x_hi),
                        "y_lo": float(y_lo),
                        "y_hi": float(y_hi),
                        "nx": int(nx),
                        "ny": int(ny),
                        "intents": list(intents or ["Reactor"]),
                        "include_outputs": bool(include_outputs),
                    }
                    artifact = None
                    if callable(build_scan_artifact):
                        try:
                            artifact = build_scan_artifact(
                                report=rep,
                                settings=settings,
                                metadata=dict(rep.get("metadata") or {}),
                                reason_code="run_ok",
                                freeze_statement=f"Scan Lab frozen (schema v{int(SCAN_SCHEMA_VERSION or 1)})",
                            )
                        except Exception:
                            artifact = None
                    if isinstance(artifact, dict):
                        st.session_state["scan_cartography_artifact"] = artifact
                        try:
                            _v98_record_run("scan_cartography", artifact, mode="scan_lab")
                        except Exception:
                            pass
                    else:
                        try:
                            _v98_record_run("scan_cartography", rep, mode="scan_lab")
                        except Exception:
                            pass
                    st.success(f"Scan complete: {rep.get('n_points')} points")
                except Exception as e:
                    st.error(f"Scan failed: {e}")

        rep = st.session_state.get("scan_cartography_report")
        if not isinstance(rep, dict):
            st.info("No cartography scan results yet.")
            return

        # (One-glance truth + intentional emptiness messaging are rendered below.)

        # One-glance truth strip (radical clarity)
        nar_all = rep.get("narrative") or {}
        nar_int = (nar_all.get("intents") or {}) if isinstance(nar_all, dict) else {}
        intents_strip = rep.get("intents") or []
        if not intents_strip:
            intents_strip = ["Reactor"]

        # derive summary for primary intent (first in list)
        it_primary = str(intents_strip[0])
        n0 = nar_int.get(it_primary, {}) if isinstance(nar_int, dict) else {}
        feasible0 = float(n0.get("blocking_feasible_rate", 0.0)) if isinstance(n0, dict) else 0.0
        top0 = (n0.get("dominance_ranked") or []) if isinstance(n0, dict) else []
        dom0 = (top0[0].get("constraint") if top0 else None) or "(none)"
        cliff0 = float(n0.get("cliffiness_proxy", 0.0)) if isinstance(n0, dict) else 0.0

        # robustness verdict (simple, honest): use feasible fraction bands
        if feasible0 >= 0.85:
            rb0 = "Robust"
        elif feasible0 >= 0.55:
            rb0 = "Balanced"
        elif feasible0 >= 0.25:
            rb0 = "Brittle"
        else:
            rb0 = "Knife-edge"

        st.markdown("### Oneâ€‘glance truth")
        a, b, c, d = st.columns(4)
        a.metric("Dominant constraint", str(dom0))
        b.metric(f"Feasible fraction ({it_primary})", f"{feasible0*100:.0f}%")
        c.metric("Robustness verdict", rb0)
        d.metric("Cliffiness proxy", f"{cliff0:.2f}")

        # The final test: can a user learn something fundamental from one scan?
        with st.expander("One-scan benchmark", expanded=False):
            st.caption("A lightweight self-check for freeze readiness. This is optional and does not affect results.")
            st.checkbox("After one scan, I learned something fundamental about what limits this design space.", key="scan_benchmark_learned")
            st.text_area("If yes: what was it? (optional)", value="", height=90, key="scan_benchmark_note")

        # Make absence intentional (no-feasible region)
        try:
            all_zero = True
            worst = {}
            for it in intents_strip:
                nn = nar_int.get(str(it), {}) if isinstance(nar_int, dict) else {}
                ff = float(nn.get("blocking_feasible_rate", 0.0)) if isinstance(nn, dict) else 0.0
                if ff > 0.0:
                    all_zero = False
                rk = (nn.get("dominance_ranked") or []) if isinstance(nn, dict) else []
                if rk:
                    worst[str(it)] = rk[0].get("constraint")
            if all_zero:
                st.warning(
                    "No blocking-feasible region exists in this Xâ€“Y space (under the selected assumptions)."
                )
                st.markdown(
                    "**Why this is empty (most likely):**\n"
                    + "\n".join([f"- Under **{k}** intent, **{v}** limits essentially everywhere." for k, v in worst.items()])
                )
                st.caption("Try widening bounds, changing axes, or switching intent to test whether this is a structural limit or a policy lens.")
        except Exception:
            pass

        
        
        # --- Post-run Cartography Workbench (v196.3) ---
        # Goal: eliminate scroll-fatigue by treating the map as the center of gravity.
        # This is UI-only; it does not change evaluator truth or scan semantics.

        intents2 = rep.get("intents") or []
        x_vals = rep.get("x_vals") or []
        y_vals = rep.get("y_vals") or []
        pts = rep.get("points") or []
        try:
            grid = {(int(p["i"]), int(p["j"])): p for p in pts if isinstance(p, dict) and "i" in p and "j" in p}
        except Exception:
            grid = {}

        if not intents2:
            intents2 = ["Reactor"]

        # --- Sticky-ish truth bar (best-effort in Streamlit) ---
        try:
            st.markdown(
                """
<style>
/* Make the next container behave like a lightweight "truth bar" */
div[data-testid="stVerticalBlockBorderWrapper"].shams_truthbar { position: sticky; top: 0.5rem; z-index: 50; background: white; }
</style>
                """,
                unsafe_allow_html=True,
            )
        except Exception:
            pass

        # We keep your Oneâ€‘glance truth metrics above; now we add a compact context line.
        st.caption(f"Postâ€‘run workspace: **{rep.get('x_key')}** vs **{rep.get('y_key')}** Â· intents: **{' / '.join([str(i) for i in intents2])}** Â· n={int(rep.get('n_points') or 0)}")

        st.markdown("### ðŸ—ºï¸ Cartography workbench")
        st.caption("Orient â†’ look â†’ probe â†’ explain â†’ compare. (Cartography/interpretability only; no optimization.)")

        # ---------- helpers ----------
        def _cell(intent: str, i: int, j: int) -> dict:
            c = grid.get((int(i), int(j)), {}) if isinstance(grid, dict) else {}
            return ((c.get("intent") or {}).get(str(intent)) or {}) if isinstance(c, dict) else {}

        def _dominance_labels(intent: str):
            labels = set()
            for j in range(len(y_vals)):
                for i in range(len(x_vals)):
                    s = _cell(intent, i, j)
                    if bool(s.get("blocking_feasible")):
                        labels.add("PASS")
                    else:
                        labels.add(str(s.get("dominant_blocking") or "FAIL (unknown)"))
            lab = sorted(list(labels))
            if "PASS" in lab:
                lab = ["PASS"] + [x for x in lab if x != "PASS"]
            return lab

        def _render_dominance_map(intent: str):
            dom = np.empty((len(y_vals), len(x_vals)), dtype=object)
            for j in range(len(y_vals)):
                for i in range(len(x_vals)):
                    s = _cell(intent, i, j)
                    if bool(s.get("blocking_feasible")):
                        dom[j, i] = "PASS"
                    else:
                        dom[j, i] = s.get("dominant_blocking") or "FAIL (unknown)"
            labels = _dominance_labels(intent)
            lut = {lab: k for k, lab in enumerate(labels)}
            z = np.vectorize(lambda s: lut.get(str(s), 0))(dom)

            fig, ax = plt.subplots(figsize=(7.6, 4.4))
            try:
                from tools.scan_visual_identity import build_palette
                palette = build_palette(labels)
            except Exception:
                palette = ['#E0E0E0', '#4C78A8', '#F58518', '#54A24B', '#E45756', '#72B7B2', '#B279A2', '#FF9DA6', '#9D755D', '#BAB0AC', '#2F4B7C', '#7A5195', '#EF5675', '#FFA600']
            if labels and labels[0] == 'PASS':
                palette[0] = '#E0E0E0'
            cmap = ListedColormap(palette[:max(len(labels), 1)])
            ax.imshow(z, origin='lower', aspect='auto', cmap=cmap, vmin=-0.5, vmax=len(labels)-0.5)
            ax.set_xlabel(f"{rep.get('x_key')} - {str(rep.get('x_label') or '')}")
            ax.set_ylabel(f"{rep.get('y_key')} - {str(rep.get('y_label') or '')}")
            ax.set_title(f"Dominant blocking constraint - intent: {intent}")
            return fig, labels

        def _render_feasible_map(intent: str):
            z = np.zeros((len(y_vals), len(x_vals)), dtype=float)
            for j in range(len(y_vals)):
                for i in range(len(x_vals)):
                    s = _cell(intent, i, j)
                    z[j, i] = 1.0 if bool(s.get("blocking_feasible")) else 0.0
            fig, ax = plt.subplots(figsize=(7.6, 4.4))
            ax.imshow(z, origin='lower', aspect='auto')
            ax.set_xlabel(f"{rep.get('x_key')}")
            ax.set_ylabel(f"{rep.get('y_key')}")
            ax.set_title(f"Blocking feasibility (1=feasible, 0=infeasible) - intent: {intent}")
            return fig

        def _render_robustness_proxy(intent: str):
            z = np.full((len(y_vals), len(x_vals)), float("nan"))
            for j in range(len(y_vals)):
                for i in range(len(x_vals)):
                    s = _cell(intent, i, j)
                    try:
                        z[j, i] = float(s.get("local_p_feasible"))
                    except Exception:
                        z[j, i] = float("nan")
            fig, ax = plt.subplots(figsize=(7.6, 4.4))
            ax.imshow(z, origin='lower', aspect='auto')
            ax.set_xlabel(f"{rep.get('x_key')}")
            ax.set_ylabel(f"{rep.get('y_key')}")
            ax.set_title(f"Robustness proxy (local p-feasible) - intent: {intent}")
            return fig

        def _render_operating_contours(intent: str, field: str):
            fc = rep.get("field_cube") if isinstance(rep, dict) else None
            arr = None
            try:
                arr = (fc.get("vars") or {}).get(str(field)) if isinstance(fc, dict) else None
            except Exception:
                arr = None
            if not isinstance(arr, list):
                # fallback: build from per-point outputs (sparse)
                z = np.full((len(y_vals), len(x_vals)), float("nan"))
                for j in range(len(y_vals)):
                    for i in range(len(x_vals)):
                        cell = grid.get((i, j), {}) if isinstance(grid, dict) else {}
                        outs = cell.get("outputs") or {}
                        if isinstance(outs, dict) and field in outs:
                            try:
                                z[j, i] = float(outs.get(field))
                            except Exception:
                                z[j, i] = float("nan")
            else:
                z = np.array(arr, dtype=float)
            fig, ax = plt.subplots(figsize=(7.6, 4.4))
            # Filled contours + feasibility overlay
            try:
                ax.contourf(z, levels=12, origin="lower")
            except Exception:
                ax.imshow(z, origin="lower", aspect="auto")
            ax.set_xlabel(f"{rep.get('x_key')}")
            ax.set_ylabel(f"{rep.get('y_key')}")
            ax.set_title(f"Operating contours: {field} - intent context: {intent}")
            return fig

        # ---------- layout ----------
        nav, mapcol, insp = st.columns([1.05, 2.2, 1.15], gap="large")

        with nav:
            st.markdown("**Navigate**")
            it_active = st.selectbox("Primary intent (view)", options=intents2, index=0, key="scan_wb_intent_active")

            view = st.radio(
                "View",
                options=[
                    "Dominance (blocking)",
                    "Feasibility (blocking)",
                    "Robustness (proxy)",
                    "Operating contours (outputs)",
                ],
                index=0,
                key="scan_wb_view",
            )

            
            out_key = None
            if str(view).startswith("Operating"):
                # Requires include_outputs=True when running the scan
                fc = rep.get("field_cube") if isinstance(rep, dict) else None
                keys = []
                try:
                    keys = sorted(list((fc.get("vars") or {}).keys())) if isinstance(fc, dict) else []
                except Exception:
                    keys = []
                if not keys:
                    st.warning("No output fields found. Re-run the scan with **Include outputs** enabled.")
                else:
                    out_key = st.selectbox("Contour field", options=keys, index=0, key="scan_wb_out_key")
            compare = False
            if len(intents2) >= 2:
                compare = st.checkbox("Compare intents (side-by-side)", value=False, key="scan_wb_compare")

            with st.expander("Legend / meaning", expanded=False):
                st.caption("PASS means blocking-feasible at that cell. Failures are colored by the dominant blocking constraint (worst margin).")
                try:
                    labs = _dominance_labels(it_active)
                    st.write(labs)
                    if labs == ["PASS"]:
                        st.info("All cells are feasible in this slice â†’ dominance map is intentionally neutral/gray.")
                except Exception:
                    pass

            with st.expander("Trust & export", expanded=False):
                st.write({
                    "n_points": rep.get("n_points"),
                    "run_seconds": rep.get("run_seconds"),
                    "report_id": rep.get("id"),
                    "version": rep.get("shams_version"),
                })
                # Downloads (report + artifact) if present
                rep_dl = _shams_json_dumps(rep, indent=2).encode("utf-8")
                st.download_button("Download cartography report (JSON)", data=rep_dl, file_name="shams_cartography_report.json", mime="application/json", use_container_width=True, key="scan_wb_dl_rep")
                art = st.session_state.get("scan_cartography_artifact")
                if isinstance(art, dict):
                    art_dl = _shams_json_dumps(art, indent=2).encode("utf-8")
                    st.download_button("Download scan artifact (JSON, schema v1)", data=art_dl, file_name="shams_scan_artifact_v1.json", mime="application/json", use_container_width=True, key="scan_wb_dl_art")

                    # Optional: boundary segments + field-cube exports (Scan Lab v218 additions)
                    try:
                        bnd = rep.get("boundaries") if isinstance(rep, dict) else None
                        if isinstance(bnd, dict) and bnd:
                            bnd_dl = _shams_json_dumps(bnd, indent=2).encode("utf-8")
                            st.download_button(
                                "Download boundaries (segments JSON)",
                                data=bnd_dl,
                                file_name="shams_scan_boundaries_segments.json",
                                mime="application/json",
                                use_container_width=True,
                                key="scan_wb_dl_boundaries",
                            )
                        fc = rep.get("field_cube") if isinstance(rep, dict) else None
                        if isinstance(fc, dict) and fc:
                            fc_dl = _shams_json_dumps(fc, indent=2).encode("utf-8")
                            st.download_button(
                                "Download field-cube (labelled arrays JSON)",
                                data=fc_dl,
                                file_name="shams_scan_field_cube_v1.json",
                                mime="application/json",
                                use_container_width=True,
                                key="scan_wb_dl_field_cube",
                            )
                    except Exception:
                        pass

        with mapcol:
            if compare and len(intents2) >= 2:
                a, b = st.columns(2)
                for col, it in [(a, intents2[0]), (b, intents2[1])]:
                    with col:
                        if view.startswith("Dominance"):
                            fig, _labs = _render_dominance_map(str(it))
                            st.pyplot(fig, use_container_width=True)
                        elif view.startswith("Feasibility"):
                            fig = _render_feasible_map(str(it))
                            st.pyplot(fig, use_container_width=True)
                        else:
                            if view.startswith("Operating") and out_key:
                                fig = _render_operating_contours(str(it), str(out_key))
                            else:
                                fig = _render_robustness_proxy(str(it))
                            st.pyplot(fig, use_container_width=True)
            else:
                if view.startswith("Dominance"):
                    fig, _labs = _render_dominance_map(str(it_active))
                    st.pyplot(fig, use_container_width=True)
                elif view.startswith("Feasibility"):
                    fig = _render_feasible_map(str(it_active))
                    st.pyplot(fig, use_container_width=True)
                else:
                    if view.startswith("Operating") and out_key:
                        fig = _render_operating_contours(str(it_active), str(out_key))
                    else:
                        fig = _render_robustness_proxy(str(it_active))
                    st.pyplot(fig, use_container_width=True)

            st.caption("Tip: use the Inspector on the right to probe a cell and see the full constraint stack (descriptive only).")

        with insp:
            st.markdown("**Probe / Inspector**")

            if len(x_vals) == 0 or len(y_vals) == 0:
                st.warning("No grid values found in report.")
            else:
                # Probe controls (index-based, reliable across render backends)
                ii = int(st.slider("i (x index)", 0, max(0, len(x_vals) - 1), int(st.session_state.get("scan_wb_i", 0)), 1, key="scan_wb_i"))
                jj = int(st.slider("j (y index)", 0, max(0, len(y_vals) - 1), int(st.session_state.get("scan_wb_j", 0)), 1, key="scan_wb_j"))

                cell0 = grid.get((ii, jj), {}) if isinstance(grid, dict) else {}
                if not cell0:
                    st.warning("Selected cell not found in grid.")
                else:
                    st.write({"x": cell0.get("x"), "y": cell0.get("y"), "i": ii, "j": jj})

                    def _show_intent_block(it: str):
                        s = ((cell0.get("intent") or {}).get(str(it)) or {})
                        st.markdown(f"**Intent:** {it}")
                        st.write({
                            "blocking_feasible": bool(s.get("blocking_feasible")),
                            "dominant_blocking": s.get("dominant_blocking"),
                            "min_blocking_margin": s.get("min_blocking_margin"),
                            "robustness": s.get("robustness"),
                            "local_p_feasible (proxy)": s.get("local_p_feasible"),
                        })
                        fb = s.get("failed_blocking") or []
                        if fb:
                            st.caption("Failed blocking constraints")
                            st.write(list(fb)[:15])
                        mh = (cell0.get("margins_hard") or {}) if isinstance(cell0, dict) else {}
                        if isinstance(mh, dict) and mh:
                            rows = [{"constraint": k, "margin_frac": float(v)} for k, v in mh.items()]
                            rows.sort(key=lambda r: r["margin_frac"])
                            st.caption("Hard-constraint margins (fractional, worst first)")
                            try:
                                import pandas as _pd
                                st.dataframe(_pd.DataFrame(rows), use_container_width=True, height=210)
                            except Exception:
                                st.json(rows[:25], expanded=False)

                    if compare and len(intents2) >= 2:
                        itA, itB = intents2[0], intents2[1]
                        _show_intent_block(str(itA))
                        st.markdown("---")
                        _show_intent_block(str(itB))
                    else:
                        _show_intent_block(str(it_active))

                    # Canonical promotion hook: probe-cell -> Point Designer
                    with st.expander("Promote this probed cell to ðŸ§­ Point Designer", expanded=False):
                        st.caption("Reconstructs a PointInputs candidate from the scan base_inputs + the probed x/y cell and promotes it into Point Designer.")
                        try:
                            base_inputs_dict = rep.get("base_inputs") if isinstance(rep, dict) else None
                            if not isinstance(base_inputs_dict, dict):
                                base_inputs_dict = {}
                            cand = dict(base_inputs_dict)
                            cand[str(rep.get('x_key'))] = float(cell0.get('x'))
                            cand[str(rep.get('y_key'))] = float(cell0.get('y'))
                            st.write({"x_key": str(rep.get('x_key')), "y_key": str(rep.get('y_key')), "x": cand.get(str(rep.get('x_key'))), "y": cand.get(str(rep.get('y_key')))} )
                            if st.button("Promote to Point Designer", use_container_width=True, key="scan_wb_promote_pd"):
                                stage_pd_candidate_apply(cand, source="ðŸ—ºï¸ Scan Lab / Workbench Probe", note="Probed scan cell")
                                st.success("Promoted probed cell to Point Designer. Switch to ðŸ§­ Point Designer to review/evaluate.")
                        except Exception as _e:
                            st.info(f"Promotion unavailable: {_e}")

                    fo = cell0.get("failure_order_any") or []
                    if fo:
                        with st.expander("Failure order (hard, worst-first)", expanded=False):
                            st.write(list(fo))

        st.markdown("---")
        with st.expander("Advanced / deep dives (optional)", expanded=False):
            st.caption("Everything below is optional. The workbench above is the primary post-run experience.")
            show_deep = st.checkbox("Show deep dives", value=False, key="scan_show_deep")
        if not bool(show_deep):
            return

# Expert argument tools (claim builder + falsification)
        with st.expander("Expert argument tools (Claim Builder + Falsification)", expanded=False):
            st.caption("Turn scan results into audit-grade, evidence-backed claims. Includes a falsification lens.")

            intents2 = rep.get('intents') or []
            it0 = st.selectbox("Intent", options=intents2 if intents2 else ["Reactor"], index=0, key="scan_claim_intent")
            claim_type = st.selectbox("Claim type", options=["Dominance", "Robustness"], index=0, key="scan_claim_type")

            # Suggested expected value from narrative
            nar = ((rep.get("narrative") or {}).get("intents") or {}).get(it0, {})
            expected_default = ""
            if claim_type == "Dominance":
                expected_default = str((nar.get("dominance_ranked") or [{}])[0].get("constraint") or nar.get("dominant") or "")
            if claim_type == "Robustness":
                expected_default = "Balanced"

            expected = st.text_input("Expected (for falsification)", value=expected_default, key="scan_claim_expected")
            title = st.text_input("Claim title", value=f"Scan claim - {claim_type}", key="scan_claim_title")
            statement = st.text_area(
                "Claim statement",
                value=(
                    f"Under intent {it0}, the scan is dominated by {expected_default or '[constraint]'} across most of the Xâ€“Y space."
                    if claim_type == "Dominance" else
                    f"Under intent {it0}, this landscape is {expected_default} in the local neighborhood sense."
                ),
                height=120,
                key="scan_claim_statement",
            )
            notes = st.text_input("Notes (optional)", value="", key="scan_claim_notes")

            # Build evidence and show a stability badge (assumption-sensitivity)
            ev_blob = {}
            try:
                if callable(build_claim_evidence):
                    ev_blob = build_claim_evidence(rep, str(it0))
            except Exception:
                ev_blob = {}

            # Conclusion stability badge (heuristic): if feasible fraction is low or many components -> sensitive
            stability = "Stable"
            try:
                ff = float((ev_blob.get("stats") or {}).get("blocking_feasible_fraction"))
                comps = float((ev_blob.get("cliffs") or {}).get("n_components") or 1)
                if ff < 0.25 or comps >= 3:
                    stability = "Assumptionâ€‘sensitive"
                elif ff < 0.5:
                    stability = "Conditionally stable"
            except Exception:
                stability = "Conditionally stable"
            st.info(f"Conclusion stability: **{stability}**")

            # Why nature forces this (synthesis)
            try:
                dom_top = ((ev_blob.get('stats') or {}).get('dominance_top') or [])
                dom_name = dom_top[0][0] if dom_top else (nar.get('dominant') or 'PASS')
                why = (
                    f"Nature forces this landscape to cluster along the **{dom_name}** boundary because it is the dominant blocking limiter across the scan. "
                    f"Where dominance flips, you are crossing a regime boundary: the leverage of {rep.get('x_key')} vs {rep.get('y_key')} changes sign in terms of minimum margin."
                )
                st.caption("Why nature forces this")
                st.write(why)
            except Exception:
                pass

            # Falsification (counterexamples)
            if st.button("Try to falsify this claim", use_container_width=True, key="scan_claim_falsify"):
                try:
                    ct = "dominance" if claim_type == "Dominance" else "robustness"
                    fx = falsify_claim(rep, intent=str(it0), claim_type=ct, expected=str(expected)) if callable(falsify_claim) else {"ok": False, "reason": "falsify_unavailable"}
                    st.session_state["scan_claim_falsify_last"] = fx
                except Exception as e:
                    st.session_state["scan_claim_falsify_last"] = {"ok": False, "reason": str(e)}

            fx = st.session_state.get("scan_claim_falsify_last")
            if isinstance(fx, dict) and fx:
                st.write({"counterexamples": fx.get("n_counterexamples"), "note": fx.get("note")})
                ex = fx.get("examples") or []
                if ex:
                    try:
                        import pandas as pd
                        st.dataframe(pd.DataFrame(ex), use_container_width=True)
                    except Exception:
                        st.json(ex[:10], expanded=False)

            # Export claim as a 1-page PDF slide
            if st.button("Export Claim (1-page PDF)", use_container_width=True, key="scan_claim_export"):
                try:
                    cl = ScanClaim(title=str(title), statement=str(statement), intent=str(it0), claim_type=str(claim_type), notes=str(notes))
                    mp = st.session_state.get("scan_cart_map_pngs") or {}
                    map_png = mp.get(str(it0))
                    import os
                    repo_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
                    fps = compute_fingerprints(repo_root) if callable(compute_fingerprints) else {}
                    pdfb = build_claim_pdf_bytes(claim=cl, evidence=ev_blob, map_png=bytes(map_png) if isinstance(map_png, (bytes, bytearray)) else None, fingerprint=fps) if callable(build_claim_pdf_bytes) else b""
                    st.session_state["scan_claim_pdf_bytes"] = pdfb
                    st.session_state["scan_claim_last"] = {"title": str(title), "statement": str(statement), "intent": str(it0), "type": str(claim_type)}
                except Exception as e:
                    st.error(f"Claim export failed: {e}")

            pdfb = st.session_state.get("scan_claim_pdf_bytes")
            if isinstance(pdfb, (bytes, bytearray)) and len(pdfb) > 100:
                st.download_button("Download claim PDF", data=pdfb, file_name="shams_scan_claim.pdf", mime="application/pdf", use_container_width=True, key="scan_claim_dl")

        # Curated scan library (institutional memory)
        with st.expander("Curated scan library (local)", expanded=False):
            st.caption("Save notable scans locally to build a personal reference library.")
            tag = st.text_input("Tag", value="interesting", key="scan_lib_tag")
            note = st.text_area("Why this scan mattered (one paragraph)", value="", height=90, key="scan_lib_note")
            if st.button("Save scan + note", use_container_width=True, key="scan_lib_save"):
                try:
                    import os, json
                    repo_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
                    lib_path = os.path.join(repo_root, "docs", "scan_library.json")
                    lib = []
                    if os.path.exists(lib_path):
                        lib = json.loads(open(lib_path, "r", encoding="utf-8").read() or "[]")
                    lib.append({
                        "id": rep.get("id"),
                        "tag": str(tag),
                        "note": str(note).strip(),
                        "x": rep.get("x_key"),
                        "y": rep.get("y_key"),
                        "intents": rep.get("intents"),
                        "shams_version": rep.get("shams_version"),
                        "fingerprint": (rep.get("metadata") or {}).get("fingerprints", {}).get("fingerprint") if isinstance(rep.get("metadata"), dict) else None,
                    })
                    with open(lib_path, "w", encoding="utf-8") as f:
                        f.write(_shams_json_dumps(lib, indent=2))
                    st.success(f"Saved to {lib_path}")
                except Exception as e:
                    st.error(f"Save failed: {e}")

        # Next-tier 0-D insight suite (v191)
        with st.expander("Nextâ€‘tier insights (0â€‘D, no optimization)", expanded=False):
            st.caption("These tools turn scans into understanding (laws, regimes, impossibility). They never modify Point Designer truth.")

            insight = st.selectbox(
                "Pick an insight",
                options=[
                    "Local scaling law near a cell",
                    "Regime label at a cell",
                    "Explain impossible / infeasible region",
                    "Constraint irrelevance (never active)",
                    "Assumption stress hotspots (near-threshold)",
                    "Counterfactual lens (drop one constraint)",
                    "Projection stability (vary 3rd variable)",
                    "Path-follow scan (hold a target output)",
                    "Surprise detector (high-entropy neighborhoods)",
                    "Guided insight mode (10â€‘minute walkthrough)",
                    "Export reference atlas (PDF)",
                    "Export SHAMS Signature Atlas (10 pages)",
                ],
                index=0,
                key="scan_next_tier_pick",
            )

            # point picker shared across views
            try:
                ii = int(st.number_input("Cell i", min_value=0, max_value=max(0, len(x_vals)-1), value=0, step=1, key="scan_nt_i"))
                jj = int(st.number_input("Cell j", min_value=0, max_value=max(0, len(y_vals)-1), value=0, step=1, key="scan_nt_j"))
            except Exception:
                ii, jj = 0, 0

            picked = grid.get((ii, jj), {})

            if insight == "Local scaling law near a cell":
                if not callable(local_powerlaw_fit):
                    st.warning("Local scaling fit unavailable.")
                else:
                    intent_fit = st.selectbox("Intent lens", options=intents2, index=0, key="scan_nt_scal_int")
                    yvar = st.selectbox(
                        "Quantity to fit",
                        options=["min_blocking_margin", "local_p_feasible"] + ["q_div_MW_m2", "sigma_vm_MPa", "B_peak_T", "q95", "TBR", "P_fus_MW"],
                        index=0,
                        key="scan_nt_scal_yvar",
                    )
                    out = local_powerlaw_fit(report=rep, intent=intent_fit, i0=ii, j0=jj, yvar=yvar, radius=2)
                    if out.get("ok"):
                        st.write(out)
                    else:
                        st.warning(out.get("reason"))

            elif insight == "Regime label at a cell":
                if not callable(label_regime):
                    st.warning("Regime labeling unavailable.")
                else:
                    it0 = st.selectbox("Intent lens", options=intents2, index=0, key="scan_nt_reg_int")
                    st.write(label_regime(report=rep, intent=it0, i0=ii, j0=jj))

            elif insight == "Explain impossible / infeasible region":
                if not callable(explain_impossible_region):
                    st.warning("Region explanation unavailable.")
                else:
                    it0 = st.selectbox("Intent lens", options=intents2, index=0, key="scan_nt_imp_int")
                    st.write(explain_impossible_region(report=rep, intent=it0))

            elif insight == "Constraint irrelevance (never active)":
                if not callable(detect_irrelevant_constraints):
                    st.warning("Irrelevance detection unavailable.")
                else:
                    st.write(detect_irrelevant_constraints(report=rep))

            elif insight == "Assumption stress hotspots (near-threshold)":
                if not callable(assumption_stress_hotspots):
                    st.warning("Hotspot detection unavailable.")
                else:
                    it0 = st.selectbox("Intent lens", options=intents2, index=0, key="scan_nt_hot_int")
                    st.write(assumption_stress_hotspots(report=rep, intent=it0, topk=20))

            elif insight == "Counterfactual lens (drop one constraint)":
                if not callable(counterfactual_lens):
                    st.warning("Counterfactual lens unavailable.")
                else:
                    drop = st.text_input("Constraint name to drop (visualization only)", value="TBR", key="scan_nt_drop")
                    it0 = st.selectbox("Intent lens", options=intents2, index=0, key="scan_nt_cf_int")
                    cf = counterfactual_lens(report=rep, intent=it0, drop_constraint=drop)
                    st.write({"removed_constraint": cf.get("removed_constraint"), "ok": cf.get("ok"), "note": cf.get("note")})
                    # basic map: feasible grid under counterfactual
                    try:
                        cf_ok = np.array(cf.get("grid"))
                        fig_cf, ax_cf = plt.subplots(figsize=(7.6, 4.0))
                        ax_cf.imshow(cf_ok.astype(float), origin='lower', aspect='auto')
                        ax_cf.set_title(f"Counterfactual feasible map (drop={drop})")
                        ax_cf.set_xlabel(f"{x_key}")
                        ax_cf.set_ylabel(f"{y_key}")
                        st.pyplot(fig_cf, use_container_width=True)
                    except Exception:
                        pass

            elif insight == "Projection stability (vary 3rd variable)":
                if not callable(projection_stability_check) or Evaluator is None:
                    st.warning("Projection stability check unavailable.")
                else:
                    z_key = st.selectbox("3rd variable (z)", options=klist, index=0, key="scan_nt_z")
                    it0 = st.selectbox("Intent lens", options=intents2, index=0, key="scan_nt_proj_int")
                    rel = float(st.slider("z variation Â±%", 1, 20, 5, 1, key="scan_nt_zrel")) / 100.0
                    ev_local = _dsg_evaluator(origin="UI", cache_enabled=True)
                    st.write(projection_stability_check(evaluator=ev_local, base_inputs=base, report=rep, intent=it0, i0=ii, j0=jj, z_key=z_key, rel_step=rel))

            elif insight == "Path-follow scan (hold a target output)":
                if not callable(path_follow_scan) or Evaluator is None:
                    st.warning("Path-follow scan unavailable.")
                else:
                    it0 = st.selectbox("Intent lens", options=intents2, index=0, key="scan_nt_path_int")
                    target_key = st.selectbox("Target output to hold", options=["q95", "B_peak_T", "q_div_MW_m2", "P_fus_MW"], index=0, key="scan_nt_tgt")
                    st.caption("This follows a trajectory by adjusting y to hold the target output approximately constant as x varies.")
                    if st.button("Run path-follow", use_container_width=True, key="scan_nt_run_path"):
                        ev_local = _dsg_evaluator(origin="UI", cache_enabled=True)
                        out = path_follow_scan(evaluator=ev_local, base_inputs=base, x_key=x_key, y_key=y_key, x_vals=list(x_vals), target_output=target_key)
                        st.session_state["scan_path_follow_last"] = out
                    out = st.session_state.get("scan_path_follow_last")
                    if isinstance(out, dict):
                        st.write(out.get("summary"))
                        try:
                            dfp = pd.DataFrame(out.get("path") or [])
                            st.dataframe(dfp, use_container_width=True)
                        except Exception:
                            pass

            elif insight == "Surprise detector (high-entropy neighborhoods)":
                if not callable(surprise_detector):
                    st.warning("Surprise detector unavailable.")
                else:
                    it0 = st.selectbox("Intent lens", options=intents2, index=0, key="scan_nt_sur_int")
                    rad = int(st.slider("Neighborhood radius", 1, 3, 1, 1, key="scan_nt_sur_rad"))
                    st.write(surprise_detector(report=rep, intent=it0, radius=rad))

            elif insight == "Guided insight mode (10â€‘minute walkthrough)":
                if not callable(guided_steps):
                    st.warning("Guided mode unavailable.")
                else:
                    st.markdown("**Walkthrough steps**")
                    for s in guided_steps():
                        st.write(f"{s.get('step')}. {s.get('title')} - {s.get('hint')}")
                    st.caption("Tip: start with a Golden scan, then follow steps 1â†’5.")

            elif insight == "Export reference atlas (PDF)":
                if not callable(build_scan_atlas_pdf_bytes):
                    st.warning("Atlas export unavailable.")
                else:
                    st.caption("Build a multi-page PDF atlas from the current scan (one page per intent).")
                    title = st.text_input("Atlas title", value="SHAMS - Scan Lab Atlas", key="scan_nt_atlas_title")
                    if st.button("Build atlas PDF", use_container_width=True, key="scan_nt_build_atlas"):
                        map_pngs = st.session_state.get("scan_cart_map_pngs") or {}
                        pages = []
                        for it0 in intents2:
                            png = map_pngs.get(str(it0))
                            if isinstance(png, (bytes, bytearray)):
                                pages.append({
                                    "report": rep,
                                    "intent": str(it0),
                                    "map_png": bytes(png),
                                    "page_title": f"{title} - {it0}",
                                })
                        pdfb = build_scan_atlas_pdf_bytes(pages=pages, title=str(title))
                        st.session_state["scan_atlas_pdf_bytes"] = pdfb
                    pdfb = st.session_state.get("scan_atlas_pdf_bytes")
                    if isinstance(pdfb, (bytes, bytearray)) and len(pdfb) > 100:
                        st.download_button("Download atlas (PDF)", data=pdfb, file_name="shams_scan_atlas.pdf", mime="application/pdf", use_container_width=True, key="scan_nt_dl_atlas")

            elif insight == "Export SHAMS Signature Atlas (10 pages)":
                if not callable(build_signature_atlas_pdf_bytes):
                    st.warning("Signature atlas export unavailable.")
                else:
                    st.caption("Build the fixed 10-page SHAMS Signature Atlas (contract + provenance + key scan views).")
                    title = st.text_input("Atlas title", value="SHAMS - Scan Lab Signature Atlas", key="scan_sig_atlas_title")
                    if st.button("Build Signature Atlas (10 pages)", use_container_width=True, key="scan_sig_build"):
                        # Map PNGs for intents (from rendered dominance maps)
                        map_pngs = st.session_state.get("scan_cart_map_pngs") or {}
                        # Intent split map PNG (generated when both intents exist)
                        split_png = st.session_state.get("scan_intent_split_png")

                        # Fingerprints (citation-grade)
                        import os
                        repo_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
                        fps = {}
                        try:
                            if callable(compute_fingerprints):
                                fps = compute_fingerprints(repo_root)
                        except Exception:
                            fps = {}

                        # Optional claim to embed
                        cl = st.session_state.get("scan_claim_last")
                        pdfb = build_signature_atlas_pdf_bytes(
                            report=rep,
                            title=str(title),
                            contract_md=str(SCAN_LAB_CONTRACT),
                            fingerprints=fps,
                            map_png_by_intent={str(k): bytes(v) for k, v in map_pngs.items() if isinstance(v, (bytes, bytearray))},
                            intent_split_png=bytes(split_png) if isinstance(split_png, (bytes, bytearray)) else None,
                            claim=cl if isinstance(cl, dict) else None,
                        )
                        st.session_state["scan_signature_atlas_pdf_bytes"] = pdfb

                        # Publish into docs as SHAMS signature artifact (best-effort)
                        try:
                            out_path = os.path.join(repo_root, "docs", "SHAMS_Scan_Lab_Atlas_Signature.pdf")
                            with open(out_path, "wb") as f:
                                f.write(pdfb)
                            st.success("Published signature atlas into docs/SHAMS_Scan_Lab_Atlas_Signature.pdf")
                        except Exception:
                            pass

                    pdfb = st.session_state.get("scan_signature_atlas_pdf_bytes")
                    if isinstance(pdfb, (bytes, bytearray)) and len(pdfb) > 100:
                        st.download_button("Download Signature Atlas (PDF)", data=pdfb, file_name="shams_scan_signature_atlas.pdf", mime="application/pdf", use_container_width=True, key="scan_sig_dl")

        # PDF one-page summary + PNG map exports per intent
        try:
            from tools.reports.scan_summary import build_scan_summary_pdf_bytes
        except Exception:
            build_scan_summary_pdf_bytes = None  # type: ignore


        # Render maps
        intents2 = rep.get("intents") or []
        x_vals = rep.get("x_vals") or []
        y_vals = rep.get("y_vals") or []
        pts = rep.get("points") or []

        # Build quick lookup
        grid = {(int(p["i"]), int(p["j"])): p for p in pts if isinstance(p, dict) and "i" in p and "j" in p}

        it_primary = str(intents2[0]) if intents2 else 'Reactor'
        # Secondary intent lenses (collapsible) - reduces scroll fatigue.
        for it in intents2:
            if str(it) == str(it_primary):
                continue
            with st.expander(f"Intent lens: {it}", expanded=False):
                nar = ((rep.get("narrative") or {}).get("intents") or {}).get(it, {})
                if nar:
                    st.info(nar.get("plain_language", ""))

            # Topology change alerts (disconnected islands / holes)
                try:
                    topo = (rep.get('topology') or {}).get(it, {})
                    if isinstance(topo, dict) and topo:
                        if int(topo.get('n_components', 1)) > 1:
                            st.warning(f"Topology: {topo.get('n_components')} disconnected feasible islands detected (intent {it}).")
                        if bool(topo.get('has_holes')):
                            st.warning(f"Topology: hole-like infeasible pockets detected (count={topo.get('hole_count')}).")
                except Exception:
                    pass

                # arrays
                dom = np.empty((len(y_vals), len(x_vals)), dtype=object)
                ok = np.zeros((len(y_vals), len(x_vals)), dtype=float)
                mm = np.zeros((len(y_vals), len(x_vals)), dtype=float)
                rb = np.empty((len(y_vals), len(x_vals)), dtype=object)
                for j in range(len(y_vals)):
                    for i in range(len(x_vals)):
                        p = grid.get((i, j), {})
                        s = ((p.get("intent") or {}).get(it) or {})
                        ok[j, i] = 1.0 if bool(s.get("blocking_feasible")) else 0.0
                        # Prevent â€œall grayâ€ ambiguity when dominance labels are missing.
                        if ok[j, i] > 0.5:
                            dom[j, i] = "PASS"
                        else:
                            dom[j, i] = s.get("dominant_blocking") or "FAIL (unknown)"
                        mm[j, i] = float(s.get("min_blocking_margin") or float("nan"))
                        rb[j, i] = s.get("robustness") or "Unknown"

            # Categorical dominance to integer map
            labels = sorted(list({str(x) for x in dom.flatten().tolist()}))
            lut = {lab: idx for idx, lab in enumerate(labels)}
            z = np.vectorize(lambda s: lut.get(str(s), 0))(dom)

            fig, ax = plt.subplots(figsize=(7.6, 4.4))
            # Stable categorical colormap (PASS is neutral)
            labs = labels
            if 'PASS' in labs:
                labs = ['PASS'] + [x for x in labs if x != 'PASS']
                labels = labs
                lut = {lab: idx for idx, lab in enumerate(labels)}
                z = np.vectorize(lambda s: lut.get(str(s), 0))(dom)
            # Frozen visual semantics: constraintâ†’color mapping (PASS is neutral)
            try:
                from tools.scan_visual_identity import build_palette
                palette = build_palette(labels)
            except Exception:
                palette = ['#E0E0E0', '#4C78A8', '#F58518', '#54A24B', '#E45756', '#72B7B2', '#B279A2', '#FF9DA6', '#9D755D', '#BAB0AC', '#2F4B7C', '#7A5195', '#EF5675', '#FFA600']
            # Guarantee PASS stays neutral even if the mapping changes
            if labels and labels[0] == 'PASS':
                palette[0] = '#E0E0E0'
            cmap = ListedColormap(palette[:max(len(labels), 1)])
            im = ax.imshow(z, origin='lower', aspect='auto', cmap=cmap, vmin=-0.5, vmax=len(labels)-0.5)
            ax.set_xlabel(f"{x_key} - {key_to_label.get(x_key,x_key)}")
            ax.set_ylabel(f"{y_key} - {key_to_label.get(y_key,y_key)}")
            ax.set_title("Constraintâ€‘Dominance Cartography (blocking)")
            # Dominance boundary emphasis
            try:
                b = np.zeros_like(z, dtype=float)
                b[1:, :] |= (z[1:, :] != z[:-1, :])
                b[:, 1:] |= (z[:, 1:] != z[:, :-1])
                ax.contour(b, levels=[0.5], colors='k', linewidths=0.7, origin='lower')
            except Exception:
                pass
            # ticks: keep sparse
            ax.set_xticks([0, len(x_vals)//2, len(x_vals)-1])
            ax.set_xticklabels([f"{x_vals[0]:.3g}", f"{x_vals[len(x_vals)//2]:.3g}", f"{x_vals[-1]:.3g}"])
            ax.set_yticks([0, len(y_vals)//2, len(y_vals)-1])
            ax.set_yticklabels([f"{y_vals[0]:.3g}", f"{y_vals[len(y_vals)//2]:.3g}", f"{y_vals[-1]:.3g}"])
            st.pyplot(fig, use_container_width=True)

            # If everything is PASS, the map is intentionally neutral/gray.
            try:
                if labels == ['PASS']:
                    st.info("This map is neutral/gray because **all sampled points are blocking-feasible** in this slice (dominant constraint = PASS everywhere).")
            except Exception:
                pass

            # Capture a high-DPI PNG for exports (summary PDF / atlas)
            try:
                _buf = io.BytesIO()
                fig.savefig(_buf, format="png", dpi=220, bbox_inches="tight")
                mp = st.session_state.get("scan_cart_map_pngs")
                if not isinstance(mp, dict):
                    mp = {}
                mp[str(it)] = _buf.getvalue()
                st.session_state["scan_cart_map_pngs"] = mp
            except Exception:
                pass

            # Intent-split map: hatched region = Research-feasible but Reactor-infeasible
            if it == 'Reactor' and 'Research' in intents2 and 'Reactor' in intents2:
                try:
                    ok_r = ok
                    ok_s = np.zeros_like(ok)
                    for j in range(len(y_vals)):
                        for i in range(len(x_vals)):
                            p = grid.get((i, j), {})
                            ok_s[j, i] = 1.0 if bool(((p.get('intent') or {}).get('Research') or {}).get('blocking_feasible')) else 0.0
                    only_research = (ok_s > 0.5) & (ok_r < 0.5)
                    if only_research.any():
                        fig_os, ax_os = plt.subplots(figsize=(7.6, 4.4))
                        ax_os.imshow(z, origin='lower', aspect='auto', cmap=cmap, vmin=-0.5, vmax=len(labels)-0.5)
                        ax_os.contourf(only_research.astype(float), levels=[0.5, 1.5], colors='none', hatches=['////'], origin='lower')
                        ax_os.set_title('Intent-split overlay (hatched = Research-only feasible)')
                        st.pyplot(fig_os, use_container_width=True)

                        # Capture PNG for signature atlas
                        try:
                            _buf2 = io.BytesIO()
                            fig_os.savefig(_buf2, format="png", dpi=220, bbox_inches="tight")
                            st.session_state["scan_intent_split_png"] = _buf2.getvalue()
                        except Exception:
                            pass
                except Exception:
                    pass

            with st.expander("Legend (dominant blocking constraint)", expanded=False):
                # Export map for this intent
                try:
                    import io
                    _buf = io.BytesIO()
                    fig.savefig(_buf, format='png', dpi=200, bbox_inches='tight')
                    _buf.seek(0)
                    st.download_button(f'Download map (PNG) - {it}', data=_buf.getvalue(), file_name=f'shams_scan_map_{it.lower()}.png', mime='image/png', use_container_width=True, key=f'scan_map_png_{it}')
                    if callable(build_scan_summary_pdf_bytes):
                        _pdf = build_scan_summary_pdf_bytes(report=rep, intent=str(it), map_png=_buf.getvalue())
                        st.download_button(f'Download 1-page PDF - {it}', data=_pdf, file_name=f'shams_scan_summary_{it.lower()}.pdf', mime='application/pdf', use_container_width=True, key=f'scan_map_pdf_{it}')
                except Exception:
                    pass
                st.write([{"id": int(lut[k]), "constraint": k} for k in labels])

            # Iso-constraint manifolds (margin=0 contours)
            with st.expander("Iso-constraint manifolds (margin=0)", expanded=False):
                st.caption("Shows contour lines where a selected hard-constraint margin crosses zero (approx feasibility boundary).")
                # gather constraint names
                names = set()
                for p in pts:
                    mh = p.get('margins_hard') if isinstance(p, dict) else None
                    if isinstance(mh, dict):
                        for nm in mh.keys():
                            names.add(str(nm))
                names = sorted([n for n in names if n])
                if not names:
                    st.info("Hard-constraint margins not present in this report.")
                else:
                    pick_nm = st.selectbox("Constraint", options=names, index=0, key=f"scan_iso_pick_{it}")
                    M = np.full((len(y_vals), len(x_vals)), np.nan)
                    for j in range(len(y_vals)):
                        for i in range(len(x_vals)):
                            p = grid.get((i, j), {})
                            mh = p.get('margins_hard') if isinstance(p, dict) else None
                            if isinstance(mh, dict) and pick_nm in mh:
                                try:
                                    M[j, i] = float(mh[pick_nm])
                                except Exception:
                                    pass
                    if np.isfinite(M).any():
                        figc, axc = plt.subplots(figsize=(7.6, 4.4))
                        axc.imshow(ok, origin='lower', aspect='auto')
                        try:
                            axc.contour(M, levels=[0.0], colors='k', linewidths=1.0, origin='lower')
                        except Exception:
                            st.info("No margin=0 contour found in the current bounds.")
                        axc.set_title(f"Iso-contour: {pick_nm} margin = 0")
                        st.pyplot(figc, use_container_width=True)
                    else:
                        st.info("No finite margin data for this constraint in the scanned region.")

            # Firstâ€‘failure topology: show the failure order at a selected point
            with st.expander("Firstâ€‘Failure Topology (pick a cell)", expanded=False):
                ci, cj = st.columns(2)
                with ci:
                    ii = int(st.slider("i (x index)", 0, max(0, len(x_vals)-1), len(x_vals)//2, 1, key=f"scan_pick_i_{it}"))
                with cj:
                    jj = int(st.slider("j (y index)", 0, max(0, len(y_vals)-1), len(y_vals)//2, 1, key=f"scan_pick_j_{it}"))
                p = grid.get((ii, jj), {})
                st.write({"x": float(p.get("x", float('nan'))), "y": float(p.get("y", float('nan'))), "blocking_feasible": bool(((p.get('intent') or {}).get(it) or {}).get('blocking_feasible'))})
                st.write("Failure order (hard constraints, worst margin first):")
                st.write(p.get("failure_order_any") or [])
                st.write("Intent summary:")
                st.json(((p.get("intent") or {}).get(it) or {}), expanded=False)
                if st.button("Push this point to Point Designer", key=f"scan_push_pd_{it}"):
                    try:
                        # Canonical cross-panel handoff: set pd_candidate_apply.
                        # Point Designer will consume this payload and push it into widget keys.
                        from dataclasses import replace
                        _inp = replace(base, **{x_key: float(p.get("x")), y_key: float(p.get("y"))})
                        try:
                            from dataclasses import asdict
                            stage_pd_candidate_apply(asdict(_inp), source="ðŸ—ºï¸ Scan Lab / Topology Picker", note="Picked scan cell")
                        except Exception:
                            stage_pd_candidate_apply(dict(getattr(_inp, "__dict__", {})), source="ðŸ—ºï¸ Scan Lab / Topology Picker", note="Picked scan cell")
                        st.session_state["last_point_inp"] = _inp
                        st.success("Promoted this cell to Point Designer. Switch to ðŸ§­ Point Designer to review/evaluate.")
                    except Exception as e:
                        st.error(f"Could not apply point: {e}")

                st.markdown("---")
                st.markdown("##### Local insight tools")
                tabs = st.tabs(["Causality", "Time-to-failure", "Uncertainty", "Null direction"])

                # Build point overrides for this cell
                point_overrides = {x_key: float(p.get("x")), y_key: float(p.get("y"))}

                with tabs[0]:
                    st.caption("Local sensitivity trace for the currently dominant blocking constraint (finite differences).")
                    domc = (((p.get('intent') or {}).get(it) or {}).get('dominant_blocking') or '').strip()
                    if not domc or domc.upper() == 'PASS':
                        st.info("This cell passes blocking constraints; causality trace is most useful on a failing cell.")
                    elif not callable(build_causality_trace) or Evaluator is None:
                        st.info("Causality engine unavailable.")
                    else:
                        knobs = [x_key, y_key, 'R0_m', 'Bt_T', 'Ip_MA', 'fG', 'Paux_MW', 'a_m']
                        knobs = list(dict.fromkeys([k for k in knobs if hasattr(base, k)]))
                        rel_step = st.slider("Sensitivity step (relative)", 0.005, 0.05, 0.01, 0.005, key=f"scan_caus_step_{it}")
                        if st.button("Compute causality trace", key=f"scan_caus_run_{it}", use_container_width=True):
                            try:
                                ev_local = _dsg_evaluator(origin="UI", cache_enabled=True)
                                tr = build_causality_trace(
                                    evaluator=ev_local,
                                    base_inputs=base,
                                    point_overrides=point_overrides,
                                    constraint_name=domc,
                                    knobs=knobs,
                                    rel_step=float(rel_step),
                                )
                                st.json(tr, expanded=False)
                            except Exception as e:
                                st.error(f"Causality trace failed: {e}")

                with tabs[1]:
                    st.caption("How much you can push a knob before the point becomes blocking-infeasible.")
                    if not callable(time_to_failure_along_knob) or Evaluator is None:
                        st.info("Time-to-failure engine unavailable.")
                    else:
                        knob = st.selectbox("Knob", options=[x_key, y_key], index=0, key=f"scan_ttf_knob_{it}")
                        direction = st.radio("Direction", options=["Increase", "Decrease"], horizontal=True, key=f"scan_ttf_dir_{it}")
                        d = 1.0 if direction == "Increase" else -1.0
                        max_rel = st.slider("Max relative push", 0.05, 1.0, 0.5, 0.05, key=f"scan_ttf_max_{it}")
                        if st.button("Compute push-to-fail", key=f"scan_ttf_run_{it}", use_container_width=True):
                            try:
                                ev_local = _dsg_evaluator(origin="UI", cache_enabled=True)
                                rel = time_to_failure_along_knob(
                                    evaluator=ev_local,
                                    base_inputs=base,
                                    point_overrides=point_overrides,
                                    intent=str(it),
                                    knob=str(knob),
                                    direction=float(d),
                                    max_rel=float(max_rel),
                                )
                                if rel is None:
                                    st.info("No failure found within bounds (or point not feasible).")
                                else:
                                    st.success(f"Fails after â‰ˆ {100.0*rel:.1f}% {direction.lower()} in {knob}.")
                            except Exception as e:
                                st.error(f"Time-to-failure failed: {e}")

                with tabs[2]:
                    st.caption("Stress-test this cell under small uncertainty on nuisance inputs. Reports worst-case margin and dominant-constraint probabilities.")
                    if not callable(uncertainty_stress_test) or Evaluator is None:
                        st.info("Uncertainty engine unavailable.")
                    else:
                        nuis_all = [k for k in ['R0_m','a_m','Bt_T','Ip_MA','fG','Paux_MW','kappa','Ti_keV'] if hasattr(base, k)]
                        nuis = st.multiselect("Nuisance keys", options=nuis_all, default=nuis_all[:3], key=f"scan_unc_keys_{it}")
                        rel_unc = st.slider("Relative uncertainty", 0.0, 0.15, 0.03, 0.01, key=f"scan_unc_rel_{it}")
                        n_samples = int(st.slider("Samples", 10, 200, 60, 10, key=f"scan_unc_n_{it}"))
                        seed = int(st.number_input("Seed", value=7, step=1, key=f"scan_unc_seed_{it}"))
                        if st.button("Run uncertainty stress-test", key=f"scan_unc_run_{it}", use_container_width=True):
                            try:
                                ev_local = _dsg_evaluator(origin="UI", cache_enabled=True)
                                u = uncertainty_stress_test(
                                    evaluator=ev_local,
                                    base_inputs=base,
                                    point_overrides=point_overrides,
                                    intent=str(it),
                                    nuisance_keys=list(nuis),
                                    rel_unc=float(rel_unc),
                                    n_samples=int(n_samples),
                                    seed=int(seed),
                                )
                                st.json(u, expanded=False)
                            except Exception as e:
                                st.error(f"Uncertainty test failed: {e}")

                with tabs[3]:
                    st.caption("Null direction = locally flat direction in scan space (perpendicular to margin gradient).")
                    if not callable(null_direction_2d):
                        st.info("Null-direction helper unavailable.")
                    else:
                        try:
                            # local gradient of min margin from neighbors
                            gx = 0.0
                            gy = 0.0
                            if 0 < ii < len(x_vals)-1 and 0 < jj < len(y_vals)-1:
                                gx = (mm[jj, ii+1] - mm[jj, ii-1]) / max((x_vals[ii+1]-x_vals[ii-1]), 1e-12)
                                gy = (mm[jj+1, ii] - mm[jj-1, ii]) / max((y_vals[jj+1]-y_vals[jj-1]), 1e-12)
                            nd = null_direction_2d(gx, gy)
                            st.write({"grad_dir": nd.get('grad_dir'), "flat_dir": nd.get('flat_dir')})
                            st.caption("Interpretation: moving along flat_dir tends to keep min margin nearly constant locally.")
                        except Exception as e:
                            st.error(f"Null direction unavailable: {e}")

            # Optional vector field
            with st.expander("Margin vector field (optional)", expanded=False):
                st.caption("Arrows point toward increasing min blocking margin (local safety direction).")
                if len(x_vals) >= 3 and len(y_vals) >= 3:
                    # finite differences
                    gx = np.zeros_like(mm)
                    gy = np.zeros_like(mm)
                    for j in range(1, len(y_vals)-1):
                        for i in range(1, len(x_vals)-1):
                            gx[j,i] = (mm[j,i+1] - mm[j,i-1]) / max((x_vals[i+1]-x_vals[i-1]), 1e-12)
                            gy[j,i] = (mm[j+1,i] - mm[j-1,i]) / max((y_vals[j+1]-y_vals[j-1]), 1e-12)
                    fig2, ax2 = plt.subplots(figsize=(7.6, 4.4))
                    ax2.imshow(ok, origin="lower", aspect="auto")
                    step = max(1, int(max(len(x_vals), len(y_vals)) / 20))
                    X, Y = np.meshgrid(np.arange(len(x_vals)), np.arange(len(y_vals)))
                    ax2.quiver(X[::step,::step], Y[::step,::step], gx[::step,::step], gy[::step,::step], angles='xy', scale_units='xy', scale=None)
                    ax2.set_title("Vector field over blocking feasibility (background)")
                    ax2.set_xlabel(f"{x_key}")
                    ax2.set_ylabel(f"{y_key}")
                    st.pyplot(fig2, use_container_width=True)
                else:
                    st.info("Increase Nx/Ny to at least 3 for a vector field.")

            # Robustness map (labels)
            with st.expander("Robustness (brutally honest)", expanded=False):
                # summarize counts
                flat = [str(x) for x in rb.flatten().tolist()]
                counts = {k: flat.count(k) for k in sorted(set(flat))}
                st.write({k: f"{v} ({v/len(flat):.0%})" for k, v in counts.items()})

        # Constraint interaction map (coupling view)
        with st.expander("Constraint interaction map (coupling)", expanded=False):
            st.caption("Matrix: how often constraint A appears before B in the local failure order. Descriptive only.")
            inter = rep.get('interaction') or {}
            if isinstance(inter, dict):
                iit = st.selectbox("Intent", options=intents2, index=0, key="scan_inter_intent")
                blob = (inter.get('intents') or {}).get(iit, {}) if isinstance(inter.get('intents'), dict) else {}
                names = blob.get('names') if isinstance(blob, dict) else None
                mat = blob.get('before_counts') if isinstance(blob, dict) else None
                if isinstance(names, list) and isinstance(mat, dict):
                    import pandas as pd
                    dfm = pd.DataFrame(mat)
                    dfm = dfm.reindex(index=names, columns=names)
                    st.dataframe(dfm, use_container_width=True)
                else:
                    st.info("Interaction data unavailable in this report.")
            else:
                st.info("Interaction data unavailable.")

        # Intent split overlay guidance
        if len(intents2) >= 2:
            st.markdown("### Intent-split insight")
            st.caption("Run the same scan with both intents to see how Research-feasible regions differ from Reactor-feasible regions.")

    # Render world-class scan panel
    try:
        _v188_scan_lab_panel()
    except Exception as _e:
        st.error(f"Scan Lab error: {_e}")

    # NOTE: Scan Lab freeze/legacy/parameter-guide/mapping panels are rendered inside Scan Lab
    # (before Cartography) to reduce scroll fatigue and keep the instrument literacy in one place.
if _deck == "ðŸ“ˆ Pareto Lab":
    # DSG: auto edge-kind tagging by active panel (exploration only)
    if bool(st.session_state.get("dsg_edge_kind_auto", True)):
        st.session_state["dsg_context_edge_kind"] = "pareto"

    st.header("ðŸ“ˆ Pareto Lab")
    st.caption("Trade-off observatory over the feasible set. External optimization is firewalled; truth remains frozen.")
    render_mode_scope("pareto")
    # --- Pareto freeze (read-only semantics) ---
    st.info(PARETO_LOCK_LINE)

    # --- Deck selector (v230.0: external optimizer console) ---
    _pareto_deck_keys = [
        "Internal Pareto Frontier",
        "Robust Pareto Frontier (Phase+UQ)",
        "Regime-Conditioned Pareto Atlas 2.0",
        "Certified Optimization Orchestrator",
        "Feasible Optimizer (External)",
        "Concept Optimization Cockpit",
        "External Optimization Workbench",
        "External Optimization Interpretation",
        "Design Family Narratives",
        "External Optimizer Co-Pilot",
        "External Optimizer Suite",
        "Optimization Evidence Packs",
    ]
    _pareto_deck_labels = {
        "Internal Pareto Frontier": "ðŸ§­ Internal Pareto Frontier",
        "Robust Pareto Frontier (Phase+UQ)": "ðŸ›¡ï¸ Robust Pareto Frontier (Phase+UQ)",
        "Regime-Conditioned Pareto Atlas 2.0": "ðŸ§­ Regime-Conditioned Pareto Atlas 2.0",
        "Certified Optimization Orchestrator": "ðŸ§¾ Certified Optimization Orchestrator",
        "Feasible Optimizer (External)": "ðŸ§² Feasible Optimizer (External)",
        "Concept Optimization Cockpit": "ðŸ§¬ Concept Optimization Cockpit",
        "External Optimization Workbench": "ðŸ“ˆ External Optimization Workbench",
        "External Optimization Interpretation": "ðŸ§ª External Optimization Interpretation",
        "Design Family Narratives": "ðŸ§¬ Design Family Narratives",
        "External Optimizer Co-Pilot": "ðŸ§­ External Optimizer Co-Pilot",
        "External Optimizer Suite": "ðŸ“¦ External Optimizer Suite",
        "Optimization Evidence Packs": "ðŸ§¾ Optimization Evidence Packs",
    }

    # Back-compat for older stored values that included emojis in the raw key.
    _legacy_to_key = {
        "ðŸ§¬ Concept Optimization Cockpit": "Concept Optimization Cockpit",
        "ðŸ“ˆ External Optimization Workbench": "External Optimization Workbench",
        "ðŸ“¦ External Optimizer Suite": "External Optimizer Suite",
    }
    try:
        _legacy = st.session_state.get("pareto_deck_selector_v230")
        if isinstance(_legacy, str) and _legacy in _legacy_to_key:
            st.session_state["pareto_deck_selector_v230"] = _legacy_to_key[_legacy]
    except Exception:
        pass

    _pareto_deck = st.radio(
        "Pareto Lab deck",
        options=_pareto_deck_keys,
        index=0,
        horizontal=True,
        format_func=lambda k: _pareto_deck_labels.get(str(k), str(k)),
        help=(
            "Choose the Pareto Lab deck. External optimizer tooling runs outside the frozen evaluator "
            "and does not modify physics truth."
        ),
        key="pareto_deck_selector_v230",
    )

    if _pareto_deck == "Feasible Optimizer (External)":
        render_external_optimizer_launcher(Path(__file__).resolve().parent.parent)
        st.stop()

    if _pareto_deck == "Certified Optimization Orchestrator":
        from ui.certified_opt_orchestrator import render_certified_optimization_orchestrator

        render_certified_optimization_orchestrator(Path(__file__).resolve().parent.parent)
        st.stop()

    if _pareto_deck == "Concept Optimization Cockpit":
        from ui.concept_opt_cockpit import render_concept_optimization_cockpit

        render_concept_optimization_cockpit(Path(__file__).resolve().parent.parent)
        st.stop()

    if _pareto_deck == "External Optimization Workbench":
        from ui.extopt_workbench import render_extopt_workbench

        render_extopt_workbench(Path(__file__).resolve().parent.parent)
        st.stop()

    if _pareto_deck == "External Optimization Interpretation":
        from ui.extopt_interpretation import render_extopt_interpretation

        render_extopt_interpretation(Path(__file__).resolve().parent.parent)
        st.stop()

    if _pareto_deck == "Design Family Narratives":
        from ui.design_families import render_design_families

        render_design_families(Path(__file__).resolve().parent.parent)
        st.stop()

    if _pareto_deck == "External Optimizer Co-Pilot":
        from ui.extopt_copilot import render_extopt_copilot

        render_extopt_copilot(Path(__file__).resolve().parent.parent)
        st.stop()

    if _pareto_deck == "External Optimizer Suite":
        from ui.extopt_suite import render_extopt_suite

        render_extopt_suite(Path(__file__).resolve().parent.parent)
        st.stop()

    if _pareto_deck == "Optimization Evidence Packs":
        render_optimizer_evidence_packs(Path(__file__).resolve().parent.parent)
        st.stop()

    if _pareto_deck == "Robust Pareto Frontier (Phase+UQ)":
        from ui.robust_pareto_lab import render_robust_pareto_lab

        render_robust_pareto_lab(Path(__file__).resolve().parent.parent)
        st.stop()

    if _pareto_deck == "Regime-Conditioned Pareto Atlas 2.0":
        from ui.regime_conditioned_atlas import render_regime_conditioned_atlas

        render_regime_conditioned_atlas(Path(__file__).resolve().parent.parent)
        st.stop()


    # Summary card (filled after run if results exist)
    with st.container(border=True):
        st.markdown("**Frontier Dashboard**")
        _sum_cols = st.columns(5)
        # placeholders populated later via session state when available
        _ps = st.session_state.get("pareto_last_summary", {})
        _sum_cols[0].metric("Feasible points", _ps.get("n_feasible", "-"))
        _sum_cols[1].metric("Pareto points", _ps.get("n_pareto", "-"))
        _sum_cols[2].metric("Top constraint", _ps.get("top_constraint", "-"))
        _sum_cols[3].metric("Robust mix", _ps.get("robust_mix", "-"))
        _sum_cols[4].metric("Confidence", _ps.get("confidence", "-"))

    # Definition of Pareto optimal (SHAMS-specific)
    with st.expander("â„¹ï¸ What does â€œPareto optimalâ€ mean here? (frontier concept)", expanded=False):
        st.markdown(PARETO_OPTIMAL_DEF)

    # Trust boundaries
    with st.expander("Trust boundaries (what you can and cannot conclude)", expanded=False):
        for _t in TRUST_BOUNDARIES:
            st.markdown(f"- {_t}")

        st.caption("Pareto Lab does **not** recommend or select designs. It maps unavoidable trade-offs among **feasible** designs only.")

    # --- Governance (read-only) ---
    with st.expander("Pareto Mode governance (constitution / freeze / contribution rules)", expanded=False):
        st.caption("Read-only governance documents that protect Pareto from drifting into optimization or recommendations.")
        try:
            _c = (Path(__file__).resolve().parent.parent / "docs" / "PARETO_MODE_CONSTITUTION.md").read_text(encoding="utf-8")
            _f = (Path(__file__).resolve().parent.parent / "docs" / "PARETO_V1_FREEZE_DECLARATION.md").read_text(encoding="utf-8")
            _r = (Path(__file__).resolve().parent.parent / "docs" / "PARETO_POST_FREEZE_CONTRIBUTION_RULES.md").read_text(encoding="utf-8")
            _t = (Path(__file__).resolve().parent.parent / "docs" / "PARETO_TEACHING_FREEZE_POLICY.md").read_text(encoding="utf-8")
        except Exception:
            _c=_f=_r=_t="(missing doc file in this build)"
        cols = st.columns(4)
        cols[0].download_button("Download Constitution", data=_c, file_name="PARETO_MODE_CONSTITUTION.md", mime="text/markdown", use_container_width=True)
        cols[1].download_button("Download Freeze", data=_f, file_name="PARETO_V1_FREEZE_DECLARATION.md", mime="text/markdown", use_container_width=True)
        cols[2].download_button("Download Rules", data=_r, file_name="PARETO_POST_FREEZE_CONTRIBUTION_RULES.md", mime="text/markdown", use_container_width=True)
        cols[3].download_button("Download Teaching Policy", data=_t, file_name="PARETO_TEACHING_FREEZE_POLICY.md", mime="text/markdown", use_container_width=True)

    # --- Replay (read-only) ---
    with st.expander("Replay capsule (read-only)", expanded=False):
        st.caption("Load a previously exported Pareto artifact and reproduce the same front without re-sampling. This is audit/review mode.")
        art_file = st.file_uploader("Upload Pareto artifact (.json)", type=["json"], key="pareto_replay_uploader")
        if art_file is not None:
            try:
                art = json.load(art_file)
                st.success("Artifact loaded.")
                st.json({k: art.get(k) for k in ["schema", "version", "intent_mode", "n_samples", "seed", "objectives"] if k in art}, expanded=False)
                _front = pd.DataFrame(art.get("pareto", []) or [])
                _feas = pd.DataFrame(art.get("feasible", []) or [])
                if len(_front):
                    st.markdown("#### Replayed Pareto front")
                    try:
                        import plotly.express as px
                        _objs = list((art.get("objectives") or {}).keys())
                        if len(_objs) >= 2:
                            x, y = _objs[0], _objs[1]
                            fig = px.scatter(_front, x=x, y=y, color="dominant_constraint" if "dominant_constraint" in _front.columns else None, hover_data=_front.columns)
                            st.plotly_chart(fig, use_container_width=True)
                        else:
                            st.dataframe(_front, use_container_width=True)
                    except Exception:
                        st.dataframe(_front, use_container_width=True)
                if len(_feas):
                    st.markdown("#### Replayed feasible set (sampled)")
                    st.dataframe(_feas.head(200), use_container_width=True)
            except Exception as e:
                st.error(f"Could not load artifact: {e}")

    st.markdown(
        "This mode performs a deterministic-feeling **LHS sampling study** inside explicit bounds, filters **intent-aware feasible** points, "
        "and constructs **constraint-annotated Pareto fronts** for explicit objectives."
    )

    # --- Objective Contract (explicit, publishable) ---
    _OBJ_CATALOG = {
        "R0_m": {"units": "m", "desc": "Major radius"},
        "Bt_T": {"units": "T", "desc": "Toroidal field on axis"},
        "Ip_MA": {"units": "MA", "desc": "Plasma current"},
        "fG": {"units": "-", "desc": "Greenwald fraction"},
        "B_peak_T": {"units": "T", "desc": "Peak TF field"},
        "P_e_net_MW": {"units": "MW", "desc": "Net electric power"},
        "Q_DT_eqv": {"units": "-", "desc": "Equivalent DT gain"},
        "q_div_MW_m2": {"units": "MW/m^2", "desc": "Divertor heat-flux proxy"},
        "sigma_vm_MPa": {"units": "MPa", "desc": "Von Mises stress proxy"},
        "hts_margin_cs": {"units": "-", "desc": "HTS margin (critical surface)"},
        "TBR": {"units": "-", "desc": "Tritium breeding ratio"},
    }

    base0 = st.session_state.get("last_point_inp")
    if base0 is None:
        base0 = PointInputs(R0_m=1.85, a_m=0.57, kappa=1.8, Bt_T=12.2, Ip_MA=8.0, Ti_keV=15.0, fG=0.8, Paux_MW=20.0)

    with st.expander("Bounds (sampling hyper-rectangle)", expanded=False):
        st.caption("Bounds are applied to the chosen variables during sampling.")
        bcols = st.columns(4)
        b_R0 = (float(_safe_get(base0, 'R0_m')*0.8), float(_safe_get(base0, 'R0_m')*1.25))
        b_Bt = (float(_safe_get(base0, 'Bt_T')*0.7), float(_safe_get(base0, 'Bt_T')*1.15))
        b_Ip = (float(_safe_get(base0, 'Ip_MA')*0.6), float(_safe_get(base0, 'Ip_MA')*1.6))
        b_fG = (0.3, 1.1)
        R0_lo = _num("R0 min [m]", b_R0[0], 0.01)
        R0_hi = _num("R0 max [m]", b_R0[1], 0.01)
        Bt_lo = _num("Bt min [T]", b_Bt[0], 0.1)
        Bt_hi = _num("Bt max [T]", b_Bt[1], 0.1)
        Ip_lo = _num("Ip min [MA]", b_Ip[0], 0.1)
        Ip_hi = _num("Ip max [MA]", b_Ip[1], 0.1)
        fG_lo = _num("fG min [-]", b_fG[0], 0.05)
        fG_hi = _num("fG max [-]", b_fG[1], 0.05)

        bounds = {
            "R0_m": (float(R0_lo), float(R0_hi)),
            "Bt_T": (float(Bt_lo), float(Bt_hi)),
            "Ip_MA": (float(Ip_lo), float(Ip_hi)),
            "fG": (float(fG_lo), float(fG_hi)),
        }

    with st.expander("Objective Contract (explicit)", expanded=False):
        st.caption("Objectives are explicit and unit-aware. No hidden scoring. The contract is included in exports.")
        # Objective templates (smart presets, not recommendations)
        _OBJ_TEMPLATES = {
            "Custom": None,
            "Reactor - Compact power": {"R0_m":"min","P_e_net_MW":"max","q_div_MW_m2":"min","sigma_vm_MPa":"min","TBR":"max"},
            "Reactor - Max gain": {"Q_DT_eqv":"max","P_e_net_MW":"max","R0_m":"min","q_div_MW_m2":"min"},
            "Research - High current/density": {"Ip_MA":"max","fG":"max","R0_m":"min","Bt_T":"max"},
            "Research - High field": {"Bt_T":"max","B_peak_T":"max","R0_m":"min"},
        }
        tmpl = st.selectbox("Objective template", options=list(_OBJ_TEMPLATES.keys()), index=0, help="Populates objectives with common expert framing. This is not a recommendation.")
        if "pareto_template_last" not in st.session_state:
            st.session_state.pareto_template_last = "Custom"
        # When template changes, update defaults in session_state (deterministic)
        if tmpl != st.session_state.pareto_template_last and _OBJ_TEMPLATES.get(tmpl):
            st.session_state.pareto_sel_objs = list(_OBJ_TEMPLATES[tmpl].keys())
            st.session_state.pareto_obj_senses = dict(_OBJ_TEMPLATES[tmpl])
            st.session_state.pareto_template_last = tmpl
        elif tmpl != st.session_state.pareto_template_last:
            st.session_state.pareto_template_last = tmpl

        intent_mode = st.radio("Design Intent", ["Reactor", "Research", "Both (overlay)"] , index=0, horizontal=True)
        obj_keys = list(_OBJ_CATALOG.keys())
        c1, c2, c3 = st.columns([2, 1, 1])
        with c1:
            sel_objs = st.multiselect("Objectives", options=obj_keys, default=st.session_state.get("pareto_sel_objs", ["R0_m", "B_peak_T", "P_e_net_MW"]), key="pareto_sel_objs")
        with c2:
            st.write("\n")
            robust_margin_thr = float(st.number_input("Robust margin threshold", value=0.10, step=0.05))
        with c3:
            st.write("\n")
            n_samples = int(st.slider("Samples", min_value=50, max_value=4000, value=300, step=50))
        seed = int(st.number_input("Sampling seed", value=1, step=1))

        objectives = {}
        for k in sel_objs:
            meta = _OBJ_CATALOG.get(k, {})
            cols = st.columns([2, 1, 2])
            with cols[0]:
                st.write(f"**{k}**")
            with cols[1]:
                objectives[k] = st.selectbox(f"sense_{k}", ["min", "max"], index=(0 if st.session_state.get("pareto_obj_senses", {}).get(k, ("min" if k in ("R0_m","B_peak_T","q_div_MW_m2","sigma_vm_MPa") else "max"))=="min" else 1), label_visibility="collapsed")
            with cols[2]:
                st.caption(f"{meta.get('desc','')}  [{meta.get('units','-')}]".strip())

        if len(objectives) < 2:
            st.warning("Select at least 2 objectives for a meaningful Pareto front.")

        # Objective sanity validator (warnings only; does not block)
        st.divider()
        with st.expander("Objective sanity checks (warnings only)", expanded=False):
            warns=[]
            if any(k.upper().startswith("TBR") for k in objectives.keys()) and str(intent_mode).startswith("Research"):
                warns.append("TBR is typically **ignored as a blocking constraint** in Research intent. Using TBR as an objective in Research may be uninformative.")
            if any(k in ["P_e_net_MW"] for k in objectives.keys()) and str(intent_mode).startswith("Research"):
                warns.append("Net electric power is usually not a Research driver; ensure this objective is meaningful for your study.")
            if len(set(objectives.keys())) != len(objectives):
                warns.append("Duplicate objective keys detected (this should not happen).")
            if len(warns)==0:
                st.success("No obvious objective-contract red flags.")
            else:
                for w in warns:
                    st.warning(w)

        # Redundancy hint
        st.divider()
        st.caption("Redundancy detection runs after sampling (correlation-based).")

    if st.button("Run Pareto (feasible-only)", type="primary", use_container_width=True):
        import time
        t0=time.time()
        try:
            from solvers.optimize import pareto_optimize
            intents = ["Reactor", "Research"] if str(intent_mode).startswith("Both") else [str(intent_mode)]
            all_runs = []
            all_fronts = []
            all_samples = []
            for it in intents:
                res = pareto_optimize(base0, bounds=bounds, objectives=objectives, n_samples=n_samples, seed=seed, intent_key=it)
                feasible = res.get("feasible", [])
                front = res.get("pareto", [])
                all_samp = res.get("all", [])
                if all_samp:
                    dfA = pd.DataFrame(all_samp)
                    dfA["intent"] = it
                    all_samples.append(dfA)
                if feasible:
                    dfF = pd.DataFrame(feasible)
                    dfF["intent"] = it
                    all_runs.append(dfF)
                if front:
                    dfP = pd.DataFrame(front)
                    dfP["intent"] = it
                    all_fronts.append(dfP)

            dfF_all = pd.concat(all_runs, ignore_index=True) if all_runs else pd.DataFrame()
            dfP_all = pd.concat(all_fronts, ignore_index=True) if all_fronts else pd.DataFrame()
            dfA_all = pd.concat(all_samples, ignore_index=True) if all_samples else pd.DataFrame()
            st.session_state.pareto_last = {
                "objectives": objectives,
                "intent_mode": intent_mode,
                "bounds": bounds,
                "seed": seed,
                "n_samples": n_samples,
                "feasible": dfF_all.to_dict(orient="records") if len(dfF_all) else [],
                "pareto": dfP_all.to_dict(orient="records") if len(dfP_all) else [],
                "robust_margin_thr": robust_margin_thr,
            }

            st.success(f"Done. Feasible points: {len(dfF_all)} / {n_samples*len(intents)}. Pareto points: {len(dfP_all)}. ({time.time()-t0:.1f}s)")
            # Explain why not (if feasibility/front is empty)
            if len(dfF_all) == 0 or len(dfP_all) == 0:
                with st.expander("Explain why not (empty feasibility or empty Pareto front)", expanded=False):
                    if len(dfF_all) == 0:
                        st.warning("No feasible designs were found in the sampled bounds for the selected intent(s). This is not a plotting issue; it means feasibility was not achieved under the frozen evaluator.")
                    elif len(dfP_all) == 0:
                        st.warning("Feasible designs exist, but no non-dominated Pareto set was produced (often due to objective redundancy or insufficient variation).")
                    try:
                        if len(dfA_all) and "first_failure" in dfA_all.columns:
                            vc = dfA_all["first_failure"].fillna("(none)").astype(str).value_counts().head(8)
                            st.markdown("**Top blocking constraints in sampled space (first-failure counts):**")
                            st.dataframe(vc.rename("count").to_frame(), use_container_width=True)
                            st.caption("Tip: If a single constraint dominates all failures, the bounds likely never enter a feasible basin for that intent.")
                        else:
                            st.info("No failure-atlas data available in this run.")
                    except Exception:
                        st.info("Could not summarize failure modes for this run.")

            # --- Sampling honesty panel ---

            with st.expander("Sampling honesty (coverage / density / what was explored)", expanded=False):
                st.caption("Pareto conclusions are only as strong as sampling coverage. This panel reports coverage proxies (no smoothing).")
                if len(dfA_all):
                    st.write({
                        "n_samples_total": int(len(dfA_all)),
                        "n_feasible": int(len(dfF_all)),
                        "feasible_fraction": float(len(dfF_all)/max(len(dfA_all),1)),
                        "intents": intents,
                        "seed": seed,
                    })
                    # Density proxy in objective space (kNN distance)
                    try:
                        obj_keys=list(objectives.keys())
                        if len(obj_keys)>=2 and len(dfF_all)>=10:
                            X=dfF_all[obj_keys].astype(float).to_numpy()
                            k=min(10, len(X)-1)
                            d2=((X[:,None,:]-X[None,:,:])**2).sum(axis=2)
                            np.fill_diagonal(d2, np.inf)
                            knn=np.sort(d2,axis=1)[:,:k]
                            rho=np.sqrt(np.mean(knn,axis=1))
                            st.metric("Median local spacing (objective-space)", float(np.median(rho)))
                            st.metric("95% spacing (thin regions)", float(np.percentile(rho,95)))
                        else:
                            st.info("Density proxy requires â‰¥10 feasible points and â‰¥2 objectives.")
                    except Exception as _e:
                        st.info(f"Could not compute density proxy: {_e}")
                else:
                    st.info("No sampling summary available for this run.")

            # --- Failure atlas (infeasible shadow) ---
            with st.expander("Failure atlas (infeasible shadow / what blocks the frontier)", expanded=False):
                st.caption("Faintly shows sampled infeasible points and the first blocking constraint. This does not relax constraints.")
                if len(dfA_all):
                    dfI = dfA_all[~dfA_all.get("is_feasible", False)].copy() if "is_feasible" in dfA_all.columns else pd.DataFrame()
                    if len(dfI) and len(objectives)>=2:
                        obj_keys=list(objectives.keys())
                        x,y=obj_keys[0], obj_keys[1]
                        try:
                            import plotly.express as px
                            figI = px.scatter(dfI, x=x, y=y, color="first_failure" if "first_failure" in dfI.columns else None,
                                              opacity=0.25, hover_data=dfI.columns)
                            st.plotly_chart(figI, use_container_width=True)
                        except Exception:
                            st.dataframe(dfI.head(200), use_container_width=True)
                    else:
                        st.info("No infeasible shadow available (or not enough objectives selected).")
                else:
                    st.info("No infeasible shadow available for this run.")
            if len(dfF_all):
                st.markdown("### Feasible set (intent-aware)")
                st.dataframe(dfF_all, use_container_width=True, height=260)

                # Objective redundancy detection (correlation over feasible set)
                try:
                    corr = dfF_all[list(objectives.keys())].corr(numeric_only=True)
                    bad = []
                    for i, a in enumerate(corr.columns):
                        for j, b in enumerate(corr.columns):
                            if j <= i:
                                continue
                            v = float(corr.loc[a, b])
                            if abs(v) >= 0.92:
                                bad.append((a, b, v))
                    if bad:
                        st.warning("Objective redundancy detected (high correlation on feasible set):")
                        st.dataframe(pd.DataFrame(bad, columns=["objective_a", "objective_b", "corr"]), use_container_width=True)
                except Exception:
                    pass


                # Objective interaction matrix (sign-only couplings on feasible manifold)
                # Non-trade-off region marker (when objectives co-improve)
                try:
                    if "pareto_front" in st.session_state:
                        _pf = st.session_state["pareto_front"]
                        if isinstance(_pf, pd.DataFrame) and len(_pf) >= 10:
                            _okeys = list(objectives.keys())
                            if len(_okeys) >= 2:
                                _c = _pf[_okeys[:2]].corr(numeric_only=True).iloc[0, 1]
                                if _c == _c and float(_c) > 0.80:
                                    st.info("No meaningful trade-off detected in this projection (objectives tend to improve together here).")
                except Exception:
                    pass



                with st.expander("Objective interaction matrix (sign-only, descriptive)", expanded=False):
                    st.caption("Shows how objectives tend to co-vary across the feasible set (not recommendations). '+' means tends to increase together, '-' means trade-off, '~' means weak/none.")
                    try:
                        obj_keys = list(objectives.keys())
                        if len(obj_keys) >= 2 and len(dfF_all) >= 8:
                            C = dfF_all[obj_keys].corr(numeric_only=True)
                            def _sg(v: float) -> str:
                                if not (v == v):
                                    return ""
                                if abs(v) < 0.25:
                                    return "~"
                                return "+" if v > 0 else "-"
                            M = C.copy()
                            for a in obj_keys:
                                for b in obj_keys:
                                    M.loc[a, b] = _sg(float(C.loc[a, b]))
                            st.dataframe(M, use_container_width=True)
                        else:
                            st.info("Not enough feasible points/objectives to form an interaction matrix.")
                    except Exception:
                        st.info("Interaction matrix unavailable in this run.")

                # Epistemic confidence + incompleteness detector (sampling honesty â†’ confidence)
                with st.expander("Confidence & incompleteness (epistemic, not physics)", expanded=False):
                    st.caption("These are sampling-based confidence signals: they estimate where the Pareto picture is solid vs where it may be incomplete due to limited coverage.")
                    try:
                        feas_frac = float(len(dfF_all) / max(int(len(dfA_all)), 1)) if len(dfA_all) else float(len(dfF_all) / max(int(n_samples*len(intents)), 1))
                        st.write({"n_feasible": int(len(dfF_all)), "n_front": int(len(dfP_all)), "feasible_fraction": feas_frac})
                        # Heuristic incompleteness flags
                        flags = []
                        if len(dfF_all) < max(50, 0.01 * n_samples * max(len(intents),1)):
                            flags.append("Feasible sample is sparse; Pareto front may be incomplete.")
                        if feas_frac < 0.002:
                            flags.append("Feasible fraction is very low; consider that the sampled bounds may mostly miss the feasible basin.")
                        if flags:
                            st.warning(" | ".join(flags))
                        else:
                            st.success("No major incompleteness flags triggered by sampling proxies.")
                    except Exception:
                        st.info("Confidence/incompleteness summary unavailable.")

                # Active question suggestions (guides thinking; does not choose designs)
                with st.expander("Possible next questions (guidance, not recommendations)", expanded=False):
                    qs = []
                    try:
                        if 'bad' in locals() and bad:
                            qs.append("Two objectives appear redundant here - would you like to hide one and re-run to sharpen the front?")
                    except Exception:
                        pass
                    try:
                        if len(dfP_all) and "min_signed_margin" in dfP_all.columns:
                            frac_frag = float((pd.to_numeric(dfP_all["min_signed_margin"], errors="coerce") < float(robust_margin_thr)).mean())
                            if frac_frag > 0.5:
                                qs.append("Most Pareto points are fragile under the chosen margin threshold - would you like to view robust-only by default?")
                    except Exception:
                        pass
                    if str(intent_mode).startswith("Both"):
                        qs.append("Would you like to compare intent-split fronts side-by-side (Research vs Reactor) on the same axes?")
                    qs.append("Would you like to export a publication pack (artifact + CSV + narrative + PNG) for this run?")
                    qs.append("Would you like to click a segment and read the regime explanation (what pins this trade-off)?")
                    for q in qs[:6]:
                        st.write("â€¢ " + q)

                # Self-audit (continuous proof of integrity)
                with st.expander("Pareto self-audit (read-only integrity checklist)", expanded=False):
                    st.caption("This checklist is informational; it summarizes SHAMS guarantees for this mode.")
                    st.write(
                        {
                            "feasible_only": True,
                            "deterministic": True,
                            "no_recommendations": True,
                            "policy_explicit": True,
                            "intent_explicit": True,
                            "sampling_honesty_reported": True,
                        }
                    )

                # Language calibration (scientific phrasing guardrail)
                with st.expander("Language calibration (how to read statements)", expanded=False):
                    st.caption("All Pareto statements are conditional on bounds, intent, policy, and sampling. They are descriptive (not prescriptive).")
                    st.write("Example: â€œStress dominates hereâ€ means â€œGiven the selected bounds and policy, Ïƒ_vm is most limiting along this segment in the sampled feasible set.â€")
            if len(dfP_all):
                st.markdown("### Pareto fronts (constraint-annotated)")
                st.dataframe(dfP_all, use_container_width=True, height=260)

                # Promote a selected Pareto point back into Point Designer (canonical handoff)
                with st.expander("Promote a Pareto point to ðŸ§­ Point Designer", expanded=False):
                    st.caption("Select a point from the Pareto front and promote it into Point Designer inputs (no evaluation performed here).")
                    try:
                        _idxs = list(range(int(len(dfP_all))))
                        _pick = int(st.selectbox("Pareto row index", options=_idxs, index=0, key="pareto_promote_row")) if _idxs else 0
                        _row = dfP_all.iloc[_pick].to_dict() if len(dfP_all) else {}
                        st.write({k: _row.get(k) for k in list(objectives.keys())[:4] + ["dominant_constraint", "min_constraint_margin", "intent"] if k in _row})
                        if st.button("Promote to Point Designer", use_container_width=True, key="pareto_promote_btn"):
                            try:
                                # Reconstruct a full PointInputs dict from the baseline + sampled decision variables.
                                from dataclasses import asdict
                                _base_dict = asdict(base0) if base0 is not None else {}
                            except Exception:
                                _base_dict = dict(getattr(base0, "__dict__", {})) if base0 is not None else {}

                            # Decision variables are the bound keys for this run.
                            for _k in list(bounds.keys()):
                                if _k in _row and _row.get(_k) is not None:
                                    try:
                                        _base_dict[_k] = float(_row.get(_k))
                                    except Exception:
                                        pass

                            stage_pd_candidate_apply(dict(_base_dict), source="ðŸ“ˆ Pareto Lab / Internal Pareto", note="Selected Pareto row")
                            st.success("Promoted selected Pareto point to Point Designer. Switch to ðŸ§­ Point Designer to review/evaluate.")
                    except Exception as _e:
                        st.info(f"Promotion UI unavailable for this run: {_e}")

                # Robust envelope (proxy): filter by min_constraint_margin
                dfP_all["robust"] = (pd.to_numeric(dfP_all.get("min_constraint_margin"), errors="coerce") >= robust_margin_thr)
                dfP_robust = dfP_all[dfP_all["robust"]].copy()

                # Freedom-left indicator (2D, selected axes)
                xkey = st.selectbox("x-axis", options=list(objectives.keys()), index=0)
                ykey = st.selectbox("y-axis", options=[k for k in objectives.keys() if k != xkey], index=0)
                ckey = st.selectbox("color", options=["dominant_constraint", "intent", "robust"], index=0)

                def _classify_freedom(df: pd.DataFrame) -> pd.Series:
                    try:
                        d = df.sort_values(xkey)
                        x = pd.to_numeric(d[xkey], errors="coerce").values
                        y = pd.to_numeric(d[ykey], errors="coerce").values
                        dy = np.gradient(y)
                        dx = np.gradient(x)
                        slope = np.abs(dy / (dx + 1e-12))
                        out = []
                        for s in slope:
                            if not np.isfinite(s):
                                out.append("Tight")
                            elif s < 0.15:
                                out.append("Flat")
                            elif s < 0.6:
                                out.append("Tight")
                            else:
                                out.append("Exhausted")
                        return pd.Series(out, index=d.index)
                    except Exception:
                        return pd.Series(["-"]*len(df), index=df.index)

                dfP_all["freedom_left"] = _classify_freedom(dfP_all)
                if len(dfP_robust):
                    dfP_robust["freedom_left"] = _classify_freedom(dfP_robust)

                st.caption("Front segments are annotated with dominant constraint and margin. Robust front is a conservative proxy filter; no uncertainty optimizer is used.")

                # Plot (matplotlib if available, else streamlit)
                try:
                    if _HAVE_MPL and plt is not None:
                        fig = plt.figure()
                        ax = fig.add_subplot(111)
                        # nominal (optionally color by categorical key)
                        if ckey in ("dominant_constraint", "intent") and ckey in dfP_all.columns:
                            cats = list(pd.Series(dfP_all[ckey]).fillna("(none)").astype(str).unique())
                            cmap_vals = pd.Series(dfP_all[ckey]).fillna("(none)").astype(str).map({c:i for i,c in enumerate(cats)})
                            sc = ax.scatter(dfP_all[xkey], dfP_all[ykey], c=cmap_vals, s=20, label="Nominal")
                            cb = fig.colorbar(sc, ax=ax)
                            cb.set_ticks(list(range(len(cats))))
                            cb.set_ticklabels(cats)
                            cb.set_label(ckey)
                        else:
                            ax.scatter(dfP_all[xkey], dfP_all[ykey], s=18, label="Nominal")
                        # robust overlay
                        if len(dfP_robust):
                            ax.scatter(dfP_robust[xkey], dfP_robust[ykey], s=26, marker="x", label="Robust (proxy)")
                        ax.set_xlabel(f"{xkey} [{_OBJ_CATALOG.get(xkey,{}).get('units','-')}]" )
                        ax.set_ylabel(f"{ykey} [{_OBJ_CATALOG.get(ykey,{}).get('units','-')}]" )
                        ax.grid(True, alpha=0.25)
                        ax.legend()
                        st.pyplot(fig, use_container_width=True)
                    else:
                        st.scatter_chart(dfP_all[[xkey, ykey]], x=xkey, y=ykey)
                except Exception:
                    st.scatter_chart(dfP_all[[xkey, ykey]], x=xkey, y=ykey)

                # -----------------------------
                # Pareto v2: world-class interpretability layers (still 0-D, still non-optimizing)
                # -----------------------------
                def _sense_sign(s: str) -> int:
                    return -1 if str(s).lower().strip() == "min" else 1

                def _pareto2(df: pd.DataFrame, xk: str, yk: str, sx: str, sy: str) -> pd.DataFrame:
                    """Return non-dominated set for 2 objectives (stable, no randomness)."""
                    if df is None or len(df) == 0:
                        return df
                    d = df[[c for c in df.columns if c in set(df.columns)]].copy()
                    x = pd.to_numeric(d[xk], errors="coerce").values
                    y = pd.to_numeric(d[yk], errors="coerce").values
                    m = np.isfinite(x) & np.isfinite(y)
                    d = d.loc[m].copy()
                    x = x[m]; y = y[m]
                    # Convert to minimization
                    if str(sx).lower() == "max":
                        x = -x
                    if str(sy).lower() == "max":
                        y = -y
                    order = np.lexsort((y, x))
                    x = x[order]; y = y[order]
                    d = d.iloc[order].copy()
                    best_y = np.inf
                    keep = []
                    for i in range(len(d)):
                        if y[i] < best_y - 1e-12:
                            keep.append(True)
                            best_y = y[i]
                        else:
                            keep.append(False)
                    return d.loc[keep].reset_index(drop=True)

                def _confidence_from_density(df_feas: pd.DataFrame, df_front: pd.DataFrame, xk: str, yk: str) -> pd.Series:
                    """Proxy confidence: kNN distance in objective space (smaller => higher confidence)."""
                    try:
                        if df_feas is None or len(df_feas) < 10 or df_front is None or len(df_front) == 0:
                            return pd.Series([np.nan]*len(df_front), index=df_front.index)
                        F = df_feas[[xk, yk]].copy()
                        P = df_front[[xk, yk]].copy()
                        Fx = pd.to_numeric(F[xk], errors="coerce").values
                        Fy = pd.to_numeric(F[yk], errors="coerce").values
                        Px = pd.to_numeric(P[xk], errors="coerce").values
                        Py = pd.to_numeric(P[yk], errors="coerce").values
                        mF = np.isfinite(Fx) & np.isfinite(Fy)
                        Fx, Fy = Fx[mF], Fy[mF]
                        k = int(max(5, min(25, len(Fx)//30)))
                        out = []
                        for (px, py) in zip(Px, Py):
                            if not (np.isfinite(px) and np.isfinite(py)):
                                out.append(np.nan); continue
                            d2 = (Fx - px)**2 + (Fy - py)**2
                            if len(d2) == 0:
                                out.append(np.nan); continue
                            kk = min(k, len(d2))
                            # partial sort
                            idx = np.argpartition(d2, kk-1)[:kk]
                            md = float(np.mean(np.sqrt(d2[idx]) + 1e-12))
                            out.append(md)
                        # invert and normalize
                        arr = np.asarray(out, dtype=float)
                        if np.all(~np.isfinite(arr)):
                            return pd.Series([np.nan]*len(df_front), index=df_front.index)
                        lo = np.nanmin(arr); hi = np.nanmax(arr)
                        conf = (hi - arr) / (hi - lo + 1e-12)
                        return pd.Series(conf, index=df_front.index)
                    except Exception:
                        return pd.Series([np.nan]*len(df_front), index=df_front.index)

                def _segment_ids(df_front: pd.DataFrame) -> pd.Series:
                    """Assign segment ids where dominant constraint is constant (cliff => new segment)."""
                    if df_front is None or len(df_front) == 0 or "dominant_constraint" not in df_front.columns:
                        return pd.Series([0]*len(df_front), index=df_front.index)
                    dom = df_front["dominant_constraint"].fillna("(none)").astype(str).values
                    seg = []
                    cur = 0
                    prev = dom[0] if len(dom) else "(none)"
                    for d0 in dom:
                        if d0 != prev:
                            cur += 1
                        seg.append(cur)
                        prev = d0
                    return pd.Series(seg, index=df_front.index)

                # Confidence halo + geography tags
                dfP_all = dfP_all.copy()
                dfP_all["confidence"] = _confidence_from_density(dfF_all, dfP_all, xkey, ykey)
                dfP_all["segment_id"] = _segment_ids(dfP_all)
                try:
                    # Geography = cognitive metaphor on frozen data (no new math)
                    geo = []
                    dom = dfP_all["dominant_constraint"].fillna("(none)").astype(str).values if "dominant_constraint" in dfP_all.columns else ["(none)"]*len(dfP_all)
                    for i in range(len(dfP_all)):
                        cliff = (i > 0 and dom[i] != dom[i-1])
                        if cliff:
                            geo.append("Cliff")
                        else:
                            # Ridge proxy: tight margin + stable constraint; Plain: flat + good margin
                            mm = float(dfP_all.iloc[i].get("min_constraint_margin", np.nan))
                            fl = str(dfP_all.iloc[i].get("freedom_left", "-"))
                            if np.isfinite(mm) and mm < max(0.05, robust_margin_thr*0.5):
                                geo.append("Ridge")
                            elif fl == "Flat" and (not np.isfinite(mm) or mm >= robust_margin_thr):
                                geo.append("Plain")
                            else:
                                geo.append("Slope")
                    dfP_all["geography"] = geo
                except Exception:
                    dfP_all["geography"] = ["-"]*len(dfP_all)

                # Quiet opinionated defaults: robust overlay on, fragile points visually de-emphasized
                st.markdown("### Pareto Interpretability Layers")

                # Question-driven exploration (wizard chooses views, not designs)
                qcols = st.columns([2, 2])
                with qcols[0]:
                    question = st.selectbox(
                        "What are you trying to learn?",
                        [
                            "Custom view",
                            "Where does robustness collapse?",
                            "Where is heat exhaust (q_div) limiting?",
                            "Where is stress (Ïƒ_vm) limiting?",
                            "Where is TBR policy shaping the trade-off?",
                            "Where do constraints switch (cliffs)?",
                        ],
                        index=0,
                        help="This changes *views and lenses only*. It does not select designs or optimize.",
                    )
                with qcols[1]:
                    focus_metrics = st.multiselect(
                        "Focus metrics (always shown in inspectors)",
                        options=["min_constraint_margin","dominant_constraint","q_div_MW_m2","sigma_vm_MPa","hts_margin_cs","TBR","Q_DT_eqv","P_e_net_MW","B_peak_T"],
                        default=["min_constraint_margin","dominant_constraint","q_div_MW_m2","sigma_vm_MPa"],
                        help="Personalization only: controls what the inspector emphasizes.",
                    )

                # Default lens settings (may be overridden by question-driven preset below)
                _def_geo, _def_conf, _def_policy, _def_teach = True, True, False, False
                if 'question' in locals() and question != "Custom view":
                    if question == "Where does robustness collapse?":
                        _def_geo, _def_conf, _def_policy, _def_teach = True, True, False, True
                    elif question == "Where is heat exhaust (q_div) limiting?":
                        _def_geo, _def_conf, _def_policy, _def_teach = True, True, False, True
                    elif question == "Where is stress (Ïƒ_vm) limiting?":
                        _def_geo, _def_conf, _def_policy, _def_teach = True, True, False, True
                    elif question == "Where is TBR policy shaping the trade-off?":
                        _def_geo, _def_conf, _def_policy, _def_teach = True, True, True, True
                    elif question == "Where do constraints switch (cliffs)?":
                        _def_geo, _def_conf, _def_policy, _def_teach = True, True, False, True

                l1, l2, l3, l4 = st.columns([1.2, 1.2, 1.2, 1.4])
                with l1:
                    show_geography = st.checkbox("Geography view", value=_def_geo, help="Terrain metaphor: ridges/cliffs/plains (purely descriptive).")
                with l2:
                    show_conf_halo = st.checkbox("Confidence halo", value=_def_conf, help="Density-based proxy confidence (no smoothing).")
                with l3:
                    show_policy = st.checkbox("Policy change compare", value=_def_policy, help="Compare fronts under explicit policy thresholds (filter-only lens).")
                with l4:
                    teaching_mode = st.checkbox("Teaching mode", value=_def_teach, help="Adds short callouts and guardrails; does not change results.")

                if teaching_mode:
                    st.caption("Tip: A Pareto front is a set of *non-dominated feasible designs*. This tool does not select or recommend designs.")

                # Segment-level explanation
                with st.expander("Explain a front segment", expanded=False):
                    st.caption("Segments are contiguous parts of the front pinned by the same dominant constraint (constraint-switches appear as cliffs).")
                    seg_ids = sorted(dfP_all["segment_id"].unique().tolist()) if len(dfP_all) else [0]
                    sel_seg = int(st.selectbox("Segment", options=seg_ids, index=0))
                    seg_df = dfP_all[dfP_all["segment_id"] == sel_seg].copy()
                    if len(seg_df):
                        domc = str(seg_df["dominant_constraint"].iloc[0]) if "dominant_constraint" in seg_df.columns else "(unknown)"
                        st.write(f"**Dominant constraint:** `{domc}`  |  Points: {len(seg_df)}")
                        # identify strongest driver variable among sampled knobs
                        drivers = [k for k in ("R0_m", "Bt_T", "Ip_MA", "fG") if k in seg_df.columns]
                        driver_msg = ""
                        try:
                            targ = ykey
                            corrs = []
                            for dv in drivers:
                                a = pd.to_numeric(seg_df[dv], errors="coerce")
                                b = pd.to_numeric(seg_df[targ], errors="coerce")
                                if a.notna().sum() > 3 and b.notna().sum() > 3:
                                    corrs.append((dv, float(a.corr(b))))
                            corrs = [c for c in corrs if np.isfinite(c[1])]
                            if corrs:
                                dv, cc = sorted(corrs, key=lambda kv: -abs(kv[1]))[0]
                                driver_msg = f"Within this segment, `{targ}` is most correlated with `{dv}` (corrâ‰ˆ{cc:.2f})."
                        except Exception:
                            pass
                        if driver_msg:
                            st.caption(driver_msg)
                        # causal chain template (descriptive)
                        chain = f"In this segment, pushing `{ykey}` tends to move designs along the front until `{domc}` becomes limiting. "
                        chain += "This reflects coupled 0-D physics and engineering proxies; it is descriptive, not a recommendation."
                        st.write(chain)
                    else:
                        st.info("No points in selected segment.")

                # Objective relevance lens
                with st.expander("Objective relevance lens", expanded=False):
                    st.caption("Shows where objectives genuinely shape the front vs. where they are mostly redundant or flat (descriptive).")
                    try:
                        rel = []
                        for ok in objectives.keys():
                            if ok not in dfP_all.columns:
                                continue
                            vF = float(pd.to_numeric(dfF_all.get(ok, pd.Series([],dtype=float)), errors="coerce").var()) if len(dfF_all) else np.nan
                            vP = float(pd.to_numeric(dfP_all.get(ok, pd.Series([],dtype=float)), errors="coerce").var()) if len(dfP_all) else np.nan
                            ratio = (vP / (vF + 1e-12)) if (np.isfinite(vP) and np.isfinite(vF)) else np.nan
                            rel.append({"objective": ok, "var_front": vP, "var_feasible": vF, "relevance_ratio": ratio})
                        rel_df = pd.DataFrame(rel).sort_values("relevance_ratio", ascending=False)
                        st.dataframe(rel_df, use_container_width=True, hide_index=True)
                        if teaching_mode and len(rel_df):
                            st.caption("High relevance_ratio means the objective varies significantly along the front; low means it may be redundant in this domain.")
                    except Exception:
                        st.info("Relevance lens unavailable.")

                # Policy change compare (filter-only lens; no physics changes)
                dfP_policy = None
                if show_policy:
                    with st.expander("Policy change compare (filter-only lens)", expanded=False):
                        st.caption("This is a policy lens: it filters feasible points using explicit thresholds, then recomputes the non-dominated set. No constraints are relaxed; no evaluator changes are made here.")
                        p1, p2, p3, p4 = st.columns(4)
                        with p1:
                            tbr_min = float(st.number_input("TBR â‰¥", value=1.10, step=0.01))
                        with p2:
                            sigma_max = float(st.number_input("Ïƒ_vm â‰¤ [MPa]", value=700.0, step=10.0))
                        with p3:
                            qdiv_max = float(st.number_input("q_div â‰¤ [MW/mÂ²]", value=10.0, step=0.5))
                        with p4:
                            hts_min = float(st.number_input("HTS margin â‰¥", value=0.10, step=0.01))
                        dpol = dfF_all.copy()
                        if "TBR" in dpol.columns:
                            dpol = dpol[pd.to_numeric(dpol["TBR"], errors="coerce") >= tbr_min]
                        if "sigma_vm_MPa" in dpol.columns:
                            dpol = dpol[pd.to_numeric(dpol["sigma_vm_MPa"], errors="coerce") <= sigma_max]
                        if "q_div_MW_m2" in dpol.columns:
                            dpol = dpol[pd.to_numeric(dpol["q_div_MW_m2"], errors="coerce") <= qdiv_max]
                        if "hts_margin_cs" in dpol.columns:
                            dpol = dpol[pd.to_numeric(dpol["hts_margin_cs"], errors="coerce") >= hts_min]
                        dfP_policy = _pareto2(dpol, xkey, ykey, objectives.get(xkey,"min"), objectives.get(ykey,"min"))
                        st.write(f"Policy-filtered feasible points: {len(dpol)} | policy-front points: {len(dfP_policy) if dfP_policy is not None else 0}")

                # Reference fronts (runtime presets; deterministic)
                with st.expander("Reference fronts (runtime presets)", expanded=False):
                    st.caption("Generate canonical reference fronts for comparison (deterministic presets; no hidden scoring).")
                    ref = st.selectbox("Reference family", ["None", "ITER-like", "SPARC-like", "ARC-like"], index=0)
                    if ref != "None":
                        st.info("Reference fronts are generated on demand using fixed presets and the frozen evaluator. This does not recommend designs.")
                    # This upgrade only provides UI hooks; generation uses the same run button / settings.

                # No free lunch detector
                with st.expander("No free lunch detector", expanded=False):
                    st.caption("Flags regions where both selected objectives appear to improve together - often due to projection, redundancy, or shared drivers.")
                    try:
                        sx = objectives.get(xkey, "min"); sy = objectives.get(ykey, "min")
                        dx = _sense_sign(sx); dy = _sense_sign(sy)
                        d = dfP_all.sort_values(xkey).copy()
                        x = pd.to_numeric(d[xkey], errors="coerce").values
                        y = pd.to_numeric(d[ykey], errors="coerce").values
                        good = []
                        for i in range(1, len(d)):
                            if not (np.isfinite(x[i-1]) and np.isfinite(x[i]) and np.isfinite(y[i-1]) and np.isfinite(y[i])):
                                continue
                            imp_x = dx*(x[i]-x[i-1]) < 0  # improvement
                            imp_y = dy*(y[i]-y[i-1]) < 0
                            if imp_x and imp_y:
                                good.append(int(i))
                        if good:
                            st.warning(f"Detected {len(good)} local steps where both objectives improve together. This may indicate redundancy/shared drivers or a misleading projection.")
                            if teaching_mode:
                                st.caption("Try changing axes, adding a third objective, or inspecting dominance and segment explanations to interpret this correctly.")
                        else:
                            st.success("No obvious 'free lunch' steps detected along the chosen front ordering.")
                    except Exception:
                        st.info("Detector unavailable.")

                # Narrative timeline along the front
                # Knee candidates (descriptive, not "best")
                # Knee candidates (descriptive, not "best")
                with st.expander("Notable compromise regions (knee candidates)", expanded=False):
                    st.caption("Highlights regions of the front where trade-offs tighten rapidly (geometric knee proxy). These are **not** recommendations.")
                    if len(dfP_all) >= 5:
                        try:
                            d = dfP_all[[xkey, ykey, "segment_id", "dominant_constraint", "min_constraint_margin", "confidence", "intent"]].copy()
                            d = d.sort_values(xkey).reset_index(drop=True)
                            x = pd.to_numeric(d[xkey], errors="coerce").values
                            y = pd.to_numeric(d[ykey], errors="coerce").values
                            m = np.isfinite(x) & np.isfinite(y)
                            d = d.loc[m].reset_index(drop=True)
                            x = x[m]; y = y[m]
                            if len(d) >= 5:
                                xn = (x - np.min(x)) / (np.ptp(x) + 1e-12)
                                yn = (y - np.min(y)) / (np.ptp(y) + 1e-12)
                                kappa = np.zeros(len(d))
                                for ii in range(1, len(d)-1):
                                    v1 = np.array([xn[ii]-xn[ii-1], yn[ii]-yn[ii-1]])
                                    v2 = np.array([xn[ii+1]-xn[ii], yn[ii+1]-yn[ii]])
                                    n1 = np.linalg.norm(v1) + 1e-12
                                    n2 = np.linalg.norm(v2) + 1e-12
                                    ang = np.arccos(np.clip(np.dot(v1, v2)/(n1*n2), -1.0, 1.0))
                                    kappa[ii] = float(ang)
                                d["knee_score"] = kappa
                                topk = d.sort_values("knee_score", ascending=False).head(min(8, len(d)))
                                st.dataframe(topk, use_container_width=True, hide_index=True)
                                if teaching_mode:
                                    st.caption("High knee_score means the front bends sharply in the chosen projection; confirm with segment explanations and dominance to avoid projection traps.")
                            else:
                                st.info("Not enough clean points for knee scoring.")
                        except Exception:
                            st.info("Knee scoring unavailable for this run.")
                    else:
                        st.info("Need at least 5 Pareto points to compute knee candidates.")

                with st.expander("Pareto timeline (scrub along the front)", expanded=False):
                    st.caption("Scrub a slider along the front to see objective/constraint transitions as a narrative.")
                    if len(dfP_all):
                        k = int(st.slider("Front index", min_value=0, max_value=max(len(dfP_all)-1, 0), value=min(0, len(dfP_all)-1), step=1))
                        row = dfP_all.iloc[k].to_dict()
                        st.write(f"**Index {k}** | segment={row.get('segment_id')} | geography={row.get('geography')} | dominant={row.get('dominant_constraint')} | margin={row.get('min_constraint_margin')}")
                        st.json({xkey: row.get(xkey), ykey: row.get(ykey), "intent": row.get("intent"), "dominant_constraint": row.get("dominant_constraint"), "confidence": row.get("confidence")}, expanded=False)
                    else:
                        st.info("No front points to scrub.")
                # Point Inspector
                with st.expander("Pareto Point Inspector", expanded=False):
                    idx = int(st.number_input("Row index (in table above)", min_value=0, max_value=max(len(dfP_all)-1,0), value=0, step=1))
                    try:
                        row = dfP_all.iloc[idx].to_dict()
                        st.json(row, expanded=False)
                        # Focus metrics (personalized)
                        try:
                            _fm = focus_metrics if "focus_metrics" in locals() else ["min_constraint_margin","dominant_constraint"]
                            _show = {k: row.get(k) for k in _fm if k in row}
                            if _show:
                                st.markdown("**Focus metrics**")
                                st.dataframe(pd.DataFrame([_show]), use_container_width=True, hide_index=True)
                        except Exception:
                            pass

                        cA, cB, cC = st.columns([1, 1, 1])
                        with cA:
                            if st.button("View in Scan Lab", use_container_width=True, key="pareto_to_scanlab"):
                                # Best-effort cross-link: store a focus payload and instruct user to switch tab.
                                st.session_state.scanlab_focus_from_pareto = {
                                    "inputs": {k: row.get(k) for k in ("R0_m", "Bt_T", "Ip_MA", "fG") if k in row},
                                    "objectives": {k: row.get(k) for k in objectives.keys()},
                                    "intent": str(row.get("intent", intent_mode)),
                                    "dominant_constraint": row.get("dominant_constraint"),
                                    "min_constraint_margin": row.get("min_constraint_margin"),
                                }
                                st.info("Stored a Scan Lab focus hint in session state. Switch to Scan Lab to view the highlighted context.")
                        with cB:
                            st.caption("Read-only: no auto-apply.")
                        st.caption("To inspect physics/constraints deeply, paste these inputs into Point Designer or Systems Mode. Pareto does not auto-apply.")
                        with cC:
                            if st.button("Queue in Systems Mode", use_container_width=True, key="pareto_to_systems"):
                                # Queue a reversible base-apply payload for Systems Mode (no solving here).
                                st.session_state.systems_pending_base_apply = {k: row.get(k) for k in ("R0_m","a_m","kappa","Bt_T","Ip_MA","Ti_keV","fG","Paux_MW","t_shield_m") if k in row}
                                st.session_state.systems_pending_base_apply_source = "Pareto Lab point"
                                st.info("Queued. Switch to Systems Mode and review the pending Apply card (reversible).")

                    except Exception:
                        st.info("Select a valid row index.")

                # Narrative summary (deterministic)
                with st.expander("Trade-off summary (deterministic)", expanded=False):
                    try:
                        dom_counts = dfP_all["dominant_constraint"].value_counts().to_dict() if "dominant_constraint" in dfP_all.columns else {}
                        dom_top = sorted(dom_counts.items(), key=lambda kv: -kv[1])[:3]
                        msg = ""
                        if dom_top:
                            msg += "Dominant-limiting segments (by count along the front): " + ", ".join([f"{k} ({v})" for k,v in dom_top]) + ". "
                        msg += f"Freedom-left classification along chosen axes: Flat={int((dfP_all['freedom_left']=='Flat').sum())}, Tight={int((dfP_all['freedom_left']=='Tight').sum())}, Exhausted={int((dfP_all['freedom_left']=='Exhausted').sum())}. "
                        msg += "These statements are descriptive; no design choice is implied."
                        st.session_state.pareto_narrative_summary = msg
                        st.write(msg)
                    except Exception:
                        st.write("Summary unavailable.")

                # Export artifact (JSON) + CSV
                c1, c2 = st.columns(2)
                with c1:
                    st.download_button(
                        "Download Pareto front (CSV)",
                        data=dfP_all.to_csv(index=False),
                        file_name="shams_pareto_front.csv",
                        mime="text/csv",
                        use_container_width=True,
                    )
                with c2:
                    art = {
                        "schema": "shams.pareto.v1",
                        "version": str(st.session_state.get("app_version","")),
                        "intent_mode": intent_mode,
                        "objectives": {k: {"sense": v, **_OBJ_CATALOG.get(k, {})} for k, v in objectives.items()},
                        "bounds": bounds,
                        "seed": seed,
                        "n_samples": n_samples,
                        "robust_margin_thr": robust_margin_thr,
                        "feasible": dfF_all.to_dict(orient="records") if len(dfF_all) else [],
                        "pareto": dfP_all.to_dict(orient="records") if len(dfP_all) else [],
                    }
                    st.download_button(
                        "Download Pareto artifact (JSON)",
                        data=json.dumps(art, indent=2, sort_keys=True),
                        file_name="shams_pareto_artifact.json",
                        mime="application/json",
                        use_container_width=True,
                    )
                    # Publication-ready export pack (zip): artifact + CSVs + narrative
                    try:
                        import io, zipfile as _zip
                        buf = io.BytesIO()
                        with _zip.ZipFile(buf, "w", compression=_zip.ZIP_DEFLATED) as zf:
                            zf.writestr("pareto/shams_pareto_artifact.json", json.dumps(art, indent=2, sort_keys=True))
                            if len(dfP_all):
                                zf.writestr("pareto/pareto_front.csv", dfP_all.to_csv(index=False))
                            if len(dfF_all):
                                zf.writestr("pareto/feasible_set_sampled.csv", dfF_all.to_csv(index=False))
                            # Narrative summary
                            zf.writestr("pareto/narrative_summary.md", str(st.session_state.get("pareto_narrative_summary","")).strip() or "(no narrative)")
                            # Repro capsule
                            zf.writestr("pareto/README.md", "\n".join([
                                "# SHAMS Pareto Publication Pack",
                                "",
                                "- Includes the JSON artifact (authoritative), CSV exports, and the deterministic narrative summary.",
                                "- Pareto Mode is feasible-only and non-optimizing.",
                                "",
                                f"- intent_mode: {intent_mode}",
                                f"- n_samples: {n_samples}",
                                f"- seed: {seed}",
                                f"- objectives: {list(objectives.keys())}",
                            ]))
                        buf.seek(0)
                        st.download_button(
                            "Download publication pack (.zip)",
                            data=buf.getvalue(),
                            file_name="shams_pareto_publication_pack.zip",
                            mime="application/zip",
                            use_container_width=True,
                        )
                    except Exception:
                        pass
                    # Interactive reproducibility capsule (trust panel)
                    with st.expander("Reproducibility capsule (what is guaranteed / what is not)", expanded=False):
                        st.caption("This makes Pareto audit-ready. It describes guarantees and limitations of the current run (no hidden assumptions).")
                        st.markdown("**Guaranteed**")
                        st.markdown("- Feasibility is evaluated by the frozen Point Designer evaluator (intent-aware).")
                        st.markdown("- Pareto uses explicit objectives only (Objective Contract).")
                        st.markdown("- Sampling is seeded; reruns with same seed/bounds/objectives are reproducible up to floating-point nondeterminism.")
                        st.markdown("**Not guaranteed**")
                        st.markdown("- Global coverage: the front is only as complete as the sampling density.")
                        st.markdown("- Projection honesty: 2D views can hide higher-dimensional dominance changes.")
                        st.markdown("- Counterfactual/policy lenses are filter-only overlays; they do not modify evaluator physics.")
                        st.divider()
                        st.json({
                            "version": str(st.session_state.get("app_version","")),
                            "intent_mode": intent_mode,
                            "seed": seed,
                            "n_samples": n_samples,
                            "bounds": bounds,
                            "objectives": objectives,
                        }, expanded=False)

        except Exception as e:
            st.error(f"Pareto study error: {e}")

    # -----------------------------
    


    # --- Freeze badge (Pareto) ---
    with st.container():
        st.caption(" Pareto Mode v1.0 - Frozen. Descriptive trade-off cartography only.")
        try:
            _pf = (Path(__file__).resolve().parent.parent / "docs" / "PARETO_V1_FREEZE_DECLARATION.md").read_text(encoding="utf-8")
        except Exception:
            _pf = "(missing docs/PARETO_FREEZE.md)"
        st.caption(FREEZE_STAMP)
        st.download_button("Download Pareto Freeze Statement", data=_pf, file_name="PARETO_V1_FREEZE_DECLARATION.md", mime="text/markdown", use_container_width=False)


if _deck == "ðŸ§ª Trade Study Studio":
    # DSG: auto edge-kind tagging by active panel (exploration only)
    if bool(st.session_state.get("dsg_edge_kind_auto", True)):
        st.session_state["dsg_context_edge_kind"] = "trade"

    try:
        from ui.trade_study_studio import render_trade_study_studio
        render_trade_study_studio(st, repo_root=Path(__file__).resolve().parent.parent)
    except Exception as e:
        st.error(f"Trade Study Studio failed to load: {e}")


if _deck == "âš’ï¸ Reactor Design Forge":
    st.header("âš’ï¸ Reactor Design Forge")
    st.caption("Concept assembly + candidate archives + traces. Feeds the frozen evaluator; does not replace it.")
    render_mode_scope("forge")

    # --- Legacy v93 stateful download compatibility (read-only) ---
    try:
        _v93_stateful_sandbox_panel()
    except Exception:
        pass

    st.subheader("ðŸ§­ Forge Bridgehead")
    # v208: Review Mode (review-room posture; no knobs/search actions)
    if "forge_review_mode" not in st.session_state:
        st.session_state["forge_review_mode"] = False
    st.session_state["forge_review_mode"] = st.toggle(
        "Review Mode (locks exploration controls)",
        value=bool(st.session_state.get("forge_review_mode")),
        key="forge_review_mode_toggle",
        help="Review Mode is a UI posture: inputs are locked; only review artifacts and comparisons are shown.",
    )

    if bool(st.session_state.get("forge_review_mode")):
        st.info(
            f"{_LANG.get('non_prescriptive_banner')}  \
{_LANG.get('margin_first')}",
            icon="ðŸ§‘â€âš–ï¸",
        )
    else:
        st.caption("Candidate-generation workspace (external to truth): global pattern â†’ surrogate acceleration â†’ local refinement. All candidates are audited by the frozen evaluator. No relaxation; no auto-apply.")
    st.info(
        "Non-authoritative workspace: this mode produces **candidate archives** + **traces**. "
        "Truth remains in the frozen evaluator. Nothing is applied automatically.",
        icon="ðŸ§±",
    )

    # v208: Review Mode locks exploration controls (read-only review posture).
    forge_lock = bool(st.session_state.get("forge_review_mode"))

    # --- Forge deck (no scroll walls) ---
    _forge_deck = st.radio(
        "Forge deck",
        options=["ðŸ§© Intent Compiler", "ðŸš€ Machine Finder", "ðŸ” Capsules"],
        index=0,
        horizontal=True,
        key="forge_deck",
        help="Deck-based navigation: render one Forge workspace at a time (no scroll walls).",
    )


    # --- Imports (local to keep UI start-up fast) ---
    from src.models.inputs import PointInputs
    from physics.hot_ion import hot_ion_point
    from constraints.system import build_constraints_from_outputs
    from tools.process_compat.process_compat import (
        constraints_to_records,
        active_constraints,
        feasibility_flag,
        failure_mode,
    )
    from tools.sandbox.hybrid_engine import (
        Objective, VarSpec, run_hybrid_machine_finder,
        global_de_phase, surrogate_phase, local_refine_phase, surface_surf_phase,
        build_archive, resistance_atlas, variable_correlations, build_feasibility_skeleton,
    )
    from tools.sandbox.optimizer_engines import default_objective_packs
    from tools.sandbox.feasibility_ladder import classify_candidate
    from tools.sandbox.resistance_report import build_resistance_report
    from tools.sandbox.persistence import save_run_capsule_v2
    from tools.sandbox.export_capsule import export_run_capsule_zip
    from tools.sandbox.export_capsule import import_run_capsule_zip
    from tools.sandbox.persistence import load_run_capsule_v2, diff_capsules
    from tools.sandbox.advanced_features import constraint_surface_map
    from tools.sandbox.conflict_atlas import new_atlas, update_atlas, summarize_atlas
    from tools.sandbox.design_navigation import steering_cues_from_surface_map, filter_cues
    from tools.sandbox.lineage_graph import build_lineage_edges, compute_tree_layout
    from tools.sandbox.spend_map import build_spend_scatter
    from tools.sandbox.robustness_envelope import robustness_envelope_from_records
    from tools.sandbox.narrative_pack import build_narrative
    from tools.sandbox.design_card import build_design_card_md
    from tools.sandbox.existence_report import existence_report
    from tools.sandbox.archive_intelligence import ladder_histogram, regime_clusters_summary
    from tools.sandbox.confidence_sweep import confidence_sweep
    from tools.sandbox.design_packet import build_design_packet_files
    from tools.sandbox.review_room import build_review_trinity, build_attack_simulation

    # v203 Reactor Design Forge: PROCESS-independence instruments
    from tools.sandbox.closure_console import closure_console
    from tools.sandbox.margin_budget import margin_budget
    from tools.sandbox.reality_gates import reality_gates
    from tools.sandbox.report_pack import build_report_pack


    from src.economics.cost import cost_proxies
    # Tier 5â€“6 instruments
    from tools.sandbox.tier56 import (
        ConstraintCred,
        apply_credibility_overlay,
        counterfactual_gate,
        build_intent_trajectory,
        why_not_report,
        discovered_relations,
        export_relations_markdown,
        inverse_design_residual,
    )

    # Tier 7 + Epistemic guarantees (collaboration + standards)
    from tools.sandbox.tier7 import (
        repo_fingerprint,
        candidate_fingerprint,
        generate_cert_badge_svg,
        export_doi_ready_pack,
        new_review_session,
        default_sessions_dir,
        save_review_session,
        load_review_session,
        export_review_session_zip,
        import_review_session_zip,
        run_regression_suite,
    )

    # Tier 8â€“9: design-space jurisprudence, intent-conditional laws, genealogy, counter-optimization
    from tools.sandbox.tier89 import (
        feasibility_confidence_from_trace,
        candidate_verdict,
        region_verdict,
        intent_conditional_laws,
        reconstruct_genealogy,
        counter_optimization_report,
    )

    

    # -------------------------
    # Intent Compiler (v285.0)
    # -------------------------
    if _forge_deck == "ðŸ§© Intent Compiler":
        st.markdown("### ðŸ§© Intent Compiler")
        st.caption("Deterministic algebraic compilation from intent â†’ candidate PointInputs. Produces candidates only; truth remains in Point Designer.")

        try:
            from tools.sandbox.intent_compiler import compile_intent_to_candidate
        except Exception as _e:
            st.error(f"Intent compiler import failed: {_e}")
            compile_intent_to_candidate = None  # type: ignore

        # Use last Point Designer inputs as base if available
        _base_obj = st.session_state.get('pd_last_inputs_obj')
        if _base_obj is None:
            _base_obj = st.session_state.get('base_point_inputs_obj')
        if not isinstance(_base_obj, PointInputs):
            # fall back to a safe, minimal default
            _base_obj = PointInputs(R0_m=3.0, a_m=1.0, kappa=1.8, Bt_T=12.0, Ip_MA=8.0, Ti_keV=10.0, fG=0.8, Paux_MW=20.0)

        c1, c2 = st.columns(2)
        Pfus = c1.number_input('Target fusion power P_fus (MW)', value=140.0, step=10.0, min_value=0.0)
        Q = c2.number_input('Target Q (proxy)', value=2.0, step=0.1, min_value=0.01)

        st.markdown('**Optional direct overrides (applied after compilation)**')
        o1, o2, o3, o4 = st.columns(4)
        o_R0 = o1.number_input('Override R0 (m) (0=ignore)', value=0.0, step=0.1)
        o_a  = o2.number_input('Override a (m) (0=ignore)', value=0.0, step=0.05)
        o_Bt = o3.number_input('Override Bt (T) (0=ignore)', value=0.0, step=0.5)
        o_Ip = o4.number_input('Override Ip (MA) (0=ignore)', value=0.0, step=0.5)

        overrides = {}
        if o_R0 > 0: overrides['R0_m'] = float(o_R0)
        if o_a  > 0: overrides['a_m']  = float(o_a)
        if o_Bt > 0: overrides['Bt_T'] = float(o_Bt)
        if o_Ip > 0: overrides['Ip_MA'] = float(o_Ip)

        if st.button('Compile candidate', type='primary', use_container_width=True, disabled=(compile_intent_to_candidate is None)):
            status, payload = compile_intent_to_candidate(_base_obj, Pfus_target_MW=float(Pfus), Q_target=float(Q), overrides=overrides)
            st.session_state['forge_intent_compiler_last'] = {'status': status, **payload}

        last = st.session_state.get('forge_intent_compiler_last')
        if isinstance(last, dict):
            st.info(f"Compiler status: **{last.get('status','?')}**")
            if last.get('reason'):
                st.error(str(last.get('reason')))
            if last.get('trace'):
                with st.expander('Compilation trace', expanded=False):
                    for ln in list(last.get('trace') or []):
                        st.markdown(f"- {ln}")
            cand = last.get('candidate_inputs')
            if isinstance(cand, dict):
                with st.expander('Candidate inputs (dict)', expanded=False):
                    st.json(cand)
                if st.button('Apply candidate in Point Designer', use_container_width=True):
                    st.session_state['pd_candidate_apply'] = dict(cand)
                    st.success('Candidate applied: go to "ðŸ§­ Point Designer" and press Evaluate.')

        # End Intent Compiler deck

# -------------------------
    # Replay / Diff (capsules)
    # -------------------------
    if _forge_deck == "ðŸ” Capsules":
        with st.expander("ðŸ” Replay & Diff â€” Run Capsules", expanded=False):
            st.caption("Load a previously exported Optimization Run Capsule (.zip or .json) to restore the Workbench. "
                       "This is metadata replay; truth remains the frozen evaluator.")
    
            c1, c2 = st.columns(2)
            up1 = c1.file_uploader("Restore capsule", type=["zip", "json"], key="opt_restore_capsule")
            if up1 is not None:
                try:
                    # zip capsule
                    if str(up1.name).lower().endswith(".zip"):
                        tmp = Path(".shams_state") / "_uploads"
                        tmp.mkdir(parents=True, exist_ok=True)
                        p = tmp / str(up1.name)
                        p.write_bytes(up1.getbuffer())
                        data = import_run_capsule_zip(p)
                        capsule = data.get("capsule") or {}
                    else:
                        capsule = json.loads(up1.getvalue().decode("utf-8"))
                    if str(capsule.get("schema")) != "shams.opt_sandbox.run_capsule.v2":
                        st.error(f"Unsupported capsule schema: {capsule.get('schema')}")
                    else:
                        st.session_state["opt_workbench_run"] = {
                            "kind": "optimization_sandbox_replay",
                            "intent": capsule.get("intent"),
                            "seed": capsule.get("seed", 1),
                            "objectives": capsule.get("lens", {}).get("objectives", []),
                            "var_specs": capsule.get("var_specs", []),
                            "budgets": {"bounds": capsule.get("bounds", {})},
                            "archive": capsule.get("archive", []),
                            "trace": capsule.get("trace", []),
                            "telemetry": capsule.get("telemetry", {}),
                            "resistance_report": capsule.get("resistance_report"),
                            "capsule_v2": capsule,
                            "non_authoritative_notice": "Replayed from capsule. Truth remains the frozen evaluator.",
                        }
                        st.success("Capsule restored into the Workbench.")
                except Exception as e:
                    st.error(f"Failed to restore capsule: {e}")
    
            st.markdown("---")
            st.caption("Diff two capsules (lens/bounds/counts/ladder histogram).")
            upA = c1.file_uploader("Capsule A", type=["json"], key="opt_diff_a")
            upB = c2.file_uploader("Capsule B", type=["json"], key="opt_diff_b")
            if upA is not None and upB is not None:
                try:
                    ca = json.loads(upA.getvalue().decode("utf-8"))
                    cb = json.loads(upB.getvalue().decode("utf-8"))
                    d = diff_capsules(ca, cb)
                    st.json(d)
                except Exception as e:
                    st.error(f"Diff failed: {e}")
    
        st.stop()

    # -------------------------
    # Evaluator (frozen truth)
    # -------------------------
    def _evaluate_candidate(inp: dict, intent: str) -> dict:
        """Audit a candidate with frozen evaluator. Returns a rich dict for archive/trace."""
        # Build PointInputs (fills defaults and validates)
        pi = PointInputs(**inp)
        outputs = hot_ion_point(pi)
        cons = build_constraints_from_outputs(outputs, design_intent=intent)
        records = constraints_to_records(cons)
        feas = feasibility_flag(records, design_intent=intent)
        act = active_constraints(records, design_intent=intent)
        fm = failure_mode(records, design_intent=intent)
        # compute min margin
        min_sm = None
        for r in records:
            try:
                sm = float(r.get("signed_margin"))
            except Exception:
                continue
            if min_sm is None or sm < min_sm:
                min_sm = sm
        # optional cost proxies (pure outputs->cost)
        cost = cost_proxies(outputs) if isinstance(outputs, dict) else {}
        closure_bundle = closure_console(outputs=outputs, cost_proxy=cost) if isinstance(outputs, dict) else {"ok": False, "reason": "outputs_not_dict"}
        mb = margin_budget(records)
        rg = reality_gates(records, closure_bundle if isinstance(closure_bundle, dict) else None)
        rp = build_report_pack(intent=str(intent), inputs=dict(inp), outputs=outputs, constraints=records, closure_bundle=closure_bundle if isinstance(closure_bundle, dict) else None, margin_budget=mb, reality_gates=rg)

        return {
            "inputs": dict(inp),
            "outputs": outputs,
            "constraints": records,
            "feasible": bool(feas),
            "active_constraints": act,
            "failure_mode": fm,
            "min_signed_margin": float(min_sm) if min_sm is not None else float("nan"),
            "cost": cost,
            "closure_bundle": closure_bundle,
            "margin_budget": mb,
            "reality_gates": rg,
            "report_pack": rp,
            "closure_certificate": (rp.get("json") or {}).get("closure_certificate"),
            "design_class": (rp.get("json") or {}).get("design_class"),
            "citation_blocks": (rp.get("json") or {}).get("citation_blocks"),
            "reference_context": (rp.get("json") or {}).get("reference_context"),

        }

    # -------------------------
    # Guided runs (user friendly)
    # -------------------------
    st.markdown("### ðŸŽ¯ Intent & Lens (explicit contract)")
    st.caption("Pick a goal pack; SHAMS will *show the exact objectives and bounds* before running.")

    # Intent selection
    intent_label = st.selectbox("Design intent", ["Power Reactor (net-electric)", "Experimental Device (research)"], index=0, key="opt_intent")

    # Internal canonical intent key (feeds constraint policy)
    intent = "Reactor" if intent_label.lower().startswith("power") else "Research"

    # Objective packs (explicit)
    packs = default_objective_packs(intent)
    pack_names = [p.name for p in packs] + ["Custom (manual objectives)"]
    pack_choice = st.selectbox("Objective pack", pack_names, index=0, key="opt_pack_choice")

    # Anchor: either current Point Designer inputs (if available) or a sensible baseline
    anchor_default = {}
    if "point_inputs_last" in st.session_state and isinstance(st.session_state["point_inputs_last"], dict):
        anchor_default = dict(st.session_state["point_inputs_last"])
    # fallback: minimal anchor (PointInputs will fill defaults)
    if not anchor_default:
        anchor_default = {"R0_m": 6.2, "a_m": 2.0, "kappa": 1.8, "delta": 0.33, "Bt_T": 5.3, "Ip_MA": 15.0, "Paux_MW": 50.0}

    # Choose variables + bounds (table-style)
    st.markdown("### ðŸ§¬ Degrees of Freedom (search space)")
    st.caption("You control what the machine finder is allowed to change. Frozen variables never move.")

    default_vars = ["R0_m", "Bt_T", "Ip_MA", "Paux_MW"]
    all_keys = list(anchor_default.keys())
    # Add common knobs even if absent
    for k in ["R0_m","a_m","kappa","delta","Bt_T","Ip_MA","Paux_MW","nbar_1e20_m3","Ti_keV"]:
        if k not in all_keys:
            all_keys.append(k)

    var_keys = st.multiselect("Variables to optimize", options=all_keys, default=default_vars, key="opt_var_keys")
    st.caption("Tip: start with 3â€“5 variables for stability; expand later.")

    # Bounds helper
    bound_mode = st.radio("Bounds mode", ["Tight (Â±10%)", "Medium (Â±20%)", "Wide (Â±35%)", "Custom"], index=1, horizontal=True, key="opt_bound_mode")
    frac = {"Tight (Â±10%)":0.10, "Medium (Â±20%)":0.20, "Wide (Â±35%)":0.35}.get(bound_mode, 0.20)

    bounds = {}
    for k in var_keys:
        v0 = float(anchor_default.get(k, 0.0))
        if bound_mode != "Custom":
            lo, hi = v0*(1-frac), v0*(1+frac)
        else:
            lo, hi = v0*(1-0.2), v0*(1+0.2)
        bounds[k] = (lo, hi)

    with st.expander("Edit bounds (table)", expanded=False):
        cols = st.columns([2,2,2])
        cols[0].markdown("**Variable**")
        cols[1].markdown("**Min**")
        cols[2].markdown("**Max**")
        for k in var_keys:
            lo, hi = bounds[k]
            c1, c2, c3 = st.columns([2,2,2])
            c1.write(k)
            lo2 = c2.number_input(f"{k}_lo", value=float(lo), key=f"b_lo_{k}")
            hi2 = c3.number_input(f"{k}_hi", value=float(hi), key=f"b_hi_{k}")
            if hi2 < lo2:
                hi2 = lo2
            bounds[k] = (float(lo2), float(hi2))

    # Objectives display + custom editor
    if pack_choice != "Custom (manual objectives)":
        pack = next(p for p in packs if p.name == pack_choice)
        objectives = [Objective(**o.__dict__) for o in pack.objectives]
        st.info(f"**Pack:** {pack.description}")
    else:
        st.caption("Custom objectives: add 1â€“3 objectives. Sense: max/min. Weight: explicit.")
        obj_rows = st.number_input("Number of objectives", 1, 3, 2, key="opt_n_obj")
        objectives = []
        for i in range(int(obj_rows)):
            c1, c2, c3 = st.columns([3,1,1])
            key = c1.text_input(f"Objective {i+1} key", value=["P_e_net_MW","Q_DT_eqv","q_div_MW_m2"][i] if i<3 else "Q_DT_eqv", key=f"obj_key_{i}")
            sense = c2.selectbox(f"Sense {i+1}", ["max","min"], index=0, key=f"obj_sense_{i}")
            weight = c3.number_input(f"Weight {i+1}", value=1.0, key=f"obj_w_{i}")
            objectives.append(Objective(key=key, sense=sense, weight=float(weight)))


    # Program Lens (objective contract) - explicit, exported (no hidden ranking)
    lens_contract = {
        "name": str(pack_choice),
        "description": str(pack.description) if pack_choice != "Custom (manual objectives)" else "Custom objectives (manual)",
        "intent": str(intent),
        "objectives": [{"key": o.key, "sense": o.sense, "weight": float(o.weight)} for o in (objectives or [])],
    }
    st.session_state["opt_lens_contract"] = lens_contract

    # Optional costing layer
    st.markdown("### ðŸ’¸ Transparent costing layer (optional)")
    use_cost = st.checkbox("Enable cost proxies in objectives/filters (transparent)", value=False, key="opt_use_cost")
    if use_cost:
        st.caption("Cost proxies are computed from outputs; assumptions are explicit and exported with the run.")

    # Budgets / engine tuning (simple)
    st.markdown("### âš™ï¸ Run budget (fast-first â†’ deeper)")
    cA,cB,cC,cD = st.columns(4)
    pop_size = cA.number_input("Pop size", 20, 200, 64, key="opt_pop")
    generations = cB.number_input("Global generations", 5, 200, 40, key="opt_gens")
    surrogate_rounds = cC.number_input("Surrogate rounds", 0, 30, 6, key="opt_surr")
    local_steps = cD.number_input("Local steps", 0, 300, 70, key="opt_local")
    archive_topk = st.slider("Archive size (top-k diverse)", 20, 200, 60, key="opt_topk")

    # Guardrails
    st.markdown("### ðŸ›¡ï¸ Guardrails (feasibility governance)")
    min_margin = st.number_input("Require min signed margin â‰¥ (optional)", value=0.0, step=0.01, key="opt_min_margin")
    require_feasible_only = st.checkbox("Archive: keep feasible only (recommended)", value=True, key="opt_feas_only")

    # Advanced capabilities (Tier 1â€“4)
    st.markdown("### ðŸ§  Advanced instruments (Tier 1â€“4)")
    c1, c2, c3 = st.columns(3)
    enable_surface = c1.checkbox("Constraint-surface surfing", value=True, key="opt_adv_surface")
    enable_skeleton = c2.checkbox("Feasibility skeleton", value=True, key="opt_adv_skeleton")
    enable_memory = c3.checkbox("Active learning across runs (opt-in)", value=False, key="opt_adv_memory")
    c4, c5 = st.columns(2)
    enable_multi_intent = c4.checkbox("Track distance to the other Intent", value=False, key="opt_adv_multi_intent")
    staged = c5.checkbox("Staged run (human-in-the-loop phases)", value=False, key="opt_adv_staged")
    if staged:
        st.caption("Staged run executes phases one-by-one (Global â†’ Surrogate â†’ Local â†’ Surf). Useful for steering and learning.")

    # Build an evaluator closure for this panel so post-run instruments (cartography/UQ)
    # can reuse it even when the UI reruns.
    def _make_eval_fn():
        def _fn(cand_inputs: dict):
            res = _evaluate_candidate(cand_inputs, intent=intent)
            # Expose cost proxies as objective keys (transparent, explicit)
            try:
                if isinstance(res.get("outputs"), dict) and isinstance(res.get("cost"), dict):
                    for ck, cv in res["cost"].items():
                        if ck not in res["outputs"]:
                            res["outputs"][ck] = cv
            except Exception:
                pass

            # Multi-intent instrumentation (distance-to-other)
            if enable_multi_intent:
                other_intent = "Research" if str(intent) == "Reactor" else "Reactor"
                try:
                    oth = _evaluate_candidate(cand_inputs, intent=other_intent)
                    oth_v = 0.0
                    for rr in (oth.get("constraints") or []):
                        try:
                            sm = float(rr.get("signed_margin", float("nan")))
                            if sm < 0:
                                oth_v += (-sm)
                        except Exception:
                            continue
                    res["other_intent"] = other_intent
                    res["other_feasible"] = bool(oth.get("feasible", False))
                    res["other_violation"] = float(oth_v)
                    res["other_min_signed_margin"] = float(oth.get("min_signed_margin", float("nan")))
                    res["other_failure_mode"] = oth.get("failure_mode")
                except Exception:
                    pass

            # Guardrails (do not change evaluator truth; mark infeasible for archive filtering)
            if min_margin and float(min_margin) > 0:
                try:
                    if float(res.get("min_signed_margin", float("nan"))) < float(min_margin):
                        res["feasible"] = False
                        res["failure_mode"] = res.get("failure_mode") or "min_margin_guardrail"
                except Exception:
                    pass
            return res
        return _fn

    eval_fn = _make_eval_fn()

    # Run control
    run_now = st.button("Run machine finder", type="primary", use_container_width=True, key="opt_run_button")

    if run_now:
        # Build VarSpecs
        var_specs = [VarSpec(key=k, lo=bounds[k][0], hi=bounds[k][1]) for k in var_keys]
        budgets = {
            "pop_size": int(pop_size),
            "generations": int(generations),
            "surrogate_rounds": int(surrogate_rounds),
            "propose_per_round": 36,
            "local_steps": int(local_steps),
            "archive_topk": int(archive_topk),
            "resistance_window": 250,
            "enable_surface_surf": bool(enable_surface),
            "enable_skeleton": bool(enable_skeleton),
            "use_knowledge_store": bool(enable_memory),
        }
        if staged:
            # Human-in-the-loop staged run (no background execution): phases are executed
            # one at a time and stored in session_state.
            st.session_state["opt_stage_state"] = {
                "intent": intent,
                "anchor": dict(anchor_default),
                "var_specs": [v.__dict__ for v in var_specs],
                "objectives": [o.__dict__ for o in objectives],
                "budgets": dict(budgets),
                "all_points": [],
                "trace": [],
                "done": {"global": False, "surrogate": False, "local": False, "surf": False},
                "seed": 1,
            }
            st.info("Staged run initialized. Use the phase controls below in the Workbench.")
            run = None
        else:
            run = run_hybrid_machine_finder(
                evaluate_fn=eval_fn,
                intent=intent,
                anchor_inputs=anchor_default,
                var_specs=var_specs,
                objectives=objectives,
                budgets=budgets,
                seed=1,
            )

        if isinstance(run, dict):
            # Post-filter archive if requested
            if require_feasible_only:
                run["archive"] = [a for a in run.get("archive", []) if a.get("feasible", False)]

            # vNext: rebuild archive with diversity + dominance annotation (explicit objectives)
            try:
                run["archive"] = build_archive(run.get("archive", []) or [], var_specs, topk=int(archive_topk), objectives=objectives)
            except Exception:
                pass

            # vNext: feasibility ladder classification (archive + trace)
            try:
                for c in (run.get("archive") or []):
                    c.update(classify_candidate(c, dominant=bool(c.get("is_dominant", False))))
                for t in (run.get("trace") or []):
                    t.update(classify_candidate(t))
            except Exception:
                pass

            # vNext: resistance report (descriptive)
            try:
                lens_contract = st.session_state.get("opt_lens_contract") or {}
                bounds_dict = {k: list(bounds[k]) for k in var_keys if k in bounds}
                var_specs_dicts = [v.__dict__ for v in var_specs]
                run["resistance_report"] = build_resistance_report(
                    trace=run.get("trace") or [],
                    archive=run.get("archive") or [],
                    intent=intent,
                    lens_contract=lens_contract,
                    bounds=bounds_dict,
                    var_specs=var_specs_dicts,
                )
            except Exception:
                pass

            st.session_state["opt_workbench_run"] = run
            st.success("Run complete. Workbench updated below.")

    # -------------------------
    # Post-run Workbench
    # -------------------------
    stage_state = st.session_state.get("opt_stage_state")
    run = st.session_state.get("opt_workbench_run")

    # If staged run is active, provide phase controls and build a live workbench run view
    if stage_state is not None and isinstance(stage_state, dict):
        st.markdown("---")
        st.markdown("## Forge Workbench (Staged run)")
        st.caption("Execute phases one-by-one. Nothing runs in the background.")

        # Rehydrate
        _intent = stage_state.get("intent")
        _anchor = stage_state.get("anchor") or {}
        _seed = int(stage_state.get("seed", 1))
        _var_specs = [VarSpec(**v) for v in (stage_state.get("var_specs") or [])]
        _objectives = [Objective(**o) for o in (stage_state.get("objectives") or [])]
        _budgets = stage_state.get("budgets") or {}

        done = stage_state.get("done") or {}
        ph1, ph2, ph3, ph4 = st.columns(4)
        if ph1.button("Run Global", use_container_width=True, disabled=forge_lock or bool(done.get("global"))):
            pts, tr = global_de_phase(
                evaluate_fn=eval_fn,
                anchor_inputs=_anchor,
                var_specs=_var_specs,
                objectives=_objectives,
                pop_size=int(_budgets.get("pop_size", 64)),
                generations=int(_budgets.get("generations", 40)),
                seed=_seed,
            )
            stage_state["all_points"] = (stage_state.get("all_points") or []) + pts
            stage_state["trace"] = (stage_state.get("trace") or []) + tr
            stage_state["done"]["global"] = True
            st.session_state["opt_stage_state"] = stage_state
            st.success("Global phase complete.")
        if ph2.button("Run Surrogate", use_container_width=True, disabled=forge_lock or (not bool(done.get("global")) or bool(done.get("surrogate")))):
            pts, tr = surrogate_phase(
                evaluate_fn=eval_fn,
                anchor_inputs=_anchor,
                var_specs=_var_specs,
                objectives=_objectives,
                seed=_seed,
                history=list(stage_state.get("all_points") or []),
                rounds=int(_budgets.get("surrogate_rounds", 6)),
                propose_per_round=int(_budgets.get("propose_per_round", 36)),
            )
            stage_state["all_points"] = (stage_state.get("all_points") or []) + pts
            stage_state["trace"] = (stage_state.get("trace") or []) + tr
            stage_state["done"]["surrogate"] = True
            st.session_state["opt_stage_state"] = stage_state
            st.success("Surrogate phase complete.")
        if ph3.button("Run Local", use_container_width=True, disabled=forge_lock or (not bool(done.get("global")) or bool(done.get("local")))):
            pts, tr = local_refine_phase(
                evaluate_fn=eval_fn,
                anchor_inputs=_anchor,
                var_specs=_var_specs,
                objectives=_objectives,
                seed=_seed,
                seeds=list(stage_state.get("all_points") or []),
                steps=int(_budgets.get("local_steps", 70)),
            )
            stage_state["all_points"] = (stage_state.get("all_points") or []) + pts
            stage_state["trace"] = (stage_state.get("trace") or []) + tr
            stage_state["done"]["local"] = True
            st.session_state["opt_stage_state"] = stage_state
            st.success("Local phase complete.")
        if ph4.button("Run Surf", use_container_width=True, disabled=forge_lock or (not bool(done.get("local")) or bool(done.get("surf")))):
            pts, tr = surface_surf_phase(
                evaluate_fn=eval_fn,
                anchor_inputs=_anchor,
                var_specs=_var_specs,
                objectives=_objectives,
                seed=_seed,
                seeds=list(stage_state.get("all_points") or []),
                steps=int(_budgets.get("surf_steps", 80)),
            )
            stage_state["all_points"] = (stage_state.get("all_points") or []) + pts
            stage_state["trace"] = (stage_state.get("trace") or []) + tr
            stage_state["done"]["surf"] = True
            st.session_state["opt_stage_state"] = stage_state
            st.success("Surf phase complete.")

        # Build a live run view from staged state
        _archive = build_archive(list(stage_state.get("all_points") or []), _var_specs, topk=int(_budgets.get("archive_topk", 60)))
        _trace = list(stage_state.get("trace") or [])
        _resist = resistance_atlas(_trace, last_n=int(_budgets.get("resistance_window", 250)))
        _corr = variable_correlations(_archive, _var_specs)
        _skel = build_feasibility_skeleton(_archive, _var_specs) if enable_skeleton else None
        run = {
            "kind": "optimization_sandbox_hybrid_run_staged",
            "intent": str(_intent),
            "seed": int(_seed),
            "objectives": [o.__dict__ for o in _objectives],
            "var_specs": [v.__dict__ for v in _var_specs],
            "budgets": dict(_budgets),
            "archive": _archive,
            "trace": _trace,
            "resistance": _resist,
            "variable_correlations": _corr,
            "feasibility_skeleton": _skel,
        }
        st.session_state["opt_workbench_run"] = run

    if isinstance(run, dict) and run.get("archive") is not None:
        st.markdown("---")
        st.markdown("## Forge Workbench")

        # -------------------------
        # Run Dashboard (workflow view)
        # -------------------------
        d1, d2, d3, d4 = st.columns(4)
        with d1:
            st.markdown("**Run Contract**")
            st.write({
                "intent": run.get("intent"),
                "lens": (run.get("capsule_v2") or {}).get("lens", st.session_state.get("opt_lens_contract")),
                "seed": run.get("seed"),
            })
        with d2:
            st.markdown("**Live Trace**")
            tr = run.get("trace") or []
            n_t = len(tr)
            n_f = sum(1 for t in tr if bool(t.get("feasible", False)))
            st.write({"n_evaluated": int(n_t), "n_feasible": int(n_f)})
            # top failure mode snapshot
            fm = {}
            for t in tr[-250:]:
                k = str(t.get("failure_mode") or "")
                if not k:
                    continue
                fm[k] = fm.get(k, 0) + 1
            if fm:
                top = sorted(fm.items(), key=lambda x: x[1], reverse=True)[:3]
                st.caption("Recent failure modes")
                st.write({k: int(v) for k, v in top})
        with d3:
            st.markdown("**Candidate Archive**")
            ar = run.get("archive") or []
            n_a = len(ar)
            n_af = sum(1 for a in ar if bool(a.get("feasible", False)))
            n_dom = sum(1 for a in ar if bool(a.get("is_dominant", False)))
            st.write({"n_archive": int(n_a), "n_feasible": int(n_af), "n_dominant": int(n_dom)})
        with d4:
            st.markdown("**Resistance**")
            rr = run.get("resistance_report")
            if isinstance(rr, dict):
                topb = rr.get("primary_blockers") or rr.get("blockers") or []
                if isinstance(topb, list) and topb:
                    st.write({"top_blocker": topb[0].get("name", topb[0]) if isinstance(topb[0], dict) else topb[0]})
                else:
                    st.write("(no blockers reported)")
            else:
                st.write("(no resistance report)")

        # Budget allocation (transparent scheduler)
        ba = run.get("budget_allocation")
        if isinstance(ba, dict):
            with st.expander("Budget allocation (feasibility-first scheduler)", expanded=False):
                st.json(ba)

        # Conflict atlas (accumulates across runs in-session)
        rr = run.get("resistance_report")
        if "opt_conflict_atlas" not in st.session_state or not isinstance(st.session_state.get("opt_conflict_atlas"), dict):
            st.session_state["opt_conflict_atlas"] = new_atlas()
        if isinstance(rr, dict):
            try:
                st.session_state["opt_conflict_atlas"] = update_atlas(st.session_state["opt_conflict_atlas"], rr)
            except Exception:
                pass
        with st.expander("Conflict atlas (across runs, descriptive)", expanded=False):
            st.caption("Accumulated from Resistance Reports. Descriptive only - not causal, not prescriptive.")
            rows = summarize_atlas(st.session_state.get("opt_conflict_atlas") or {}, top_n=25)
            if rows:
                import pandas as _pd
                st.dataframe(_pd.DataFrame(rows), use_container_width=True)
            else:
                st.info("No conflicts accumulated yet. Run optimization at least once.")
            st.download_button(
                "Download conflict atlas (json)",
                data=json.dumps(st.session_state.get("opt_conflict_atlas") or {}, indent=2, sort_keys=True),
                file_name="shams_opt_conflict_atlas.json",
                mime="application/json",
                use_container_width=True,
                key="opt_dl_conflict_atlas",
            )

        # vNext: Run capsule (v2) + resistance report (descriptive, exportable)
        with st.expander("Run capsule + resistance report (export)", expanded=False):
            lens_contract = st.session_state.get("opt_lens_contract") or {}
            rr = run.get("resistance_report")
            if isinstance(rr, dict):
                st.caption("Resistance report (descriptive)")
                st.json(rr, expanded=False)
            else:
                st.info("Resistance report not available for this run.")

            if st.button("Build capsule zip (v2)", use_container_width=True, key="opt_build_capsule_zip"):
                try:
                    import time, os, json
                    run_id = f"run_{int(time.time())}"
                    settings = {
                        "bounds": {k: list(v) for k, v in (st.session_state.get("opt_bounds") or {}).items()} if isinstance(st.session_state.get("opt_bounds"), dict) else {},
                        "var_specs": run.get("var_specs") or [],
                        "objectives": run.get("objectives") or [],
                    }
                    evaluator_hash = str(run.get("fingerprint") or "")
                    cap_path = save_run_capsule_v2(
                        run,
                        run_id=run_id,
                        settings=settings,
                        evaluator_hash=evaluator_hash,
                        archive=run.get("archive") or [],
                        trace=run.get("trace") or [],
                        lens_contract=lens_contract,
                        resistance_report=rr if isinstance(rr, dict) else None,
                    )
                    # Also export a compact zip with manifest
                    from pathlib import Path
                    out_zip = Path(cap_path).with_suffix(".zip")
                    export_run_capsule_zip(
                        capsule=json.loads(Path(cap_path).read_text(encoding="utf-8")),
                        archive={"schema":"shams.opt_sandbox.archive_snapshot.v1","archive": run.get("archive") or []},
                        resistance_report=rr if isinstance(rr, dict) else None,
                        out_path=out_zip,
                    )
                    st.session_state["opt_capsule_zip_bytes"] = out_zip.read_bytes()
                    st.session_state["opt_capsule_zip_name"] = out_zip.name
                    st.success("Capsule zip built.")
                except Exception as e:
                    st.error(f"Capsule build failed: {e}")

            if st.session_state.get("opt_capsule_zip_bytes") is not None:
                st.download_button(
                    "Download capsule zip",
                    data=st.session_state["opt_capsule_zip_bytes"],
                    file_name=str(st.session_state.get("opt_capsule_zip_name","opt_capsule.zip")),
                    mime="application/zip",
                    use_container_width=True,
                    key="opt_dl_capsule_zip",
                )

        # Sticky truth bar (simple CSS)
        st.markdown(
            """
            <style>
            div[data-testid="stVerticalBlock"] div:has(> div.shams-sticky) { position: sticky; top: 0; z-index: 999; background: white; }
            .shams-sticky { border: 1px solid rgba(49,51,63,0.15); padding: 10px; border-radius: 10px; }
            </style>
            """,
            unsafe_allow_html=True
        )

        best = run.get("best_feasible")
        feas_rate = run.get("resistance", {}).get("feasible_rate")
        dom = run.get("resistance", {}).get("dominant_constraints", {})
        dom_top = sorted(dom.items(), key=lambda kv: kv[1], reverse=True)[:1]
        dom_txt = dom_top[0][0] if dom_top else "-"

        with st.container():
            st.markdown('<div class="shams-sticky">', unsafe_allow_html=True)
            t1,t2,t3,t4,t5 = st.columns([1,1,1,1,2])
            t1.metric("Intent", run.get("intent","-"))
            t2.metric("Feasible rate (recent)", f"{(float(feas_rate)*100.0):.1f}%" if feas_rate is not None else "-")
            # Keep score strictly labeled as non-authoritative (legacy search utility).
            t3.metric("Archive score (non-authoritative)", f"{float(best.get('_score')):.3g}" if isinstance(best, dict) else "-")
            t4.metric("Archive size", str(len(run.get("archive") or [])))
            t5.write(f"**Dominant resistance:** {dom_txt}")
            st.markdown('</div>', unsafe_allow_html=True)

        left, center, right = st.columns([1.2, 2.2, 1.4], vertical_alignment="top")

        # LEFT: nav/setup for post-run views
        with left:
            st.markdown("### Navigate")

            # v206: reduce scrolling fatigue for experts by using a searchable selectbox
            # and a "Cockpit mode" that keeps the most-used instruments on one screen.
            cockpit_mode = st.toggle(
                "Forge Cockpit Mode",
                value=True,
                help="Compact, low-scroll layout for experts. Keeps the core instruments together.",
                key="rdf_cockpit_mode",
                disabled=forge_lock,
            )
            if forge_lock:
                cockpit_mode = True

            _views_core = [
                "Casebook",
                "Candidate Archive",
                "Forge Timeline",
                "Machine Dossier",
                "Review Trinity",
                "Attack Simulation",
                "Resistance Brief",
                "Scan â†” Forge Grounding",
                "Conflict Atlas",
                "Boundary Navigator",
                "Constraint Spend Map",
                "Reactor Accounting Console",
                "Margin Ledger",
                "Reality Gates",
                "Closure Certificate",
                "Provenance Graph",
                "Engineering Reality Budget",
                "Failure-Mode Canon",
                "Design Class",
                "Citation Blocks",
                "Reference Reproduction",
                "Economics Deck",
                "Robustness Envelope",
                "Design Narrative",
                "Design Card",
                "Design Packet",
                "Confidence Sweep",
                "Expert Compare (no ranking)",
                "Exposure Readiness",
                "Epistemic Gap Map",
                "Constraint Personas",
                "Design Genealogy",
                "Doâ€‘Notâ€‘Build Brief",
                "Process of Elimination",
                "Paperâ€‘Ready Signals",
                "Silence Mode"
                "Sensitivity Fingerprint",
                "Reviewer Packet"
            ]
            _views_full = _views_core + [
                "Archive regimes & coverage",
                "Machine existence report",
                "Design navigation (steering)",
                "Pareto (if multi-objective)",
                "Report Pack",
                "Trace Telemetry",
                "Feasibility skeleton",
                "Local cartography (adaptive)",
                "Uncertainty (Monte Carlo)",
                "Intent trajectories (Researchâ†’Reactor)",
                "Inverse design / Why not?",
                "Discovered relations (laws)",
                "Counterfactual lens",
                "PROCESS parity benchmarks",
                "Parity validation packs (PASS/WARN/FAIL)",
                "Parity calibration (reference deltas)",
                "Decision scenarios (program lens)",
                "Collaboration (review sessions)",
                "Epistemic guarantees (regression suite)",
                "Standards & DOI export",
                "Design-space verdicts (Allowed/Forbidden)",
                "Epistemic confidence bounds",
                "Intent-conditional design laws",
                "Machine genealogy",
                "Counter-optimization (no interior optimum)",
                "Reproducibility",
            ]

            _views = _views_core if cockpit_mode else _views_full
            _default_view = st.session_state.get("opt_view") or ("Review Trinity" if forge_lock else "Casebook")
            if _default_view not in _views:
                _default_view = "Review Trinity" if forge_lock else "Casebook"
            view = st.selectbox(
                "Main view",
                options=_views,
                index=int(_views.index(_default_view)),
                key="opt_view",
                help="Type to search. Cockpit mode shows the core instruments first.",
            )

            # Back-compat: keep internal handlers stable while we improve naming for fusion experts.
            _view_alias = {
                "Casebook": "Casebook Runner",
                "Candidate Archive": "Archive Bay",
                "Forge Timeline": "Timeline Strip",
                "Review Trinity": "Review Trinity",
                "Attack Simulation": "Attack Simulation",
                "Scan â†” Forge Grounding": "Scan â†” Forge Grounding",
                "Exposure Readiness": "Exposure Readiness",
            }
            view = _view_alias.get(str(view), str(view))
            st.markdown("### Archive filters")
            only_robust = st.checkbox("Keep only marginâ‰¥0", value=False, key="opt_filter_robust")
            min_score = st.number_input("Min score (optional)", value=float("-inf"), key="opt_filter_minscore")
            if use_cost:
                st.markdown("### ðŸ’° Cost filter")
                max_coe = st.number_input("Max COE proxy (optional)", value=float("inf"), key="opt_filter_coe")



            # ------------------------------
            # Review Bench (Compare Tray)
            # ------------------------------
            with st.expander("Review Bench (compare tray)", expanded=False):
                st.caption("Pin a handful of candidates for side-by-side review. Descriptive only - no ranking.")

                # Current candidate (filtered archive index)
                _cur = None
                try:
                    if filt:
                        _i = int(st.session_state.get("opt_inspect_idx", 0) or 0)
                        _i = max(0, min(len(filt)-1, _i))
                        _cur = filt[_i]
                except Exception:
                    _cur = None

                if "opt_review_bench" not in st.session_state or not isinstance(st.session_state.get("opt_review_bench"), list):
                    st.session_state["opt_review_bench"] = []

                cA, cB = st.columns([1,1])
                with cA:
                    if st.button("Pin current candidate", use_container_width=True, key="opt_pin_current"):
                        if _cur is None:
                            st.warning("No current candidate to pin.")
                        else:
                            try:
                                _fid = candidate_fingerprint(_cur)
                            except Exception:
                                _fid = f"idx:{int(st.session_state.get('opt_inspect_idx',0) or 0)}"
                            # avoid duplicates
                            if not any(str(x.get("id")) == str(_fid) for x in st.session_state["opt_review_bench"]):
                                inp = _cur.get("inputs") or {}
                                out = _cur.get("outputs") or {}
                                st.session_state["opt_review_bench"].append({
                                    "id": str(_fid),
                                    "idx": int(st.session_state.get("opt_inspect_idx", 0) or 0),
                                    "R0_m": inp.get("R0_m"),
                                    "Bt_T": inp.get("Bt_T"),
                                    "Ip_MA": inp.get("Ip_MA"),
                                    "P_e_net_MW": out.get("P_e_net_MW"),
                                    "Pfus_total_MW": out.get("Pfus_total_MW"),
                                    "first_failure": _cur.get("first_failure"),
                                    "min_signed_margin": _cur.get("min_signed_margin"),
                                })
                                st.success("Pinned.")
                            else:
                                st.info("Already pinned.")
                with cB:
                    if st.button("Clear bench", use_container_width=True, key="opt_clear_bench"):
                        st.session_state["opt_review_bench"] = []
                        st.success("Cleared.")

                bench = st.session_state.get("opt_review_bench") or []
                if bench:
                    import pandas as _pd
                    dfb = _pd.DataFrame(bench)
                    st.dataframe(dfb, use_container_width=True, hide_index=True)

                    # Exports
                    csv_bytes = dfb.to_csv(index=False).encode("utf-8")
                    st.download_button("Download Review Bench (CSV)", data=csv_bytes, file_name="review_bench.csv", mime="text/csv", use_container_width=True)
                    md = "|" + "|".join(dfb.columns) + "|\n"
                    md += "|" + "|".join(["---"]*len(dfb.columns)) + "|\n"
                    for _, r in dfb.iterrows():
                        md += "|" + "|".join(str(r[c]) for c in dfb.columns) + "|\n"
                    st.download_button("Download Review Bench (Markdown)", data=md.encode("utf-8"), file_name="review_bench.md", mime="text/markdown", use_container_width=True)
                else:
                    st.info("Bench is empty. Pin 2â€“5 candidates during review.")

            # ------------------------------
            # Tier 5â€“6 controls (integrated)
            # ------------------------------
            with st.expander("Tier 5â€“6 instruments (optional)", expanded=False):
                st.caption(
                    "These are advanced instruments that PROCESS cannot provide. "
                    "They never modify frozen truth; they add explanatory lenses and workflows."
                )

                # Counterfactual constraint gate
                st.markdown("**Counterfactual gate (hypothetical)**")
                st.caption("Disable a constraint only in the feasibility *gate* for analysis. Raw constraints remain unchanged.")
                _all_names = []
                try:
                    if (run.get("archive") or []) and (run.get("archive")[0].get("constraints") or []):
                        _all_names = [str(r.get("name")) for r in (run.get("archive")[0].get("constraints") or []) if r.get("name")]
                        _all_names = sorted(list(dict.fromkeys(_all_names)))
                except Exception:
                    _all_names = []
                disabled_cons = st.multiselect(
                    "Disable constraints (hypothetical)",
                    options=_all_names,
                    default=[],
                    key="opt_cf_disable",
                )

                # Credibility overlay
                st.markdown("**Constraint credibility overlay**")
                st.caption("Optional epistemic lens: maturity/uncertainty adjusts *displayed* margins and filters. Does not change feasibility truth.")
                use_cred = st.checkbox("Enable credibility overlay", value=False, key="opt_use_cred")
                cred_map = {}
                if use_cred and _all_names:
                    # Keep a small editable set (top 8 by occurrence in archive)
                    top = _all_names[:8]
                    for nm in top:
                        c1,c2,c3 = st.columns([2,1,1])
                        with c1:
                            st.write(nm)
                        with c2:
                            mat = st.slider(f"maturity_{nm}", 0.0, 1.0, 0.7, 0.05, key=f"cred_m_{nm}")
                        with c3:
                            unc = st.slider(f"uncertainty_{nm}", 0.0, 0.5, 0.10, 0.01, key=f"cred_u_{nm}")
                        cred_map[nm] = ConstraintCred(name=nm, maturity=float(mat), uncertainty_frac=float(unc), conservative=True)
                # Persist for center/inspector usage
                st.session_state["opt_cred_map"] = {
                    k: {"name": v.name, "maturity": float(v.maturity), "uncertainty_frac": float(v.uncertainty_frac), "conservative": bool(v.conservative)}
                    for k, v in cred_map.items()
                }

                # Trajectory settings
                st.markdown("**Intent trajectories**")
                st.caption("Build a simple Researchâ†’Reactor highway from the current archive (and current variable list).")
                traj_steps = st.slider("Max path steps", 2, 10, 5, key="opt_traj_steps")
                if st.button("Build trajectory", use_container_width=True, key="opt_traj_build"):
                    st.session_state["opt_traj"] = None
                    try:
                        # Use current archive (pre-filter) to keep it deterministic
                        cands = run.get("archive") or []
                        st.session_state["opt_traj"] = build_intent_trajectory(
                            evaluate_fn=_evaluate_candidate,
                            candidates=cands,
                            var_keys=var_keys,
                            from_intent="Research",
                            to_intent="Reactor",
                            k_steps=int(traj_steps),
                            seed=int(run.get("seed", 0)),
                        )
                    except Exception as _e:
                        st.session_state["opt_traj"] = {"ok": False, "reason": str(_e)}

                # Inverse design target capture (used in center view)
                st.markdown("**Inverse design targets**")
                st.caption("Define desired outputs; SHAMS finds the closest feasible candidate *within your declared bounds*. No relaxation.")
                inv_cols = st.columns(2)
                inv_k = inv_cols[0].text_input("Target key", value="P_e_net_MW", key="opt_inv_key")
                inv_v = inv_cols[1].number_input("Target value", value=500.0, key="opt_inv_val")
                st.session_state["opt_inv_targets"] = {str(inv_k): float(inv_v)}

        archive = run.get("archive") or []
        # Apply filters
        filt = []
        for a in archive:
            if only_robust and float(a.get("min_signed_margin", float("nan"))) < 0:
                continue
            if float(a.get("_score", -1e30)) < float(min_score):
                continue
            if use_cost:
                coe = (a.get("cost") or {}).get("COE_proxy")
                if coe is not None and float(coe) > float(max_coe):
                    continue
            filt.append(a)

        # -------------------------
        # v206: expert signals (descriptive)
        # -------------------------
        def _regime_signature(_cand: dict) -> list:
            """Fusion-expert friendly regime tags (descriptive only)."""
            tags = []
            inp = _cand.get("inputs") or {}
            out = _cand.get("outputs") or {}

            def _get_num(d, k):
                try:
                    v = d.get(k)
                    return None if v is None else float(v)
                except Exception:
                    return None

            R0 = _get_num(inp, "R0_m") or _get_num(out, "R0_m")
            a = _get_num(inp, "a_m") or _get_num(out, "a_m")
            Ip = _get_num(inp, "Ip_MA") or _get_num(out, "Ip_MA")
            B0 = _get_num(inp, "B0_T") or _get_num(out, "B0_T") or _get_num(out, "Bt_T")
            Pf = _get_num(out, "Pfus_total_MW")

            # Geometry regime
            if R0 is not None:
                if R0 < 2.5:
                    tags.append("compact")
                elif R0 > 6.0:
                    tags.append("large-R")
            if R0 is not None and a is not None and a > 0:
                A = R0 / a
                if A < 2.2:
                    tags.append("spherical")
                elif A > 3.2:
                    tags.append("high-aspect")

            # Field/current regimes (heuristic, descriptive)
            if B0 is not None:
                if B0 >= 10.0:
                    tags.append("high-field")
                elif B0 <= 4.0:
                    tags.append("low-field")
            if Ip is not None:
                if Ip >= 12.0:
                    tags.append("high-current")

            # Power density proxy
            if Pf is not None and R0 is not None and R0 > 0:
                pd = Pf / (R0 ** 3)
                if pd >= 20.0:
                    tags.append("power-dense")

            # Feasibility state
            fs = _cand.get("feasibility_state")
            if fs:
                tags.append(str(fs).replace("feasible_", ""))
            return tags[:8]

        def _first_kill(_cand: dict) -> dict:
            """Return the tightest constraint (first-kill) from margin ledger rows."""
            mb = _cand.get("margin_budget") or {}
            rows = mb.get("rows") or []
            if not rows:
                return {"name": _cand.get("first_failure") or _cand.get("failure_mode") or "-", "signed_margin": _cand.get("min_signed_margin")}
            best = None
            for r in rows:
                try:
                    sm = float(r.get("signed_margin"))
                except Exception:
                    continue
                if best is None or sm < best[0]:
                    best = (sm, r)
            if best is None:
                return {"name": _cand.get("first_failure") or "-", "signed_margin": _cand.get("min_signed_margin")}
            rr = best[1]
            return {"name": rr.get("name") or rr.get("constraint") or "-", "signed_margin": float(best[0])}

        def _constraint_spend_rate(_cand: dict) -> dict:
            """Local heuristic: margin change per objective change vs parent (if lineage exists)."""
            pid = _cand.get("parent_id") or _cand.get("parent")
            if not pid:
                return {"ok": False, "reason": "no parent link"}
            parent = None
            for c in (run.get("archive") or []):
                if (c.get("_id") or c.get("fingerprint")) == pid:
                    parent = c
                    break
            if parent is None:
                return {"ok": False, "reason": "parent not found in archive"}

            # choose one objective if available
            obj = None
            lens = run.get("lens") or {}
            objs = lens.get("objectives") if isinstance(lens, dict) else None
            if isinstance(objs, list) and objs:
                obj = objs[0].get("key") if isinstance(objs[0], dict) else None
            if not obj:
                obj = "P_e_net_MW" if "P_e_net_MW" in (_cand.get("outputs") or {}) else None
            if not obj:
                return {"ok": False, "reason": "no objective key"}

            def _val(c, key):
                try:
                    return float((c.get("outputs") or {}).get(key))
                except Exception:
                    return None

            d_obj = None
            c_obj = _val(_cand, obj)
            p_obj = _val(parent, obj)
            if c_obj is not None and p_obj is not None:
                d_obj = c_obj - p_obj

            try:
                d_m = float(_cand.get("min_signed_margin")) - float(parent.get("min_signed_margin"))
            except Exception:
                d_m = None

            if d_obj is None or d_m is None or abs(d_obj) < 1e-12:
                return {"ok": False, "reason": "insufficient delta"}

            return {
                "ok": True,
                "objective": obj,
                "delta_objective": d_obj,
                "delta_min_margin": d_m,
                "margin_spend_per_objective": (d_m / d_obj),
                "note": "Local heuristic vs parent only (descriptive).",
            }

        # --- v208: Scan â†” Forge grounding (descriptive topology context) ---
        def _scan_grounding(_cand: dict, _scan_artifact: dict, *, intent: str) -> dict:
            """Attach nearest-point scan context to a candidate (descriptive only)."""
            try:
                art = _scan_artifact or {}
                rep = (art.get("report") or {}) if isinstance(art, dict) else {}
                pts = rep.get("points") or []
                xk = rep.get("x_key"); yk = rep.get("y_key")
                if not pts or not xk or not yk:
                    return {"ok": False, "reason": "scan artifact missing points/x_key/y_key"}
                cin = _cand.get("inputs") or {}
                if xk not in cin or yk not in cin:
                    return {"ok": False, "reason": "candidate lacks scan axes", "x_key": xk, "y_key": yk}
                cx = float(cin.get(xk)); cy = float(cin.get(yk))
                best = None
                best_d = None
                for p in pts:
                    try:
                        dx = float(p.get("x")) - cx
                        dy = float(p.get("y")) - cy
                        d = (dx*dx + dy*dy) ** 0.5
                    except Exception:
                        continue
                    if best is None or (best_d is not None and d < best_d) or (best_d is None):
                        best = p; best_d = d
                if best is None:
                    return {"ok": False, "reason": "no valid scan points"}
                it = str(intent or "Reactor")
                it_sum = ((best.get("intent") or {}).get(it) if isinstance(best.get("intent"), dict) else None) or {}
                top = ((rep.get("topology") or {}).get(it) if isinstance(rep.get("topology"), dict) else None) or {}
                return {
                    "ok": True,
                    "scan_id": (rep.get("id") or art.get("report_hash") or art.get("artifact_hash")),
                    "x_key": xk,
                    "y_key": yk,
                    "candidate_xy": {"x": cx, "y": cy},
                    "nearest": {
                        "i": int(best.get("i", -1)),
                        "j": int(best.get("j", -1)),
                        "x": float(best.get("x")),
                        "y": float(best.get("y")),
                        "robustness": it_sum.get("robustness"),
                        "dominant_blocking": it_sum.get("dominant_blocking"),
                        "min_blocking_margin": it_sum.get("min_blocking_margin"),
                    },
                    "distance": float(best_d) if best_d is not None else None,
                    "topology": top,
                    "note": "Descriptive grounding only. Does not modify evaluator truth.",
                }
            except Exception as e:
                return {"ok": False, "reason": f"grounding_error: {e}"}

        # CENTER: main canvas
        with center:
            st.markdown("### Canvas")

            # v208: Margin-first framing (always-on summary for the inspected candidate)
            try:
                if filt:
                    _i = int(st.session_state.get("opt_inspect_idx", 0) or 0)
                    _i = max(0, min(len(filt) - 1, _i))
                    _c = filt[_i]
                    _mb = _c.get("margin_budget")
                    if not isinstance(_mb, dict):
                        _mb = margin_budget(_c.get("constraints") or [])
                        _c["margin_budget"] = _mb
                    _rows = _mb.get("rows") or []
                    _tight = []
                    for r in _rows:
                        if isinstance(r, dict) and r.get("name"):
                            _tight.append(r)
                    _tight = sorted(_tight, key=lambda rr: float(rr.get("margin_frac", 1e30) or 1e30))[:5]
                    with st.expander(f"{_LANG.get('margin_first')}", expanded=False if forge_lock else False):
                        c1,c2,c3 = st.columns(3)
                        c1.metric("Min signed margin", str(_c.get("min_signed_margin")))
                        c2.metric("Dominant resistance", str(_c.get("first_failure") or _c.get("failure_mode") or "-"))
                        c3.metric("Feasible", str(bool(_c.get("feasible"))))
                        if _tight:
                            st.write({str(r.get("name")): r.get("margin_frac") for r in _tight})
            except Exception:
                pass

            # v206: dedicated Conflict Atlas view (also shown in the right rail in cockpit mode)
            if view == "Conflict Atlas":
                st.caption("Constraint Conflict Atlas (descriptive, accumulated across runs).")
                rows = summarize_atlas(st.session_state.get("opt_conflict_atlas") or {}, top_n=50)
                if rows:
                    import pandas as _pd
                    st.dataframe(_pd.DataFrame(rows), use_container_width=True, height=520)
                else:
                    st.info("No conflicts accumulated yet. Run at least one case.")
                st.stop()

            # --- v204â€“v205: Design intelligence + confidence instruments ---
            if view == "Timeline Strip":
                st.caption("v204: Timeline strip of the current run (phases + evaluations).")
                tr = run.get("trace") or []
                if not tr:
                    st.info("No trace available.")
                else:
                    try:
                        import pandas as _pd
                        rows = []
                        for i, t in enumerate(tr):
                            rows.append({
                                "i": i,
                                "phase": t.get("phase") or t.get("step") or "",
                                "feasible": bool(t.get("feasible")) if t.get("feasible") is not None else None,
                                "failure": t.get("failure_mode") or t.get("failure") or "",
                                "score": t.get("score") or t.get("_score"),
                            })
                        df = _pd.DataFrame(rows)
                        st.dataframe(df, use_container_width=True, height=420)
                    except Exception:
                        st.json(tr[:200])
                st.stop()

            if view == "Lineage Graph":
                st.caption("v204: Design lineage graph based on recorded parents (audit-clean).")
                if not (run.get("archive") or []):
                    st.info("No archive available.")
                    st.stop()
                edges = build_lineage_edges(run.get("archive") or [])
                if not edges:
                    st.info("No explicit parent links found in archive candidates. (Fallback: use 'Machine genealogy' for reconstructed ancestry.)")
                    st.stop()
                layout = compute_tree_layout(edges)
                try:
                    import pandas as _pd
                    import plotly.graph_objects as _go

                    # Edges as segments
                    xs = []
                    ys = []
                    for p, c in edges:
                        if p not in layout or c not in layout:
                            continue
                        xs += [layout[p]["x"], layout[c]["x"], None]
                        ys += [layout[p]["y"], layout[c]["y"], None]
                    fig = _go.Figure()
                    fig.add_trace(_go.Scatter(x=xs, y=ys, mode="lines", name="lineage"))

                    # Nodes
                    ndf = _pd.DataFrame([
                        {"id": nid, "x": v["x"], "y": v["y"], "depth": v["depth"]}
                        for nid, v in layout.items()
                    ])
                    fig.add_trace(_go.Scatter(x=ndf["x"], y=ndf["y"], mode="markers+text", text=ndf["id"], textposition="top center", name="nodes"))
                    fig.update_layout(height=520, margin=dict(l=10, r=10, t=30, b=10), title="Lineage Graph")
                    st.plotly_chart(fig, use_container_width=True)
                except Exception:
                    st.write({"edges": edges[:200], "layout": layout})
                st.stop()

            if view == "Constraint Spend Map":
                st.caption("v204: Spend map - where feasibility margin is being spent.")
                if not filt:
                    st.info("No candidates available.")
                    st.stop()
                xk, yk = st.columns(2)
                x_key = xk.text_input("X axis input key", value=var_keys[0] if var_keys else "Ip_MA", key="spend_x")
                y_key = yk.text_input("Y axis input key", value=var_keys[1] if len(var_keys) > 1 else "R0_m", key="spend_y")
                mode = st.selectbox("Color by", ["min_margin", "feasibility_state", "constraint_margin"], index=0, key="spend_color")
                con_key = None
                if mode == "constraint_margin":
                    con_key = st.text_input("Constraint key", value="q_div", key="spend_ck")
                scat = build_spend_scatter(filt, x_key=str(x_key), y_key=str(y_key), color_by=str(mode), constraint_key=str(con_key) if con_key else None)
                try:
                    import pandas as _pd
                    import plotly.express as _px
                    df = _pd.DataFrame({"x": scat["x"], "y": scat["y"], "c": scat["c"], "id": scat["ids"]})
                    fig = _px.scatter(df, x="x", y="y", hover_data=["id"], color="c")
                    fig.update_layout(height=520, margin=dict(l=10, r=10, t=20, b=10))
                    st.plotly_chart(fig, use_container_width=True)
                except Exception:
                    st.json(scat)
                st.stop()

            if view == "Robustness Envelope":
                st.caption("v205: Robustness envelope (first-order margin perturbation sweep).")
                if not filt:
                    st.info("No candidates available.")
                    st.stop()
                idx_sel = int(st.session_state.get("opt_inspect_idx", 0))
                cand = filt[int(max(0, min(idx_sel, len(filt)-1)))]
                env = robustness_envelope_from_records(cand.get("constraints") or [])
                if not env.get("ok"):
                    st.info(env.get("reason"))
                else:
                    try:
                        import pandas as _pd
                        st.line_chart(_pd.DataFrame({"pass_fraction": env["pass_fraction"]}, index=[str(p) for p in env["perturbations"]]))
                    except Exception:
                        st.write(env)
                st.json(env, expanded=False)
                st.stop()

            if view == "Design Narrative":
                st.caption("v205: Design narrative pack (review-grade, no recommendations).")
                if not filt:
                    st.info("No candidates available.")
                    st.stop()
                idx_sel = int(st.session_state.get("opt_inspect_idx", 0))
                cand = filt[int(max(0, min(idx_sel, len(filt)-1)))]
                nar = build_narrative(cand)
                st.markdown(nar.get("markdown") or "")
                st.download_button("Download narrative (md)", data=(nar.get("markdown") or "").encode("utf-8"), file_name="design_narrative.md", mime="text/markdown")
                st.json(nar, expanded=False)
                st.stop()

            if view == "Design Card":
                st.caption("v205: One-page design card (printable, reviewer-friendly).")
                if not filt:
                    st.info("No candidates available.")
                    st.stop()
                idx_sel = int(st.session_state.get("opt_inspect_idx", 0))
                cand = filt[int(max(0, min(idx_sel, len(filt)-1)))]
                card = build_design_card_md(cand)
                st.markdown(card)
                st.download_button("Download Design Card (md)", data=card.encode("utf-8"), file_name="design_card.md", mime="text/markdown")
                st.stop()

            if view == "Design Packet":
                st.caption("v207: Design Packet - narrative + card + key tables (PDF best-effort).")
                if not filt:
                    st.info("No candidates available.")
                    st.stop()
                idx_sel = int(st.session_state.get("opt_inspect_idx", 0))
                cand = filt[int(max(0, min(idx_sel, len(filt)-1)))]
                nar = build_narrative(cand)
                card = build_design_card_md(cand)
                title = f"SHAMS Design Packet - Candidate {idx_sel}"
                pkt = build_design_packet_files(title=title, card_md=card, narrative_md=(nar.get('markdown') or ''), candidate=cand)
                md = pkt.get('markdown') or ''
                st.markdown(md)
                st.download_button("Download Design Packet (md)", data=md.encode('utf-8'), file_name="design_packet.md", mime="text/markdown", use_container_width=True)
                pdfb = pkt.get('pdf_bytes')
                if pdfb:
                    st.download_button("Download Design Packet (pdf)", data=pdfb, file_name="design_packet.pdf", mime="application/pdf", use_container_width=True)
                else:
                    st.info("PDF rendering unavailable (markdown export is authoritative).")
                with st.expander("Packet metadata (json)", expanded=False):
                    st.json({k: v for k, v in pkt.items() if k not in ('markdown','pdf_bytes')}, expanded=False)
                st.stop()

            if view == "Confidence Sweep":
                st.caption("v207: Confidence Sweep - explicit declared perturbations (no hidden penalties, no recommendations).")
                if not filt:
                    st.info("No candidates available.")
                    st.stop()
                idx_sel = int(st.session_state.get("opt_inspect_idx", 0))
                cand = filt[int(max(0, min(idx_sel, len(filt)-1)))]
                recs = cand.get('constraints') or []
                cb = cand.get('closure_bundle') or {}
                cs = confidence_sweep(records=recs, closure_bundle=cb)
                if not cs.get('ok'):
                    st.warning(cs.get('reason'))
                else:
                    c1,c2,c3 = st.columns(3)
                    c1.metric("Verdict", cs.get('verdict'))
                    c2.metric("Min pass fraction", f"{float(cs.get('min_pass_fraction',0.0))*100:.1f}%")
                    fk = cs.get('first_kill_tally') or {}
                    top = sorted(fk.items(), key=lambda kv: kv[1], reverse=True)[:1]
                    c3.metric("Most common first-kill", top[0][0] if top else '-')
                    try:
                        import pandas as _pd
                        df = _pd.DataFrame({"pass_fraction": cs.get('pass_fraction') or []}, index=[str(x) for x in (cs.get('margin_deltas') or [])])
                        st.line_chart(df)
                    except Exception:
                        pass
                    with st.expander("First-kill tally", expanded=False):
                        st.write(fk)
                    with st.expander("Proxy headlines", expanded=False):
                        st.write(cs.get('proxy_headlines') or [])
                st.json(cs, expanded=False)
                st.download_button("Download Confidence Sweep (json)", data=json.dumps(cs, indent=2, sort_keys=True), file_name="confidence_sweep.json", mime="application/json", use_container_width=True)
                st.stop()

            # --- v208: Review-room instruments ---
            if view == "Scan â†” Forge Grounding":
                st.caption("Ground the current candidate in Scan Lab topology (descriptive).")
                sa = st.session_state.get("scan_cartography_artifact")
                if not isinstance(sa, dict):
                    st.info("No Scan Lab artifact found in session. Run Scan Lab or upload a scan artifact there first.")
                    st.stop()
                if not filt:
                    st.info("No candidates available.")
                    st.stop()
                idx_sel = int(st.session_state.get("opt_inspect_idx", 0))
                cand = filt[int(max(0, min(idx_sel, len(filt)-1)))]
                sg = _scan_grounding(cand, sa, intent=str(run.get("intent") or "Reactor"))
                if not sg.get("ok"):
                    st.warning(str(sg.get("reason")))
                st.json(sg, expanded=False)
                st.download_button("Download grounding (json)", data=json.dumps(sg, indent=2, sort_keys=True), file_name="scan_forge_grounding.json", mime="application/json", use_container_width=True)
                st.stop()

            # ---- Supremacy Instruments (descriptive, review-room)
            if view == "Epistemic Gap Map":
                from tools.sandbox import forge_supremacy_plus as fsp
                st.caption("Epistemic Gap Map: make model limits explicit (honesty signaling, not UQ).")
                ctx = {
                    "assumptions_ledger": st.session_state.get("assumptions_ledger_text") or "",
                    "model_ledger": st.session_state.get("model_ledger_text") or "",
                    "notes": st.session_state.get("run_notes") or "",
                }
                gaps = fsp.epistemic_gap_map(ctx)
                for k, items in gaps.items():
                    with st.expander(k, expanded=False):
                        for it in items:
                            st.write(f"- {it}")
                st.download_button("Download Gap Map (JSON)",
                                   data=json.dumps(gaps, indent=2).encode("utf-8"),
                                   file_name="epistemic_gap_map.json",
                                   mime="application/json")
                st.stop()

            if view == "Constraint Personas":
                from tools.sandbox import forge_supremacy_plus as fsp
                st.caption("Constraint Personas: memorable behavioral profiles (descriptive).")
                personas = fsp.constraint_personas()
                for cname, prof in personas.items():
                    with st.expander(cname, expanded=False):
                        for kk, vv in prof.items():
                            st.write(f"**{kk}:** {vv}")
                st.download_button("Download Personas (JSON)",
                                   data=json.dumps(personas, indent=2).encode("utf-8"),
                                   file_name="constraint_personas.json",
                                   mime="application/json")
                st.stop()

            if view == "Design Genealogy":
                from tools.sandbox import forge_supremacy_plus as fsp
                st.caption("Design Genealogy: lineage view (when lineage metadata exists).")
                cand_list = st.session_state.get("opt_archive_filtered") or st.session_state.get("opt_archive") or []
                md = fsp.genealogy_markdown(list(cand_list) if isinstance(cand_list, list) else [])
                st.markdown(md)
                st.download_button("Download Genealogy (MD)",
                                   data=md.encode("utf-8"),
                                   file_name="design_genealogy.md",
                                   mime="text/markdown")
                st.stop()

            if view == "Doâ€‘Notâ€‘Build Brief":
                from tools.sandbox import forge_supremacy_plus as fsp
                st.caption("Doâ€‘Notâ€‘Build Brief: reasons *not* to build (trust ledger).")
                filt = st.session_state.get("opt_archive_filtered") or st.session_state.get("opt_archive") or []
                if not filt:
                    st.info("No candidates available.")
                    st.stop()
                idx_sel = int(st.session_state.get("opt_inspect_idx", 0))
                cand = filt[int(max(0, min(idx_sel, len(filt)-1)))]
                ctx = {
                    "margin_ledger": st.session_state.get("opt_margin_ledger"),
                    "conflicts": st.session_state.get("opt_conflicts"),
                    "first_kill_under_uncertainty": st.session_state.get("opt_first_kill_uncertainty"),
                }
                brief = fsp.do_not_build_brief(cand, ctx)
                st.subheader(brief["title"])
                st.caption(brief.get("posture", ""))
                for r in brief["reasons"]:
                    st.write(f"- {r}")
                st.download_button("Download Doâ€‘Notâ€‘Build Brief (JSON)",
                                   data=json.dumps(brief, indent=2).encode("utf-8"),
                                   file_name="do_not_build_brief.json",
                                   mime="application/json")
                st.stop()

            if view == "Process of Elimination":
                from tools.sandbox import forge_supremacy_plus as fsp
                st.caption("Process of Elimination: why most machines cannot exist (constraint narrative).")
                ctx = {
                    "first_failure_histogram": st.session_state.get("scan_first_failure_hist"),
                    "dominant_killers": st.session_state.get("scan_dominant_killers"),
                }
                md = fsp.elimination_narrative(ctx)
                st.markdown(md)
                st.download_button("Download Elimination Narrative (MD)",
                                   data=md.encode("utf-8"),
                                   file_name="process_of_elimination.md",
                                   mime="text/markdown")
                st.stop()

            if view == "Paperâ€‘Ready Signals":
                from tools.sandbox import forge_supremacy_plus as fsp
                st.caption("Paperâ€‘Ready Signals: stable figure/table IDs for deterministic replay.")
                filt = st.session_state.get("opt_archive_filtered") or st.session_state.get("opt_archive") or []
                if not filt:
                    st.info("No candidates available.")
                    st.stop()
                idx_sel = int(st.session_state.get("opt_inspect_idx", 0))
                cand = filt[int(max(0, min(idx_sel, len(filt)-1)))]
                sig = fsp.paper_ready_signals(cand)
                for item in sig["paper_ready_signals"]:
                    st.write(f"**{item['id']}** - {item['title']}  Â·  `{item['ref']}`")
                st.download_button("Download Paperâ€‘Ready Signals (JSON)",
                                   data=json.dumps(sig, indent=2).encode("utf-8"),
                                   file_name="paper_ready_signals.json",
                                   mime="application/json")
                st.stop()

            if view == "Silence Mode":
                st.caption("Silence Mode: review-room calm. Suppresses celebratory UI noise (no effect on physics).")
                st.session_state["silence_mode"] = st.toggle("Enable Silence Mode", value=bool(st.session_state.get("silence_mode", False)))
                if st.session_state.get("silence_mode"):
                    st.info("Silence Mode is ON. Prefer artifacts over narration.")
                else:
                    st.info("Silence Mode is OFF.")
                st.stop()
            if view == "Sensitivity Fingerprint":
                st.caption("Constraint Sensitivity Fingerprint: small perturbation fragility tags (screening-level).")
                from tools.sandbox import sensitivity_fingerprint as sf
                filt = st.session_state.get("opt_archive_filtered") or st.session_state.get("opt_archive") or []
                if not filt:
                    st.info("No candidates available.")
                    st.stop()
                idx_sel = int(st.session_state.get("opt_inspect_idx", 0))
                cand = filt[int(max(0, min(idx_sel, len(filt)-1)))]
                # Use existing point evaluator wrapper if available; otherwise show info only.
                evaluator = st.session_state.get("_frozen_point_evaluator")
                if evaluator is None:
                    st.warning("No evaluator wrapper found in session. Run any evaluation to enable fingerprints.")
                    st.stop()
                fp = sf.build_fingerprint(cand, evaluator=evaluator)
                for t in fp.get("tags", []):
                    st.write(f"- {t}")
                if fp.get("notes"):
                    with st.expander("Notes", expanded=False):
                        for n in fp["notes"]:
                            st.write(f"- {n}")
                st.download_button("Download Fingerprint (JSON)",
                                   data=json.dumps(fp, indent=2).encode("utf-8"),
                                   file_name="sensitivity_fingerprint.json",
                                   mime="application/json")
                st.stop()

            if view == "Reviewer Packet":
                st.caption("One-click Reviewer Packet: Markdown bundle + key artifacts (descriptive, deterministic).")
                filt = st.session_state.get("opt_archive_filtered") or st.session_state.get("opt_archive") or []
                if not filt:
                    st.info("No candidates available.")
                    st.stop()
                idx_sel = int(st.session_state.get("opt_inspect_idx", 0))
                cand = filt[int(max(0, min(idx_sel, len(filt)-1)))]
                from tools.sandbox.reviewer_packet_builder import ReviewerPacketOptions, build_reviewer_packet_zip
                with st.expander("Packet composition", expanded=False):
                    c1, c2, c3 = st.columns(3)
                    include_report = c1.checkbox("Include Report Pack", value=True, key="v322_rp_inc_report")
                    include_trinity = c2.checkbox("Include Review Trinity", value=True, key="v322_rp_inc_trinity")
                    include_attack = c3.checkbox("Include Attack Simulation", value=True, key="v322_rp_inc_attack")
                    c4, c5, c6 = st.columns(3)
                    include_scan = c4.checkbox("Include Scan Grounding", value=True, key="v322_rp_inc_scan")
                    include_capsule = c5.checkbox("Include Run Capsule", value=True, key="v322_rp_inc_capsule")
                    include_manifests = c6.checkbox("Include Repo Manifests", value=True, key="v322_rp_inc_manif")

                cap = run.get("capsule_v2") if isinstance(run, dict) else None
                sa = st.session_state.get("scan_cartography_artifact")
                sg = _scan_grounding(cand, sa, intent=str(run.get("intent") or "Reactor")) if isinstance(sa, dict) else None
                dnb = st.session_state.get("do_not_build_brief_latest")
                opts = ReviewerPacketOptions(
                    include_core_docs=True,
                    include_candidate_snapshot=True,
                    include_report_pack=bool(include_report),
                    include_review_trinity=bool(include_trinity),
                    include_attack_simulation=bool(include_attack),
                    include_scan_grounding=bool(include_scan),
                    include_run_capsule=bool(include_capsule),
                    include_do_not_build_brief=True,
                    include_repo_manifests=bool(include_manifests),
                )
                zip_bytes, summary = build_reviewer_packet_zip(
                    candidate=cand,
                    repo_root=Path(__file__).resolve().parent.parent,
                    run_capsule=cap if isinstance(cap, dict) else None,
                    scan_grounding=sg if isinstance(sg, dict) else None,
                    do_not_build_brief=dnb if isinstance(dnb, dict) else None,
                    options=opts,
                )
                st.download_button(
                    "Download Reviewer Packet (ZIP)",
                    data=zip_bytes,
                    file_name="shams_reviewer_packet.zip",
                    mime="application/zip",
                    use_container_width=True,
                )
                with st.expander("Packet manifest (preview)", expanded=False):
                    st.json(summary.get("manifest") or {}, expanded=False)
                st.stop()
            if view == "Review Trinity":
                st.caption("Review Trinity: Existence Proof â†’ Stress Story â†’ Positioning.")
                if not filt:
                    st.info("No candidates available.")
                    st.stop()
                idx_sel = int(st.session_state.get("opt_inspect_idx", 0))
                cand = filt[int(max(0, min(idx_sel, len(filt)-1)))]
                sa = st.session_state.get("scan_cartography_artifact")
                sg = _scan_grounding(cand, sa, intent=str(run.get("intent") or "Reactor")) if isinstance(sa, dict) else {}
                tri = build_review_trinity(candidate=cand, scan_grounding=sg if isinstance(sg, dict) else {})
                st.markdown(tri.get("markdown") or "")
                st.download_button("Download Review Trinity (md)", data=(tri.get("markdown") or "").encode("utf-8"), file_name="review_trinity.md", mime="text/markdown", use_container_width=True)
                st.download_button("Download Review Trinity (json)", data=json.dumps(tri, indent=2, sort_keys=True), file_name="review_trinity.json", mime="application/json", use_container_width=True)
                st.stop()

            if view == "Attack Simulation":
                st.caption("Hostile review rehearsal scaffold (no invented answers).")
                if not filt:
                    st.info("No candidates available.")
                    st.stop()
                idx_sel = int(st.session_state.get("opt_inspect_idx", 0))
                cand = filt[int(max(0, min(idx_sel, len(filt)-1)))]
                cap = run.get("capsule_v2") if isinstance(run, dict) else None
                atk = build_attack_simulation(candidate=cand, run_capsule=cap if isinstance(cap, dict) else None)
                st.markdown(atk.get("markdown") or "")
                st.download_button("Download Attack Simulation (md)", data=(atk.get("markdown") or "").encode("utf-8"), file_name="attack_simulation.md", mime="text/markdown", use_container_width=True)
                st.download_button("Download Attack Simulation (json)", data=json.dumps(atk, indent=2, sort_keys=True), file_name="attack_simulation.json", mime="application/json", use_container_width=True)
                st.stop()

            if view == "Exposure Readiness":
                st.caption(_LANG.get("external_exposure_gate"))
                try:
                    _chk = (Path(__file__).resolve().parent.parent / "docs" / "EXTERNAL_EXPOSURE_CHECKLIST.md").read_text(encoding="utf-8")
                except Exception:
                    _chk = "(missing docs/EXTERNAL_EXPOSURE_CHECKLIST.md)"
                st.markdown(_chk)
                st.download_button("Download exposure checklist (md)", data=_chk.encode("utf-8"), file_name="EXTERNAL_EXPOSURE_CHECKLIST.md", mime="text/markdown", use_container_width=True)
                st.stop()
            if view in ("Reactor Accounting Console","Margin Ledger","Reality Gates","Economics Deck","Report Pack","Closure Certificate","Provenance Graph","Engineering Reality Budget","Failure-Mode Canon","Design Class","Citation Blocks","Reference Reproduction"):
                if not filt:
                    st.info("No candidates available in the archive (after filters).")
                else:
                    import pandas as _pd
                    def _lab(i):
                        a = filt[i]
                        ms = a.get("min_signed_margin")
                        try:
                            ms = float(ms)
                        except Exception:
                            ms = float('nan')
                        return f"{i:03d} | min_margin={ms:.3g} | {str(a.get('failure_mode') or '')}".strip()
                    idx = st.selectbox("Candidate", options=list(range(len(filt))), format_func=_lab, key="v203_rdf_candidate_pick")
                    a = filt[int(idx)]
                    if view == "Reactor Accounting Console":
                        st.caption("Explicit plant closure (derived). No hidden penalties.")
                        st.json(a.get("closure_bundle") or {}, expanded=False)
                    elif view == "Margin Ledger":
                        mb = a.get("margin_budget") or {}
                        rows = mb.get("rows") or []
                        if rows:
                            st.dataframe(_pd.DataFrame(rows), use_container_width=True)
                        else:
                            st.json(mb, expanded=False)
                    elif view == "Reality Gates":
                        st.json(a.get("reality_gates") or {}, expanded=False)
                    elif view == "Economics Deck":
                        cb = a.get("closure_bundle") or {}
                        env = (cb.get("economics_envelopes") if isinstance(cb, dict) else None) or []
                        if env:
                            st.dataframe(_pd.DataFrame(env), use_container_width=True)
                        else:
                            st.json(cb, expanded=False)
                    elif view == "Report Pack":
                        rp = a.get("report_pack") or {}
                        md = rp.get("markdown") or "(no report)"
                        st.markdown(md)
                        st.download_button("Download report JSON", data=json.dumps(rp.get("json") or {}, indent=2, sort_keys=True), file_name="shams_reactor_design_forge_report.json", mime="application/json", use_container_width=True, key="v203_dl_rdf_json")
                        st.download_button("Download report Markdown", data=str(md), file_name="shams_reactor_design_forge_report.md", mime="text/markdown", use_container_width=True, key="v203_dl_rdf_md")
                        st.download_button("Download report CSV", data=str(rp.get("csv") or ""), file_name="shams_reactor_design_forge_report.csv", mime="text/csv", use_container_width=True, key="v203_dl_rdf_csv")

                    elif view == "Closure Certificate":
                        from tools.sandbox.closure_certificate import build_closure_certificate
                        cert = a.get("closure_certificate") or build_closure_certificate(a)
                        st.subheader(f"Feasibility Closure Certificate - {cert.get('verdict')}")
                        st.json(cert, expanded=False)
                        st.download_button("Download Closure Certificate (JSON)",
                                           data=json.dumps(cert, indent=2, sort_keys=True),
                                           file_name="shams_closure_certificate.json",
                                           mime="application/json",
                                           use_container_width=True,
                                           key="rdf_dl_fcc")
                    elif view == "Provenance Graph":
                        from tools.sandbox.constraint_provenance_graph import build_cpg_for_constraint
                        st.caption("Constraint Provenance Graph: where each limit comes from (structure, not new physics).")
                        cname = st.text_input("Constraint name (e.g., q_div, sigma_vm, HTS margin, TBR, net_electric)", value=str(a.get("failure_mode") or "q_div"))
                        cpg = build_cpg_for_constraint(cname, intent=str(run.get("intent") or ""))
                        st.json(cpg, expanded=False)
                        st.download_button("Download CPG (JSON)",
                                           data=json.dumps(cpg, indent=2, sort_keys=True),
                                           file_name="shams_constraint_provenance_graph.json",
                                           mime="application/json",
                                           use_container_width=True,
                                           key="rdf_dl_cpg")
                    elif view == "Engineering Reality Budget":
                        st.caption("Engineering Reality Budget: grouped margin currency (descriptive).")
                        mb = a.get("margin_budget") or {}
                        rows = mb.get("rows") or []
                        groups = {"plasma": [], "materials": [], "thermal": [], "economics": [], "other": []}
                        for r in rows:
                            nm = str(r.get("name") or "").lower()
                            if any(k in nm for k in ["q95","bet","greenwald","h98","kappa","delta","stability","plasma"]):
                                groups["plasma"].append(r)
                            elif any(k in nm for k in ["sigma","stress","strain","tf","structure","hts","coil"]):
                                groups["materials"].append(r)
                            elif any(k in nm for k in ["q_div","heat","divert","sol","thermal","wall"]):
                                groups["thermal"].append(r)
                            elif any(k in nm for k in ["coe","cost","econ","net","recirc","electric"]):
                                groups["economics"].append(r)
                            else:
                                groups["other"].append(r)
                        for g,rs in groups.items():
                            with st.expander(f"{g.title()} budget", expanded=(g in ["plasma","materials","thermal"])):
                                if rs:
                                    import pandas as _pd
                                    st.dataframe(_pd.DataFrame(rs), use_container_width=True)
                                else:
                                    st.write("(no rows in this bucket)")
                        st.download_button("Download Reality Budget (JSON)",
                                           data=json.dumps(groups, indent=2, sort_keys=True),
                                           file_name="shams_engineering_reality_budget.json",
                                           mime="application/json",
                                           use_container_width=True,
                                           key="rdf_dl_erb")
                    elif view == "Failure-Mode Canon":
                        st.caption("Failure-Mode Canon: standardized non-prescriptive archetypes.")
                        canon = {
                            "heat-flux dominated": ["q_div", "divertor", "sol"],
                            "stress-limited": ["sigma", "stress", "vm"],
                            "hts-margin collapse": ["hts", "margin"],
                            "breeding-limited": ["tbr", "breed"],
                            "recirculation-trapped": ["recirc", "net", "electric"],
                            "coupled failure": ["+", "and", "coupled"],
                        }
                        fm_now = str(a.get("failure_mode") or "")
                        tag = "unclassified"
                        fml = fm_now.lower()
                        for k, toks in canon.items():
                            if any(t in fml for t in toks):
                                tag = k
                                break
                        st.write({"failure_mode": fm_now, "canonical_tag": tag})
                        st.json(canon, expanded=False)
                        st.download_button("Download Failure-Mode Canon (JSON)",
                                           data=json.dumps({"canonical_tag": tag, "canon": canon}, indent=2, sort_keys=True),
                                           file_name="shams_failure_mode_canon.json",
                                           mime="application/json",
                                           use_container_width=True,
                                           key="rdf_dl_fmc")
                    elif view == "Design Class":
                        dc = a.get("design_class") or {}
                        st.subheader(f"{dc.get('code','')} - {dc.get('name','')}")
                        st.json(dc, expanded=False)
                        st.download_button("Download Design Class (JSON)",
                                           data=json.dumps(dc, indent=2, sort_keys=True),
                                           file_name="shams_design_class.json",
                                           mime="application/json",
                                           use_container_width=True,
                                           key="rdf_dl_dc")
                    elif view == "Citation Blocks":
                        cb = a.get("citation_blocks") or {}
                        st.caption("Paste-ready Methods + citation scaffold (local repo content).")
                        st.text_area("Methods block", value=str(cb.get("methods_block") or ""), height=200)
                        if cb.get("citation_cff"):
                            with st.expander("CITATION.cff", expanded=False):
                                st.code(cb.get("citation_cff"))
                        st.download_button("Download Citation Blocks (JSON)",
                                           data=json.dumps(cb, indent=2, sort_keys=True),
                                           file_name="shams_citation_blocks.json",
                                           mime="application/json",
                                           use_container_width=True,
                                           key="rdf_dl_cb")
                    elif view == "Reference Reproduction":
                        rc = a.get("reference_context") or {}
                        st.caption("Historical reproduction: compare candidate to reference presets (anchors, not targets).")
                        refs = rc.get("refs") or []
                        if refs:
                            for ref in refs:
                                with st.expander(str(ref.get("ref")), expanded=False):
                                    st.json(ref.get("comparison") or {}, expanded=False)
                        else:
                            st.info("No reference context available for this candidate.")
                        st.download_button("Download Reference Context (JSON)",
                                           data=json.dumps(rc, indent=2, sort_keys=True),
                                           file_name="shams_reference_context.json",
                                           mime="application/json",
                                           use_container_width=True,
                                           key="rdf_dl_ref")

                st.stop()

            # --- v203 Reactor Design Forge panels (PROCESS-independence) ---
            def _pick_candidate(_cands):
                if not _cands:
                    return None
                opts = []
                for idx,a in enumerate(_cands):
                    ms = a.get('min_signed_margin')
                    try:
                        msf = float(ms)
                    except Exception:
                        msf = None
                    fp = a.get('fingerprint') or a.get('_id') or str(idx)
                    opts.append((f"{idx:03d} | min_margin={msf if msf is not None else 'na'} | {fp}", idx))
                label_to_i = {l:i for l,i in opts}
                lab = st.selectbox('Select candidate', options=[l for l,_ in opts], index=0, key='rdf_pick')
                return _cands[int(label_to_i.get(lab,0))]

            if view in ["Reactor Accounting Console","Margin Ledger","Reality Gates","Economics Deck","Report Pack"]:
                sel = _pick_candidate(filt)
                if sel is None:
                    st.info('Archive is empty. Run a case first.')
                else:
                    if view == "Reactor Accounting Console":
                        st.caption('Explicit plant/accounting closure derived from frozen truth outputs. No hidden assumptions.')
                        cb = sel.get('closure_bundle')
                        if not isinstance(cb, dict):
                            cb = closure_console(outputs=sel.get('outputs') or {}, cost_proxy=sel.get('cost') or {})
                        st.json(cb, expanded=False)
                    elif view == "Margin Ledger":
                        st.caption('Constraint margin budget (engineering accounting). Not a score.')
                        mb = sel.get('margin_budget')
                        if not isinstance(mb, dict):
                            mb = margin_budget(sel.get('constraints') or [])
                        import pandas as _pd
                        st.dataframe(_pd.DataFrame(mb.get('rows') or []), use_container_width=True)
                        st.write({'min_signed_margin': mb.get('min_signed_margin'), 'tight_constraints': mb.get('tight_constraints')})
                    elif view == "Reality Gates":
                        st.caption('Declared buildability gates (toggleable, descriptive).')
                        rg = sel.get('reality_gates')
                        if not isinstance(rg, dict):
                            rg = reality_gates(sel.get('constraints') or [], sel.get('closure_bundle') if isinstance(sel.get('closure_bundle'), dict) else None)
                        st.json(rg, expanded=False)
                    elif view == "Economics Deck":
                        st.caption('Explicit economics envelopes (Optimistic / Nominal / Conservative).')
                        cb = sel.get('closure_bundle')
                        if not isinstance(cb, dict):
                            cb = closure_console(outputs=sel.get('outputs') or {}, cost_proxy=sel.get('cost') or {})
                        env = (cb.get('economics_envelopes') or []) if isinstance(cb, dict) else []
                        import pandas as _pd
                        st.dataframe(_pd.DataFrame(env), use_container_width=True)
                    elif view == "Report Pack":
                        st.caption('PROCESS-recognizable report pack (audit-clean). No ranking, no recommendations.')
                        rp = sel.get('report_pack')
                        if not isinstance(rp, dict):
                            rp = build_report_pack(intent=str(run.get('intent')), inputs=sel.get('inputs') or {}, outputs=sel.get('outputs') or {}, constraints=sel.get('constraints') or [], closure_bundle=sel.get('closure_bundle') if isinstance(sel.get('closure_bundle'), dict) else None, margin_budget=sel.get('margin_budget') if isinstance(sel.get('margin_budget'), dict) else None, reality_gates=sel.get('reality_gates') if isinstance(sel.get('reality_gates'), dict) else None)
                        st.download_button('Download report JSON', data=json.dumps(rp.get('json') or {}, indent=2, sort_keys=True), file_name='shams_report_pack.json', mime='application/json', use_container_width=True)
                        st.download_button('Download report markdown', data=str(rp.get('markdown') or ''), file_name='shams_report_pack.md', mime='text/markdown', use_container_width=True)
                        st.download_button('Download report CSV', data=str(rp.get('csv') or ''), file_name='shams_report_pack.csv', mime='text/csv', use_container_width=True)
                        with st.expander('Preview markdown', expanded=False):
                            st.markdown(str(rp.get('markdown') or ''))

            if view == "Design navigation (steering)":
                st.caption("Steering cues are derived from a local linear surface map built from evaluated archive data. Descriptive only.")
                # derive variable keys from var_specs
                vs = run.get("var_specs") or []
                var_k = [str(v.get("key")) for v in vs if isinstance(v, dict) and v.get("key")]
                # constraint list
                cnames = []
                try:
                    if (run.get("archive") or []) and ((run.get("archive")[0].get("constraints") or [])):
                        cnames = sorted(list({str(c.get("name")) for c in (run.get("archive")[0].get("constraints") or []) if c.get("name")}))
                except Exception:
                    cnames = []
                if not cnames:
                    st.info("No constraint names available in archive.")
                else:
                    csel = st.selectbox("Constraint to navigate", options=cnames, index=0, key="opt_nav_constraint")
                    fam = st.selectbox("Lever family", options=["All","Geometry","ðŸ”¥ Plasma","Power","ðŸ§² Magnets","Other"], index=0, key="opt_nav_family")
                    smap = constraint_surface_map(archive=run.get("archive") or [], var_keys=var_k, constraint_name=str(csel))
                    if not smap.get("ok"):
                        st.warning(f"Surface map not available: {smap.get('reason')}")
                        st.json(smap)
                    else:
                        cues = steering_cues_from_surface_map(smap)
                        cues = filter_cues(cues, family=fam, top_n=15)
                        st.markdown("#### Steering cues (local, descriptive)")
                        if cues:
                            import pandas as _pd
                            st.dataframe(_pd.DataFrame(cues)[["family","lever","cue","signed","strength"]], use_container_width=True)
                        else:
                            st.info("No cues available (insufficient gradient data).")
                        with st.expander("Surface map details", expanded=False):
                            st.json(smap, expanded=False)

            elif view == "Machine existence report":
                st.caption("Explains why the currently selected candidate exists (or how close it is). No recommendations.")
                idx2 = int(st.session_state.get("opt_inspect_idx", 0))
                cand = filt[idx2] if filt and idx2 < len(filt) else (filt[0] if filt else None)
                if cand is None:
                    st.info("No candidate available.")
                else:
                    rep = existence_report(cand)
                    st.info(rep.get("narrative"))
                    c1, c2 = st.columns(2)
                    with c1:
                        st.markdown("**Tight constraints**")
                        st.write(rep.get("tight", []))
                    with c2:
                        st.markdown("**Slack constraints**")
                        st.write(rep.get("slack", []))
                    with st.expander("Full existence report (json)", expanded=False):
                        st.json(rep, expanded=False)

            elif view == "Archive regimes & coverage":
                st.caption("Descriptive regime clustering and coverage cues for the feasible archive.")
                ar0 = run.get("archive") or []
                vs = run.get("var_specs") or []
                var_k = [str(v.get("key")) for v in vs if isinstance(v, dict) and v.get("key")]
                h = ladder_histogram(ar0)
                st.markdown("#### Feasibility ladder histogram")
                st.write(h)
                summ = regime_clusters_summary(archive=ar0, var_keys=var_k, max_k=10, seed=int(run.get("seed", 0) or 0))
                if not summ.get("ok"):
                    st.warning(summ.get("reason"))
                else:
                    import pandas as _pd
                    st.markdown("#### Regime clusters (feasible points)")
                    st.dataframe(_pd.DataFrame(summ.get("clusters") or []), use_container_width=True)
                    st.caption("Clusters are descriptive only; they do not imply optimality.")

            elif view == "Machine Dossier":
                st.caption("A structured, exportable dossier for the selected candidate. Descriptive only - no recommendations.")
                # Select candidate
                cand = None
                if filt:
                    try:
                        idx = int(st.session_state.get("opt_inspect_idx", 0) or 0)
                        idx = max(0, min(len(filt)-1, idx))
                        cand = filt[idx]
                    except Exception:
                        cand = filt[0]
                if cand is None:
                    st.info("No candidate available. Run the Machine Finder to populate the archive.")
                else:
                    # Tabs: Truth, Closure, Margins, Costs, Existence, Neighborhood
                    t_truth, t_close, t_marg, t_cost, t_exist, t_neigh = st.tabs([
                        "Truth",
                        "Closure",
                        "Margins",
                        "ðŸ’° Economics",
                        "Why it exists",
                        "Neighborhood",
                    ])
                    with t_truth:
                        st.markdown("#### Inputs")
                        st.json(cand.get("inputs") or {}, expanded=False)
                        st.markdown("#### Key outputs")
                        out = cand.get("outputs") or {}
                        # show a compact subset if present
                        keys = ["Pfus_total_MW","P_e_net_MW","Q_DT_eqv","q_div_MW_m2","min_signed_margin"]
                        st.write({k: out.get(k) for k in keys if k in out} or out)
                        st.markdown("#### Constraint ladder")
                        st.write({
                            "feasibility_state": cand.get("feasibility_state"),
                            "robustness_class": cand.get("robustness_class"),
                            "first_failure": cand.get("first_failure"),
                            "failure_mode": cand.get("failure_mode"),
                        })

                        st.markdown("#### Expert signals (descriptive)")
                        tags = _regime_signature(cand)
                        fk = _first_kill(cand)
                        st.write({
                            "regime_signature": tags,
                            "first_kill": fk,
                        })
                        sr = _constraint_spend_rate(cand)
                        if isinstance(sr, dict) and sr.get("ok"):
                            with st.expander("Constraint spend rate vs parent (heuristic)", expanded=False):
                                st.json(sr, expanded=False)
                        with st.expander("Constraint records", expanded=False):
                            st.json(cand.get("constraints") or [], expanded=False)

                    with t_close:
                        st.caption("Plant closure and accounting are computed explicitly (parity layer). They do not modify frozen truth.")
                        try:
                            from src.parity import parity_plant_closure, parity_magnets, parity_cryo
                            from src.models.inputs import PointInputs
                            pi = cand.get("_point_inputs_obj")
                            if pi is None:
                                pi = PointInputs(**(cand.get("inputs") or {}))
                            outputs = cand.get("outputs") or {}
                            plant = parity_plant_closure(pi, outputs)
                            magnets = parity_magnets(pi, outputs)
                            cryo = parity_cryo(pi, outputs)
                            c1,c2,c3 = st.columns(3)
                            c1.metric("Net electric (MW)", f"{plant.get('derived',{}).get('P_e_net_MW', float('nan')):.3g}")
                            c2.metric("Recirc electric (MW)", f"{plant.get('derived',{}).get('P_recirc_e_MW', float('nan')):.3g}")
                            c3.metric("Qe", f"{plant.get('derived',{}).get('Qe', float('nan')):.3g}")
                            with st.expander("Plant closure", expanded=False):
                                st.json(plant, expanded=False)
                            with st.expander("ðŸ§² Magnets", expanded=False):
                                st.json(magnets, expanded=False)
                            with st.expander("Cryogenics", expanded=False):
                                st.json(cryo, expanded=False)
                        except Exception as e:
                            st.error(f"Closure console unavailable for this candidate: {e}")

                    with t_marg:
                        st.caption("Margin budget view: tight vs slack constraints (descriptive).")
                        rows=[]
                        for r in (cand.get("constraints") or []):
                            try:
                                rows.append({
                                    "name": r.get("name"),
                                    "ok": bool(r.get("ok")),
                                    "signed_margin": r.get("signed_margin"),
                                    "value": r.get("value"),
                                    "limit": r.get("limit"),
                                })
                            except Exception:
                                pass
                        if rows:
                            import pandas as _pd
                            df=_pd.DataFrame(rows)
                            df=df.sort_values(by="signed_margin", ascending=True, na_position="last")
                            st.dataframe(df, use_container_width=True, hide_index=True)
                        else:
                            st.info("No constraint margin records for this candidate.")

                    with t_cost:
                        st.caption("Economics envelopes are explicit lenses (optimistic/nominal/conservative).")
                        try:
                            from src.parity import parity_costing_envelope, parity_costing
                            from src.models.inputs import PointInputs
                            pi = cand.get("_point_inputs_obj")
                            if pi is None:
                                pi = PointInputs(**(cand.get("inputs") or {}))
                            outputs = cand.get("outputs") or {}
                            env = parity_costing_envelope(pi, outputs)
                            base = parity_costing(pi, outputs)
                            c1,c2,c3 = st.columns(3)
                            c1.metric("CAPEX (MUSD)", f"{base.get('derived',{}).get('CAPEX_MUSD', float('nan')):.3g}")
                            c2.metric("LCOE (USD/MWh)", f"{base.get('derived',{}).get('LCOE_USD_per_MWh', float('nan')):.3g}")
                            # show envelope headline
                            if isinstance(env, dict) and env.get("nominal"):
                                st.caption(
                                    f"Envelope LCOE - Opt {env.get('optimistic',{}).get('LCOE_USD_per_MWh', float('nan')):.3g} | "
                                    f"Nom {env.get('nominal',{}).get('LCOE_USD_per_MWh', float('nan')):.3g} | "
                                    f"Con {env.get('conservative',{}).get('LCOE_USD_per_MWh', float('nan')):.3g}"
                                )
                            with st.expander("Economics envelope", expanded=False):
                                st.json(env, expanded=False)
                            with st.expander("Base costing (proxy)", expanded=False):
                                st.json(base.get("raw", base), expanded=False)
                        except Exception as e:
                            st.error(f"Economics deck unavailable: {e}")

                    with t_exist:
                        try:
                            rep = existence_report(cand)
                            st.json(rep, expanded=False)
                        except Exception as e:
                            st.error(f"Existence report failed: {e}")

                    with t_neigh:
                        st.caption("Bridge to Scan Lab: open a local slice around this candidate (no auto-run).")
                        try:
                            if st.button("Set Scan Lab slice around candidate (Ip vs R0)", use_container_width=True, key="opt_set_scan_slice"):
                                inp = cand.get("inputs") or {}
                                st.session_state["scan_x"] = "R0_m"
                                st.session_state["scan_y"] = "Ip_MA"
                                R0=float(inp.get("R0_m", 1.0))
                                Ip=float(inp.get("Ip_MA", 1.0))
                                st.session_state["scan_bounds"] = {
                                    "R0_m": [max(0.2, 0.7*R0), 1.3*R0],
                                    "Ip_MA": [max(0.1, 0.7*Ip), 1.3*Ip],
                                }
                                st.success("Scan Lab slice parameters set in session state. Switch to Scan Lab to run.")
                        except Exception as e:
                            st.error(f"Could not set Scan Lab slice: {e}")

            elif view == "Expert Compare (no ranking)":
                st.caption("Compare a handful of candidates side-by-side. No ranking, no recommendation - just numbers and margins.")
                ar0 = filt or (run.get("archive") or [])
                if not ar0:
                    st.info("No archive available.")
                else:
                    max_n = min(12, len(ar0))
                    idxs = st.multiselect(
                        "Select candidate indices (from filtered archive order)",
                        options=list(range(len(ar0))),
                        default=list(range(min(3, len(ar0)))),
                        key="opt_compare_idxs",
                    )
                    rows=[]
                    for idx in idxs[:max_n]:
                        a=ar0[int(idx)]
                        out=a.get("outputs") or {}
                        inp=a.get("inputs") or {}
                        rows.append({
                            "idx": int(idx),
                            "feasibility_state": a.get("feasibility_state"),
                            "robustness": a.get("robustness_class"),
                            "first_failure": a.get("first_failure"),
                            "R0_m": inp.get("R0_m"),
                            "a_m": inp.get("a_m"),
                            "Bt_T": inp.get("Bt_T"),
                            "Ip_MA": inp.get("Ip_MA"),
                            "Pfus_total_MW": out.get("Pfus_total_MW"),
                            "P_e_net_MW": out.get("P_e_net_MW"),
                            "Q_DT_eqv": out.get("Q_DT_eqv"),
                            "q_div_MW_m2": out.get("q_div_MW_m2"),
                            "min_signed_margin": out.get("min_signed_margin", a.get("min_signed_margin")),
                        })
                    if rows:
                        import pandas as _pd
                        st.dataframe(_pd.DataFrame(rows), use_container_width=True, hide_index=True)
                    st.caption("Tip: use the Inspector index to sync with Machine Dossier.")

            elif view == "Casebook Runner":
                st.caption("Run a small set of declared cases (intent+lens+bounds) and compare feasibility distributions. No recommendations.")

                # Optional: load the packaged flagship casebook (review-room demo)
                if st.button("Load flagship casebook (packaged)", use_container_width=True, key="opt_load_flagship_casebook", disabled=forge_lock):
                    try:
                        from pathlib import Path as _P
                        _p = _P("scenarios") / "flagship_casebook.json"
                        if _p.exists():
                            st.session_state["opt_casebook"] = json.loads(_p.read_text(encoding="utf-8"))
                            st.success("Flagship casebook loaded.")
                        else:
                            st.warning("flagship_casebook.json not found in scenarios/.")
                    except Exception as _e:
                        st.warning(f"Could not load flagship casebook: {_e}")
                if "opt_casebook" not in st.session_state or not isinstance(st.session_state.get("opt_casebook"), list):
                    st.session_state["opt_casebook"] = []
                # Case definition UI
                c1,c2,c3 = st.columns([2,2,1])
                with c1:
                    case_name = st.text_input("Case name", value=f"Case {len(st.session_state['opt_casebook'])+1}", key="opt_case_name")
                with c2:
                    case_lens = st.selectbox("Lens", list((default_objective_packs(_design_intent_key()) or {}).keys()) or ["default"], key="opt_case_lens")
                with c3:
                    case_seed = st.number_input("Seed", value=int(run.get("seed", 0) or 0), step=1, key="opt_case_seed")
                if st.button("Add case to casebook", use_container_width=True, key="opt_add_case", disabled=forge_lock):
                    st.session_state["opt_casebook"].append({"name": case_name, "lens": case_lens, "seed": int(case_seed)})
                    st.success("Case added.")
                # Display current casebook
                if st.session_state["opt_casebook"]:
                    import pandas as _pd
                    st.dataframe(_pd.DataFrame(st.session_state["opt_casebook"]), use_container_width=True, hide_index=True)
                else:
                    st.info("Casebook is empty. Add 2â€“5 cases to run.")
                # Run cases (small budgets unless expert toggles)
                budget = int(st.number_input("Per-case evaluation budget", value=120, min_value=20, max_value=5000, step=20, key="opt_case_budget"))
                if st.button("Run casebook", use_container_width=True, key="opt_run_casebook", disabled=forge_lock):
                    results=[]
                    for case in (st.session_state["opt_casebook"] or [])[:10]:
                        try:
                            # Reuse current bounds and var specs from session if present
                            _packs = default_objective_packs(_design_intent_key()) or {}
                            pack = _packs.get(case["lens"]) or next(iter(_packs.values())) if _packs else {}
                            # minimal var specs: use session var specs if present
                            var_specs = run.get("var_specs") or st.session_state.get("opt_var_specs") or []
                            bounds = st.session_state.get("opt_bounds") or {}
                            rcase = run_hybrid_machine_finder(
                                seed=int(case.get("seed",0)),
                                intent=_design_intent_key(),
                                objective_pack=pack,
                                bounds=bounds,
                                var_specs=var_specs,
                                budget=int(budget),
                                cache_enabled=bool(st.session_state.get("opt_cache_enabled", True)),
                                cache_max=int(st.session_state.get("opt_cache_max", 256)),
                            )
                            tr = rcase.get("trace") or []
                            n=len(tr); nf=sum(1 for t in tr if bool(t.get("feasible")))
                            results.append({"case": case["name"], "lens": case["lens"], "seed": case["seed"], "n_eval": n, "n_feasible": nf})
                        except Exception as e:
                            results.append({"case": case["name"], "lens": case["lens"], "seed": case["seed"], "n_eval": 0, "n_feasible": 0, "error": str(e)})
                    st.session_state["opt_casebook_results"] = results
                res = st.session_state.get("opt_casebook_results") or []
                if res:
                    import pandas as _pd
                    st.markdown("#### Casebook results (summary)")
                    st.dataframe(_pd.DataFrame(res), use_container_width=True, hide_index=True)
        

            elif view == "Archive Bay":
                # pick axes
                xkey = st.selectbox("x-axis", ["R0_m","Bt_T","Ip_MA","P_e_net_MW","Pfus_total_MW","q_div_MW_m2","min_signed_margin","_score"], index=0, key="opt_scatter_x")
                ykey = st.selectbox("y-axis", ["P_e_net_MW","Pfus_total_MW","Q_DT_eqv","q_div_MW_m2","min_signed_margin","_score"], index=0, key="opt_scatter_y")
                rows=[]
                for a in filt:
                    o=a.get("outputs") or {}
                    i=a.get("inputs") or {}
                    def get(k):
                        if k in ("_score",):
                            return float(a.get("_score",-1e30))
                        if k in i:
                            return float(i.get(k, float("nan")))
                        return float(o.get(k, float("nan")))
                    rows.append({"x": get(xkey), "y": get(ykey), "feasible": bool(a.get("feasible", False)), "idx": len(rows)})
                if rows:
                    import pandas as _pd
                    df=_pd.DataFrame(rows)
                    st.scatter_chart(df, x="x", y="y")
                    st.caption("Use the Inspector to select a candidate by index from the filtered archive.")
                else:
                    st.info("No points after filtering. Relax filters or rerun with wider bounds/budget.")
            elif view == "Reactor Accounting Console":
                st.caption(
                    "Optional PROCESS-parity accounting: plant closure + magnets + cryo + costing. "
                    "This is a *lens* and does not change frozen evaluator truth."
                )
                # choose candidate
                cand = None
                if filt:
                    cand = filt[int(min(len(filt)-1, int(st.session_state.get("opt_inspect_idx", 0))))]
                elif isinstance(run.get("best_feasible"), dict):
                    cand = run.get("best_feasible")
                if cand is None:
                    st.info("No candidate available. Run the Machine Finder to populate the archive.")
                else:
                    try:
                        from src.parity import parity_plant_closure, parity_magnets, parity_cryo, parity_costing, parity_costing_envelope
                        from src.parity.calibration import economics_local_sensitivity
                        from src.parity.report_pack import build_parity_report_pack, report_pack_to_csv_rows, report_pack_to_markdown
                        pi = cand.get("_point_inputs_obj")
                        if pi is None:
                            # reconstruct PointInputs if present
                            from src.models.inputs import PointInputs
                            pi = PointInputs(**(cand.get("inputs") or {}))
                        outputs = cand.get("outputs") or {}
                        parity = {
                            "plant": parity_plant_closure(pi, outputs),
                            "magnets": parity_magnets(pi, outputs),
                            "cryo": parity_cryo(pi, outputs),
                            "costing": parity_costing(pi, outputs),
                            "costing_envelope": parity_costing_envelope(pi, outputs),
                        }
                        # summary cards
                        c1,c2,c3,c4 = st.columns(4)
                        c1.metric("Net electric (MW)", f"{parity['plant']['derived'].get('P_e_net_MW', float('nan')):.3g}")
                        c2.metric("Qe", f"{parity['plant']['derived'].get('Qe', float('nan')):.3g}")
                        c3.metric("CAPEX (MUSD)", f"{parity['costing']['derived'].get('CAPEX_MUSD', float('nan')):.3g}")
                        c4.metric("LCOE (USD/MWh)", f"{parity['costing']['derived'].get('LCOE_USD_per_MWh', float('nan')):.3g}")
                        # Cost envelope (Phase-2)
                        env = parity.get('costing_envelope', {})
                        posture = st.session_state.get('ppl_cost_posture', 'Nominal')
                        if isinstance(env, dict) and env.get('nominal'):
                            nom = env.get('nominal', {})
                            opt = env.get('optimistic', {})
                            con = env.get('conservative', {})
                            st.caption(
                                f"Economics envelope - Optimistic {opt.get('LCOE_USD_per_MWh', float('nan')):.3g} | "
                                f"Nominal {nom.get('LCOE_USD_per_MWh', float('nan')):.3g} | "
                                f"Conservative {con.get('LCOE_USD_per_MWh', float('nan')):.3g}  (posture: {posture})"
                            )
                        with st.expander("Plant closure", expanded=False):
                            st.json(parity["plant"], expanded=False)
                        with st.expander("ðŸ§² Magnets", expanded=False):
                            st.json(parity["magnets"], expanded=False)
                        with st.expander("Cryogenics", expanded=False):
                            st.json(parity["cryo"], expanded=False)
                        with st.expander("Costing (proxy)", expanded=False):
                            st.json(parity["costing"].get("raw", parity["costing"]), expanded=False)

                        # --- v2 additions: CAPEX breakdown + local sensitivities ---
                        st.markdown("#### ðŸ’° Economics breakdown")
                        bd = (parity.get("costing") or {}).get("derived", {}).get("breakdown_MUSD", {}) or {}
                        if bd:
                            import pandas as _pd
                            df = _pd.DataFrame({"component": list(bd.keys()), "CAPEX_MUSD": [float(bd[k]) for k in bd.keys()]})
                            st.bar_chart(df, x="component", y="CAPEX_MUSD")
                            st.caption("CAPEX proxy breakdown (component bars). Total CAPEX is the sum of these components.")
                        else:
                            st.info("No CAPEX breakdown available for this candidate.")

                        with st.expander("Local sensitivity (economics knobs)", expanded=False):
                            sens = economics_local_sensitivity(inputs=pi, outputs=outputs, perturb_frac=0.10)
                            st.json(sens.get("base", {}), expanded=False)
                            rows = sens.get("rows") or []
                            if rows:
                                import pandas as _pd
                                st.dataframe(_pd.DataFrame(rows), use_container_width=True)
                            st.caption("Finite-difference lens (+10% knob perturbation). Not a gradient; used for intuition.")

                        import json as _json
                        pack = build_parity_report_pack(
                            inputs=pi,
                            outputs=outputs,
                            parity=parity,
                            run_id=str(run.get("run_id", "")),
                            version=str(run.get("version", "")),
                        )
                        md = report_pack_to_markdown(pack)
                        header, row = report_pack_to_csv_rows(pack)
                        csv = ",".join(header) + "\n" + ",".join([str(x) for x in row]) + "\n"
                        st.download_button(
                            "Download parity report (JSON)",
                            data=_json.dumps(pack, indent=2).encode("utf-8"),
                            file_name="shams_parity_report_pack.json",
                            mime="application/json",
                            use_container_width=True,
                        )
                        st.download_button(
                            "Download parity report (Markdown)",
                            data=md.encode("utf-8"),
                            file_name="shams_parity_report_pack.md",
                            mime="text/markdown",
                            use_container_width=True,
                        )
                        st.download_button(
                            "Download parity flat row (CSV)",
                            data=csv.encode("utf-8"),
                            file_name="shams_parity_report_pack.csv",
                            mime="text/csv",
                            use_container_width=True,
                        )
                    except Exception as _e:
                        st.error(f"Parity layer failed: {_e}")
            elif view == "Parity validation packs (PASS/WARN/FAIL)":

                st.caption("Named validation packs with explicit tolerances. Load your own reference values to certify parity.")
                from pathlib import Path
                from src.parity.validation_packs import load_validation_packs, evaluate_pack_candidate, compare_to_reference

                packs_path = Path("benchmarks/ppl_validation_packs_v3.json")
                refs_path = Path("benchmarks/ppl_validation_refs_v3.json")
                packs = load_validation_packs(packs_path)

                st.markdown("#### Validation pack selection")
                pack_titles = [f"{p.title}  -  ({p.pack_id})" for p in packs]
                sel = st.selectbox("Pack", pack_titles, index=0, key="ppl_pack_sel")
                pack = packs[pack_titles.index(sel)]

                st.markdown("#### Reference table")
                use_builtin = st.checkbox("Use built-in reference (placeholder)", value=True, key="ppl_use_builtin_refs")
                ref_dict = {}
                if use_builtin and refs_path.exists():
                    try:
                        ref_dict = __import__("json").loads(refs_path.read_text(encoding="utf-8")).get("refs", {}).get(pack.pack_id, {})
                    except Exception as _e:
                        st.warning(f"Could not read built-in refs: {_e}")

                up = st.file_uploader("Or upload reference JSON (expects {'refs': {pack_id: {metric_key: value}}})", type=["json"], key="ppl_ref_upload")
                if up is not None:
                    try:
                        payload = __import__("json").loads(up.read().decode("utf-8"))
                        ref_dict = payload.get("refs", {}).get(pack.pack_id, {})
                        st.success("Loaded uploaded reference table.")
                    except Exception as _e:
                        st.error(f"Could not parse reference JSON: {_e}")

                tol_scale = st.slider("Tolerance scale (multiplies per-metric tolerances)", 0.25, 3.0, 1.0, 0.05, key="ppl_tol_scale")

                if st.button("Run validation pack", key="ppl_run_pack"):
                    try:
                        preset, outputs, metrics, meta = evaluate_pack_candidate(pack)
                        # apply tol scaling on-the-fly
                        scaled_pack = type(pack)(
                            pack_id=pack.pack_id,
                            title=pack.title,
                            preset_key=pack.preset_key,
                            design_intent=pack.design_intent,
                            compare_keys=pack.compare_keys,
                            tolerances_rel={k: float(v) * float(tol_scale) for k, v in pack.tolerances_rel.items()},
                            severities=pack.severities,
                        )
                        res = compare_to_reference(pack=scaled_pack, metrics=metrics, reference=ref_dict or {})
                        st.session_state["ppl_last_validation"] = {"pack": pack.pack_id, "res": res, "meta": meta, "metrics": metrics, "reference": ref_dict}
                    except Exception as _e:
                        st.error(f"Validation run failed: {_e}")

                last = st.session_state.get("ppl_last_validation")
                if last and last.get("pack") == pack.pack_id:
                    res = last["res"]
                    st.markdown("#### Verdict")
                    if res["status"] == "PASS":
                        st.success(f"PASS - worst relative error: {res['worst_rel_err']:.3f}")
                    elif res["status"] == "WARN":
                        st.warning(f"WARN - worst relative error: {res['worst_rel_err']:.3f}")
                    else:
                        st.error(f"FAIL - worst relative error: {res['worst_rel_err']:.3f}")

                    st.markdown("#### Deltas")
                    rows = res["rows"]
                    # render as a small table
                    import pandas as _pd
                    df = _pd.DataFrame(rows)
                    st.dataframe(df, use_container_width=True, hide_index=True)

                    with st.expander("Assumptions & constraints context", expanded=False):
                        st.json(last.get("meta", {}))
                        st.json({"metrics": last.get("metrics", {}), "reference": last.get("reference", {})})


            elif view == "Parity calibration (reference deltas)":
                st.caption(
                    "Compare SHAMS Parity outputs against a reference table (e.g., published study values). "
                    "Built-in references are placeholders; upload your program's reference JSON to calibrate."
                )
                up = st.file_uploader("Optional reference JSON (same schema as benchmarks/parity_v2_refs.json)", type=["json"], key="opt_parity_ref_upload")
                ref_path = None
                if up is not None:
                    try:
                        tmp = Path(ROOT) / ".tmp_parity_refs.json"
                        tmp.write_bytes(up.getvalue())
                        ref_path = str(tmp)
                    except Exception:
                        ref_path = None
                if st.button("Run calibration", use_container_width=True, key="opt_parity_calib_run"):
                    try:
                        from tools.parity_calibrate import run_parity_calibration
                        st.session_state["opt_parity_calib_res"] = run_parity_calibration(refs_path=ref_path)
                    except Exception as _e:
                        st.session_state["opt_parity_calib_res"] = {"ok": False, "reason": str(_e)}
                res = st.session_state.get("opt_parity_calib_res")
                if res:
                    st.json({k: res.get(k) for k in ["ok", "n_cases", "refs_path", "note"]}, expanded=False)
                    for case in res.get("results", [])[:50]:
                        nm = case.get("name")
                        ok = bool(case.get("ok"))
                        with st.expander(f"{nm} - {'PASS' if ok else 'CHECK'}", expanded=not ok):
                            st.caption(str(case.get("notes") or ""))
                            rows = case.get("rows") or []
                            if rows:
                                import pandas as _pd
                                st.dataframe(_pd.DataFrame(rows), use_container_width=True)
                            else:
                                st.info("No reference metrics defined for this case.")
            elif view == "PROCESS parity benchmarks":
                st.caption("Run the internal parity regression suite (local, deterministic).")
                upd = st.checkbox("Update golden (developer)", value=False, key="opt_parity_update_golden")
                if st.button("Run parity benchmarks", key="opt_parity_bench", use_container_width=True):
                    try:
                        from tools.parity_bench import run_parity_benchmarks
                        st.session_state["opt_parity_bench_res"] = run_parity_benchmarks(update_golden=bool(upd))
                    except Exception as _e:
                        st.session_state["opt_parity_bench_res"] = {"ok": False, "reason": str(_e)}
                if st.session_state.get("opt_parity_bench_res"):
                    st.json(st.session_state.get("opt_parity_bench_res"), expanded=False)
            
            elif view == "Decision scenarios (program lens)":
                st.caption(
                    "Scenario presets that bundle objective intent, economics conservatism, and credibility lens. "
                    "They do not change frozen truth; they set *study posture*."
                )
                scenarios = {
                    "Conservative engineering": {
            "opt_intent": "Reactor",
            "opt_pack_choice": "Compact feasible reactor",
            "ppl_cost_posture": "Conservative",
            "credibility": "Conservative",
            "note": "Bias toward margin and conservative economics; useful for program-safe screening.",
                    },
                    "Nominal program baseline": {
            "opt_intent": "Reactor",
            "opt_pack_choice": "Compact feasible reactor",
            "ppl_cost_posture": "Nominal",
            "credibility": "Nominal",
            "note": "Default posture for comparisons and internal benchmarking.",
                    },
                    "Aggressive HTS / high-field": {
            "opt_intent": "Reactor",
            "opt_pack_choice": "High-field HTS stress frontier",
            "ppl_cost_posture": "Optimistic",
            "credibility": "Aggressive",
            "note": "Exploration posture; expect fragility and tighter trust boundaries.",
                    },
                    "Research toward Reactor": {
            "opt_intent": "Research",
            "opt_pack_choice": "Closest-to-reactor feasibility",
            "ppl_cost_posture": "Nominal",
            "credibility": "Nominal",
            "note": "Use Research intent but track distance-to-reactor; strategic R&D planning.",
                    },
                }

                names = list(scenarios.keys())
                sel = st.selectbox("Scenario", names, index=1, key="ppl_scenario_sel")
                s = scenarios[sel]
                st.info(s["note"], icon="ðŸ§­")

                colA, colB = st.columns([1, 1])
                with colA:
                    st.markdown("**Scenario settings**")
                    st.json({k: v for k, v in s.items() if k != "note"})
                with colB:
                    st.markdown("**Apply**")
                    if st.button("Apply scenario to session", key="ppl_apply_scenario"):
                        # apply to optimization setup controls
                        st.session_state["opt_intent"] = s["opt_intent"]
                        st.session_state["opt_pack_choice"] = s["opt_pack_choice"]
                        st.session_state["ppl_cost_posture"] = s["ppl_cost_posture"]
                        st.session_state["ppl_credibility"] = s["credibility"]
                        st.success("Scenario applied. Return to Setup and run Machine Finder, or inspect Parity Workbench.")
                    st.markdown("**Economics posture**")
                    posture = st.selectbox(
            "Cost envelope posture",
            ["Optimistic", "Nominal", "Conservative"],
            index=["Optimistic","Nominal","Conservative"].index(st.session_state.get("ppl_cost_posture","Nominal")),
            key="ppl_cost_posture",
                    )
                    st.caption("Used for economics envelope display and scenario labeling (nominal truth is unchanged).")

            elif view == "Trace Telemetry":
                tr = run.get("trace") or []
                if tr:
                    import pandas as _pd
                    df=_pd.DataFrame(tr)
                    st.line_chart(df[["score"]])
                    st.caption("Trace shows score progression; feasibility and resistance are summarized in the atlas.")
                else:
                    st.info("No trace recorded.")
            elif view == "Resistance Brief":
                st.json(run.get("resistance", {}))
                st.json(run.get("variable_correlations", {}))
            elif view == "Boundary Navigator":
                st.caption(
                    "Local linear surface model for a single constraint, fitted from near-boundary points in the archive. "
                    "This is an instrument to understand geometry; it does not recommend designs."
                )
                vs = run.get("var_specs") or []
                vkeys = []
                for v in vs:
                    if isinstance(v, dict) and v.get("key"):
                        vkeys.append(str(v.get("key")))
                    else:
                        try:
                            vkeys.append(str(getattr(v, "key")))
                        except Exception:
                            pass
                # available constraints
                names = []
                sample = None
                for a in (run.get("archive") or []):
                    if (a.get("constraints") or []):
                        sample = a
                        break
                if sample is not None:
                    for c in (sample.get("constraints") or []):
                        nm = str(c.get("name"))
                        if nm and nm not in names:
                            names.append(nm)
                if not vkeys:
                    st.info("No optimized variables found in this run.")
                elif not names:
                    st.info("No constraint records found in archive candidates.")
                else:
                    cn = st.selectbox("Constraint", options=names, index=0, key="opt_surface_constraint")
                    use_archive = st.checkbox("Use filtered archive", value=True, key="opt_surface_use_filtered")
                    data_src = filt if (use_archive and isinstance(filt, list) and filt) else (run.get("archive") or [])
                    m = constraint_surface_map(archive=data_src, var_keys=vkeys, constraint_name=cn)
                    st.json(m)
            elif view == "Feasibility skeleton":
                sk = run.get("feasibility_skeleton") or {}
                if not sk:
                    st.info("Skeleton not available (need feasible points).")
                else:
                    st.metric("Feasible points", str(sk.get("n_feasible", "-")))
                    st.metric("Components", str(sk.get("n_components", "-")))
                    st.write({"component_sizes": sk.get("components", [])})
                    with st.expander("Bottleneck edges (longest kNN edges)", expanded=False):
                        st.write(sk.get("bottleneck_edges", []))
                    st.caption("Use this to see whether feasible truth is one connected basin or multiple islands.")
            elif view == "Local cartography (adaptive)":
                st.caption("A small 2D scan around the selected/ best candidate. This is cartography, not optimization.")
                base = None
                if isinstance(run.get("best_feasible"), dict):
                    base = run.get("best_feasible")
                if filt:
                    base = filt[int(min(len(filt)-1, int(st.session_state.get("opt_inspect_idx", 0))))]
                if base is None:
                    st.info("No candidate to center local cartography on.")
                else:
                    vs = run.get("var_specs") or []
                    vkeys = [v.get("key") for v in vs if isinstance(v, dict) and v.get("key")]
                    if len(vkeys) < 2:
                        st.info("Need at least two optimized variables.")
                    else:
                        xk = st.selectbox("x variable", vkeys, index=0, key="opt_localcart_x")
                        yk = st.selectbox("y variable", vkeys, index=min(1, len(vkeys)-1), key="opt_localcart_y")
                        span = st.slider("Span (Â±% of local bounds)", 5, 60, 20, key="opt_localcart_span")
                        ngrid = st.slider("Grid", 9, 41, 21, step=2, key="opt_localcart_ng")
                        if st.button("Run local cartography", key="opt_localcart_run"):
                            import numpy as _np
                            import pandas as _pd
                            bx = (base.get("inputs") or {}).get(xk)
                            by = (base.get("inputs") or {}).get(yk)
                            # bounds
                            bmap = {v.get("key"):(float(v.get("lo")), float(v.get("hi"))) for v in vs if isinstance(v, dict) and v.get("key")}
                            xlo,xhi = bmap.get(xk,(float(bx)*0.8,float(bx)*1.2))
                            ylo,yhi = bmap.get(yk,(float(by)*0.8,float(by)*1.2))
                            xmid = float(bx) if bx is not None else 0.5*(xlo+xhi)
                            ymid = float(by) if by is not None else 0.5*(ylo+yhi)
                            dx = (xhi-xlo)*float(span)/100.0
                            dy = (yhi-ylo)*float(span)/100.0
                            xs = _np.linspace(max(xlo, xmid-dx), min(xhi, xmid+dx), int(ngrid))
                            ys = _np.linspace(max(ylo, ymid-dy), min(yhi, ymid+dy), int(ngrid))
                            rows=[]
                            for xv in xs:
                                for yv in ys:
                                    cand_in = dict(base.get("inputs") or {})
                                    cand_in[xk] = float(xv)
                                    cand_in[yk] = float(yv)
                                    r = eval_fn(cand_in)
                                    rows.append({"x":float(xv),"y":float(yv),"feasible":bool(r.get("feasible",False)),"score":float(r.get("_score",-1e30)),"violation":float(r.get("_violation",1e30)),"min_margin":float(r.get("min_signed_margin",float("nan")))})
                            df=_pd.DataFrame(rows)
                            st.session_state["opt_localcart_df"] = df
                        df = st.session_state.get("opt_localcart_df")
                        if df is not None:
                            try:
                                import numpy as _np
                                import matplotlib.pyplot as _plt
                                dff = df.pivot(index="y", columns="x", values="feasible")
                                fig = _plt.figure()
                                _plt.imshow(dff.values[::-1, :], aspect="auto")
                                _plt.title("Feasibility map (local)")
                                st.pyplot(fig)
                                st.caption("Heatmap shows feasible (1) vs infeasible (0). Use Inspector for details.")
                            except Exception:
                                st.dataframe(df)
            elif view == "Uncertainty (Monte Carlo)":
                st.caption("Monte Carlo robustness is optional. It does **not** change feasibility truth; it samples around a candidate.")
                if not filt:
                    st.info("No candidates available.")
                else:
                    cand = filt[int(min(len(filt)-1, int(st.session_state.get("opt_inspect_idx", 0))))]
                    ns = st.number_input("Samples", 20, 2000, 200, step=20, key="opt_uq_ns")
                    pct = st.slider("Perturbation (Â±% on optimized vars)", 1, 25, 5, key="opt_uq_pct")
                    if st.button("Run robustness Monte Carlo", key="opt_uq_run"):
                        import numpy as _np
                        rng = _np.random.default_rng(1)
                        vs = run.get("var_specs") or []
                        vkeys = [v.get("key") for v in vs if isinstance(v, dict) and v.get("key")]
                        base_in = dict(cand.get("inputs") or {})
                        feas=0
                        scores=[]
                        for _ in range(int(ns)):
                            ci = dict(base_in)
                            for k in vkeys:
                                v0 = float(base_in.get(k, 0.0))
                                dv = v0 * float(pct)/100.0
                                ci[k] = float(rng.uniform(v0-dv, v0+dv))
                            r = eval_fn(ci)
                            if r.get("feasible", False):
                                feas += 1
                                scores.append(float(r.get("_score", -1e30)))
                        st.session_state["opt_uq_res"] = {
                            "samples": int(ns),
                            "pct": float(pct),
                            "feasible_rate": float(feas)/float(ns),
                            "mean_score_feasible": float(_np.mean(scores)) if scores else None,
                            "n_feasible": int(feas),
                        }
                    if st.session_state.get("opt_uq_res"):
                        st.json(st.session_state.get("opt_uq_res"))
            elif view == "Intent trajectories (Researchâ†’Reactor)":
                st.caption(
                    "Tier-5: a simple *intent trajectory* instrument. "
                    "It tries to build a Researchâ†’Reactor 'highway' using the current archive and the currently selected variables. "
                    "This does not optimize; it organizes what you already found."
                )
                traj = st.session_state.get("opt_traj")
                if not traj:
                    st.info("Click **Build trajectory** in the left Tier 5â€“6 expander to compute a path.")
                else:
                    if not traj.get("ok", False):
                        st.warning(f"Trajectory unavailable: {traj.get('reason')}")
                        st.json(traj, expanded=False)
                    else:
                        st.success(f"Trajectory built: {traj.get('from_intent')} â†’ {traj.get('to_intent')} (steps={len(traj.get('nodes') or [])})")
                        nodes = traj.get("nodes") or []
                        edges = traj.get("edges") or []
                        import pandas as _pd
                        rows = []
                        for i, n in enumerate(nodes):
                            rows.append({
                                "step": i,
                                "from_feasible": bool(n.get("from_feasible")),
                                "to_feasible": bool(n.get("to_feasible")),
                                "from_score": float(n.get("from_score", -1e30)),
                                "to_score": float(n.get("to_score", -1e30)),
                            })
                        st.dataframe(_pd.DataFrame(rows), use_container_width=True)
                        with st.expander("Step inputs", expanded=False):
                            for i, n in enumerate(nodes):
                                st.markdown(f"**Step {i} inputs**")
                                st.json(n.get("inputs") or {}, expanded=False)
                                if i < len(edges):
                                    st.caption(f"Î” to next (L2 dist â‰ˆ {float(edges[i].get('dist', 0.0)):.4g})")
                                    st.json(edges[i].get("delta") or {}, expanded=False)
                        import json as _json
                        st.download_button(
                            "Download trajectory (json)",
                            data=_json.dumps(traj, indent=2).encode("utf-8"),
                            file_name="shams_intent_trajectory.json",
                            mime="application/json",
                            use_container_width=True,
                        )

            elif view == "Inverse design / Why not?":
                st.caption(
                    "Tier-5: inverse design (closest feasible to a target) + 'why not' explanation. "
                    "This never relaxes constraints; it searches only within your declared bounds."
                )
                targets = st.session_state.get("opt_inv_targets") or {}
                st.markdown("**Targets**")
                st.json(targets, expanded=False)

                n_samples = st.number_input("Inverse search samples", 50, 5000, 600, step=50, key="opt_inv_ns")
                if st.button("Run inverse search (closest feasible)", key="opt_inv_run", use_container_width=True):
                    # Sample uniformly in declared bounds and pick feasible with min residual.
                    import numpy as _np
                    rng = _np.random.default_rng(int(run.get("seed", 0)) + 17)
                    vs = run.get("var_specs") or []
                    vkeys = [v.get("key") for v in vs if isinstance(v, dict) and v.get("key")]
                    # If var_specs not present, fall back to current var_keys + bounds
                    if not vkeys:
                        vkeys = list(bounds.keys())
                    best_res = None
                    best_eval = None
                    best_inputs = None
                    for _ in range(int(n_samples)):
                        cand_in = dict(anchor_default)
                        for k in vkeys:
                            lo, hi = bounds.get(k, (float(cand_in.get(k, 0.0)), float(cand_in.get(k, 0.0))))
                            cand_in[k] = float(rng.uniform(float(lo), float(hi)))
                        r = _evaluate_candidate(cand_in, intent)
                        if not r.get("feasible", False):
                            continue
                        resid = inverse_design_residual(r.get("outputs") or {}, targets)
                        if best_res is None or resid < best_res:
                            best_res = float(resid)
                            best_eval = r
                            best_inputs = cand_in
                    st.session_state["opt_inv_best"] = {"residual": best_res, "eval": best_eval, "inputs": best_inputs}

                inv_best = st.session_state.get("opt_inv_best")
                if inv_best and inv_best.get("eval"):
                    st.success(f"Best feasible inverse match residual: {float(inv_best.get('residual')):.4g}")
                    st.json(inv_best.get("inputs") or {}, expanded=False)
                    st.json((inv_best.get("eval") or {}).get("outputs") or {}, expanded=False)
                    with st.expander("Why-not style report (for this candidate)", expanded=False):
                        st.json(why_not_report(eval_res=inv_best.get("eval") or {}, disabled_constraints=st.session_state.get("opt_cf_disable") or []), expanded=False)
                else:
                    st.info("Run inverse search to find the closest feasible candidate to your targets.")

                # Why-not for selected candidate in inspector
                st.divider()
                st.markdown("**Why not for the currently selected candidate (Inspector index)**")
                if filt:
                    cand = filt[int(min(len(filt)-1, int(st.session_state.get("opt_inspect_idx", 0))))]
                    wr = why_not_report(
                        eval_res=cand,
                        disabled_constraints=st.session_state.get("opt_cf_disable") or [],
                    )
                    st.json(wr, expanded=False)
                else:
                    st.info("No candidate selected.")

            elif view == "Discovered relations (laws)":
                st.caption(
                    "Tier-6: mine simple, explainable relations from the feasible archive. "
                    "This is not a physics claim; it's a data-derived hint to guide exploration."
                )
                import pandas as _pd
                x_opts = ["R0_m","a_m","Bt_T","Ip_MA","Paux_MW","kappa","delta","nbar_1e20_m3","Ti_keV"]
                y_opts = ["P_e_net_MW","Pfus_total_MW","Q_DT_eqv","q_div_MW_m2","min_signed_margin"]
                x_sel = st.multiselect("x variables", x_opts, default=["R0_m","Bt_T","Ip_MA"], key="opt_rel_x")
                y_sel = st.multiselect("y metrics", y_opts, default=["P_e_net_MW","q_div_MW_m2"], key="opt_rel_y")
                feas_only = st.checkbox("Use feasible candidates only", value=True, key="opt_rel_feas")
                if st.button("Compute discovered relations", key="opt_rel_run", use_container_width=True):
                    st.session_state["opt_rel"] = discovered_relations(
                        candidates=archive,
                        x_keys=x_sel,
                        y_keys=y_sel,
                        feasible_only=bool(feas_only),
                        top_k=8,
                    )
                rel = st.session_state.get("opt_rel")
                if rel and rel.get("ok"):
                    st.success(f"Computed relations from n={rel.get('n')} candidates")
                    st.markdown("**Top linear fits**")
                    st.write(rel.get("top_linear_fits"))
                    st.markdown("**Top correlations**")
                    st.write(rel.get("top_corrs"))
                    md = export_relations_markdown(rel)
                    st.download_button(
                        "Download relations report (markdown)",
                        data=md.encode("utf-8"),
                        file_name="shams_discovered_relations.md",
                        mime="text/markdown",
                        use_container_width=True,
                    )
                else:
                    st.info("Compute relations to see the strongest correlations and simple fits.")

            elif view == "Counterfactual lens":
                st.caption(
                    "Tier-6: counterfactual lens. You can disable one or more constraints in the **feasibility gate** only. "
                    "Raw constraints stay unchanged; this is a hypothetical planning tool."
                )
                disabled = st.session_state.get("opt_cf_disable") or []
                if not disabled:
                    st.info("Select constraints to disable in the left Tier 5â€“6 expander.")
                else:
                    st.warning(f"Counterfactual disabled constraints: {disabled}")
                    cf_feas = 0
                    cf_best = None
                    cf_best_score = -1e30
                    for c in filt:
                        cf = counterfactual_gate(c, disabled).get("counterfactual") or {}
                        if cf.get("feasible", False):
                            cf_feas += 1
                            sc = float(c.get("_score", -1e30))
                            if sc > cf_best_score:
                                cf_best_score = sc
                                cf_best = c
                    st.metric("Counterfactual feasible (filtered)", f"{cf_feas} / {len(filt)}")
                    if cf_best is not None:
                        st.markdown("**Best (by score) among counterfactual-feasible**")
                        st.json(cf_best.get("inputs") or {}, expanded=False)
                        st.json((cf_best.get("outputs") or {}), expanded=False)
                        with st.expander("Counterfactual gate details", expanded=False):
                            st.json(counterfactual_gate(cf_best, disabled).get("counterfactual"), expanded=False)
                    st.divider()
                    st.markdown("**Inspector candidate under counterfactual**")
                    if filt:
                        cand = filt[int(min(len(filt)-1, int(st.session_state.get("opt_inspect_idx", 0))))]
                        st.json(counterfactual_gate(cand, disabled).get("counterfactual"), expanded=False)

            elif view == "Collaboration (review sessions)":
                st.caption(
                    "Tier-7: multi-user deliberation without external services. "
                    "Create a review session to attach comments/votes/tags to candidates, then export a session bundle."
                )
                _repo_root = Path(__file__).resolve().parent.parent
                _eval_fp = repo_fingerprint(_repo_root)
                sessions_dir = default_sessions_dir()
                st.markdown(f"**Local sessions dir:** `{sessions_dir}`")

                colA, colB = st.columns(2)
                with colA:
                    title = st.text_input("New session title", value=st.session_state.get("opt_review_title", "Optimization review"), key="opt_review_title")
                    notes = st.text_area("Session notes (optional)", value=st.session_state.get("opt_review_notes", ""), key="opt_review_notes")
                    if st.button("Create new session", key="opt_review_create", use_container_width=True):
                        sess = new_review_session(title=title, evaluator_fp=_eval_fp, intent=run.get("intent", ""), notes=notes)
                        path = sessions_dir / f"{sess.session_id}.json"
                        save_review_session(sess, path)
                        st.session_state["opt_review_path"] = str(path)
                        st.success(f"Created session: {sess.session_id}")

                with colB:
                    existing = sorted([p.name for p in sessions_dir.glob("*.json")])
                    pick = st.selectbox("Load existing session", options=[""] + existing, index=0, key="opt_review_pick")
                    if pick:
                        path = sessions_dir / pick
                        try:
                            sess = load_review_session(path)
                            st.session_state["opt_review_path"] = str(path)
                            st.success(f"Loaded session: {sess.session_id}")
                        except Exception as e:
                            st.error(f"Failed to load session: {e}")

                # Load active session
                sess = None
                sp = st.session_state.get("opt_review_path")
                if sp:
                    try:
                        sess = load_review_session(Path(sp))
                    except Exception:
                        sess = None

                if sess is None:
                    st.info("Create or load a review session to start commenting/voting.")
                else:
                    st.markdown("### Session")
                    st.json({
                        "session_id": sess.session_id,
                        "title": sess.title,
                        "created_at": sess.created_at,
                        "intent": sess.intent,
                        "evaluator_fp": sess.evaluator_fp[:12],
                        "n_candidates": len(sess.candidates or []),
                        "n_comments": len(sess.comments or []),
                        "n_votes": len(sess.votes or []),
                    }, expanded=False)

                    # Add current inspector candidate
                    if filt:
                        idx = int(st.session_state.get("opt_inspect_idx", 0))
                        idx = int(max(0, min(len(filt)-1, idx)))
                        cand = filt[idx]
                        if st.button("Add current inspector candidate to session", key="opt_review_add", use_container_width=True):
                            _c_fp = candidate_fingerprint(cand.get("inputs", {}) or {}, intent=run.get("intent", ""), evaluator_fp=_eval_fp)
                            # de-dup
                            if not any((c.get("candidate_fp") == _c_fp) for c in (sess.candidates or [])):
                                sess.candidates.append({
                                    "candidate_fp": _c_fp,
                                    "score": cand.get("_score"),
                                    "feasible": cand.get("feasible"),
                                    "min_signed_margin": cand.get("min_signed_margin"),
                                    "inputs": cand.get("inputs", {}),
                                    "failure_mode": cand.get("failure_mode"),
                                })
                                save_review_session(sess, Path(sp))
                                st.success("Added.")
                            else:
                                st.info("Candidate already in session.")

                    st.divider()
                    st.markdown("### Comment / vote")
                    cand_opts = [c.get("candidate_fp", "")[:12] + "â€¦" for c in (sess.candidates or [])]
                    if not cand_opts:
                        st.info("Add candidates to the session to enable comments and votes.")
                    else:
                        sel = st.selectbox("Candidate", options=list(range(len(cand_opts))), format_func=lambda i: cand_opts[i], key="opt_review_cand_sel")
                        comment = st.text_area("Comment", key="opt_review_comment")
                        vote = st.slider("Vote (1â€“5)", 1, 5, 3, key="opt_review_vote")
                        tag = st.text_input("Tag (optional)", value="", key="opt_review_tag")
                        cols = st.columns(3)
                        if cols[0].button("Add comment", key="opt_review_add_comment"):
                            sess.comments.append({
                                "ts": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
                                "candidate_fp": sess.candidates[int(sel)].get("candidate_fp"),
                                "text": comment,
                            })
                            save_review_session(sess, Path(sp))
                            st.success("Comment added.")
                        if cols[1].button("Cast vote", key="opt_review_add_vote"):
                            sess.votes.append({
                                "ts": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
                                "candidate_fp": sess.candidates[int(sel)].get("candidate_fp"),
                                "vote": int(vote),
                            })
                            save_review_session(sess, Path(sp))
                            st.success("Vote recorded.")
                        if cols[2].button("Add tag", key="opt_review_add_tag"):
                            if tag.strip():
                                sess.tags.append({
                                    "ts": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
                                    "candidate_fp": sess.candidates[int(sel)].get("candidate_fp"),
                                    "tag": tag.strip(),
                                })
                                save_review_session(sess, Path(sp))
                                st.success("Tag added.")

                    with st.expander("Session data", expanded=False):
                        st.json(sess.to_dict(), expanded=False)

                    st.divider()
                    st.markdown("### ðŸ“¤ Export / import")
                    st.download_button(
                        "Download review session bundle (.zip)",
                        data=export_review_session_zip(sess),
                        file_name=f"shams_review_session_{sess.session_id}.zip",
                        mime="application/zip",
                        use_container_width=True,
                    )
                    up = st.file_uploader("Import review session bundle", type=["zip"], key="opt_review_import")
                    if up is not None:
                        try:
                            sess2 = import_review_session_zip(up.read())
                            path = sessions_dir / f"{sess2.session_id}.json"
                            save_review_session(sess2, path)
                            st.session_state["opt_review_path"] = str(path)
                            st.success(f"Imported session: {sess2.session_id}")
                        except Exception as e:
                            st.error(f"Import failed: {e}")

            elif view == "Epistemic guarantees (regression suite)":
                st.caption(
                    "Ultimate differentiator: epistemic guarantees. Run the golden regression suite to detect unintended semantic drift. "
                    "This does not validate against reality; it enforces stability of the frozen evaluator and artifact contracts."
                )
                _repo_root = Path(__file__).resolve().parent.parent
                _eval_fp = repo_fingerprint(_repo_root)
                st.markdown(f"**Evaluator fingerprint:** `{_eval_fp}`")
                rtol = st.number_input("Relative tolerance", value=0.01, min_value=0.0, max_value=0.2, step=0.005, key="opt_reg_rtol")
                atol = st.number_input("Absolute tolerance", value=1e-6, min_value=0.0, max_value=1e-2, step=1e-6, format="%.1e", key="opt_reg_atol")
                if st.button("Run regression suite now", key="opt_reg_run", use_container_width=True):
                    st.session_state["opt_reg_report"] = run_regression_suite(_repo_root, rtol=float(rtol), atol=float(atol))
                rep = st.session_state.get("opt_reg_report")
                if rep:
                    if rep.get("ok"):
                        st.success("Regression suite PASSED.")
                    else:
                        st.error("Regression suite FAILED.")
                    with st.expander("Runner output", expanded=False):
                        st.code(rep.get("output", ""))
                    d = rep.get("diff") or {}
                    if d:
                        st.markdown("### Diff summary")
                        try:
                            st.json({
                                "numeric_failures": d.get("numeric", {}).get("summary"),
                                "structural": d.get("structural", {}).get("severity"),
                            }, expanded=False)
                        except Exception:
                            st.json(d, expanded=False)
                else:
                    st.info("Run the suite to get a pass/fail report + structured diffs.")

            elif view == "Standards & DOI export":
                st.caption(
                    "Tier-7 standards: export DOI-ready packs + SHAMS-certified feasibility badges (descriptive, non-ranking)."
                )
                _repo_root = Path(__file__).resolve().parent.parent
                _eval_fp = repo_fingerprint(_repo_root)
                version = str(st.session_state.get("app_version", ""))
                best = run.get("best_feasible")
                if not isinstance(best, dict):
                    st.info("No feasible best candidate yet. Run the machine finder first.")
                else:
                    cfp = candidate_fingerprint(best.get("inputs", {}) or {}, intent=run.get("intent", ""), evaluator_fp=_eval_fp)
                    svg = generate_cert_badge_svg(
                        candidate_fp=cfp,
                        intent=run.get("intent", ""),
                        feasible=bool(best.get("feasible", False)),
                        version=version,
                        evaluator_fp=_eval_fp,
                        note="audited by frozen evaluator",
                    )
                    st.download_button(
                        "Download SHAMS-certified badge (SVG)",
                        data=svg.encode("utf-8"),
                        file_name=f"shams_cert_badge_{cfp[:12]}.svg",
                        mime="image/svg+xml",
                        use_container_width=True,
                    )

                    # DOI-ready export pack (includes archive + trace)
                    archive_rows = run.get("archive") or []
                    trace_rows = run.get("trace") or []
                    run_meta = {
                        "schema": "shams.optimization_pack.v1",
                        "version": version,
                        "intent": run.get("intent"),
                        "seed": run.get("seed"),
                        "fingerprint": run.get("fingerprint"),
                        "evaluator_fp": _eval_fp,
                        "objectives": run.get("objectives"),
                        "var_specs": run.get("var_specs"),
                        "budgets": run.get("budgets"),
                        "notes": "Export pack is descriptive. No hidden preferences.",
                    }
                    extra = [(f"badges/shams_cert_badge_{cfp[:12]}.svg", svg.encode("utf-8"))]
                    # Attach review session if loaded
                    sp = st.session_state.get("opt_review_path")
                    if sp:
                        try:
                            sess = load_review_session(Path(sp))
                            extra.append(("review_session.json", json.dumps(sess.to_dict(), indent=2, sort_keys=True).encode("utf-8")))
                        except Exception:
                            pass
                    pack = export_doi_ready_pack(
                        repo_root=_repo_root,
                        run_meta=run_meta,
                        archive_rows=archive_rows,
                        trace_rows=trace_rows,
                        extra_files=extra,
                    )
                    st.download_button(
                        "Download DOI-ready publication pack (.zip)",
                        data=pack,
                        file_name=f"shams_optimization_publication_pack_{run.get('fingerprint','')[:12]}.zip",
                        mime="application/zip",
                        use_container_width=True,
                    )

            elif view == "Design-space verdicts (Allowed/Forbidden)":
                st.caption(
                    "Tier-8: Design-space jurisprudence. This is not a recommendation engine - it classifies what is supported by evidence in the explored region. "
                    "Forbidden here means *locally forbidden within the explored neighborhood*, not a universal impossibility theorem."
                )
                ci = feasibility_confidence_from_trace(run.get("trace") or [], window=500)
                rv = region_verdict(run.get("trace") or [], window=500)
                st.markdown("#### Region verdict (recent window)")
                st.write({
                    "verdict": rv.label,
                    "confidence": f"{rv.confidence:.2f}",
                    "rate": ci.get("rate"),
                    "ci_95": [ci.get("ci_lo"), ci.get("ci_hi")],
                    "rationale": rv.rationale,
                })
                st.markdown("#### Candidate verdict (selected)")
                if filt:
                    cand = filt[int(st.session_state.get("opt_inspect_idx", 0))]
                    vv = candidate_verdict(cand, archive=run.get("archive") or [], var_keys=var_keys, robust_margin=float(min_margin) if min_margin else 0.0)
                    st.write({"verdict": vv.label, "confidence": f"{vv.confidence:.2f}", "rationale": vv.rationale})
                else:
                    st.info("No candidates available.")

            elif view == "Epistemic confidence bounds":
                st.caption(
                    "Tier-8: Epistemic confidence bounds on feasibility rates (Wilson interval). This quantifies how strongly the *recent search evidence* supports feasibility/infeasibility."
                )
                w = st.slider("Window (last N evaluations)", 50, 2000, 500, 50, key="opt_ci_window")
                ci = feasibility_confidence_from_trace(run.get("trace") or [], window=int(w))
                st.write(ci)
                st.markdown("**Interpretation**")
                st.write(
                    "- If `k=0` and the upper CI is very small, the explored region is strongly supported as locally infeasible.\n"
                    "- If `k>0`, feasibility is established for the explored region; the CI describes how often feasibility occurs under the current proposal distribution." 
                )

            elif view == "Intent-conditional design laws":
                st.caption(
                    "Tier-8: Intent-conditional design laws. We take top feasible candidates under the current intent and re-evaluate them under the other intent, then compare correlations."
                )
                if not filt:
                    st.info("No candidates available.")
                else:
                    other_intent = "Research" if run.get("intent") == "Reactor" else "Reactor"
                    key_y = st.text_input("Output key to analyze", value="P_e_net_MW", key="opt_laws_keyy")
                    topn = st.slider("Top feasible candidates to compare", 10, 120, 40, 5, key="opt_laws_topn")
                    def _eval_other(inp: dict) -> dict:
                        return _evaluate_candidate(inp, intent=other_intent)
                    laws = intent_conditional_laws(_eval_other, archive=run.get("archive") or [], var_keys=var_keys, key_y=str(key_y), top_n=int(topn))
                    st.write({"primary_intent": run.get("intent"), "other_intent": other_intent, "n_primary": laws.get("n_primary"), "n_other": laws.get("n_other")})
                    try:
                        import pandas as _pd
                        df = _pd.DataFrame(laws.get("rows") or [])
                        st.dataframe(df, use_container_width=True)
                    except Exception:
                        st.json(laws)

            elif view == "Machine genealogy":
                st.caption(
                    "Tier-9: Machine genealogy. When engines do not record parents explicitly, SHAMS reconstructs a conservative ancestry graph: each candidate's parent is its nearest better neighbor in variable space."
                )
                if not filt:
                    st.info("No candidates available.")
                else:
                    maxch = st.slider("Max children per parent (for readability)", 3, 30, 12, 1, key="opt_gene_maxch")
                    g = reconstruct_genealogy(run.get("archive") or [], var_keys=var_keys, max_children_per_parent=int(maxch))
                    st.write({"roots": g.get("roots"), "num_nodes": len(g.get("parents") or {})})
                    # Render a small textual tree for the top few roots
                    roots = list(g.get("roots") or [])[:5]
                    parents = g.get("parents") or {}
                    children = g.get("children") or {}
                    def _node_label(i: int) -> str:
                        try:
                            a = (run.get("archive") or [])[int(i)]
                            return f"#{i} | feas={bool(a.get('feasible'))} | score={float(a.get('_score', -1e30)):.3g} | m={float(a.get('min_signed_margin', float('nan'))):.3g}"
                        except Exception:
                            return f"#{i}"
                    def _render(i: int, depth: int = 0, maxd: int = 3):
                        lines = ["  "*depth + "- " + _node_label(i)]
                        if depth >= maxd:
                            return lines
                        for ch in (children.get(i) or [])[:10]:
                            lines.extend(_render(int(ch), depth+1, maxd))
                        return lines
                    with st.expander("Genealogy tree (top roots)", expanded=False):
                        text = []
                        for r in roots:
                            text.extend(_render(int(r), 0, 3))
                        st.code("\n".join(text))

            elif view == "Counter-optimization (no interior optimum)":
                st.caption(
                    "Tier-9: Counter-optimization. This does not claim mathematical proofs; it reports evidence that improvement is boundary-limited (no interior optimum) under the current search space."
                )
                key_obj = st.text_input("Objective key (default: _score)", value="_score", key="opt_counter_key")
                rep = counter_optimization_report(run.get("archive") or [], key_obj=str(key_obj))
                if rep.get("status") == "ok":
                    if rep.get("boundary_limited"):
                        st.warning(rep.get("message"))
                    else:
                        st.info(rep.get("message"))
                else:
                    st.info(rep.get("message"))
                st.json(rep, expanded=False)

            elif view == "Reproducibility":
                st.caption("Audit capsule: fingerprint + config + optional citation.")
                st.json({
                    "fingerprint": run.get("fingerprint"),
                    "intent": run.get("intent"),
                    "seed": run.get("seed"),
                    "objectives": run.get("objectives"),
                    "var_specs": run.get("var_specs"),
                    "budgets": run.get("budgets"),
                }, expanded=False)
                try:
                    from pathlib import Path as _Path
                    cff = (_Path(__file__).resolve().parent.parent / "CITATION.cff").read_text(encoding="utf-8")
                    with st.expander("CITATION.cff", expanded=False):
                        st.code(cff, language="yaml")
                    try:
                        pm_path = _Path(__file__).resolve().parent.parent / "examples" / "published_machines.json"
                        if pm_path.exists():
                            import json as _json
                            pm = _json.loads(pm_path.read_text(encoding="utf-8"))
                            with st.expander("Published machine overlay (optional)", expanded=False):
                                st.json(pm)
                    except Exception:
                        pass
                except Exception:
                    pass
            else:
                st.info("Pareto view: available when â‰¥2 objectives are configured.")

        # RIGHT: inspector
        with right:
            st.markdown("### Inspector")
            idx = st.number_input("Candidate index (in filtered archive)", 0, max(0, len(filt)-1), 0, key="opt_inspect_idx")
            if filt:
                cand = filt[int(idx)]
                st.markdown("**Candidate summary**")
                st.write({
                    "feasible": bool(cand.get("feasible", False)),
                    "score": float(cand.get("_score", -1e30)),
                    "min_signed_margin": float(cand.get("min_signed_margin", float("nan"))),
                    "failure_mode": cand.get("failure_mode"),
                    "dominant_constraints": cand.get("active_constraints", [])[:5],
                })
                with st.expander("Inputs", expanded=False):
                    st.json(cand.get("inputs", {}))
                with st.expander("Key outputs", expanded=False):
                    outs = cand.get("outputs", {}) or {}
                    keys_show = ["P_e_net_MW","Pfus_total_MW","Q_DT_eqv","q_div_MW_m2","B_peak_T","Ti_keV","nbar_1e20_m3"]
                    st.json({k: outs.get(k) for k in keys_show if k in outs})
                with st.expander("Constraint margins (blocking/diagnostic/ignored)", expanded=False):
                    _cons = cand.get("constraints", []) or []
                    # Optional credibility overlay (display-only)
                    try:
                        if st.session_state.get("opt_use_cred") and st.session_state.get("opt_cred_map"):
                            _cm = {
                                k: ConstraintCred(
                                    name=v.get("name", k),
                                    maturity=float(v.get("maturity", 0.7)),
                                    uncertainty_frac=float(v.get("uncertainty_frac", 0.10)),
                                    conservative=bool(v.get("conservative", True)),
                                )
                                for k, v in (st.session_state.get("opt_cred_map") or {}).items()
                            }
                            _cons = apply_credibility_overlay(_cons, _cm)
                    except Exception:
                        pass
                    st.write(_cons[:80])
                if enable_multi_intent and cand.get("other_intent"):
                    with st.expander("Other Intent distance (instrumentation)", expanded=False):
                        st.write({
                            "other_intent": cand.get("other_intent"),
                            "other_feasible": cand.get("other_feasible"),
                            "other_violation": cand.get("other_violation"),
                            "other_min_signed_margin": cand.get("other_min_signed_margin"),
                            "other_failure_mode": cand.get("other_failure_mode"),
                        })
                if use_cost:
                    with st.expander("Cost proxies (transparent)", expanded=False):
                        st.json(cand.get("cost", {}))

                st.markdown("**Actions (explicit, reversible)**")
                if st.button("Send to Systems Mode (as starting point)", key="opt_send_systems"):
                    st.session_state["systems_seed_inputs"] = dict(cand.get("inputs", {}))
                    st.success("Sent to Systems Mode seed (session-only).")
                if st.button("Open in Point Designer (read-only)", key="opt_send_point"):
                    st.session_state["point_inputs_last"] = dict(cand.get("inputs", {}))
                    st.success("Loaded into Point Designer inputs (session-only).")

                if st.button("Open Scan Lab slice around this candidate", key="opt_send_scan"):
                    try:
                        inp = cand.get("inputs") or {}
                        # Default axes chosen for expert usefulness
                        xk, yk = "Ip_MA", "R0_m"
                        x0 = float(inp.get(xk, 0.0)); y0 = float(inp.get(yk, 0.0))
                        # Â±10% local neighborhood (bounded away from zero)
                        def _band(v: float) -> tuple[float,float]:
                            dv = max(0.05*abs(v), 0.01)
                            return (v - dv, v + dv)
                        xlo, xhi = _band(x0)
                        ylo, yhi = _band(y0)
                        st.session_state["scan_cart_x_key"] = xk
                        st.session_state["scan_cart_y_key"] = yk
                        st.session_state["scan_cart_x_lo"] = float(xlo)
                        st.session_state["scan_cart_x_hi"] = float(xhi)
                        st.session_state["scan_cart_y_lo"] = float(ylo)
                        st.session_state["scan_cart_y_hi"] = float(yhi)
                        st.session_state["scan_cart_nx"] = 31
                        st.session_state["scan_cart_ny"] = 25
                        st.session_state["scan_cart_intents"] = [str(run.get("intent") or "Reactor")]
                        st.session_state["scan_cart_inc_out"] = False
                        st.success("Scan Lab settings prepared. Click the Scan Lab tab and press Run cartography scan.")
                    except Exception as e:
                        st.error(f"Failed to prepare Scan Lab slice: {e}")

            else:
                st.info("No candidates to inspect.")


if _deck == "ðŸ†š Compare":
    st.header("ðŸ†š Compare")
    st.caption("Side-by-side artifact comparison to isolate mechanism and constraint-margin deltas.")
    render_mode_scope("compare")
    st.markdown("### Compare sources")
    st.caption("Compare uses either **session slots** (recommended) or uploaded JSON artifacts.")

    _slotA = st.session_state.get("cmp_slot_A")
    _slotB = st.session_state.get("cmp_slot_B")
    _metaA = st.session_state.get("cmp_slot_A_meta") or {}
    _metaB = st.session_state.get("cmp_slot_B_meta") or {}
    _have_slotA = isinstance(_slotA, dict)
    _have_slotB = isinstance(_slotB, dict)

    ca, cb, cc = st.columns([1.2, 1.2, 1.0])
    with ca:
        use_slotA = st.checkbox("Use session Slot A", value=bool(_have_slotA), key="cmp_use_slot_A")
        if _have_slotA:
            st.caption(f"Slot A: {str(_metaA.get('label',''))} | {str(_metaA.get('inputs_hash',''))[:8]}")
        else:
            st.caption("Slot A: (empty)")
    with cb:
        use_slotB = st.checkbox("Use session Slot B", value=bool(_have_slotB), key="cmp_use_slot_B")
        if _have_slotB:
            st.caption(f"Slot B: {str(_metaB.get('label',''))} | {str(_metaB.get('inputs_hash',''))[:8]}")
        else:
            st.caption("Slot B: (empty)")
    with cc:
        if st.button("ðŸ§¹ Clear slots", use_container_width=True, key="cmp_clear_slots"):
            st.session_state.pop("cmp_slot_A", None)
            st.session_state.pop("cmp_slot_B", None)
            st.session_state.pop("cmp_slot_A_meta", None)
            st.session_state.pop("cmp_slot_B_meta", None)
            st.success("Cleared Compare slots.")

    with st.expander("Upload artifacts (optional)", expanded=False):
        colA, colB = st.columns(2)
        with colA:
            upA = st.file_uploader("Artifact A (JSON)", type=["json"], key="cmpA")
            if upA is not None:
                if st.button("Store upload as Slot A", use_container_width=True, key="cmp_store_upload_A"):
                    try:
                        _a = json.loads(upA.getvalue().decode("utf-8"))
                    except Exception:
                        _a = json.loads(upA.getvalue())
                    st.session_state["cmp_slot_A"] = _a
                    st.session_state["cmp_slot_A_meta"] = {"ts_unix": float(time.time()), "inputs_hash": str((_a.get("inputs_hash") or "")), "label": "Uploaded"}
                    st.success("Stored upload into Slot A.")
        with colB:
            upB = st.file_uploader("Artifact B (JSON)", type=["json"], key="cmpB")
            if upB is not None:
                if st.button("Store upload as Slot B", use_container_width=True, key="cmp_store_upload_B"):
                    try:
                        _b = json.loads(upB.getvalue().decode("utf-8"))
                    except Exception:
                        _b = json.loads(upB.getvalue())
                    st.session_state["cmp_slot_B"] = _b
                    st.session_state["cmp_slot_B_meta"] = {"ts_unix": float(time.time()), "inputs_hash": str((_b.get("inputs_hash") or "")), "label": "Uploaded"}
                    st.success("Stored upload into Slot B.")

    def _load_art(uploaded):
        if uploaded is None:
            return None
        try:
            return json.loads(uploaded.getvalue().decode("utf-8"))
        except Exception:
            try:
                return json.loads(uploaded.getvalue())
            except Exception:
                return None

    # Resolve artifacts from preferred sources
    artA = None
    artB = None
    if bool(use_slotA) and _have_slotA:
        artA = _slotA
    else:
        artA = _load_art(st.session_state.get("cmpA"))
    if bool(use_slotB) and _have_slotB:
        artB = _slotB
    else:
        artB = _load_art(st.session_state.get("cmpB"))

    if bool(use_slotA) and (not _have_slotA):
        st.warning("Slot A is selected but empty. Send a run from Point Designer (Export Bay) or upload an artifact.")
    if bool(use_slotB) and (not _have_slotB):
        st.warning("Slot B is selected but empty. Send a run from Point Designer (Export Bay) or upload an artifact.")

    if artA and artB:
        outA = artA.get("outputs", {}) or {}
        outB = artB.get("outputs", {}) or {}
        keys = ["Q", "Pfus_total_MW", "P_e_net_MW", "betaN", "q95", "Bpeak_TF_T", "q_div_MW_m2", "neutron_wall_load_MW_m2", "COE_proxy_USD_per_MWh"]
        rows = []
        for k in keys:
            a = outA.get(k, float("nan"))
            b = outB.get(k, float("nan"))
            try:
                da = float(a); db = float(b)
                d = db - da
            except Exception:
                d = ""
            rows.append({"metric": k, "A": a, "B": b, "B-A": d})
        df = pd.DataFrame(rows)
        st.dataframe(df, use_container_width=True)

        consA = pd.DataFrame(artA.get("constraints", []) or [])
        consB = pd.DataFrame(artB.get("constraints", []) or [])
        st.markdown("### Constraints (worst margins first)")
        c1, c2 = st.columns(2)
        with c1:
            st.markdown("**Artifact A**")
            if len(consA):
                st.dataframe(consA.sort_values("residual", ascending=False).head(20), use_container_width=True)
        with c2:
            st.markdown("**Artifact B**")
            if len(consB):
                st.dataframe(consB.sort_values("residual", ascending=False).head(20), use_container_width=True)

        # simple markdown diff export
        diff_md = ["# SHAMS Artifact Comparison", "", "## Key metrics", ""]
        diff_md.append(df.to_markdown(index=False))
        diff_md.append("")
        diff_md.append("## Worst constraints (A)")
        diff_md.append("")
        if len(consA):
            diff_md.append(consA.sort_values("residual", ascending=False).head(20).to_markdown(index=False))
        diff_md.append("")
        diff_md.append("## Worst constraints (B)")
        diff_md.append("")
        if len(consB):
            diff_md.append(consB.sort_values("residual", ascending=False).head(20).to_markdown(index=False))
        st.download_button("Download comparison (markdown)", data="\n".join(diff_md), file_name="artifact_comparison.md", mime="text/markdown", use_container_width=True)

# (v372.8.7) Studies manager is rendered inside Control Room â†’ Studies tab (no tab-handle leakage).
# The previous module-scope `with tab_studies:` block was removed to satisfy UI law.

if _deck == "âš’ï¸ Reactor Design Forge":
    st.subheader("ðŸ§ª Operating envelope check (multi-point)")
    st.caption("Evaluates startup / nominal / end-of-life proxy points and reports the worst constraint.")
    colA, colB = st.columns([1,3])
    with colA:
        run_env = st.button("Run envelope check", use_container_width=True)
    if run_env:
        try:
            from envelope.points import default_envelope_points
            from physics.hot_ion import hot_ion_point
            from constraints.system import build_constraints_from_outputs, summarize_constraints
            base_inp = st.session_state.get("last_point_inp", None)
            if base_inp is None:
                st.warning("No current point inputs available.")
            else:
                _warn_unrealistic_point_inputs(base_inp, context="Envelope check")
                pts = default_envelope_points(base_inp)
                env_rows = []
                worst = None
                for i, p in enumerate(pts):
                    out = hot_ion_point(p)
                    cs = build_constraints_from_outputs(out)
                    summ = summarize_constraints(cs)
                    dom = summ.get("dominant", {})
                    row = {
                        "point": i,
                        "all_ok": bool(summ.get("all_ok", False)),
                        "dominant": dom.get("name", ""),
                        "residual": dom.get("residual", float("nan")),
                        "margin": dom.get("margin", float("nan")),
                    }
                    env_rows.append(row)
                    if worst is None or (row["residual"] == row["residual"] and row["residual"] > worst["residual"]):
                        worst = row
                st.dataframe(env_rows, use_container_width=True)
                if worst:
                    st.info(f"Worst point: #{worst['point']} - {worst['dominant']} (residual={worst['residual']:.3g})")
        except Exception as e:
            st.error(f"Envelope check failed: {e}")


if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_model:
        st.header("0-D Tokamak Physics Model (Phaseâ€‘1)")

        with st.expander("0â€‘D Physical Models - explanations", expanded=False):
            _pm = os.path.join(ROOT, "docs", "PHYSICAL_MODELS_0D.md")
            try:
                with open(_pm, "r", encoding="utf-8") as _f:
                    st.markdown(_f.read())
            except Exception as _e:
                st.error(f"Failed to load physical model doc: {_e}")


        st.markdown(r"""
        This tab is written to be **actionable**: each section maps to code in `src/physics/`, `src/phase1_models.py`,
        `src/phase1_systems.py`, and `src/solvers/`.

        SHAMS remains a **0â€‘D / volumeâ€‘averaged / steadyâ€‘state** point-design model at its core, intended for *fast feasibility scanning*.
        Over time we have added several **external systems codesâ€‘inspired** upgrades that remain lightweight and Windowsâ€‘friendly:

        - **Optional analytic profiles ("Â½â€‘D")** for $n_e(\rho)$, $T_i(\rho)$, $T_e(\rho)$ with **normalization to the chosen volume averages**,
          plus derived averages like peaking factors and $\langle n_e^2 \rangle/\langle n_e \rangle^2$.
        - **Radiation options:** legacy fractional radiation (stable for scans) and a physicsâ€‘based path (brem + synchrotron + simple impurity line radiation).
        - **Constraint system:** engineering and plasma constraints are represented as reusable objects (external systems codesâ€‘like), usable by scans and vector solvers.
        - **Solvers:** classic nested 1â€‘D solves are still available, plus a more general bounded "targets â†’ variables" solve primitive.

        It is **not** a full transport / equilibrium / neutronics code, but it is designed to grow in that direction while staying usable.
        """)

        st.caption("Tip: expand only the models you care about - each block is independent.")

        # --- Geometry ---
        with st.expander("Geometry: volume and surface area (implemented)", expanded=False):
            st.markdown(r"""
            Implemented helpers:

            **Plasma volume** (`tokamak_volume`)
            $$
            V \approx 2\pi^2\,R\,a^2\,\kappa
            $$

            **Plasma surface area** (`tokamak_surface_area`)
            $$
            S \approx 4\pi^2\,R\,a\,\kappa
            $$

            Notes:
            - These are **engineering approximations** intended to preserve correct monotonic trends.
            - Units: $R,a$ in m, $V$ in m$^3$, $S$ in m$^2$.
            """)

        # --- Confinement ---
        with st.expander("Energy confinement: IPB98(y,2) (implemented)"):
            st.markdown(r"""
            Implemented model: `tauE_ipb98y2`.

            $$
            \tau_E = 0.0562\, I_p^{0.93} B_t^{0.15} \bar{n}^{0.41} P_{loss}^{-0.69} R^{1.97} \epsilon^{0.58} \kappa^{0.78} M^{0.19}
            $$
            where $\epsilon=a/R$.

            **Units (must match the implementation):**
            - $I_p$ in MA
            - $B_t$ in T
            - $\bar{n}$ in units of $10^{20}\,\mathrm{m^{-3}}$ (i.e. `ne20`)
            - $P_{loss}$ in MW
            - $R,a$ in m
            - $M$ in amu (default 2.5)

            Output: $\tau_E$ in seconds.
            """)

        # --- L-H threshold ---
        with st.expander("H-mode access: Martin-2008 Lâ€“H threshold (implemented)"):
            st.markdown(r"""
            Implemented model: `p_LH_martin08`.

            $$
            P_{LH} = 0.0488\, \bar{n}^{0.717} B_t^{0.803} S^{0.941}\,\left(\frac{2}{A_{eff}}\right)
            $$

            **Units:**
            - $\bar{n}$ in $10^{20}\,\mathrm{m^{-3}}$ (line-averaged)
            - $B_t$ in T
            - $S$ in m$^2$ (uses the same proxy as the geometry block)
            - $A_{eff}$ dimensionless (defaults to 2.0)

            Output: $P_{LH}$ in MW.
            """)

        # --- Greenwald ---
        with st.expander("Density limit: Greenwald (implemented)"):
            st.markdown(r"""
            Implemented helper: `greenwald_density_20`.

            $$
            n_{GW}\,[10^{20}\,\mathrm{m^{-3}}] = \frac{I_p\,[\mathrm{MA}]}{\pi a^2\,[\mathrm{m^2}]}
            $$

            In scans, an operating fraction is typically applied:
            $$
            \bar{n} = f_{nG}\,n_{GW},\qquad 0 < f_{nG} \le 1.
            $$
            """)

        # --- Screening proxies ---
        with st.expander("Screening proxies: q95, Î²N, bootstrap fraction (implemented proxies)"):
            st.markdown(r"""
            These are explicitly labeled **proxies** (trend-correct, not equilibrium/transport solutions).

            **q95 proxy** (`q95_proxy_cyl`)
            $$
            q_{95} \approx \left(\frac{2\pi R B_t}{\mu_0 I_p}\right)\left(\frac{a}{R}\right)\frac{1}{\kappa}
            $$
            with $I_p$ converted to amperes internally.

            **Normalized beta** (`betaN_from_beta`)
            $$
            \beta_N = \beta(\%)\,\frac{a\,B_t}{I_p}
            \qquad\text{with}\qquad \beta(\%)=100\,\beta
            $$
            where $\beta$ is the *fractional* beta.

            **Bootstrap fraction proxy** (`bootstrap_fraction_proxy`)
            $$
            f_{bs} \approx C_{bs}\,\frac{\beta_N}{q_{95}}
            $$
            then clamped to a configured range (default 0 to 0.95).
            """)

        # --- Fusion reactivity ---
        with st.expander("Fusion reactivity: Boschâ€“Hale âŸ¨ÏƒvâŸ© (implemented)"):
            st.markdown(r"""
            Implemented function: `bosch_hale_sigmav(T_i, reaction)`.

            This uses the Boschâ€“Hale parameterization for Maxwellian-averaged reactivity:
            $$
            \langle\sigma v\rangle(T_i)\;[\mathrm{m^3/s}]
            $$

            Internally, the implementation computes intermediate variables ($\theta$, $\xi$) from a
            reaction-specific coefficient set and returns a strictly non-negative value.

            **Important for UI users:**
            - Input $T_i$ is in **keV**.
            - Output is in **m$^3$/s**.
            """)

            # Boschâ€“Hale coefficient values used by the implementation (from `BH_COEFFS`)
            _bh_rows = []
            for _rxn in ["DT", "DD_Tp", "DD_He3n"]:
                _c = BH_COEFFS[_rxn]
                _bh_rows.append({"Reaction": _rxn, **asdict(_c)})
            _bh_df = pd.DataFrame(_bh_rows).set_index("Reaction")
            st.caption("Boschâ€“Hale coefficients used for DT and DD channels (exact values as implemented).")
            st.dataframe(_bh_df, use_container_width=True)

        # --- Fusion power / gain symbols (fixing the screenshot issue) ---
        with st.expander("Fusion power & gain definitions: P_f, P_Î±, Q (notation)"):
            st.markdown(r"""
            **What these symbols mean (and how they relate):**

            **Fusion power, $P_f$**  
            Total thermal power released by fusion reactions occurring in the plasma:
            $$
            P_f \;=\; \dot{N}_{\text{fus}}\,E_{\text{fus}}
            $$
            where $\dot{N}_{\text{fus}}$ is the fusion reaction rate [1/s] and $E_{\text{fus}}$ is the energy released per reaction.
            For Dâ€‘T, $E_{\text{fus}} = 17.6\,\mathrm{MeV}$.

            **Alpha heating power, $P_{\alpha}$**  
            Part of $P_f$ carried by *charged* alpha particles and deposited back into the plasma (selfâ€‘heating):
            $$
            P_{\alpha} \;=\; f_\alpha\,P_f
            $$
            For Dâ€‘T, $f_\alpha = \frac{3.5}{17.6} \approx 0.199$, so $P_{\alpha} \approx 0.20\,P_f$.
            (The rest is mainly neutron power: $P_n \approx 0.80\,P_f$.)

            **Fusion gain, $Q$**  
            In this UI, $Q$ is the standard *plasma gain*:
            $$
            Q \;=\; \frac{P_f}{P_{\mathrm{aux}}}
            $$
            where $P_{\mathrm{aux}}$ is the **externally applied** auxiliary heating power (e.g., NBI/RF) required to sustain the operating point.
            This is distinct from â€œwallâ€‘plugâ€ gain, which would include plant efficiencies and nonâ€‘plasma power draws.

            **How to interpret in scans**
            - Increasing $P_f$ increases $P_{\alpha}$ proportionally (more selfâ€‘heating).  
            - $Q$ improves only when $P_f$ grows faster than the required $P_{\mathrm{aux}}$.
            """)

        # --- SOL width metric ---
        with st.expander("Optional divertor/SOL risk metric: Eich Î»q (implemented)"):
            st.markdown(r"""
            Implemented metric: `lambda_q_eich14_mm`.

            $$
            \lambda_q\,[\mathrm{mm}] \approx \text{factor}\times 0.63\,B_{pol}^{-1.19}
            $$

            with $B_{pol}$ approximated by:
            $$
            B_{pol} \approx \frac{\mu_0 I_p}{2\pi a}
            $$

            This is **not** a selfâ€‘consistent divertor / edge powerâ€‘exhaust model - itâ€™s a compact, orderâ€‘ofâ€‘magnitude **screening proxy** for quickly comparing design points.
            """)

        st.info(
            "If you want the *full* step-by-step closure shown here (power balance â†’ temperatures â†’ Pf/Q), "
            "tell me which exact function in `src/phase1_core.py` you want treated as the single source of truth, "
            "and Iâ€™ll mirror it line-for-line in this tab."
        )

# -----------------------------
# Benchmarks
# -----------------------------
if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_bench:
        st.subheader("Regression Benchmarks")
        st.write("Run a small suite of SPARC-like cases to ensure recent physics/solver changes haven't broken behavior.")

        import json
        from pathlib import Path

        bench_dir = Path(__file__).resolve().parent.parent / "benchmarks"
        cases_path = bench_dir / "cases.json"
        golden_path = bench_dir / "golden.json"

        diff_path = bench_dir / "last_diff_report.json"
        with st.expander("Latest diff report (from last run)", expanded=False):
            if diff_path.exists():
                try:
                    rep = json.loads(diff_path.read_text(encoding="utf-8"))
                    st.caption(f"Generated at unix={rep.get('created_unix'):.0f} | failures={rep.get('n_failed',0)}")
                    rows = rep.get("rows", [])
                    if rows:
                        import pandas as pd

                        df_rep = pd.DataFrame(rows)
                        # show worst first
                        if "ok" in df_rep.columns and "rel_err" in df_rep.columns:
                            df_rep = df_rep.sort_values(by=["ok","rel_err"], ascending=[True, False])
                        st.dataframe(df_rep, use_container_width=True, height=260)
                    # Structural diffs (constraints/model cards) vs golden artifacts, if present
                    ss = rep.get("structural_summary")
                    if ss:
                        st.markdown("**Structural diffs vs golden artifacts**")
                        st.write({k: ss.get(k) for k in ["n_cases","n_with_changes","total_added_constraints","total_removed_constraints","total_changed_constraints","total_modelcard_changes"]})
                    structural = rep.get("structural") or {}
                    if structural:
                        with st.expander("Show structural diffs by case", expanded=False):
                            for cname, d in structural.items():
                                cadd = d.get("constraints", {}).get("added", [])
                                crem = d.get("constraints", {}).get("removed", [])
                                cchg = d.get("constraints", {}).get("changed_meta", [])
                                mc = d.get("model_cards", {})
                                mcchg = (mc.get("added", []) or []) + (mc.get("removed", []) or []) + (mc.get("changed", []) or [])
                                if not (cadd or crem or cchg or mcchg or (d.get("schema_version", {}).get("new") != d.get("schema_version", {}).get("old"))):
                                    continue
                                with st.expander(f"{cname}", expanded=False):
                                    if cadd: st.write({"constraints_added": cadd})
                                    if crem: st.write({"constraints_removed": crem})
                                    if cchg: st.write({"constraint_meta_changes": cchg})
                                    if mc.get("added"): st.write({"model_cards_added": mc.get("added")})
                                    if mc.get("removed"): st.write({"model_cards_removed": mc.get("removed")})
                                    if mc.get("changed"): st.write({"model_cards_changed": mc.get("changed")})

                    st.download_button("Download diff report JSON", data=diff_path.read_bytes(), file_name="last_diff_report.json")
                except Exception as e:
                    st.warning(f"Could not read diff report: {e}")
            else:
                st.info("No diff report yet. Run benchmarks to generate one.")


        # Release notes (auto-generated)
        with st.expander("Release notes (auto)", expanded=False):
            import subprocess, sys
            from pathlib import Path

            repo_root = Path(__file__).resolve().parent.parent
            out_md = repo_root / "RELEASE_NOTES.md"
            old_default = str((repo_root.parent / "SHAMS_old").resolve()) if (repo_root.parent / "SHAMS_old").exists() else r"..\SHAMS_old"
            old_path = st.text_input("Old SHAMS repo path", value=st.session_state.get("release_notes_old", old_default))
            st.session_state["release_notes_old"] = old_path

            auto = st.checkbox("Auto-generate if missing/out-of-date", value=True, key="release_notes_auto")
            run_now_rn = st.button("Generate release notes now", key="btn_release_notes_now")

            def _needs_rn() -> bool:
                if not out_md.exists():
                    return True
                try:
                    m_out = out_md.stat().st_mtime
                    # regenerate if diff report is newer, or tool changed
                    tool_p = repo_root / "tools" / "release_notes.py"
                    diff_p = repo_root / "benchmarks" / "last_diff_report.json"
                    newest = max([p.stat().st_mtime for p in [tool_p, diff_p] if p.exists()] + [0])
                    return newest > m_out
                except Exception:
                    return False

            if (auto and _needs_rn() and not st.session_state.get("_rn_ran_this_session", False)) or run_now_rn:
                cmd = [sys.executable, str(repo_root / "tools" / "release_notes.py"), "--old", old_path, "--new", str(repo_root), "--out", str(out_md)]
                st.caption("Running: " + " ".join(cmd))
                try:
                    p = subprocess.run(cmd, capture_output=True, text=True, cwd=str(repo_root))
                    st.session_state["_rn_last_stdout"] = p.stdout
                    st.session_state["_rn_last_stderr"] = p.stderr
                    st.session_state["_rn_last_rc"] = p.returncode
                    st.session_state["_rn_ran_this_session"] = True
                except Exception as e:
                    st.session_state["_rn_last_stderr"] = str(e)
                    st.session_state["_rn_last_rc"] = 1

            rc = st.session_state.get("_rn_last_rc")
            if rc is not None:
                if rc == 0:
                    st.success("Release notes generated.")
                else:
                    st.warning("Release notes generation had issues (see logs).")
                with st.expander("Logs", expanded=False):
                    st.code((st.session_state.get("_rn_last_stdout") or "") + "\n" + (st.session_state.get("_rn_last_stderr") or ""))

            if out_md.exists():
                st.markdown(out_md.read_text(encoding="utf-8", errors="ignore"))
                st.download_button("Download RELEASE_NOTES.md", data=out_md.read_bytes(), file_name="RELEASE_NOTES.md", mime="text/markdown")
            else:
                st.info("RELEASE_NOTES.md not found yet.")

        with st.expander("Regression comparisons", expanded=False):
            colA, colB = st.columns([1,1])
            with colA:
                run_now = st.button("Run benchmarks")
            with colB:
                regen = st.button("Regenerate golden (intentional changes)")
    
            def _safe(v):
                try:
                    return float(v)
                except Exception:
                    return float("nan")
    
            if cases_path.exists():
                _cases_raw = json.loads(cases_path.read_text())
            else:
                _cases_raw = {}
    
            # Normalize benchmark cases into a list[dict] with keys: name, inputs
            # Supports dict-form (name -> inputs), list-form (dicts), or list-form (names).
            cases = []
            if isinstance(_cases_raw, dict):
                for _name, _inp in _cases_raw.items():
                    if isinstance(_inp, dict):
                        cases.append({"name": str(_name), "inputs": _inp})
            elif isinstance(_cases_raw, list):
                for i, _c in enumerate(_cases_raw):
                    if isinstance(_c, dict):
                        _name = _c.get("name", f"case_{i}")
                        _inp = _c.get("inputs", _c.get("inp", _c.get("input", {})))
                        if isinstance(_inp, dict):
                            cases.append({"name": str(_name), "inputs": _inp})
                    else:
                        cases.append({"name": str(_c), "inputs": {}})
    
            if not cases:
                # Always provide at least one default case so the UI doesn't crash
                cases = [{"name": "default", "inputs": {"R0_m": 1.85, "a_m": 0.6, "kappa": 1.75, "Bt_T": 12.0, "Ip_MA": 8.0, "Ti_keV": 10.0, "fG": 0.85, "t_shield_m": 0.25, "Paux_MW": 25.0}}]
    
            base = PointInputs(R0_m=1.85, a_m=0.6, kappa=1.75, Bt_T=12.0, Ip_MA=8.0, Ti_keV=10.0, fG=0.85, t_shield_m=0.25, Paux_MW=25.0)
            if run_now or regen:
                results = {}
                for _case in cases:
                    name = _case.get("name","case")
                    overrides = _case.get("inputs", {})
                    # Defensive: apply only existing fields
                    d = base.__dict__.copy()
                    for k, v in overrides.items():
                        if k in d:
                            d[k] = v
                    inp_case = PointInputs(**d)
                    results[name] = hot_ion_point(inp_case)
    
                if regen:
                    golden_path.write_text(json.dumps(results, indent=2))
                    st.success(f"Wrote golden: {golden_path}")
                else:
                    if not golden_path.exists():
                        st.error("golden.json not found. Click 'Regenerate golden' once to create it.")
                    else:
                        golden = json.loads(golden_path.read_text())
                        CURATED = ["Q_DT_eqv","H98","P_fus_MW","P_SOL_MW","q_div_MW_m2","B_peak_T","sigma_hoop_MPa","hts_margin_cs","J_eng_A_mm2","t_flat_s","P_net_MW"]
                        rows = []
                        failed = 0
                        for name, cur in results.items():
                            ref = golden.get(name, {})
                            for k in CURATED:
                                a = _safe(cur.get(k))
                                b = _safe(ref.get(k))
                                if not (math.isfinite(a) and math.isfinite(b)):
                                    continue
                                atol = 1e-6
                                rtol = 5e-3
                                ok = abs(a-b) <= max(atol, rtol*max(abs(b),1e-9))
                                if not ok:
                                    failed += 1
                                rows.append({"case":name,"key":k,"got":a,"golden":b,"rel_err":(abs(a-b)/max(abs(b),1e-9)),"ok":ok})
                        import pandas as pd

                        df = pd.DataFrame(rows)
                        st.dataframe(df, use_container_width=True)

                        # Write a machine-readable diff report (used by CI and the UI)
                        try:
                            import time as _time
                            report = {
                                "created_unix": _time.time(),
                                "rtol": 5e-3,
                                "atol": 1e-6,
                                "n_rows": int(len(rows)),
                                "n_failed": int(failed),
                                "rows": rows,
                            }
                            (bench_dir / "last_diff_report.json").write_text(json.dumps(report, indent=2, sort_keys=True), encoding="utf-8")
                        except Exception:
                            pass

                        if failed==0:
                            st.success("All benchmark comparisons passed (within tolerances).")
                        else:
                            st.warning(f"{failed} comparisons exceeded tolerance. See table.")
    
    
        st.divider()
        with st.expander("Sensitivity and uncertainty (Monte Carlo)", expanded=False):
            st.subheader("Sensitivity (Monte Carlo)")
            st.write("Runs a lightweight uncertainty scan around a selected benchmark case (Windows-native).")
    
            from analysis.sensitivity import monte_carlo_feasibility
            from models.inputs import PointInputs
    
            case_names = [c.get("name", f"case_{i}") for i,c in enumerate(cases)]
            case_sel = st.selectbox("Benchmark case for sensitivity", case_names, index=0, key="sens_case_sel")
            n_mc = st.number_input("Samples", min_value=50, max_value=2000, value=50, step=50, key="sens_n")
            if st.button("Run Monte Carlo", key="run_mc_bench"):
                c = cases[case_names.index(case_sel)]
                base_inp = PointInputs(**c["inputs"])
                res = monte_carlo_feasibility(base_inp, n=int(n_mc), seed=42)
                st.metric("Feasible probability", f"{res['p_feasible']*100:.1f}%")
                st.write("Most frequently violated constraints:")
                st.dataframe(res["worst_constraints"], use_container_width=True)
    
        st.divider()
        with st.expander("Pareto search (design studies)", expanded=False):
            st.subheader("Pareto Search (LHS)")
            st.write("Finds a feasible Pareto set for a small set of design knobs around a benchmark case.")
            from solvers.optimize import pareto_optimize
    
            case_sel2 = st.selectbox("Benchmark case for Pareto", case_names, index=0, key="pareto_case_sel")
            n_lhs = st.number_input("LHS samples", min_value=50, max_value=5000, value=100, step=50, key="pareto_n")
            # simple bounds
            colp1, colp2 = st.columns(2)
            with colp1:
                R0_lo = st.number_input("R0 min [m]", value=1.5, step=0.1, key="R0_lo")
                Ip_lo = st.number_input("Ip min [MA]", value=5.0, step=0.5, key="Ip_lo")
            with colp2:
                R0_hi = st.number_input("R0 max [m]", value=2.5, step=0.1, key="R0_hi")
                Ip_hi = st.number_input("Ip max [MA]", value=12.0, step=0.5, key="Ip_hi")
            fG_lo = st.number_input("fG min", value=0.4, step=0.05, key="fG_lo")
            fG_hi = st.number_input("fG max", value=1.2, step=0.05, key="fG_hi")
    
            if st.button("Run Pareto search", key="run_pareto"):
                c = cases[case_names.index(case_sel2)]
                base_inp = PointInputs(**c["inputs"])
                bounds = {"R0_m": (float(R0_lo), float(R0_hi)), "Ip_MA": (float(Ip_lo), float(Ip_hi)), "fG": (float(fG_lo), float(fG_hi))}
                objectives = {"R0_m": "min", "B_peak_T": "min", "P_e_net_MW": "max"}
                res = pareto_optimize(base_inp, bounds=bounds, objectives=objectives, n_samples=int(n_lhs), seed=1)
                st.write(f"Feasible points: {len(res['feasible'])}  |  Pareto points: {len(res['pareto'])}")
                st.dataframe(res["pareto"], use_container_width=True)
    
        # -----------------------------
        # Variable Registry (auditable meanings/units/sources)
        # -----------------------------
if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_registry:
        st.subheader("Variable Registry")
        st.markdown(
            "A external systems codes-style registry of key SHAMS variables with units, meaning, and model provenance. "
            "Use this to keep the code **auditable** as physics/engineering fidelity increases."
        )
        q = st.text_input("Search variables", value="", placeholder="e.g., H98, q_div, HTS, TBR")
        try:
            df = registry_dataframe(q)
            st.dataframe(df, use_container_width=True, height=520)
            st.download_button(
                "Download registry (CSV)",
                data=df.to_csv(index=False),
                file_name="shams_variable_registry.csv",
                mime="text/csv",
                use_container_width=True,
            )
        except Exception as e:
            st.error(f"Registry unavailable: {e}")

# -----------------------------
# Validation (envelopes)
# -----------------------------
if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_validation:
        st.subheader("Validation envelopes")
        st.markdown(
            "Decision-grade validation in SHAMS is **envelope-based**: we check whether a solution lies within "
            "a broad reference band for key metrics, rather than trying to match a single reference point. "
            "This is robust to proxy changes and is aligned with external systems codes-style workflows."
        )
        try:
            from validation.envelopes import default_envelopes
            envs = default_envelopes()
            env_name = st.selectbox("Select envelope", list(envs.keys()), index=0, key="validation_env_sel")
            env = envs[env_name]
            st.caption(env.notes)

            out = st.session_state.get("last_point_out")
            if not out:
                st.info("Run a Point Designer solve first. The latest outputs will be checked here.")
            else:
                report = env.check(out)
                import pandas as pd

                rows = []
                n_fail = 0
                for k, r in report.items():
                    if not r.get("ok"):
                        n_fail += 1
                    rows.append({
                        "metric": k,
                        "value": r.get("value"),
                        "lo": r.get("lo"),
                        "hi": r.get("hi"),
                        "ok": bool(r.get("ok")),
                    })
                df = pd.DataFrame(rows)
                st.dataframe(df, use_container_width=True, height=360)
                if n_fail == 0:
                    st.success("All selected envelope checks passed.")
                else:
                    st.warning(f"{n_fail} envelope checks failed. This indicates the *targets/bounds* are outside the reference band (not a code error).")
        except Exception as e:
            st.error(f"Validation module unavailable: {e}")

        st.divider()
        st.subheader("Invariant guardrails")
        st.caption("Deterministic sign/bookkeeping checks (not experimental validation).")
        try:
            from validation.invariants import check_invariants
            out = st.session_state.get("last_point_out")
            if not out:
                st.info("Run a Point Designer solve first. The latest outputs will be checked here.")
            else:
                rep = check_invariants(out)
                if bool(rep.get("ok")):
                    st.success("All invariant guardrails passed.")
                else:
                    st.error("Invariant guardrail failures detected (likely bookkeeping/sign issue or invalid inputs).")
                    st.json(rep.get("failures", {}))
        except Exception as e:
            st.caption(f"Invariant checks unavailable: {e}")


# -----------------------------
# Compliance (requirements + model cards)
# -----------------------------
if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_compliance:
        st.subheader("Verification & Compliance")
        st.caption("Shows the latest verification/compliance matrix from verification/report.json (if present).")

        def _load_verification_report_ui():
            try:
                here = Path(__file__).resolve()
                root = here.parent.parent  # ui/ -> repo root
                rp = root / "verification" / "report.json"
                if rp.exists():
                    return json.loads(rp.read_text(encoding="utf-8"))
            except Exception:
                return None
            return None

        report = _load_verification_report_ui()
        if not report:
            st.info("No verification/report.json found. Run: `python verification/run_verification.py` to generate it.")
        else:
            meta = report.get("meta", {})
            st.write({
                "generated_unix": meta.get("generated_unix"),
                "python": meta.get("python"),
                "platform": meta.get("platform"),
                "git_commit": meta.get("git_commit"),
            })

            # Summary
            summary = report.get("summary", {})
            cols = st.columns(4)
            cols[0].metric("Requirements", int(summary.get("n_requirements", 0)))
            cols[1].metric("Passed", int(summary.get("n_pass", 0)))
            cols[2].metric("Failed", int(summary.get("n_fail", 0)))
            cols[3].metric("Overall", "PASS" if summary.get("all_pass") else "FAIL")

            # Detailed table
            rows = report.get("results", [])
            if rows:
                df = pd.DataFrame(rows)
                # Friendly columns ordering
                keep = [c for c in ["req_id","title","status","details","linked_model_cards"] if c in df.columns]
                df = df[keep] if keep else df
                st.dataframe(df, use_container_width=True, height=520)

            # Download JSON
            st.download_button(
                "Download verification report.json",
                data=json.dumps(report, indent=2, sort_keys=True),
                file_name="verification_report.json",
                mime="application/json",
            )


if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_docs:
        st.header("Docs")
        st.caption("Built-in documentation bundled with this repository (no internet required).")

        doc_options = {
            "Upgrade plan (transparent)": os.path.join(ROOT, "docs", "SHAMS_upgrade_plan_from_PROCESS.md"),
            "Lessons learned (systems codes)": os.path.join(ROOT, "docs", "PROCESS_lessons.md"),
            "0â€‘D Physical Models (Phaseâ€‘1)": os.path.join(ROOT, "docs", "PHYSICAL_MODELS_0D.md"),
            "Engineering closures": os.path.join(ROOT, "docs", "ENGINEERING_CLOSURES.md"),
            "Operating envelope (multi-point)": os.path.join(ROOT, "docs", "ENVELOPE.md"),
            "Studies workflows": os.path.join(ROOT, "docs", "STUDIES.md"),
            "Model cards (auditability)": os.path.join(ROOT, "docs", "MODEL_CARDS.md"),
            "Compliance & verification": os.path.join(ROOT, "docs", "COMPLIANCE.md"),
            "Regression & golden benchmarks": os.path.join(ROOT, "docs", "REGRESSION.md"),
            "Release notes generation": os.path.join(ROOT, "docs", "RELEASE_NOTES.md"),
            "UI quickstart": os.path.join(ROOT, "README_UI.md"),
        }

        doc_sel = st.selectbox("Select a document", list(doc_options.keys()), index=0, key="doc_select")
        doc_path = doc_options.get(doc_sel)

        if doc_path and os.path.exists(doc_path):
            try:
                with open(doc_path, "r", encoding="utf-8") as f:
                    st.markdown(f.read())
            except Exception as _e:
                st.error(f"Failed to read doc: {_e}")
        else:
            st.warning("Document file not found in this checkout.")


# -----------------------------
# Artifacts Explorer (new)
# -----------------------------
def _load_json_from_upload(uploaded) -> Dict[str, Any] | None:
    if uploaded is None:
        return None
    try:
        raw = uploaded.getvalue()
        if isinstance(raw, bytes):
            raw = raw.decode("utf-8")
        return json.loads(raw)
    except Exception:
        return None


def _safe_df(rows: List[Dict[str, Any]]) -> pd.DataFrame:
    try:
        return pd.DataFrame(rows)
    except Exception:
        return pd.DataFrame()


def _as_float(x) -> float | None:
    try:
        if x is None:
            return None
        if isinstance(x, bool):
            return None
        return float(x)
    except Exception:
        return None


def _numeric_delta_table(base_out: Dict[str, Any], scen_out: Dict[str, Any], limit: int = 40) -> pd.DataFrame:
    keys = sorted(set(base_out.keys()) | set(scen_out.keys()))
    rows = []
    for k in keys:
        a = _as_float(base_out.get(k))
        b = _as_float(scen_out.get(k))
        if a is None or b is None:
            continue
        d = b - a
        # skip near-identical
        if abs(d) < 1e-12:
            continue
        rows.append({"metric": k, "baseline": a, "scenario": b, "delta": d, "delta_frac": (d / a) if abs(a) > 1e-12 else None})
    df = pd.DataFrame(rows)
    if df.empty:
        return df
    df = df.reindex(df["delta"].abs().sort_values(ascending=False).index)
    return df.head(limit)


if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_artifacts:
        st.header("Artifacts Explorer")
        st.caption("Load a SHAMS run artifact and inspect new v50+ artifact sections (constraint ledger, model set, standardized tables).")

        up = st.file_uploader("Upload shams_run_artifact.json", type=["json"], key="ae_upload")
        art = _load_json_from_upload(up)

        col_a, col_b = st.columns([1.2, 1.0])
        with col_a:
            alt_path = st.text_input("...or load from local path", value="", key="ae_path")
        with col_b:
            load_btn = st.button("Load from path", key="ae_load_path")

        if load_btn and alt_path:
            try:
                with open(alt_path, "r", encoding="utf-8") as f:
                    art = json.load(f)
            except Exception as e:
                st.error(f"Failed to load JSON: {type(e).__name__}: {e}")
                art = None

        if not art:
            st.info("Upload an artifact JSON (or provide a path) to explore.")
        else:
            meta = art.get("meta", {}) or {}
            prov = art.get("provenance", {}) or {}
            st.subheader("Metadata")
            st.write({
                "schema_version": art.get("schema_version"),
                "label": meta.get("label"),
                "mode": meta.get("mode"),
                "git_commit": prov.get("git_commit"),
                "python": prov.get("python"),
                "platform": prov.get("platform"),
                "repo_version": prov.get("repo_version"),
            })

            # --- Constraint ledger ---
            st.subheader("Constraint Margin Ledger")
            ledger = art.get("constraint_ledger") or {}
            if isinstance(ledger, dict) and ledger.get("entries"):
                st.caption(f"schema={ledger.get('schema_version','(missing)')}  fingerprint={ledger.get('ledger_fingerprint_sha256','(missing)')}")
                top = ledger.get("top_blockers") or []
                if top:
                    st.markdown("**Top blockers**")
                    st.dataframe(_safe_df(top), use_container_width=True)
                with st.expander("All ledger entries"):
                    st.dataframe(_safe_df(ledger.get("entries") or []), use_container_width=True)
            else:
                st.info("No constraint_ledger found in this artifact.")

            # --- Model set / registry ---
            st.subheader("Model Set")
            model_set = art.get("model_set") or {}
            model_registry = art.get("model_registry") or {}
            if model_set:
                st.caption(f"schema={model_set.get('schema_version','(missing)')}")
                st.json(model_set)
            else:
                st.info("No model_set embedded in this artifact.")
            with st.expander("Model Registry"):
                if model_registry:
                    st.caption(f"schema={model_registry.get('schema_version','(missing)')}")
                    st.json(model_registry)
                else:
                    st.info("No model_registry embedded in this artifact.")

            # --- Standard tables ---
            st.subheader("Standard Tables")
            tables = art.get("tables") or {}
            if isinstance(tables, dict) and tables:
                for k in ["plasma", "power_balance", "tritium"]:
                    if k in tables:
                        st.markdown(f"**{k}**")
                        t = tables.get(k)
                        if isinstance(t, dict):
                            st.dataframe(pd.DataFrame([t]), use_container_width=True)
                        elif isinstance(t, list):
                            st.dataframe(pd.DataFrame(t), use_container_width=True)
                        else:
                            st.json(t)
            else:
                st.info("No tables.v1 section found in this artifact.")

            with st.expander("Full artifact JSON"):
                st.json(art)


# -----------------------------
# Case Deck Runner (new)
# -----------------------------
if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_deck:
        st.header("Case Deck Runner")
        st.caption("Run a case_deck.v1 YAML/JSON deck and view the resolved config + artifact outputs.")

        up_deck = st.file_uploader("Upload case_deck.yaml / .json", type=["yaml", "yml", "json"], key="deck_upload")
        out_root = os.path.join(ROOT, "ui_runs")
        os.makedirs(out_root, exist_ok=True)
        out_name = st.text_input("Output folder name (under ui_runs/)", value=f"deck_{int(time.time())}", key="deck_out_name")
        run_btn = st.button("Run Case Deck", key="deck_run")

        if run_btn:
            if up_deck is None:
                st.error("Please upload a case deck file first.")
            else:
                try:
                    deck_path = os.path.join(out_root, f"_uploaded_{up_deck.name}")
                    with open(deck_path, "wb") as f:
                        f.write(up_deck.getvalue())
                    out_dir = os.path.join(out_root, out_name)
                    os.makedirs(out_dir, exist_ok=True)
                    runner = os.path.join(ROOT, "tools", "run_case_deck.py")
                    proc = subprocess.run(
                        [sys.executable, runner, deck_path, "--out", out_dir],
                        cwd=ROOT,
                        capture_output=True,
                        text=True,
                        check=False,
                    )
                    st.code(proc.stdout or "", language="text")
                    if proc.returncode != 0:
                        st.error("Case deck run failed.")
                        st.code(proc.stderr or "", language="text")
                    else:
                        art_path = os.path.join(out_dir, "shams_run_artifact.json")
                        cfg_path = os.path.join(out_dir, "run_config_resolved.json")
                        st.success(f"Wrote outputs to: {out_dir}")
                        if os.path.exists(cfg_path):
                            st.subheader("Resolved config")
                            with open(cfg_path, "r", encoding="utf-8") as f:
                                st.json(json.load(f))
                        if os.path.exists(art_path):
                            st.subheader("Run artifact (preview)")
                            with open(art_path, "r", encoding="utf-8") as f:
                                st.json(json.load(f))
                except Exception as e:
                    st.error(f"{type(e).__name__}: {e}")


# -----------------------------
# Authority & Confidence (v256.0)
# -----------------------------
if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_authority_conf:
        st.header("Authority & Confidence")
        st.caption("Trust ledger: authority tiers, maturity tags, and a design-level confidence class. Post-processing only; truth unchanged.")

        colA, colB = st.columns([0.55, 0.45], gap="large")
        with colA:
            st.markdown("### Load artifact")
            up_art = st.file_uploader("shams_run_artifact.json", type=["json"], key="authconf_upload")
            art = _load_json_from_upload(up_art)
            if not art:
                # Fall back to the most recent artifact in session if present.
                art = st.session_state.get("systems_last_solve_artifact") if isinstance(st.session_state.get("systems_last_solve_artifact"), dict) else None
                if not art:
                    art = st.session_state.get("last_point_artifact") if isinstance(st.session_state.get("last_point_artifact"), dict) else None

            if not art:
                st.info("Upload an artifact, or run Point Designer / Systems Mode to populate a last artifact.")
            else:
                ac = art.get("authority_confidence") if isinstance(art, dict) else None
                if not isinstance(ac, dict):
                    st.warning("No authority_confidence found. (Older artifact?)")
                    st.json({"available_keys": sorted(list(art.keys()))[:40]}, expanded=False)
                else:
                    dc = str((ac.get("design") or {}).get("design_confidence_class", "UNKNOWN"))
                    st.markdown(f"**Design confidence class:** `{dc}`")
                    st.caption("Class is a conservative aggregation over implicated subsystems and near-binding hard constraints.")

        with colB:
            st.markdown("### Quick legend")
            st.markdown("- **A**: anchored by authoritative/external contracts (best)")
            st.markdown("- **B**: parametric / semi-authoritative closure")
            st.markdown("- **C**: proxy models or extrapolation-heavy")
            st.markdown("- **D**: speculative / unknown authority")
            st.markdown("- **UNKNOWN**: missing metadata")

        if isinstance(art, dict) and isinstance(art.get("authority_confidence"), dict):
            ac = art["authority_confidence"]
            subs = ac.get("subsystems") or {}
            rows = []
            for k in sorted(list(subs.keys())):
                v = subs.get(k) or {}
                if not isinstance(v, dict):
                    continue
                rows.append({
                    "subsystem": k,
                    "confidence": v.get("confidence_class"),
                    "authority_tier": v.get("authority_tier"),
                    "maturity": v.get("maturity"),
                    "involved": v.get("involved"),
                    "rationale": v.get("rationale"),
                })
            if rows:
                st.subheader("Subsystem trust ledger")
                st.dataframe(pd.DataFrame(rows), use_container_width=True, hide_index=True)
            else:
                st.info("No subsystem entries available.")


# -----------------------------
# Decision Consequences (v257.0)
# -----------------------------
if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_decision_conseq:
        st.header("Decision Consequences")
        st.caption(
            "Advisory governance layer: converts margins + authority into a deterministic 'posture' and risk framing. "
            "Post-processing only; truth unchanged."
        )

        colA, colB = st.columns([0.55, 0.45], gap="large")
        with colA:
            st.markdown("### Load artifact")
            up_art = st.file_uploader("shams_run_artifact.json", type=["json"], key="deccon_upload")
            art = _load_json_from_upload(up_art)
            if not art:
                art = st.session_state.get("systems_last_solve_artifact") if isinstance(st.session_state.get("systems_last_solve_artifact"), dict) else None
                if not art:
                    art = st.session_state.get("last_point_artifact") if isinstance(st.session_state.get("last_point_artifact"), dict) else None

            if not art:
                st.info("Upload an artifact, or run Point Designer / Systems Mode to populate a last artifact.")
            else:
                dc = art.get("decision_consequences") if isinstance(art, dict) else None
                if not isinstance(dc, dict):
                    st.warning("No decision_consequences found. (Older artifact?)")
                    st.json({"available_keys": sorted(list(art.keys()))[:40]}, expanded=False)
                else:
                    st.markdown(f"**Decision posture:** `{str(dc.get('decision_posture','UNKNOWN'))}`")
                    pr = str(dc.get("primary_risk_driver", "") or "")
                    if pr:
                        st.markdown(f"**Primary risk driver:** `{pr}`")
                    wh = dc.get("worst_hard_margin_frac", None)
                    try:
                        wh_s = f"{float(wh):.3f}" if wh is not None else "-"
                    except Exception:
                        wh_s = "-"
                    st.markdown(f"**Worst hard margin (frac):** {wh_s}")
                    st.caption(str(dc.get("narrative", "") or ""))

        with colB:
            st.markdown("### Posture legend")
            st.markdown("- **PROCEED**: feasible with adequate authority")
            st.markdown("- **PROCEED_TARGETED_RD**: feasible but near-binding and/or authority-limited")
            st.markdown("- **HOLD_FOUNDATIONAL**: hard-infeasible; address dominant limiter")
            st.markdown("- **UNKNOWN**: missing/legacy artifact")

        if isinstance(art, dict) and isinstance(art.get("decision_consequences"), dict):
            dc = art["decision_consequences"]
            rows = [
                {"field": "decision_posture", "value": dc.get("decision_posture")},
                {"field": "primary_risk_driver", "value": dc.get("primary_risk_driver")},
                {"field": "dominant_mechanism", "value": dc.get("dominant_mechanism")},
                {"field": "dominant_constraint", "value": dc.get("dominant_constraint")},
                {"field": "worst_hard_margin_frac", "value": dc.get("worst_hard_margin_frac")},
                {"field": "uncertainty_reduction_axis", "value": dc.get("uncertainty_reduction_axis")},
                {"field": "leverage_knobs", "value": dc.get("leverage_knobs")},
                {"field": "stamp_sha256", "value": dc.get("stamp_sha256")},
            ]
            st.subheader("Snapshot")
            st.table(rows)


# ----------------------------------
# Authority Dominance Engine (v330.0)
# ----------------------------------
if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_authority_dominance:
        st.header("Authority Dominance")
        st.caption(
            "Deterministic dominance engine: identifies the dominant feasibility killer authority "
            "(PLASMA/EXHAUST/MAGNET/CONTROL/NEUTRONICS/FUEL/PLANT) and ranks the top limiting constraints. "
            "Post-processing only; truth unchanged."
        )

        colA, colB = st.columns([0.55, 0.45], gap="large")
        with colA:
            st.markdown("### Load artifact")
            up_art = st.file_uploader("shams_run_artifact.json", type=["json"], key="authdom_upload")
            art = _load_json_from_upload(up_art)
            if not art:
                art = st.session_state.get("systems_last_solve_artifact") if isinstance(st.session_state.get("systems_last_solve_artifact"), dict) else None
                if not art:
                    art = st.session_state.get("last_point_artifact") if isinstance(st.session_state.get("last_point_artifact"), dict) else None

            if not art:
                st.info("Upload an artifact, or run Point Designer / Systems Mode to populate a last artifact.")
            else:
                ad = art.get("authority_dominance") if isinstance(art, dict) else None
                if not isinstance(ad, dict):
                    st.warning("No authority_dominance found. (Older artifact?)")
                    st.json({"available_keys": sorted(list(art.keys()))[:60]}, expanded=False)
                else:
                    st.markdown(f"**Dominance verdict:** `{str(ad.get('dominance_verdict','UNKNOWN'))}`")
                    st.markdown(f"**Dominant authority:** `{str(ad.get('dominant_authority',''))}`")
                    st.markdown(f"**Dominant constraint:** `{str(ad.get('dominant_constraint',''))}`")
                    mm = ad.get("dominant_margin_frac", None)
                    try:
                        mm_s = f"{float(mm):.4f}" if mm is not None else "-"
                    except Exception:
                        mm_s = "-"
                    st.markdown(f"**Dominant margin (frac):** {mm_s}")
                    st.caption(f"stamp_sha256: {str(ad.get('stamp_sha256',''))[:16]}â€¦")

        with colB:
            st.markdown("### Interpretation")
            st.markdown("- **INFEASIBLE**: at least one hard constraint violated; dominance points to the worst hard margin.")
            st.markdown("- **FRAGILE**: hard-feasible but the tightest hard margin is near-binding (default < 0.05).")
            st.markdown("- **FEASIBLE**: hard-feasible with comfortable margins.")

        if isinstance(art, dict) and isinstance(art.get("authority_dominance"), dict):
            ad = art["authority_dominance"]
            with st.expander("Top limiting constraints (hard)", expanded=False):
                rows = ad.get("dominance_topk") or []
                if isinstance(rows, list) and rows:
                    st.dataframe(pd.DataFrame(rows), use_container_width=True, hide_index=True)
                else:
                    st.info("No top-k rows available.")

            with st.expander("Authority ranking", expanded=False):
                rows = ad.get("authority_ranked") or []
                if isinstance(rows, list) and rows:
                    st.dataframe(pd.DataFrame(rows), use_container_width=True, hide_index=True)
                else:
                    st.info("No authority ranking available.")


# -----------------------------
# Scenario Delta Viewer (new)
# -----------------------------

if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_epoch_feas:
        st.header("Epoch Feasibility")
        st.caption(
            "Lifecycle-epoch feasibility (Startup / Nominal / End-of-Life). "
            "Constitutional reclassification only; no re-solving and no truth modification."
        )

        colA, colB = st.columns([0.55, 0.45], gap="large")
        with colA:
            st.markdown("### Load artifact")
            up_art = st.file_uploader("shams_run_artifact.json", type=["json"], key="epochfeas_upload")
            art = _load_json_from_upload(up_art)
            if not art:
                art = st.session_state.get("systems_last_solve_artifact") if isinstance(st.session_state.get("systems_last_solve_artifact"), dict) else None
                if not art:
                    art = st.session_state.get("last_point_artifact") if isinstance(st.session_state.get("last_point_artifact"), dict) else None

            if not art:
                st.info("Upload an artifact, or run Systems Mode to populate a last artifact.")
            else:
                ef = art.get("epoch_feasibility") if isinstance(art, dict) else None
                if not isinstance(ef, dict):
                    st.warning("No epoch_feasibility found. (Older artifact?)")
                    st.json({"available_keys": sorted(list(art.keys()))[:40]}, expanded=False)
                else:
                    st.markdown(f"**Overall:** `{str(ef.get('overall','UNKNOWN'))}`")
                    epochs = ef.get("epochs") or []
                    rows = []
                    for e in epochs:
                        if not isinstance(e, dict):
                            continue
                        wh = e.get("worst_hard_margin_frac", None)
                        try:
                            wh_s = f"{float(wh):.3f}" if wh is not None else "-"
                        except Exception:
                            wh_s = "-"
                        rows.append({
                            "epoch": str(e.get("epoch","")),
                            "verdict": str(e.get("verdict","")),
                            "dominant_mechanism": str(e.get("dominant_mechanism","")),
                            "dominant_constraint": str(e.get("dominant_constraint","")),
                            "worst_hard_margin": wh_s,
                            "n_blocking": len(list(e.get("blocking") or [])),
                            "n_diag": len(list(e.get("diagnostic") or [])),
                        })
                    if rows:
                        st.dataframe(rows, use_container_width=True, hide_index=True)
                    else:
                        st.warning("Epoch list empty.")
        with colB:
            st.markdown("### Constitution (selected epoch)")
            if not art or not isinstance(art, dict) or not isinstance(art.get("epoch_feasibility"), dict):
                st.caption("Load an artifact to view epoch constitutions.")
            else:
                ef = art.get("epoch_feasibility") or {}
                epochs = ef.get("epochs") or []
                labels = [str(e.get("epoch","")) for e in epochs if isinstance(e, dict)]
                sel = st.selectbox("Epoch", labels, index=0 if labels else None, key="epochfeas_pick")
                chosen = None
                for e in epochs:
                    if isinstance(e, dict) and str(e.get("epoch","")) == sel:
                        chosen = e
                        break
                if chosen is None:
                    st.info("No epoch selected.")
                else:
                    st.markdown(f"**Epoch:** `{sel}`")
                    st.json(chosen.get("constitution") or {}, expanded=False)
                    st.caption("These clauses reclassify constraint enforcement deterministically across epochs.")

if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_delta:
        st.header("Scenario Delta Viewer")
        st.caption("Compare two run artifacts (baseline vs scenario). Uses embedded scenario_delta when available; otherwise computes a transparent diff.")

        col1, col2 = st.columns(2)
        with col1:
            up_base = st.file_uploader("Baseline shams_run_artifact.json", type=["json"], key="delta_base")
        with col2:
            up_scen = st.file_uploader("Scenario shams_run_artifact.json", type=["json"], key="delta_scen")

        base = _load_json_from_upload(up_base)
        scen = _load_json_from_upload(up_scen)

        if not base or not scen:
            st.info("Upload both baseline and scenario artifacts to view deltas.")
        else:
            st.subheader("Embedded scenario_delta")
            sd = scen.get("scenario_delta")
            if sd:
                st.json(sd)
            else:
                st.info("No embedded scenario_delta found; computing diffs from inputs/outputs.")

            st.subheader("Changed inputs")
            bi = base.get("inputs") or {}
            si = scen.get("inputs") or {}
            changed = []
            for k in sorted(set(bi.keys()) | set(si.keys())):
                if bi.get(k) != si.get(k):
                    changed.append({"field": k, "baseline": bi.get(k), "scenario": si.get(k)})
            if changed:
                st.dataframe(pd.DataFrame(changed), use_container_width=True)
            else:
                st.info("No input differences detected.")

            st.subheader("Numeric output deltas")
            bo = base.get("outputs") or {}
            so = scen.get("outputs") or {}
            df = _numeric_delta_table(bo, so)
            if not df.empty:
                st.dataframe(df, use_container_width=True)
            else:
                st.info("No numeric output differences detected.")



            st.subheader("Structural / schema diff (read-only)")
            st.caption("Reports *structure* changes (constraints added/removed/meta changes, model cards) without numeric tolerances.")

            try:
                from shams_io.structural_diff import structural_diff as _structural_diff
                sd = _structural_diff(new_artifact=scen, old_artifact=base)
            except Exception as e:
                sd = None
                st.error(f"Structural diff failed: {e}")

            if isinstance(sd, dict):
                # Constraints changes
                cchg = (sd.get("constraints") or {})
                added = cchg.get("added") or []
                removed = cchg.get("removed") or []
                changed = cchg.get("changed_meta") or []
                cols = st.columns(3)
                cols[0].metric("constraints added", str(len(added)))
                cols[1].metric("constraints removed", str(len(removed)))
                cols[2].metric("constraints meta changed", str(len(changed)))

                if added:
                    with st.expander("Added constraints", expanded=False):
                        st.write(added)
                if removed:
                    with st.expander("Removed constraints", expanded=False):
                        st.write(removed)
                if changed:
                    with st.expander("Changed constraint metadata", expanded=False):
                        st.dataframe(pd.DataFrame(changed), use_container_width=True, hide_index=True)

                # Model cards changes
                mc = (sd.get("model_cards") or {})
                mc_added = mc.get("added") or []
                mc_removed = mc.get("removed") or []
                mc_changed = mc.get("changed") or []
                cols2 = st.columns(3)
                cols2[0].metric("model cards added", str(len(mc_added)))
                cols2[1].metric("model cards removed", str(len(mc_removed)))
                cols2[2].metric("model cards changed", str(len(mc_changed)))
                if mc_added or mc_removed or mc_changed:
                    with st.expander("Model card diffs", expanded=False):
                        st.json({"added": mc_added, "removed": mc_removed, "changed": mc_changed}, expanded=False)

                with st.expander("Raw structural diff JSON (audit)", expanded=False):
                    st.json(sd, expanded=False)


# -----------------------------
# Run Library (Workspace)
# -----------------------------
if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_library:
        st.header("Run Library")
        st.caption("Browse a workspace directory of SHAMS run/study artifacts (no physics changes; read-only).")

        def _scan_workspace(root: Path):
            runs = []
            studies = []
            if not root.exists():
                return runs, studies

            # Run artifacts
            for p in root.rglob("*.json"):
                if p.name.lower() in {"shams_run_artifact.json"} or p.name.lower().startswith("case_") or p.name.lower().endswith("_artifact.json"):
                    try:
                        art = read_run_artifact(p)
                        k = art.get("kpis", {}) if isinstance(art, dict) else {}
                        prov = art.get("provenance", {}) if isinstance(art, dict) else {}
                        runs.append({
                            "type": "run",
                            "path": str(p),
                            "created_unix": float(art.get("created_unix", prov.get("created_unix", float("nan")))) if isinstance(art, dict) else float("nan"),
                            "hard_ok": bool(k.get("hard_ok", False)),
                            "hard_worst_margin": k.get("hard_worst_margin", None),
                            "Q": k.get("Q_DT_eqv", k.get("Q", None)),
                            "H98": k.get("H98", None),
                            "message": ((art.get("solver") or {}).get("message") if isinstance(art.get("solver"), dict) else ""),
                        })
                    except Exception:
                        continue

            # Study indexes
            for p in root.rglob("index.json"):
                try:
                    data = json.loads(p.read_text(encoding="utf-8"))
                    if isinstance(data, dict) and data.get("schema_version") == "study_index.v1":
                        prov = data.get("provenance", {}) if isinstance(data.get("provenance"), dict) else {}
                        studies.append({
                            "type": "study",
                            "path": str(p),
                            "created_unix": float(data.get("created_unix", prov.get("created_unix", float('nan')))),
                            "n_cases": int(data.get("n_cases", 0)),
                            "elapsed_s": float(data.get("elapsed_s", float('nan'))),
                        })
                except Exception:
                    continue
            return runs, studies

        default_ws = str((Path.cwd()/ "ui_runs").resolve())
        ws = st.text_input("Workspace folder", value=st.session_state.get("ui_workspace", default_ws))
        st.session_state.ui_workspace = ws
        root = Path(ws)

        colA, colB = st.columns([1, 1])
        with colA:
            do_scan = st.button("Scan workspace", use_container_width=True)
        with colB:
            st.write("")
            st.write("")

        if do_scan:
            runs, studies = _scan_workspace(root)
            st.session_state._ws_runs = runs
            st.session_state._ws_studies = studies

        runs = st.session_state.get("_ws_runs", [])
        studies = st.session_state.get("_ws_studies", [])

        st.subheader("Runs")
        if not runs:
            st.info("No run artifacts found yet. Tip: point runs write artifacts under your chosen output directory; studies write case_XXXX.json under the study out folder.")
        else:
            df = pd.DataFrame(runs)
            # Sort: newest first when available
            if "created_unix" in df.columns:
                df = df.sort_values("created_unix", ascending=False, na_position="last")
            st.dataframe(df, use_container_width=True, hide_index=True)

            sel = st.text_input("Select a run artifact path to open", value=st.session_state.get("selected_artifact_path", ""))
            if st.button("Open selected run", use_container_width=True):
                p = Path(sel)
                if p.exists():
                    try:
                        art = read_run_artifact(p)
                        st.session_state.selected_artifact = art
                        st.session_state.selected_artifact_path = str(p)
                        st.success("Loaded run artifact into session.")
                    except Exception as e:
                        st.error(f"Failed to read artifact: {e}")
                else:
                    st.error("Path does not exist.")

        st.subheader("Studies")
        if studies:
            st.dataframe(pd.DataFrame(studies).sort_values("created_unix", ascending=False, na_position="last"), use_container_width=True, hide_index=True)
            ssel = st.text_input("Select a study index.json path to open", value=st.session_state.get("selected_study_index_path", ""))
            if st.button("Open selected study", use_container_width=True):
                p = Path(ssel)
                if p.exists():
                    try:
                        st.session_state.selected_study_index_path = str(p)
                        st.session_state.selected_study_index = json.loads(p.read_text(encoding="utf-8"))
                        st.success("Loaded study index into session.")
                    except Exception as e:
                        st.error(f"Failed to read study index: {e}")
                else:
                    st.error("Path does not exist.")
        else:
            st.caption("No study indexes found in this workspace.")

# -----------------------------
# Constraint Cockpit
# -----------------------------
if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_constraints:
        st.header("Constraint Cockpit")
        st.caption("Interactively triage constraints using the embedded constraint ledger (read-only).")

        art = st.session_state.get("selected_artifact")
        if not isinstance(art, dict):
            st.info("Load a run artifact first (Run Library or Artifacts Explorer).")
        else:
            ledger = art.get("constraint_ledger", {})
            entries = ledger.get("entries", []) if isinstance(ledger, dict) else []
            if not entries:
                st.warning("This artifact has no constraint ledger. (It should be present in v39+ artifacts.)")
            else:
                df = pd.DataFrame(entries)
                # Basic filters
                c1, c2, c3 = st.columns([1,1,1])
                with c1:
                    sev = st.multiselect("Severity", sorted(df.get("severity", pd.Series(["hard"])).dropna().unique().tolist()), default=["hard","soft"] if "soft" in df.get("severity", pd.Series([])).unique() else ["hard"])
                with c2:
                    grp = st.multiselect("Group", sorted(df.get("group", pd.Series(["general"])).dropna().unique().tolist()), default=[])
                with c3:
                    show_only_failed = st.checkbox("Only failed constraints", value=True)

                view = df.copy()
                if sev:
                    view = view[view["severity"].isin(sev)]
                if grp:
                    view = view[view["group"].isin(grp)]
                if show_only_failed and "passed" in view.columns:
                    view = view[view["passed"] == False]

                # Sort: worst first by margin_frac or margin
                if "margin_frac" in view.columns:
                    view = view.sort_values("margin_frac", ascending=True, na_position="last")
                elif "margin" in view.columns:
                    view = view.sort_values("margin", ascending=True, na_position="last")

                st.subheader("Ledger")
                st.dataframe(view, use_container_width=True, hide_index=True)

                st.subheader("Top blockers")
                top = ledger.get("top_blockers", []) if isinstance(ledger, dict) else []
                if top:
                    st.dataframe(pd.DataFrame(top), use_container_width=True, hide_index=True)
                fp = ledger.get("ledger_fingerprint_sha256")
                if fp:
                    st.caption(f"Ledger fingerprint: `{fp}`")


# -----------------------------
# Constraint Inspector (read-only)
# -----------------------------
if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_constraint_inspector:
        st.header("Constraint Inspector")
        st.caption("Read-only, equation-first inspection of a single constraint: raw inequality, margin, meaning, knobs, and provenance (when available).")

        art = st.session_state.get("selected_artifact")
        if not isinstance(art, dict):
            st.info("Load a run artifact first (Run Library or Artifacts Explorer).")
        else:
            constraints_list = art.get("constraints") or []
            # Build a name -> constraint dict map (best-effort)
            name_to_c = {}
            for c in constraints_list:
                if isinstance(c, dict) and c.get("name"):
                    name_to_c[str(c.get("name"))] = c

            ledger = art.get("constraint_ledger", {})
            entries = ledger.get("entries", []) if isinstance(ledger, dict) else []
            names = []
            # Prefer ledger order if present (it should reflect evaluation order)
            if entries:
                for e in entries:
                    n = str(e.get("name"))
                    if n and n not in names:
                        names.append(n)
            else:
                names = sorted(list(name_to_c.keys()))

            if not names:
                st.warning("No constraints found in this artifact.")
            else:
                sel = st.selectbox("Select constraint", names, index=0, key="constraint_inspector_select")

                # Pull both ledger entry (if present) and raw constraint dict (if present)
                entry = None
                if entries:
                    for e in entries:
                        if str(e.get("name")) == sel:
                            entry = e
                            break
                c = name_to_c.get(sel, {}) if isinstance(name_to_c.get(sel, {}), dict) else {}

                # Compose a canonical view (prefer ledger fields where available)
                view = {}
                for src in (c, entry or {}):
                    if isinstance(src, dict):
                        view.update({k: src.get(k) for k in src.keys()})

                # Core inequality (verbatim fields; no inferred math)
                sense = str(view.get("sense") or "")
                value = view.get("value")
                limit = view.get("limit")
                units = str(view.get("units") or "")
                meaning = str(view.get("meaning") or view.get("note") or "")

                st.subheader("Inequality")
                if sense and value is not None and limit is not None:
                    st.code(f"{sel}: value {sense} limit    (value={value}, limit={limit}, units={units})", language="text")
                else:
                    st.code(f"{sel}: (insufficient fields to render inequality)", language="text")

                # Pass/fail + margins
                cols = st.columns(4)
                cols[0].metric("passed", str(bool(view.get("passed", False))))
                if view.get("severity") is not None:
                    cols[1].metric("severity", str(view.get("severity")))
                if view.get("group") is not None:
                    cols[2].metric("group", str(view.get("group")))
                if view.get("dominance_rank") is not None:
                    cols[3].metric("dominance_rank", str(view.get("dominance_rank")))

                c1, c2, c3 = st.columns(3)
                if view.get("margin") is not None:
                    c1.metric("margin", f"{view.get('margin')}")
                if view.get("margin_frac") is not None:
                    c2.metric("margin_frac", f"{view.get('margin_frac')}")
                if view.get("violation_score") is not None:
                    c3.metric("violation_score", f"{view.get('violation_score')}")

                st.subheader("Meaning / proxy")
                if meaning.strip():
                    st.write(meaning)
                else:
                    st.info("No meaning/proxy text is attached to this constraint.")

                # Knobs + dominant inputs
                st.subheader("Knobs / dominant inputs (if present)")
                bb = view.get("best_knobs")
                di = view.get("dominant_inputs")
                kcol1, kcol2 = st.columns(2)
                with kcol1:
                    if bb:
                        st.write("**best_knobs**")
                        st.write(bb)
                    else:
                        st.caption("best_knobs: (none)")
                with kcol2:
                    if di:
                        st.write("**dominant_inputs**")
                        st.write(di)
                    else:
                        st.caption("dominant_inputs: (none)")

                # Provenance (constraint-level and artifact-level)
                st.subheader("Provenance (if present)")
                prov = {}
                if isinstance(view.get("provenance"), dict):
                    prov["constraint"] = view.get("provenance")
                if isinstance(art.get("provenance"), dict):
                    prov["artifact"] = art.get("provenance")
                if prov:
                    st.json(prov, expanded=False)
                else:
                    st.info("No provenance keys present on this constraint (artifact-level provenance may still exist under artifact.provenance).")

                # Raw views for auditability
                with st.expander("Raw JSON (audit)", expanded=False):
                    if isinstance(entry, dict):
                        st.write("**constraint_ledger entry**")
                        st.json(entry, expanded=False)
                    if isinstance(c, dict) and c:
                        st.write("**constraints[] item**")
                        st.json(c, expanded=False)


# -----------------------------
# Sensitivity Explorer
# -----------------------------
if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_sensitivity:
        st.header("Sensitivity Explorer")
        st.caption("Local finite-difference sensitivities around the current point (no model changes).")

        art = st.session_state.get("selected_artifact")
        if not isinstance(art, dict):
            st.info("Load a run artifact first (Run Library or Artifacts Explorer).")
        else:
            inp_d = art.get("inputs", {})
            if not isinstance(inp_d, dict):
                st.error("Artifact inputs missing or invalid.")
            else:
                try:
                    base = PointInputs.from_dict(inp_d)
                except Exception:
                    # Fallback: try direct constructor with expected keys
                    try:
                        base = PointInputs(**{k: inp_d[k] for k in PointInputs.__dataclass_fields__.keys() if k in inp_d})
                    except Exception as e:
                        st.error(f"Could not build PointInputs from artifact inputs: {e}")
                        base = None

                if base is not None:
                    st.subheader("Base point")
                    st.json(base.__dict__)

                    # Choose knobs + outputs
                    knob_defaults = ["Ip_MA", "fG", "Bt_T", "R0_m", "a_m", "kappa", "Paux_MW", "Ti_keV", "Te_keV"]
                    available_knobs = [k for k in knob_defaults if k in base.__dict__]
                    knobs = st.multiselect("Knobs", available_knobs, default=["Ip_MA", "fG"], key="sens_knobs_v294")

                    outputs_default = [
                        "Q_DT_eqv", "H98", "P_fus_total_MW", "Palpha_MW", "beta_N", "nbar20", "P_e_net_MW",
                        "B_peak_T", "q95", "TBR",
                    ]
                    outputs = st.multiselect("Outputs", outputs_default, default=["Q_DT_eqv", "H98"], key="sens_outs_v294")

                    step_rel = st.number_input("Step size (relative)", value=1e-3, min_value=1e-6, format="%.6f", key="sens_step_rel_v294")

                    if st.button("Compute deterministic sensitivity pack", use_container_width=True, key="sens_btn_v294"):
                        try:
                            from analysis.sensitivity import deterministic_sensitivity_pack
                            # Characteristic scales for variables when x0 == 0
                            scales = {k: 1.0 for k in knobs}
                            scales.update({"Paux_MW": 10.0, "Ip_MA": 1.0, "fG": 0.1, "Bt_T": 0.5, "R0_m": 0.5, "a_m": 0.2})
                            pack = deterministic_sensitivity_pack(base, variables={k: scales.get(k, 1.0) for k in knobs}, outputs=list(outputs), step_rel=float(step_rel))

                            # Flatten for table
                            rows = []
                            jac = pack.get("jacobian", {}) if isinstance(pack, dict) else {}
                            for o in outputs:
                                for p in knobs:
                                    try:
                                        v = float((jac.get(o) or {}).get(p))
                                    except Exception:
                                        v = float('nan')
                                    rows.append({"output": o, "knob": p, "d(output)/d(knob)": v})
                            st.dataframe(pd.DataFrame(rows), use_container_width=True, hide_index=True)

                            st.subheader("Constraint tightness (top residuals)")
                            st.dataframe(pd.DataFrame(pack.get("constraints_tightness", [])), use_container_width=True, hide_index=True)

                            with st.expander("Raw JSON (audit)", expanded=False):
                                st.json(pack)
                        except Exception as e:
                            st.error(f"Sensitivity computation failed: {e}")

# -----------------------------
# Feasibility Map Viewer
# -----------------------------
if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_feasmap:
        st.header("Feasibility Map")
        st.caption("Visualize feasibility from study sweeps (heatmap).")

        # Load study index either from session (Run Library) or by path
        p_default = st.session_state.get("selected_study_index_path", "")
        p = st.text_input("Study index.json path", value=p_default)
        idx_data = None
        if p and Path(p).exists():
            try:
                idx_data = json.loads(Path(p).read_text(encoding="utf-8"))
            except Exception as e:
                st.error(f"Could not read study index: {e}")

        if not isinstance(idx_data, dict) or idx_data.get("schema_version") != "study_index.v1":
            st.info("Provide a valid study_out/index.json (schema study_index.v1).")
        else:
            cases = idx_data.get("cases", [])
            study = idx_data.get("study", {})
            sweeps = (study.get("sweeps") if isinstance(study, dict) else None) or []
            # Determine candidate in_ variables for axes
            in_cols = []
            if cases and isinstance(cases, list) and isinstance(cases[0], dict):
                for k in cases[0].keys():
                    if k.startswith("in_"):
                        in_cols.append(k)
            # Prefer sweep variables
            sweep_vars = ["in_"+str(s.get("name")) for s in sweeps if isinstance(s, dict) and s.get("name") is not None]
            axis_candidates = [c for c in sweep_vars if c in in_cols] + [c for c in in_cols if c not in sweep_vars]
            if len(axis_candidates) < 2:
                st.warning("Need at least two swept input variables (in_*) to plot a 2D feasibility map.")
            else:
                c1, c2 = st.columns([1,1])
                with c1:
                    xcol = st.selectbox("X axis", axis_candidates, index=0)
                with c2:
                    ycol = st.selectbox("Y axis", axis_candidates, index=1 if len(axis_candidates)>1 else 0)

                df = pd.DataFrame(cases)
                if "ok" not in df.columns:
                    st.error("Study cases table missing 'ok' field.")
                else:
                    # Build pivot grid
                    xs = sorted(df[xcol].dropna().unique().tolist())
                    ys = sorted(df[ycol].dropna().unique().tolist())
                    import numpy as np
                    grid = np.full((len(ys), len(xs)), np.nan)
                    for _, r in df.iterrows():
                        try:
                            xi = xs.index(r[xcol])
                            yi = ys.index(r[ycol])
                            grid[yi, xi] = 1.0 if bool(r["ok"]) else 0.0
                        except Exception:
                            continue

                    st.subheader("Feasibility heatmap (1=feasible, 0=infeasible)")
                    try:
                        import matplotlib.pyplot as plt  # type: ignore
                        fig, ax = plt.subplots()
                        im = ax.imshow(grid, origin="lower", aspect="auto")
                        ax.set_xticks(range(len(xs)))
                        ax.set_yticks(range(len(ys)))
                        ax.set_xticklabels([str(x) for x in xs], rotation=45, ha="right")
                        ax.set_yticklabels([str(y) for y in ys])
                        ax.set_xlabel(xcol)
                        ax.set_ylabel(ycol)
                        st.pyplot(fig, clear_figure=True)
                    except Exception as e:
                        st.error(f"Plot failed: {e}")

                    st.subheader("Pick a case to open")
                    selx = st.selectbox("X value", xs, index=0)
                    sely = st.selectbox("Y value", ys, index=0)
                    sub = df[(df[xcol]==selx) & (df[ycol]==sely)]
                    if sub.empty:
                        st.info("No case for that cell.")
                    else:
                        st.dataframe(sub[["case","ok","iters","message","path"] + [xcol,ycol]], use_container_width=True, hide_index=True)
                        if st.button("Load this case artifact", use_container_width=True):
                            path = str(sub.iloc[0]["path"])
                            try:
                                art = read_run_artifact(Path(path))
                                st.session_state.selected_artifact = art
                                st.session_state.selected_artifact_path = path
                                st.success("Loaded case artifact into session.")
                            except Exception as e:
                                st.error(f"Could not load case artifact: {e}")


# -----------------------------
# UI Upgrade Pack v53 (UI-only): decision/provenance/knobs/regression/study dashboard/maturity/assumptions/export/solver introspection
# -----------------------------
def _get_active_artifact(label: str = "Use loaded artifact in session") -> dict | None:
    "Return the currently active artifact (from session_state or upload)."
    art = st.session_state.get("selected_artifact")
    if isinstance(art, dict) and art:
        st.info("Using artifact loaded into session (Run Library / Feasibility Map).")
        return art
    up = st.file_uploader("Upload shams_run_artifact.json", type=["json"], key=f"active_artifact_upload_{label}")
    return _load_json_from_upload(up)

def _guess_point_inputs_from_artifact(art: dict) -> PointInputs | None:
    "Best-effort extraction of PointInputs from an artifact. Falls back safely."
    if not isinstance(art, dict):
        return None
    cand = {}
    for k in ["inputs", "point", "point_inputs", "design_point", "config", "run_config", "resolved_config"]:
        v = art.get(k)
        if isinstance(v, dict):
            cand.update(v)
    cand.update({k: art.get(k) for k in ["R0_m","a_m","kappa","Bt_T","B0_T","Ip_MA","Ti_keV","fG","Paux_MW","Ti_over_Te","fuel_mode"] if k in art})
    if "B0_T" in cand and "Bt_T" not in cand:
        cand["Bt_T"] = cand["B0_T"]
    if "Ti_Te" in cand and "Ti_over_Te" not in cand:
        cand["Ti_over_Te"] = cand["Ti_Te"]
    if "Ti/Te" in cand and "Ti_over_Te" not in cand:
        cand["Ti_over_Te"] = cand["Ti/Te"]
    try:
        return _make_point_inputs_safe(**cand)
    except Exception:
        return None

def _decision_summary_from_artifact(art: dict) -> dict:
    kpis = art.get("kpis", {}) if isinstance(art.get("kpis"), dict) else {}
    cons = art.get("constraints", []) if isinstance(art.get("constraints"), list) else []
    ledger = art.get("constraint_ledger", {}) if isinstance(art.get("constraint_ledger"), dict) else {}
    feas = bool(art.get("is_feasible")) if "is_feasible" in art else None
    if feas is None:
        feas = all((not bool(c.get("failed"))) for c in cons) if cons else None
    top = ledger.get("top_blockers") if isinstance(ledger.get("top_blockers"), list) else []
    if not top and cons:
        failed = [c for c in cons if c.get("failed")]
        failed = failed[:8]
        top = [{"name": c.get("name"), "group": c.get("group"), "margin": c.get("margin"), "severity": c.get("severity")} for c in failed]
    return {"feasible": feas, "kpis": kpis, "top_blockers": top, "ledger": ledger, "constraints": cons}

def _download_json_button(label: str, data: dict, fname: str, key: str):
    try:
        st.download_button(label, data=json.dumps(data, indent=2, ensure_ascii=False).encode("utf-8"),
                           file_name=fname, mime="application/json", key=key)
    except Exception as e:
        st.warning(f"Download not available: {e}")

if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_decision:
        st.header("Decision Front Page Builder")
        st.caption("UI-native reconstruction of the decision-grade front-page summary from a run artifact (no physics changes).")

        art = _get_active_artifact("decision")
        if not art:
            st.info("Load an artifact to build the decision summary.")
        else:
            d = _decision_summary_from_artifact(art)
            c1, c2, c3 = st.columns([1,1,1])
            with c1:
                st.metric("Feasibility verdict", "FEASIBLE " if d["feasible"] else ("INFEASIBLE " if d["feasible"] is not None else "UNKNOWN"))
            with c2:
                st.metric("Top KPI: Q", f"{d['kpis'].get('Q_DT_eqv', d['kpis'].get('Q', '-'))}")
            with c3:
                st.metric("Top KPI: Pfus (MW)", f"{d['kpis'].get('P_fus_MW', d['kpis'].get('Pfus_MW', '-'))}")

            st.subheader("Dominant blockers")
            if d["top_blockers"]:
                st.dataframe(_safe_df(d["top_blockers"]), use_container_width=True, hide_index=True)
            else:
                st.write("No blockers found in artifact.")

            with st.expander("Full decision inputs (provenance + schema versions)"):
                prov = art.get("provenance", {}) if isinstance(art.get("provenance"), dict) else {}
                st.json({
                    "schema_version": art.get("schema_version"),
                    "repo_version": prov.get("repo_version"),
                    "git_commit": prov.get("git_commit"),
                    "python": prov.get("python"),
                    "platform": prov.get("platform"),
                    "created_unix": prov.get("created_unix"),
                })

            _download_json_button("Download decision summary JSON", d, "decision_summary.json", "dl_decision_summary")


if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_nonfeas:
        st.header("Guided Non-Feasibility Mode")
        st.caption("Turn infeasible outcomes into a structured, auditable recovery workflow (UI-only; no physics changes).")

        art = _get_active_artifact("nonfeas")
        if not art:
            st.info("Load an artifact to guide a non-feasibility recovery path.")
        else:
            cons = art.get("constraints", []) if isinstance(art.get("constraints"), list) else []
            kpis = art.get("kpis", {}) if isinstance(art.get("kpis"), dict) else {}

            # Determine hard feasibility
            feasible_hard = None
            if "feasible_hard" in kpis:
                try:
                    feasible_hard = bool(kpis.get("feasible_hard"))
                except Exception:
                    feasible_hard = None
            if feasible_hard is None and cons:
                try:
                    feasible_hard = all(
                        bool(c.get("passed", True))
                        for c in cons
                        if str(c.get("severity", "hard")).lower() == "hard"
                    )
                except Exception:
                    feasible_hard = None

            if feasible_hard is True:
                st.success("This run is hard-feasible. Guided non-feasibility mode is not needed.")
            else:
                # Get or construct a non-feasibility certificate
                cert = art.get("nonfeasibility_certificate") if isinstance(art.get("nonfeasibility_certificate"), dict) else None
                if not cert:
                    hard_failed = [
                        c for c in cons
                        if str(c.get("severity", "hard")).lower() == "hard" and not bool(c.get("passed", True))
                    ]

                    def _mkey(c):
                        try:
                            return float(c.get("margin", 0.0))
                        except Exception:
                            return 0.0

                    hard_failed.sort(key=_mkey)
                    cert = {
                        "hard_feasible": False,
                        "dominant_blockers": [{
                            "name": c.get("name", ""),
                            "group": c.get("group", ""),
                            "value": c.get("value"),
                            "limit": c.get("limit"),
                            "sense": c.get("sense"),
                            "margin": c.get("margin"),
                            "meaning": c.get("meaning", ""),
                            "best_knobs": c.get("best_knobs", []),
                            "maturity": c.get("maturity"),
                            "provenance": c.get("provenance"),
                        } for c in hard_failed[:10]],
                        "recommendation": "Move the listed best_knobs (and/or relax assumptions) until all hard constraints pass.",
                    }

                st.subheader("Non-Feasibility Certificate")
                st.json(cert)

                t1, t2, t3 = st.tabs(["1) Diagnose", "2) Minimal relaxations", "3) Create a scenario (deck)"])

                with t1:
                    st.markdown("### Dominant hard blockers (ranked)")
                    blockers = cert.get("dominant_blockers", []) if isinstance(cert.get("dominant_blockers"), list) else []
                    if blockers:
                        bdf = _safe_df(blockers)
                        pref = [c for c in ["group", "name", "margin", "value", "limit", "sense", "meaning", "best_knobs", "maturity"] if c in bdf.columns]
                        st.dataframe(bdf[pref] if pref else bdf, use_container_width=True, hide_index=True)
                    else:
                        st.warning("No dominant blockers found in certificate.")

                    # Solver hints (if present)
                    out = art.get("outputs", {}) if isinstance(art.get("outputs"), dict) else {}
                    solver = out.get("_solver") if isinstance(out.get("_solver"), dict) else art.get("solver")
                    if isinstance(solver, dict) and solver:
                        st.markdown("### Solver hints (from artifact)")
                        show = {k: solver.get(k) for k in ["status", "reason", "clamped", "clamped_on", "residuals", "ui_log"] if k in solver}
                        st.json(show or solver)

                    st.markdown("### Action principle")
                    st.write(
                        "Fix **hard** blockers first. Soft constraints are advisory unless your decision policy says otherwise. "
                        "Use the knob suggestions as **directional guidance** (not optimization)."
                    )

                with t2:
                    st.markdown("### Propose a nearest-feasible adjustment (within UI)")
                    base = _guess_point_inputs_from_artifact(art)
                    if base is None:
                        base = st.session_state.get("last_point_inp")

                    if base is None:
                        st.warning("Could not infer PointInputs from artifact. Run Point Designer once or ensure artifact includes inputs.")
                    else:
                        st.caption("Choose a dominant blocker, then adjust one or more knobs and re-evaluate.")
                        blockers = cert.get("dominant_blockers", []) if isinstance(cert.get("dominant_blockers"), list) else []
                        if blockers:
                            labels = []
                            for i, b in enumerate(blockers):
                                nm = b.get("name", "") or f"blocker_{i}"
                                mg = b.get("margin")
                                labels.append(f"{i:02d} - {nm} (margin={mg})")
                            bi = st.selectbox("Select blocker", options=list(range(len(blockers))), format_func=lambda i: labels[i], key="nf_blocker_sel")
                            b = blockers[int(bi)]
                            st.markdown("**Suggested knobs (directional):**")
                            st.write(b.get("best_knobs", []) or ["(none provided)"])
                            st.markdown("**Meaning:**")
                            st.write(b.get("meaning", "(no meaning field)"))

                        knob_fields = ["Ip_MA", "fG", "Bt_T", "R0_m", "a_m", "kappa", "Ti_keV", "Paux_MW", "Ti_over_Te"]
                        colA, colB = st.columns([2, 1])
                        with colA:
                            sel_knobs = st.multiselect("Knobs to adjust", options=knob_fields, default=["Ip_MA"], key="nf_knobs")
                        with colB:
                            mode = st.selectbox("Adjustment mode", options=["percent", "absolute"], index=0, key="nf_adj_mode")

                        deltas = {}
                        for k in sel_knobs:
                            v0 = float(getattr(base, k))
                            if mode == "percent":
                                d = st.slider(f"{k} Î” (%)", -50.0, 50.0, 5.0, step=0.5, key=f"nf_d_{k}")
                                deltas[k] = v0 * (1.0 + d / 100.0)
                            else:
                                step = 0.1 if abs < 10 else 1.0
                                d = st.number_input(f"{k} new value", value=v0, step=step, key=f"nf_abs_{k}")
                                deltas[k] = float(d)

                        fuel_mode = st.selectbox("fuel_mode", options=["DT", "DD"], index=0 if getattr(base, "fuel_mode", "DT") == "DT" else 1, key="nf_fuel_mode")

                        run = st.button("Re-evaluate adjusted point", key="nf_run_eval", use_container_width=True)
                        if run:
                            try:
                                d = base.__dict__.copy()
                                d.update({k: float(v) for k, v in deltas.items()})
                                d["fuel_mode"] = str(fuel_mode)
                                pi = PointInputs(**d)

                                out2 = hot_ion_point(pi, Paux_for_Q_MW=float(getattr(pi, "Paux_MW", 0.0)))
                                cons2 = evaluate_constraints(out2)
                                art2 = build_run_artifact(
                                    inputs=dict(pi.__dict__),
                                    outputs=dict(out2),
                                    constraints=cons2,
                                    meta={"mode": "guided_nonfeas"},
                                    baseline_inputs=dict(base.__dict__),
                                )
                                st.session_state["nf_last_artifact"] = art2
                                k2 = art2.get("kpis", {}) if isinstance(art2.get("kpis"), dict) else {}
                                st.success(f"Re-evaluated. feasible_hard={k2.get('feasible_hard')}")

                                led = art2.get("constraint_ledger", {}) if isinstance(art2.get("constraint_ledger"), dict) else {}
                                tb = led.get("top_blockers") if isinstance(led.get("top_blockers"), list) else []
                                if tb:
                                    st.subheader("New top blockers")
                                    st.dataframe(_safe_df(tb), use_container_width=True, hide_index=True)

                                with st.expander("New run artifact (raw)"):
                                    st.json(art2)

                                _download_json_button("Download adjusted run artifact", art2, "shams_run_artifact_adjusted.json", "dl_nf_adjusted_artifact")
                            except Exception as e:
                                st.error(f"Re-evaluation failed: {type(e).__name__}: {e}")

                with t3:
                    st.markdown("### Create a scenario deck for reproducible follow-up")
                    base = _guess_point_inputs_from_artifact(art) or st.session_state.get("last_point_inp")
                    last = st.session_state.get("nf_last_artifact")
                    if not isinstance(last, dict):
                        st.info("First run an adjustment in 'Minimal relaxations' to generate a proposed follow-up scenario.")
                    else:
                        try:
                            import yaml  # type: ignore
                        except Exception:
                            yaml = None  # type: ignore

                        new_inputs = last.get("inputs") if isinstance(last.get("inputs"), dict) else {}
                        base_inputs = dict(base.__dict__) if base is not None else (art.get("inputs") if isinstance(art.get("inputs"), dict) else {})

                        delta = {}
                        for k, v in new_inputs.items():
                            if k in base_inputs and base_inputs.get(k) != v:
                                delta[k] = {"from": base_inputs.get(k), "to": v}

                        st.subheader("Scenario delta (inputs)")
                        st.json(delta if delta else {"note": "No input delta detected."})

                        case_deck = {
                            "schema_version": "case_deck.v1",
                            "name": "guided_nonfeas_followup",
                            "base": {},
                            "point": new_inputs,
                            "notes": {
                                "generated_by": "Guided Non-Feasibility Mode",
                                "source_artifact_schema": art.get("schema_version"),
                            },
                        }

                        deck_txt = yaml.safe_dump(case_deck, sort_keys=False) if yaml is not None else json.dumps(case_deck, indent=2)

                        st.markdown("### Case deck")
                        st.code(deck_txt, language="yaml" if yaml is not None else "json")

                        st.download_button(
                            "Download case_deck.yaml",
                            data=deck_txt.encode("utf-8"),
                            file_name="case_deck.yaml",
                            mime="text/yaml" if yaml is not None else "application/json",
                            use_container_width=True,
                        )
                        st.download_button(
                            "Download scenario_delta.json",
                            data=json.dumps(delta, indent=2).encode("utf-8"),
                            file_name="scenario_delta.json",
                            mime="application/json",
                            use_container_width=True,
                        )


if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_cprov:
        st.header("Constraint Provenance Drill-Down")
        st.caption("Click into constraints to see definition fields, fingerprints, and maturity/provenance metadata embedded in the artifact.")

        art = _get_active_artifact("cprov")
        if not art:
            st.info("Load an artifact to inspect constraint provenance.")
        else:
            cons = art.get("constraints", [])
            if not isinstance(cons, list) or not cons:
                st.warning("No 'constraints' list found in artifact.")
            else:
                df = _safe_df(cons)
                pref_cols = [c for c in ["group","name","failed","soft_failed","severity","value","limit","margin","margin_frac","units","fingerprint","provenance_fingerprint","maturity"] if c in df.columns]
                st.dataframe(df[pref_cols] if pref_cols else df, use_container_width=True, hide_index=True)

                names = []
                for i,c in enumerate(cons):
                    n = c.get("name") or c.get("id") or f"constraint_{i}"
                    names.append(f"{i:03d} - {n}")
                sel = st.selectbox("Select constraint", options=list(range(len(cons))), format_func=lambda i: names[i], key="cprov_sel")
                c = cons[int(sel)]
                st.subheader("Selected constraint (raw)")
                st.json(c)
                if isinstance(c, dict):
                    st.markdown("**Fingerprint fields**")
                    st.code("\n".join([f"{k}: {c.get(k)}" for k in ["fingerprint","provenance_fingerprint","constraint_fingerprint_sha256"] if k in c] or ["(none found)"]))

if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_knobs:
        st.header("Knob Trade-Space Explorer")
        st.caption("Explore a 2-knob trade-space by evaluating a small grid around the active point (no optimization; feasibility-first).")

        art = _get_active_artifact("knobs")
        base = _guess_point_inputs_from_artifact(art) if art else None
        if base is None:
            base = st.session_state.get("last_point_inp")

        if base is None:
            st.info("Load an artifact (or run Point Designer) to initialize a base point.")
        else:
            st.markdown("**Base point (editable)**")
            col1, col2, col3 = st.columns(3)
            with col1:
                R0_m = st.number_input("R0 (m)", value=float(base.R0_m), step=0.01, key="knob_R0")
                a_m = st.number_input("a (m)", value=float(base.a_m), step=0.01, key="knob_a")
                kappa = st.number_input("kappa", value=float(base.kappa), step=0.05, key="knob_kappa")
            with col2:
                Bt_T = st.number_input("Bt (T)", value=float(base.Bt_T), step=0.1, key="knob_Bt")
                Ip_MA = st.number_input("Ip (MA)", value=float(base.Ip_MA), step=0.1, key="knob_Ip")
                fG = st.number_input("fG", value=float(base.fG), step=0.01, key="knob_fG")
            with col3:
                Ti_keV = st.number_input("Ti (keV)", value=float(base.Ti_keV), step=0.5, key="knob_Ti")
                Paux_MW = st.number_input("Paux (MW)", value=float(base.Paux_MW), step=1.0, key="knob_Paux")
                Ti_over_Te = st.number_input("Ti/Te", value=float(getattr(base, "Ti_over_Te", 2.0)), step=0.1, key="knob_TiTe")

            fuel_mode = st.selectbox("fuel_mode", options=["DT","DD"], index=0 if getattr(base, "fuel_mode", "DT")=="DT" else 1, key="knob_fuel")

            knobs = ["Ip_MA","fG","Bt_T","R0_m","Paux_MW","Ti_keV"]
            kx = st.selectbox("Knob X", knobs, index=0, key="knob_kx")
            ky = st.selectbox("Knob Y", knobs, index=1, key="knob_ky")

            def _getv(pi: PointInputs, k: str) -> float:
                return float(getattr(pi, k))
            def _setv(pi: PointInputs, k: str, v: float) -> PointInputs:
                d = pi.__dict__.copy()
                d[k]=float(v)
                return PointInputs(**d)

            x0=_getv(base,kx); y0=_getv(base,ky)
            colA,colB=st.columns(2)
            with colA:
                x_span = st.number_input("X span (+/-)", value=0.1*abs(x0) if abs(x0)>0 else 0.1, step=0.01, key="knob_xspan")
            with colB:
                y_span = st.number_input("Y span (+/-)", value=0.1*abs(y0) if abs(y0)>0 else 0.1, step=0.01, key="knob_yspan")
            nx = st.slider("X grid points", 3, 15, 9, key="knob_nx")
            ny = st.slider("Y grid points", 3, 15, 9, key="knob_ny")
            run = st.button("Evaluate grid", key="knob_run", use_container_width=True)

        if run:
                import numpy as np
                xs = np.linspace(x0-x_span, x0+x_span, int(nx))
                ys = np.linspace(y0-y_span, y0+y_span, int(ny))
                rows=[]
                with st.spinner("Evaluating grid..."):
                    for xv in xs:
                        for yv in ys:
                            pi = PointInputs(R0_m=float(R0_m), a_m=float(a_m), kappa=float(kappa),
                                             Bt_T=float(Bt_T), Ip_MA=float(Ip_MA), Ti_keV=float(Ti_keV),
                                             fG=float(fG), Paux_MW=float(Paux_MW), Ti_over_Te=float(Ti_over_Te),
                                             fuel_mode=str(fuel_mode))
                            pi = _setv(pi, kx, float(xv))
                            pi = _setv(pi, ky, float(yv))
                            try:
                                out = hot_ion_point(pi)
                                cons = evaluate_constraints(out, point_inputs=pi)
                                ok = all((not bool(c.get("failed"))) for c in cons)
                                top=None
                                if not ok:
                                    failed=[c for c in cons if c.get("failed")]
                                    if failed:
                                        top=failed[0].get("name")
                                rows.append({kx: float(xv), ky: float(yv), "feasible": bool(ok), "top_blocker": top,
                                             "Q": float(out.get("Q_DT_eqv", out.get("Q", float('nan')))),
                                             "Pfus_MW": float(out.get("P_fus_MW", out.get("Pfus_MW", float('nan'))))})
                            except Exception:
                                rows.append({kx: float(xv), ky: float(yv), "feasible": False, "top_blocker": "eval_error", "Q": float('nan'), "Pfus_MW": float('nan')})
                df=pd.DataFrame(rows, columns=["name","failed_A","failed_B","margin_A","margin_B","margin_delta"])
                st.subheader("Grid results (table)")
                st.dataframe(df, use_container_width=True, hide_index=True)

                try:
                    piv = df.pivot(index=ky, columns=kx, values="feasible")
                    st.subheader("Feasibility heatmap (True=1 / False=0)")
                    st.dataframe(piv.astype(int), use_container_width=True)
                except Exception as e:
                    st.warning(f"Could not pivot heatmap: {e}")

if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_regress:
        st.header("What broke? Regression Viewer")
        st.caption("Compare two artifacts: constraints, ledgers, model sets, and key KPIs. This is UI-only; it doesn't modify artifacts.")

        c1, c2 = st.columns(2)
        with c1:
            upA = st.file_uploader("Artifact A (json)", type=["json"], key="regA")
            artA = _load_json_from_upload(upA)
        with c2:
            upB = st.file_uploader("Artifact B (json)", type=["json"], key="regB")
            artB = _load_json_from_upload(upB)

        if artA and artB:
            def _kpi_df(art):
                k = art.get("kpis", {}) if isinstance(art.get("kpis"), dict) else {}
                df = pd.DataFrame([{"kpi": kk, "value": vv} for kk,vv in k.items()])
                if df.empty:
                    return pd.DataFrame(columns=["kpi","value"])
                return df.sort_values("kpi")
            st.subheader("KPI diff")
            dfA=_kpi_df(artA).set_index("kpi")
            dfB=_kpi_df(artB).set_index("kpi")
            join=dfA.join(dfB, lsuffix="_A", rsuffix="_B", how="outer")
            join["delta"]=pd.to_numeric(join["value_B"], errors="coerce")-pd.to_numeric(join["value_A"], errors="coerce")
            st.dataframe(join.reset_index().sort_values("kpi"), use_container_width=True, hide_index=True)

            st.subheader("New / worsened constraint failures")
            consA=artA.get("constraints", []) if isinstance(artA.get("constraints"), list) else []
            consB=artB.get("constraints", []) if isinstance(artB.get("constraints"), list) else []
            def _map(cons):
                m={}
                for c in cons:
                    name=c.get("name") or c.get("id")
                    if name:
                        m[name]=c
                return m
            mA=_map(consA); mB=_map(consB)
            names=sorted(set(mA.keys())|set(mB.keys()))
            rows=[]
            for n in names:
                a=mA.get(n,{}); b=mB.get(n,{})
                fa=bool(a.get("failed")); fb=bool(b.get("failed"))
                ma=a.get("margin"); mb=b.get("margin")
                rows.append({"name": n, "failed_A": fa, "failed_B": fb, "margin_A": ma, "margin_B": mb,
                             "margin_delta": (mb-ma) if isinstance(ma,(int,float)) and isinstance(mb,(int,float)) else None})
            df=pd.DataFrame(rows, columns=["name","failed_A","failed_B","margin_A","margin_B","margin_delta"])
            df_bad=df[(df["failed_B"]==True) & ((df["failed_A"]==False) | (df["failed_A"].isna()))]
            st.markdown("**New failures in B**")
            st.dataframe(df_bad.sort_values("name"), use_container_width=True, hide_index=True)
            st.markdown("**Largest margin regressions (B-A)**")
            df_reg=df.dropna(subset=["margin_delta"]).sort_values("margin_delta").head(20)
            st.dataframe(df_reg, use_container_width=True, hide_index=True)

            st.subheader("Model set comparison")
            msA=artA.get("model_set"); msB=artB.get("model_set")
            st.json({"model_set_A": msA, "model_set_B": msB})

if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_study_dash:
        st.header("Study Dashboard")
        st.caption("Manager-grade summary for study outputs (feasible fraction, dominant blockers, robustness).")

        up = st.file_uploader("Upload study index.json (study_index.v1)", type=["json"], key="sd_up")
        idx_data = _load_json_from_upload(up)
        if not idx_data:
            idx_path = st.session_state.get("selected_study_path")
            if idx_path and Path(idx_path).exists():
                try:
                    idx_data = json.loads(Path(idx_path).read_text(encoding="utf-8"))
                    st.info("Loaded study index from session.")
                except Exception:
                    idx_data = None

        if idx_data:
            st.subheader("Study headline")
            st.json({k: idx_data.get(k) for k in ["schema_version","n_cases","elapsed_s","created_unix"] if k in idx_data})
            cases = idx_data.get("cases", [])
            if isinstance(cases, list) and cases:
                df = pd.DataFrame(cases)
                if "ok" in df.columns:
                    ok_frac = float(df["ok"].mean())
                    st.metric("Feasible fraction", f"{ok_frac:.3f}")
                for col in ["dominant_blocker","top_blocker","blocker"]:
                    if col in df.columns:
                        st.subheader("Dominant blocker distribution")
                        hist = df[col].fillna("(none)").value_counts().reset_index()
                        hist.columns=[col,"count"]
                        st.dataframe(hist, use_container_width=True, hide_index=True)
                        break
                st.subheader("Cases table")
                st.dataframe(df, use_container_width=True, hide_index=True)
            else:
                st.info("No 'cases' list found in study index. (Older study output?)")

if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_maturity:
        st.header("Engineering Maturity Heatmap")
        st.caption("Visualize model maturity / validity info embedded in the artifact (model_set + model_registry).")

        art = _get_active_artifact("maturity")
        if not art:
            st.info("Load an artifact to view maturity info.")
        else:
            reg = art.get("model_registry", {})
            ms = art.get("model_set", {})
            rows=[]
            if isinstance(reg, dict):
                entries = reg.get("entries") if isinstance(reg.get("entries"), list) else None
                if entries is None:
                    if all(isinstance(v, dict) for v in reg.values()):
                        entries=[{"model_id": k, **v} for k,v in reg.items()]
                if entries:
                    selected = set()
                    if isinstance(ms, dict):
                        sel = ms.get("selected")
                        if isinstance(sel, dict):
                            selected = set(sel.values()) | set(sel.keys())
                        elif isinstance(sel, list):
                            selected = set(sel)
                    for e in entries:
                        mid = e.get("model_id", e.get("id", ""))
                        rows.append({
                            "subsystem": e.get("subsystem", e.get("domain", "")),
                            "model_id": mid,
                            "maturity": e.get("maturity", e.get("maturity_tag", "")),
                            "validity": e.get("validity", e.get("validity_range", "")),
                            "selected": (mid in selected)
                        })
            if rows:
                df=pd.DataFrame(rows, columns=["name","failed_A","failed_B","margin_A","margin_B","margin_delta"])
                st.dataframe(df.sort_values(["subsystem","model_id"]), use_container_width=True, hide_index=True)
                st.markdown("Tip: treat this as a policy gate (e.g., block decisions if maturity < required).")
            else:
                st.info("No model_registry entries found in artifact.")


if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_maintenance:
        st.header("Maintenance & Availability Authority")
        st.caption("Deterministic maintenance scheduling closure (v368.0): outage calendar proxy and schedule-dominated availability.")

        out = st.session_state.get("last_point_out")
        if not isinstance(out, dict):
            st.info("Run a point in Point Designer first (sets last_point_out).")
        else:
            enabled = bool(out.get("maintenance_contract_sha256")) and (out.get("availability_v368") == out.get("availability_v368"))
            if not enabled:
                st.warning("v368 maintenance scheduling is not enabled for the current point. Enable it in ðŸ§­ Point Designer â†’ Engineering & plant feasibility â†’ ðŸ—“ï¸ Maintenance scheduling authority (v368.0).")
            c1, c2, c3, c4 = st.columns(4)
            c1.metric("Availability (v368)", _m("availability_v368", "{:.3f}"))
            c2.metric("Outage total (v368)", _m("outage_total_frac_v368", "{:.3f}"))
            c3.metric("Net MWh/y (v368)", _m("net_electric_MWh_per_year_v368", "{:.3g}"))
            c4.metric("Repl. cost (MUSD/y)", _m("replacement_cost_MUSD_per_year_v368", "{:.3g}"))

            with st.expander("What this authority does", expanded=False):
                st.markdown(
                    "- Converts replacement cadences (FW/blanket from v367, plus HCD and tritium plant) and replacement durations into a bundled outage fraction.\n"
                    "- Combines with planned/forced baselines (and optional trips proxy) to form total outage and availability.\n"
                    "- Emits an explicit event table (maintenance_events_v368) for audit and reviewer use."
                )
            with st.expander("What this authority does not do", expanded=False):
                st.markdown(
                    "- Does not run a time-domain availability/RAMI simulation.\n"
                    "- Does not optimize schedules or negotiate constraints.\n"
                    "- Does not modify plasma truth or materials lifetime truth; it only post-processes into a schedule proxy."
                )

            st.subheader("Outage decomposition")
            rows = [
                {"term": "planned", "outage_frac": out.get("planned_outage_frac_v368")},
                {"term": "forced", "outage_frac": out.get("forced_outage_frac_v368")},
                {"term": "replacement", "outage_frac": out.get("replacement_outage_frac_v368")},
                {"term": "total", "outage_frac": out.get("outage_total_frac_v368")},
            ]
            try:
                import pandas as _pd
                df = _pd.DataFrame(rows)
                st.dataframe(df, use_container_width=True, hide_index=True)
            except Exception:
                st.write(rows)

            ev = out.get("maintenance_events_v368")
            with st.expander("Maintenance event table (v368)", expanded=False):
                if isinstance(ev, list) and ev:
                    try:
                        import pandas as _pd
                        st.dataframe(_pd.DataFrame(ev), use_container_width=True, hide_index=True)
                    except Exception:
                        st.json(ev)
                else:
                    st.info("No maintenance_events_v368 found (enable v368 and re-run).")

            with st.expander("Contract fingerprint", expanded=False):
                st.code(str(out.get("maintenance_contract_sha256", "")))


if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_profile_auth:
        st.header("Profile Authority")
        st.caption("1.5D algebraic profile diagnostics (non-iterative, conservative).")
        out = st.session_state.get("last_point_out")
        if not isinstance(out, dict):
            st.info("Run a point in Point Designer first (sets last_point_out).")
        else:
            rows=[
                {"metric":"p_peaking", "value": out.get("profile_p_peaking")},
                {"metric":"j_peaking", "value": out.get("profile_j_peaking")},
                {"metric":"li_proxy", "value": out.get("profile_li_proxy")},
                {"metric":"qmin_proxy", "value": out.get("profile_qmin_proxy")},
                {"metric":"f_bootstrap_proxy", "value": out.get("profile_f_bootstrap_proxy")},
                {"metric":"tag", "value": out.get("profile_assumption_tag")},
            ]
            st.dataframe(rows, use_container_width=True, hide_index=True)
            with st.expander("Validity flags", expanded=False):
                st.json(out.get("profile_validity", {}))

if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_impurity:
        st.header("Impurity & Radiation")
        st.caption("v320 authority: impurity radiation partitions (core/edge/SOL/div) + detachment inversion (q_div target â†’ required SOL+div radiation â†’ implied f_z).")
        out = st.session_state.get("last_point_out")
        if not isinstance(out, dict):
            st.info("Run a point in Point Designer first.")
        else:
            rows=[
                {"metric":"impurity_contract_species", "value": out.get("impurity_contract_species")},
                {"metric":"impurity_contract_f_z", "value": out.get("impurity_contract_f_z")},
                {"metric":"impurity_prad_total_MW", "value": out.get("impurity_prad_total_MW")},
                {"metric":"impurity_prad_core_MW", "value": out.get("impurity_prad_core_MW")},
                {"metric":"impurity_prad_edge_MW", "value": out.get("impurity_prad_edge_MW")},
                {"metric":"impurity_prad_sol_MW", "value": out.get("impurity_prad_sol_MW")},
                {"metric":"impurity_prad_div_MW", "value": out.get("impurity_prad_div_MW")},
                {"metric":"impurity_zeff_proxy", "value": out.get("impurity_zeff_proxy")},
                {"metric":"impurity_fuel_ion_fraction", "value": out.get("impurity_fuel_ion_fraction")},
                {"metric":"detachment_f_sol_div_required", "value": out.get("detachment_f_sol_div_required")},
                {"metric":"detachment_prad_sol_div_required_MW", "value": out.get("detachment_prad_sol_div_required_MW")},
                {"metric":"detachment_f_z_required", "value": out.get("detachment_f_z_required")},
            ]
            st.dataframe(rows, use_container_width=True, hide_index=True)
            with st.expander("Validity flags", expanded=False):
                st.json(out.get("impurity_validity", {}))

if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_disruption:
        st.header("Disruption Risk")
        st.caption("Conservative screening tier: LOW/MED/HIGH (diagnostic; not predictive).")
        out = st.session_state.get("last_point_out")
        if not isinstance(out, dict):
            st.info("Run a point in Point Designer first.")
        else:
            st.metric("Tier", str(out.get("disruption_risk_tier", "UNKNOWN")))
            cols=st.columns(3)
            with cols[0]:
                st.metric("Risk index", f"{float(out.get('disruption_risk_index', float('nan'))):.3f}" if out.get('disruption_risk_index')==out.get('disruption_risk_index') else "nan")
            with cols[1]:
                st.metric("Dominant driver", str(out.get("disruption_dominant_driver", "unknown")))
            with cols[2]:
                st.metric("fG", f"{float(getattr(st.session_state.get('last_point_inp', None),'fG', float('nan'))):.3f}" if st.session_state.get('last_point_inp') is not None else "nan")
            with st.expander("Components", expanded=False):
                st.json(out.get("disruption_risk_components", {}))

if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_stability:
        st.header("Stability Risk")
        st.caption("Conservative screening tier: LOW/MED/HIGH for vertical stability + RWM/control budgets (diagnostic; not predictive).")
        out = st.session_state.get("last_point_out")
        if not isinstance(out, dict):
            st.info("Run a point in Point Designer first.")
        else:
            st.metric("Tier", str(out.get("stability_risk_tier", "UNKNOWN")))
            cols = st.columns(4)
            with cols[0]:
                st.metric(
                    "Risk index",
                    f"{float(out.get('stability_risk_index', float('nan'))):.3f}"
                    if out.get("stability_risk_index") == out.get("stability_risk_index")
                    else "nan",
                )
            with cols[1]:
                st.metric("Dominant driver", str(out.get("stability_dominant_driver", "unknown")))
            with cols[2]:
                st.metric(
                    "vs_margin",
                    f"{float(out.get('vs_margin', float('nan'))):.3f}"
                    if out.get("vs_margin") == out.get("vs_margin")
                    else "nan",
                )
            with cols[3]:
                st.metric("RWM ok", "yes" if bool(out.get("rwm_control_ok", True)) else "no")

            st.divider()
            oc = st.columns(2)
            with oc[0]:
                st.metric("Operational tier", str(out.get("operational_risk_tier", "UNKNOWN")))
            with oc[1]:
                st.metric("Operational driver", str(out.get("operational_dominant_driver", "")) or "-")

            with st.expander("Components", expanded=False):
                st.json(out.get("stability_risk_components", {}))
            with st.expander("Control contract margins", expanded=False):
                st.json(out.get("control_contract_margins", {}))

if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_cert_search:
        st.header("Certified Search")
        st.caption("Budgeted multi-knob search (external to truth). Each candidate is verified by the frozen evaluator.")

        from dataclasses import replace
        from solvers.budgeted_search import SearchVar
        from solvers.certified_search_orchestrator import OrchestratorSpec, SearchStage, run_orchestrated_certified_search

        base = st.session_state.get("last_point_inp")
        if base is None:
            st.info("Run a point in Point Designer first so a base point exists.")
        else:
            st.subheader("Knobs")
            knob_options = [
                ("Bt_T", 2.0, 25.0),
                ("Ip_MA", 1.0, 25.0),
                ("Paux_MW", 0.0, 200.0),
                ("Ti_keV", 1.0, 40.0),
                ("fG", 0.2, 1.2),
                ("kappa", 1.0, 2.6),
                ("a_m", 0.2, 3.0),
                ("R0_m", 0.8, 12.0),
            ]
            cols = st.columns(2)
            with cols[0]:
                chosen = st.multiselect("Select up to 4 knobs", [k[0] for k in knob_options], default=["Bt_T","Ip_MA"], max_selections=4)
            with cols[1]:
                objective = st.selectbox("Score objective (PASS-only)", ["Q_DT_eqv","P_fus_MW","P_net_MW"], index=0)

            vars_=[]
            for name,lo,hi in knob_options:
                if name in chosen:
                    c1,c2=st.columns(2)
                    with c1:
                        lo_v = st.number_input(f"{name} lo", value=float(getattr(base,name)), step=0.1, key=f"cs_lo_{name}")
                    with c2:
                        hi_v = st.number_input(f"{name} hi", value=float(getattr(base,name)), step=0.1, key=f"cs_hi_{name}")
                    if hi_v <= lo_v:
                        hi_v = lo_v + 1e-6
                    vars_.append(SearchVar(name=name, lo=float(lo_v), hi=float(hi_v)))

            c1,c2,c3,c4=st.columns(4)
            with c1:
                budget = int(st.number_input("Budget", value=96, min_value=8, max_value=2048, step=8, key="cs_budget"))
            with c2:
                seed = int(st.number_input("Seed", value=0, min_value=0, max_value=10_000, step=1, key="cs_seed"))
            with c3:
                method = st.selectbox("Method", ["halton","lhs","grid"], index=0, key="cs_method")
            with c4:
                two_stage = bool(st.checkbox("Two-stage refine", value=True, key="cs_two_stage"))

            stage2_budget_frac = float(st.slider("Stage-2 budget fraction", min_value=0.10, max_value=0.80, value=0.35, step=0.05, key="cs_stage2_frac")) if two_stage else 0.0
            stage2_shrink = float(st.slider("Stage-2 local shrink", min_value=0.10, max_value=0.80, value=0.35, step=0.05, key="cs_stage2_shrink")) if two_stage else 0.0
            stage2_method = st.selectbox("Stage-2 method", ["grid","halton","lhs"], index=0, key="cs_stage2_method") if two_stage else "grid"

            st.markdown("---")
            insert_surr = bool(st.checkbox("Insert surrogate stage (feasible-first, non-authoritative)", value=False, key="cs_insert_surr"))
            surr_frac = float(
                st.slider(
                    "Surrogate budget fraction",
                    min_value=0.05,
                    max_value=0.60,
                    value=0.20,
                    step=0.05,
                    key="cs_surr_frac",
                    disabled=(not insert_surr),
                )
            )
            s1, s2, s3 = st.columns(3)
            with s1:
                surr_pool_mult = int(st.number_input("Surrogate pool multiplier", value=50, min_value=4, max_value=200, step=1, key="cs_surr_pool", disabled=(not insert_surr)))
            with s2:
                surr_kappa = float(st.slider("Surrogate kappa", min_value=0.0, max_value=2.0, value=0.5, step=0.1, key="cs_surr_kappa", disabled=(not insert_surr)))
            with s3:
                surr_ridge = float(st.number_input("Surrogate ridge alpha", value=1e-3, min_value=1e-6, max_value=1.0, format="%.6f", key="cs_surr_ridge", disabled=(not insert_surr)))

            def _builder(b, overrides):
                return replace(b, **{k: float(v) for k,v in overrides.items()})

            def _verifier(inp_obj):
                out = hot_ion_point(inp_obj)
                cons = evaluate_constraints(out, point_inputs=inp_obj)
                try:
                    from constraints.bookkeeping import summarize as _summarize_constraints
                    _cs = _summarize_constraints(cons)
                    _min_margin_frac = float(_cs.worst_hard_margin_frac) if _cs.worst_hard_margin_frac is not None else float("nan")
                    _worst_hard = str(_cs.worst_hard or "")
                except Exception:
                    _min_margin_frac = float("nan")
                    _worst_hard = ""
                ok = all((not bool(c.get("failed"))) for c in cons)
                score = float(out.get(objective, 0.0)) if ok else float("-inf")

                evidence={
                    "objective": objective,
                    "objective_value": float(out.get(objective, float("nan"))),
                    "min_margin_frac": _min_margin_frac,
                    "worst_hard": _worst_hard,
                    "worst_hard_margin_frac": float(_min_margin_frac) if _min_margin_frac == _min_margin_frac else float("nan"),
                    "n_failed": int(sum(1 for c in cons if c.get("failed"))),
                    "top_blocker": (next((c.get("name") for c in cons if c.get("failed")), None)),
                }
                return ("PASS" if ok else "FAIL"), score, evidence

            if st.button("Run certified search", use_container_width=True, key="run_cert_search"):
                if not vars_:
                    st.warning("Select at least one knob.")
                else:
                    b1 = int(max(1, round(float(budget) * (1.0 - float(stage2_budget_frac)))))
                    b2 = int(max(0, round(float(budget) * float(stage2_budget_frac))))
                    bs = int(max(0, round(float(budget) * float(surr_frac)))) if insert_surr else 0
                    # cap budgets deterministically
                    b2 = int(min(int(b2), int(max(0, budget - 1))))
                    bs = int(min(int(bs), int(max(0, budget - 1 - b2))))
                    b1 = int(max(1, int(budget) - int(b2) - int(bs)))

                    stages = [SearchStage(name="stage1", method=str(method), budget=int(b1), seed=int(seed), local_refine=False)]
                    if insert_surr and bs > 0:
                        stages.append(
                            SearchStage(
                                name="surrogate",
                                method="surrogate",
                                budget=int(bs),
                                seed=int(seed + 1),
                                local_refine=False,
                                surrogate_pool_mult=int(surr_pool_mult),
                                surrogate_kappa=float(surr_kappa),
                                surrogate_ridge_alpha=float(surr_ridge),
                                surrogate_feas_margin_key="min_margin_frac",
                            )
                        )
                    if two_stage and b2 > 0:
                        stages.append(
                            SearchStage(
                                name="stage2",
                                method=str(stage2_method),
                                budget=int(b2),
                                seed=int(seed + (2 if (insert_surr and bs > 0) else 1)),
                                local_refine=True,
                                local_shrink=float(stage2_shrink),
                            )
                        )
                    art = run_orchestrated_certified_search(
                        base,
                        OrchestratorSpec(variables=tuple(vars_), stages=tuple(stages)),
                        verifier=_verifier,
                        builder=_builder,
                    )
                    st.session_state["last_certified_search_artifact"] = art
                    st.session_state["v340_cert_search_last"] = art
                    try:
                        _v98_record_run("certified_search_orchestrated", art, mode="SystemSuite/Chronicle")
                    except Exception:
                        pass

                    n_pass = 0
                    n_tot = 0
                    try:
                        for stg in art.get("stages", []):
                            recs = stg.get("records", [])
                            n_tot += len(recs)
                            n_pass += sum(1 for r in recs if r.get("verdict") == "PASS")
                    except Exception:
                        pass
                    st.success(f"Done. Digest: {str(art.get('digest',''))[:12]} | PASS found: {n_pass}/{n_tot}")

            art = st.session_state.get("v340_cert_search_last")
            if isinstance(art, dict) and art.get("schema_version"):
                st.subheader("Results")
                # Flatten across stages for display
                rows = []
                for stg in art.get("stages", []):
                    for r in stg.get("records", []):
                        rows.append({"stage": stg.get("name"), "i": r.get("i"), "verdict": r.get("verdict"), "score": r.get("score"), **(r.get("x") or {}), **{f"e_{k}": v for k, v in (r.get("evidence") or {}).items()}})
                df = pd.DataFrame(rows)
                with st.expander("Results table", expanded=False):
                    st.dataframe(df, use_container_width=True, hide_index=True)
                if isinstance(art.get("best"), dict) and art["best"].get("x") is not None:
                    with st.expander("Best PASS candidate", expanded=False):
                        st.json(art.get("best"))

                # v297: export deterministic evidence pack
                try:
                    from tools.simple_evidence_zip import build_simple_evidence_zip_bytes
                    art2 = st.session_state.get("last_certified_search_artifact")
                    if isinstance(art2, dict):
                        if st.button("Build Certified Search evidence pack", use_container_width=True, key="cs_build_ev"):
                            b = build_simple_evidence_zip_bytes(art2, basename=f"certified_search_{art2.get('digest','')[:12]}")
                            st.session_state["certified_search_evidence_zip"] = b
                            st.success("Evidence pack built.")
                        b = st.session_state.get("certified_search_evidence_zip")
                        if isinstance(b, (bytes, bytearray)) and len(b) > 0:
                            st.download_button(
                                "Download certified_search_evidence.zip",
                                data=b,
                                file_name="certified_search_evidence.zip",
                                mime="application/zip",
                                use_container_width=True,
                                key="cs_dl_ev",
                            )
                except Exception:
                    pass

if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_repair:
        st.header("Repair Suggestions")
        st.caption("Explanatory-only: proposes bounded knob deltas to reduce dominant constraint residuals; every proposal must be verified by truth.")

        from dataclasses import replace
        from solvers.repair_suggestions import RepairKnob, propose_repair_candidates

        base_inp = st.session_state.get("last_point_inp")
        base_out = st.session_state.get("last_point_out")
        if base_inp is None or not isinstance(base_out, dict):
            st.info("Run a point in Point Designer first.")
        else:
            cons = evaluate_constraints(base_out, point_inputs=base_inp)
            failed = [c for c in cons if c.get("failed")]
            if not failed:
                st.info("Base point is already feasible; repair suggestions are not needed.")
            else:
                # Residual proxy: use 'margin' if present else 1.0
                residuals = {}
                for c in failed:
                    name = str(c.get("name", "(unnamed)"))
                    m = c.get("margin")
                    try:
                        m = float(m)
                    except Exception:
                        m = None
                    # Convert to positive residual where 0 is pass.
                    if m is None or not (m == m):
                        residuals[name] = 1.0
                    else:
                        residuals[name] = max(0.0, -m)

                st.subheader("Select knobs")
                knob_options = [
                    ("Bt_T", 2.0, 25.0),
                    ("Ip_MA", 1.0, 25.0),
                    ("Paux_MW", 0.0, 200.0),
                    ("Ti_keV", 1.0, 40.0),
                    ("fG", 0.2, 1.2),
                    ("kappa", 1.0, 2.6),
                ]
                chosen = st.multiselect("Knobs used for repair", [k[0] for k in knob_options], default=["Bt_T","Ip_MA","Paux_MW"], max_selections=6)
                knobs=[]
                for name,lo,hi in knob_options:
                    if name in chosen:
                        lo_v=float(st.number_input(f"{name} lo", value=float(getattr(base_inp,name)), step=0.1, key=f"rep_lo_{name}"))
                        hi_v=float(st.number_input(f"{name} hi", value=float(getattr(base_inp,name)), step=0.1, key=f"rep_hi_{name}"))
                        if hi_v <= lo_v:
                            hi_v = lo_v + 1e-6
                        knobs.append(RepairKnob(name=name, lo=lo_v, hi=hi_v))

                # Finite-difference jacobian (deterministic): d(residual)/dvar
                def _eval_res(inp_obj):
                    out = hot_ion_point(inp_obj)
                    cons2 = evaluate_constraints(out, point_inputs=inp_obj)
                    res={}
                    for c in cons2:
                        name=str(c.get("name","(unnamed)"))
                        m=c.get("margin")
                        try:
                            m=float(m)
                        except Exception:
                            continue
                        res[name]=max(0.0, -m)
                    return res

                if st.button("Compute repair candidates", use_container_width=True, key="run_repairs"):
                    base_res = _eval_res(base_inp)
                    jac={}
                    for c in base_res:
                        jac[c]={}
                    for kb in knobs:
                        span = kb.hi - kb.lo
                        h = 0.02*span
                        if h<=0: continue
                        x0=float(getattr(base_inp,kb.name))
                        x1=min(kb.hi, x0+h)
                        x2=max(kb.lo, x0-h)
                        # central difference when possible
                        inp_p = replace(base_inp, **{kb.name: x1})
                        inp_m = replace(base_inp, **{kb.name: x2})
                        rp=_eval_res(inp_p)
                        rm=_eval_res(inp_m)
                        denom = (x1-x2) if (x1-x2)!=0 else 1e-12
                        for c in base_res:
                            jac[c][kb.name] = (float(rp.get(c,0.0))-float(rm.get(c,0.0)))/denom

                    cands = propose_repair_candidates(residuals=base_res, jacobian=jac, knobs=knobs, k=8)
                    st.session_state["v296_repair_last"] = {"base_res": base_res, "jac": jac, "cands": cands}

                last = st.session_state.get("v296_repair_last")
                if last:
                    with st.expander("Base residuals", expanded=False):
                        st.dataframe(pd.DataFrame([{ "constraint": k, "residual": v } for k,v in sorted(last["base_res"].items(), key=lambda kv: kv[1], reverse=True)]), use_container_width=True, hide_index=True)
                    st.subheader("Candidates")
                    cands = last["cands"]
                    if cands:
                        df = pd.DataFrame([{ "rationale": c.rationale, "est_reduction": c.estimated_residual_reduction, **c.deltas } for c in cands])
                        st.dataframe(df, use_container_width=True, hide_index=True)
                    else:
                        st.info("No candidates produced (check knob selections).")

                    # v297: build + download a deterministic repair evidence pack
                    try:
                        from dataclasses import asdict
                        from tools.simple_evidence_zip import build_simple_evidence_zip_bytes

                        repair_art = {
                            "schema_version": "repair_evidence.v1",
                            "base_inputs": asdict(base_inp),
                            "base_failed_constraints": [str(c.get("name")) for c in failed][:50],
                            "base_residuals": dict(last.get("base_res", {})),
                            "knobs": [asdict(k) for k in knobs],
                            "candidates": [
                                {
                                    "rationale": getattr(c, "rationale", ""),
                                    "estimated_residual_reduction": float(getattr(c, "estimated_residual_reduction", 0.0)),
                                    "deltas": dict(getattr(c, "deltas", {})),
                                }
                                for c in (cands or [])
                            ],
                        }
                        st.session_state["last_repair_evidence_artifact"] = repair_art

                        if st.button("Build Repair evidence pack", use_container_width=True, key="rep_build_ev"):
                            b = build_simple_evidence_zip_bytes(repair_art, basename="repair_evidence")
                            st.session_state["repair_evidence_zip"] = b
                            _v98_record_run("repair_evidence", repair_art, mode="SystemSuite/Chronicle")
                            st.success("Repair evidence pack built.")

                        b = st.session_state.get("repair_evidence_zip")
                        if isinstance(b, (bytes, bytearray)) and len(b) > 0:
                            st.download_button(
                                "Download repair_evidence.zip",
                                data=b,
                                file_name="repair_evidence.zip",
                                mime="application/zip",
                                use_container_width=True,
                                key="rep_dl_ev",
                            )
                    except Exception:
                        pass

if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_refine:
        st.header("Interval Refinement")
        st.caption("Deterministic corner evaluation + contract refinement suggestions (explanatory-only).")

        from dataclasses import replace
        from uq_contracts.refinement import suggest_interval_refinements

        base = st.session_state.get("last_point_inp")
        if base is None:
            st.info("Run a point in Point Designer first so a base point exists.")
        else:
            st.subheader("Select up to 3 uncertain variables")
            var_options = ["Bt_T","Ip_MA","Paux_MW","Ti_keV","fG","kappa"]
            chosen = st.multiselect("Variables", var_options, default=["fG"], max_selections=3, key="ref_vars")
            intervals={}
            for v in chosen:
                c1,c2=st.columns(2)
                with c1:
                    lo = float(st.number_input(f"{v} lo", value=float(getattr(base,v))*0.95, step=0.1, key=f"ref_lo_{v}"))
                with c2:
                    hi = float(st.number_input(f"{v} hi", value=float(getattr(base,v))*1.05 + (0.01 if v=='fG' else 0.0), step=0.1, key=f"ref_hi_{v}"))
                if hi <= lo:
                    hi = lo + 1e-6
                intervals[v]=(lo,hi)

            if st.button("Evaluate corners", use_container_width=True, key="run_ref_corners"):
                # build corners
                vs=list(intervals.items())
                corners=[]
                def rec(i,cur):
                    if i==len(vs):
                        corners.append(dict(cur)); return
                    name,(lo,hi)=vs[i]
                    cur[name]=lo; rec(i+1,cur)
                    cur[name]=hi; rec(i+1,cur)
                    cur.pop(name,None)
                rec(0,{})

                results=[]
                for c in corners:
                    inp_obj=base
                    inp_obj=replace(inp_obj, **{k: float(v) for k,v in c.items()})
                    out = hot_ion_point(inp_obj)
                    cons = evaluate_constraints(out, point_inputs=inp_obj)
                    ok = all((not bool(cc.get("failed"))) for cc in cons)
                    dom = next((cc.get("name") for cc in cons if cc.get("failed")), None)
                    results.append({"corner": c, "verdict": "PASS" if ok else "FAIL", "dominant_mechanism": dom})

                st.session_state["v296_ref_corners"] = {"intervals": intervals, "results": results}

            last = st.session_state.get("v296_ref_corners")
            if last:
                res = last["results"]
                df = pd.DataFrame([{**r["corner"], "verdict": r["verdict"], "dominant": r.get("dominant_mechanism")} for r in res])
                st.dataframe(df, use_container_width=True, hide_index=True)
                fails=sum(1 for r in res if r["verdict"]!='PASS')
                st.metric("FAIL corners", f"{fails}/{len(res)}")
                sugg = suggest_interval_refinements(last["intervals"], res)
                if sugg:
                    st.subheader("Refinement suggestions")
                    sdf = pd.DataFrame([{"var": s.var, "current": s.current_interval, "suggested": s.suggested_interval, "rationale": s.rationale} for s in sugg])
                    st.dataframe(sdf, use_container_width=True, hide_index=True)
                else:
                    st.info("No refinement suggestions (either robust already or insufficient failure signal).")

                # v297: interval refinement evidence pack
                try:
                    from dataclasses import asdict
                    from tools.simple_evidence_zip import build_simple_evidence_zip_bytes

                    refine_art = {
                        "schema_version": "interval_refinement_evidence.v1",
                        "base_inputs": asdict(base),
                        "intervals": {k: list(v) for k, v in (last.get("intervals") or {}).items()},
                        "corner_results": list(res),
                        "suggestions": [
                            {
                                "var": s.var,
                                "current_interval": list(s.current_interval),
                                "suggested_interval": list(s.suggested_interval),
                                "rationale": s.rationale,
                            }
                            for s in (sugg or [])
                        ],
                    }
                    st.session_state["last_interval_refinement_artifact"] = refine_art

                    if st.button("Build Interval Refinement evidence pack", use_container_width=True, key="ref_build_ev"):
                        b = build_simple_evidence_zip_bytes(refine_art, basename="interval_refinement")
                        st.session_state["interval_refinement_zip"] = b
                        _v98_record_run("interval_refinement", refine_art, mode="SystemSuite/Chronicle")
                        st.success("Interval refinement evidence pack built.")

                    b = st.session_state.get("interval_refinement_zip")
                    if isinstance(b, (bytes, bytearray)) and len(b) > 0:
                        st.download_button(
                            "Download interval_refinement_evidence.zip",
                            data=b,
                            file_name="interval_refinement_evidence.zip",
                            mime="application/zip",
                            use_container_width=True,
                            key="ref_dl_ev",
                        )
                except Exception:
                    pass

if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_narrowing:
        st.header("Interval Narrowing")
        st.caption(
            "Advisory dead-region flags + interval narrowing proposals + repair contract export (no truth mutation)."
        )
        try:
            from ui.interval_narrowing import render_interval_narrowing_panel
            render_interval_narrowing_panel(st, pd, BASE_DIR, st.session_state)
        except Exception as _e:
            st.info("Panel unavailable in this build.")

if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_surrogate:
        st.header("Surrogate Overlay")
        st.caption("Non-authoritative ridge surrogate fitted to the latest Certified Search results.")

        from optimization.surrogates import fit_ridge_surrogate, predict_surrogate

        res = st.session_state.get("v296_cert_search_last")
        if res is None:
            st.info("Run Certified Search first (Chronicle â†’ Certified Search) to generate training data.")
        else:
            # train on PASS points only
            rows=[]
            for r in res.records:
                if r.verdict == "PASS":
                    rows.append({"x": r.x, "y": r.score})
            if len(rows) < 8:
                st.warning(f"Need at least 8 PASS samples to fit a surrogate; currently have {len(rows)}.")
            else:
                feat = list(rows[0]["x"].keys())
                samples=[rr["x"] for rr in rows]
                targets=[rr["y"] for rr in rows]
                ridge=float(st.number_input("Ridge", value=1e-6, format="%e"))
                if st.button("Fit surrogate", use_container_width=True, key="fit_surr"):
                    model = fit_ridge_surrogate(samples, targets, feat, ridge=ridge)
                    st.session_state["v296_surrogate_model"] = model
                    st.success("Surrogate fitted (non-authoritative).")

                model = st.session_state.get("v296_surrogate_model")
                if model is not None:
                    st.subheader("Query")
                    q={}
                    for f in model.feature_names:
                        q[f]=float(st.number_input(f"{f}", value=float(st.session_state.get('last_point_inp').__dict__.get(f, 0.0)), step=0.1, key=f"surr_q_{f}"))
                    yhat, unc = predict_surrogate(model, q)
                    st.metric("Predicted score", f"{yhat:.6g}")
                    st.metric("Uncertainty proxy", f"{unc:.3f}")
                    with st.expander("Model details", expanded=False):
                        st.write({"features": list(model.feature_names), "ridge": model.ridge})

if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_active_learning:
        st.header("Active Learning")
        st.caption("Propose new points where surrogate uncertainty is high (non-authoritative).")

        from optimization.active_learning import ALVar, propose_active_learning_points

        model = st.session_state.get("v296_surrogate_model")
        res = st.session_state.get("v296_cert_search_last")
        if model is None or res is None:
            st.info("Fit a surrogate first (Chronicle â†’ Surrogate Overlay).")
        else:
            vars_=[]
            # derive bounds from SearchSpec
            for v in res.spec.variables:
                vars_.append(ALVar(name=v.name, lo=v.lo, hi=v.hi))
            c1,c2,c3=st.columns(3)
            with c1:
                n_candidates=int(st.number_input("Candidates", value=512, min_value=64, max_value=5000, step=64))
            with c2:
                n_select=int(st.number_input("Select", value=16, min_value=4, max_value=128, step=4))
            with c3:
                seed=int(st.number_input("Seed", value=int(res.spec.seed), min_value=0, max_value=10000, step=1))

            if st.button("Propose points", use_container_width=True, key="run_al"):
                props = propose_active_learning_points(model, vars_, n_candidates=n_candidates, n_select=n_select, seed=seed)
                st.session_state["v296_al_props"] = props

            props = st.session_state.get("v296_al_props")
            if props:
                df = pd.DataFrame([{**p.x, "y_pred": p.y_pred, "uncertainty": p.uncertainty} for p in props])
                st.dataframe(df, use_container_width=True, hide_index=True)
                st.caption("Verify proposals by setting last_point_inp to a row and running Point Designer. External harness integration can automate that next.")

if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_assumptions:
        st.header("Assumption Toggle Bar")
        st.caption("Fast scenario exploration by toggling common assumptions and re-evaluating the point (still feasibility-first; no optimization).")

        art = _get_active_artifact("assumptions")
        base = _guess_point_inputs_from_artifact(art) if art else None
        if base is None:
            base = st.session_state.get("last_point_inp")
        if base is None:
            st.info("Load an artifact (or run Point Designer) to use assumption toggles.")
        else:
            col1,col2,col3=st.columns(3)
            with col1:
                fuel = st.selectbox("Fuel mode", ["DT","DD"], index=0 if getattr(base,"fuel_mode","DT")=="DT" else 1, key="ass_fuel")
            with col2:
                ti = st.number_input("Ti (keV)", value=float(base.Ti_keV), step=0.5, key="ass_Ti")
            with col3:
                paux = st.number_input("Paux (MW)", value=float(base.Paux_MW), step=1.0, key="ass_Paux")
            tite = st.number_input("Ti/Te", value=float(getattr(base,"Ti_over_Te", 2.0)), step=0.1, key="ass_TiTe")
            apply = st.button("Apply toggles and evaluate", use_container_width=True, key="ass_run")

            if apply:
                pi = PointInputs(R0_m=float(base.R0_m), a_m=float(base.a_m), kappa=float(base.kappa),
                                 Bt_T=float(base.Bt_T), Ip_MA=float(base.Ip_MA), Ti_keV=float(ti),
                                 fG=float(base.fG), Paux_MW=float(paux), Ti_over_Te=float(tite),
                                 fuel_mode=str(fuel))
                out = hot_ion_point(pi)
                cons = evaluate_constraints(out, point_inputs=pi)
                ok = all((not bool(c.get("failed"))) for c in cons)
                st.metric("Feasible", "YES " if ok else "NO ")
                st.subheader("Key outputs")
                st.json({k: out.get(k) for k in ["Q_DT_eqv","P_fus_MW","P_net_MW","betaN","q95","fG"] if k in out})
                st.subheader("Top failed constraints")
                failed=[c for c in cons if c.get("failed")]
                if failed:
                    st.dataframe(_safe_df(failed[:10]), use_container_width=True, hide_index=True)
                else:
                    st.write("No failed constraints.")

if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_export:
        st.header("Export / Communication Panel")
        st.caption("One-click export helpers (JSON, CSV, and a one-slide PNG-style summary) with provenance footer.")

        art = _get_active_artifact("export")
        if not art:
            st.info("Load an artifact to export.")
        else:
            _download_json_button("Download run artifact JSON", art, "shams_run_artifact.json", "dl_artifact")
            tables = art.get("tables", {}) if isinstance(art.get("tables"), dict) else {}
            if tables:
                for name, obj in tables.items():
                    try:
                        df = _safe_df(obj)
                        st.download_button(f"Download {name}.csv", data=df.to_csv(index=False).encode("utf-8"),
                                           file_name=f"{name}.csv", mime="text/csv", key=f"dl_csv_{name}")
                    except Exception:
                        continue
            else:
                st.info("No standardized tables found in artifact ('tables').")

            try:
                import io
                import matplotlib.pyplot as plt
                prov = art.get("provenance", {}) if isinstance(art.get("provenance"), dict) else {}
                d = _decision_summary_from_artifact(art)
                fig = plt.figure(figsize=(10, 5))
                ax = fig.add_subplot(111)
                ax.axis("off")
                title = "SHAMS Decision Summary"
                verdict = "FEASIBLE" if d["feasible"] else "INFEASIBLE"
                ax.text(0.02, 0.92, f"{title} - {verdict}", fontsize=16, weight="bold")
                ax.text(0.02, 0.82, f"Q: {d['kpis'].get('Q_DT_eqv', d['kpis'].get('Q','-'))}    Pfus(MW): {d['kpis'].get('P_fus_MW', d['kpis'].get('Pfus_MW','-'))}", fontsize=12)
                ax.text(0.02, 0.72, "Top blockers:", fontsize=12, weight="bold")
                y=0.66
                for b in (d["top_blockers"] or [])[:6]:
                    ax.text(0.04, y, f"- {b.get('group','')}: {b.get('name','')}", fontsize=11)
                    y -= 0.06
                footer = f"repo_version={prov.get('repo_version')}  git={prov.get('git_commit')}  python={prov.get('python')}"
                ax.text(0.02, 0.03, footer, fontsize=9)
                buf = io.BytesIO()
                fig.savefig(buf, format="png", dpi=150, bbox_inches="tight")
                plt.close(fig)
                st.download_button("Download one-slide summary PNG", data=buf.getvalue(), file_name="shams_one_slide.png",
                                   mime="image/png", key="dl_png_slide")
            except Exception as e:
                st.warning(f"PNG summary unavailable: {e}")

if _deck == "ðŸŽ›ï¸ Control Room":
    with tab_solver:
        st.header("Solver Introspection")
        st.caption("Inspect solver trace/clamp/residual info from artifacts or the last Point Designer run.")

        art = st.session_state.get("selected_artifact")
        if not isinstance(art, dict) or not art:
            st.info("No session artifact loaded. Upload one below to inspect solver annotations.")
            up = st.file_uploader("Upload shams_run_artifact.json", type=["json"], key="solver_up")
            art = _load_json_from_upload(up)

        if art:
            trace = art.get("solver_trace") if isinstance(art.get("solver_trace"), dict) else None
            if trace:
                st.subheader("solver_trace (artifact)")
                st.json(trace)
            else:
                st.subheader("Solver annotations (best-effort)")
                flat = {}
                for k,v in art.items():
                    if isinstance(k,str) and (k.startswith("_solver") or k.startswith("_H98") or k.startswith("_Q")):
                        flat[k]=v
                kpis = art.get("kpis", {}) if isinstance(art.get("kpis"), dict) else {}
                for k,v in kpis.items():
                    if isinstance(k,str) and (k.startswith("_solver") or k.startswith("_H98") or k.startswith("_Q")):
                        flat[f"kpis.{k}"]=v
                if flat:
                    st.json(flat)
                else:
                    st.info("No solver trace fields found in artifact.")

            st.divider()
            st.subheader("CCFS verifier (external solver firewall)")
            st.caption("Verify an external candidate bundle against frozen truth. Runs do not modify SHAMS physics.")
            up_ccfs = st.file_uploader("Upload ccfs_bundle.json", type=["json"], key="ccfs_up_v294")
            b = _load_json_from_upload(up_ccfs)
            if isinstance(b, dict):
                req_cols = st.columns(2)
                with req_cols[0]:
                    do_phase = st.checkbox("Require phase envelope PASS", value=True, key="ccfs_req_phase_v294")
                with req_cols[1]:
                    do_uq = st.checkbox("Require UQ contract not FAIL", value=False, key="ccfs_req_uq_v294")

                if st.button("Verify CCFS bundle", use_container_width=True, key="ccfs_btn_v294"):
                    try:
                        from extopt.certified_solve import verify_ccfs_bundle
                        res = verify_ccfs_bundle(b, default_request={"phase_envelope": bool(do_phase), "uq_contracts": bool(do_uq)})
                        st.success("CCFS verification complete.")
                        st.json(res, expanded=False)
                        _v98_record_run("ccfs_verify", res, mode="ControlRoom/Diagnostics")
                    except Exception as e:
                        st.error(f"CCFS verification failed: {e}")


# --- Copyright notice
st.markdown('---')
st.caption('Â© 2026 Afshin Arjhangmehr - SHAMSâ€“FUSION-X')


# =====================
# v88 UI helpers (append-only)
# =====================
def _v88_continuation_explorer(path):
    import pandas as pd, streamlit as st
    if not path:
        st.info("No continuation path available.")
        return
    df = pd.DataFrame(path)
    st.line_chart(df.select_dtypes("number"))

def _v88_boundary_map(points):
    import pandas as pd, streamlit as st
    if not points:
        st.info("No scan points available.")
        return
    df = pd.DataFrame(points)
    if "x" in df.columns and "min_signed_margin" in df.columns:
        st.scatter_chart(df, x="x", y="min_signed_margin", color="feasible")

def _v88_export_svg(fig, name):
    import io, streamlit as st
    buf = io.BytesIO()
    fig.savefig(buf, format="svg", bbox_inches="tight")
    st.download_button(
        f"Download {name}.svg",
        data=buf.getvalue(),
        file_name=f"{name}.svg",
        mime="image/svg+xml",
    )


# =====================
# v89 UI helpers (append-only)
# =====================
def _v89_margin_waterfall_fig(constraints):
    import matplotlib.pyplot as plt
    rows = []
    for r in constraints or []:
        name = r.get("name","")
        sm = r.get("signed_margin", None)
        if name and isinstance(sm, (int,float)):
            rows.append((name, float(sm)))
    if not rows:
        return None
    rows.sort(key=lambda x: x[1])
    names = [n for n,_ in rows][:40]
    vals = [v for _,v in rows][:40]
    fig, ax = plt.subplots()
    ax.barh(names, vals)
    ax.axvline(0.0)
    ax.set_xlabel("signed_margin")
    ax.set_title("Constraint Margin Waterfall")
    fig.tight_layout()
    return fig

def _v89_boundary_heatmap(points, xcol, ycol):
    import numpy as np
    import matplotlib.pyplot as plt
    xs = [p.get(xcol) for p in points]
    ys = [p.get(ycol) for p in points]
    xs = [x for x in xs if isinstance(x,(int,float))]
    ys = [y for y in ys if isinstance(y,(int,float))]
    if not xs or not ys:
        return None
    x_min, x_max = min(xs), max(xs)
    y_min, y_max = min(ys), max(ys)
    nx, ny = 40, 40
    grid = np.full((ny, nx), np.nan)
    counts = np.zeros((ny, nx), dtype=int)
    hits = np.zeros((ny, nx), dtype=int)
    for p in points:
        x = p.get(xcol); y = p.get(ycol)
        if not isinstance(x,(int,float)) or not isinstance(y,(int,float)):
            continue
        ix = int((x - x_min) / (x_max - x_min + 1e-12) * (nx-1))
        iy = int((y - y_min) / (y_max - y_min + 1e-12) * (ny-1))
        counts[iy, ix] += 1
        hits[iy, ix] += 1 if p.get("feasible", False) else 0
    mask = counts > 0
    grid[mask] = hits[mask] / counts[mask]
    fig, ax = plt.subplots()
    im = ax.imshow(grid, origin="lower", extent=[x_min,x_max,y_min,y_max], aspect="auto")
    ax.set_xlabel(xcol)
    ax.set_ylabel(ycol)
    ax.set_title("Feasibility Boundary Map (binned feasibility fraction)")
    fig.colorbar(im, ax=ax, label="feasible fraction")
    fig.tight_layout()
    return fig

def _v89_constraint_intersections(points, topn=10):
    from collections import Counter
    c = Counter()
    for p in points:
        if p.get("feasible", False):
            continue
        acts = p.get("active_constraints", [])
        if not isinstance(acts, list):
            continue
        acts = [a for a in acts if isinstance(a,str) and a][:6]
        for i in range(len(acts)):
            for j in range(i+1, len(acts)):
                pair = tuple(sorted((acts[i], acts[j])))
                c[pair] += 1
    return c.most_common(topn)


# =====================
# v89.1 UI state persistence helpers (append-only)
# =====================
def _v89_1_render_cached_point():
    import streamlit as st
    if "pd_last_outputs" not in st.session_state or "pd_last_artifact" not in st.session_state:
        st.info("No cached Point Designer result yet. Click 'Evaluate Point' to compute one.")
        return
    st.info("Showing last Point Designer results (cached). Downloads should not clear results.")

    out = st.session_state["pd_last_outputs"]
    artifact = st.session_state["pd_last_artifact"]

    # KPI summary (best effort)
    try:
        with st.expander("Point summary (cached)", expanded=False):
            kpis = headline_kpis(out)
            for i in range(0, len(kpis), 4):
                kpi_row(kpis[i:i+4])
    except Exception:
        pass

    # Exports (cached bytes if available)
    with st.expander("Exports (external systems codes-style artifacts) - cached", expanded=False):
        try:
            st.download_button(
                "Download run artifact JSON",
                data=_shams_json_dumps(artifact, indent=2, sort_keys=True),
                file_name="shams_run_artifact.json",
                mime="application/json",
                use_container_width=True,
                key="pd_cached_artifact_json",
            )
        except Exception:
            pass

        # Radial build PNG (prefer cached bytes)
        state = st.session_state.get('shams_state', None)
        rb = (
            state.last_point_radial_png
            if state and getattr(state, 'last_point_radial_png', None) is not None
            else st.session_state.get("pd_last_radial_png_bytes", None)
        )
        if isinstance(rb, (bytes, bytearray)) and len(rb) > 0:
            st.download_button(
                "Download radial build PNG",
                data=rb,
                file_name="shams_radial_build.png",
                mime="image/png",
                use_container_width=True,
                key="pd_cached_radial_png",
            )
        else:
            # Generate on demand (still should not clear results since out/artifact is cached)
            try:
                tmpdir = tempfile.mkdtemp(prefix="shams_export_")
                radial_path = os.path.join(tmpdir, "radial_build.png")
                plot_radial_build_from_artifact(artifact, radial_path)
                with open(radial_path, "rb") as f:
                    st.download_button(
                        "Download radial build PNG",
                        data=f.read(),
                        file_name="shams_radial_build.png",
                        mime="image/png",
                        use_container_width=True,
                        key="pd_cached_radial_png_gen",
                    )
            except Exception:
                st.caption("Radial-build export unavailable for this point.")


# =====================
# v89.2 UI state persistence + controls (append-only)
# =====================
def _v89_2_point_cache_ui():
    import streamlit as st
    st.subheader("Cached Point Designer Result")
    c1, c2 = st.columns([1,3])
    with c1:
        if st.button("Clear cached result", key="pd_clear_cache"):
            for k in ["pd_last_outputs", "pd_last_artifact", "pd_last_radial_png_bytes"]:
                if k in st.session_state:
                    del st.session_state[k]
            st.success("Cleared.")
            st.stop()
    with c2:
        st.caption("Cached results persist across reruns (e.g., after downloads).")

    state = st.session_state.get('shams_state', None)
    art = (state.last_point_artifact if state and getattr(state,'last_point_artifact',None) is not None else st.session_state.get("pd_last_artifact", None))
    out = (state.last_point_outputs if state and getattr(state,'last_point_outputs',None) is not None else st.session_state.get("pd_last_outputs", None))
    if art is None or out is None:
        st.info("No cached result yet. Run Point Designer once to populate this.")
        return

    rb = (state.last_point_radial_png if state and getattr(state,'last_point_radial_png',None) is not None else st.session_state.get("pd_last_radial_png_bytes", None))
    if isinstance(rb, (bytes, bytearray)) and len(rb) > 0:
        st.image(rb, caption="Radial build (cached export preview)", use_container_width=True)

    import json as _json
    st.download_button(
        "Download run artifact JSON",
        data=_json.dumps(art, indent=2, sort_keys=True),
        file_name="shams_run_artifact.json",
        mime="application/json",
        use_container_width=True,
        key="pd_dl_artifact_cached",
    )
    if isinstance(rb, (bytes, bytearray)) and len(rb) > 0:
        st.download_button(
            "Download radial build PNG",
            data=rb,
            file_name="shams_radial_build.png",
            mime="image/png",
            use_container_width=True,
            key="pd_dl_radial_cached",
        )



# =====================
# v92 UI state-machine helpers (append-only)
# =====================
# (Defined early in Phase-1 stabilization block to avoid forward-reference failures.)

# v93 UI helpers (append-only)
# =====================
# -----------------------------
# JSON safety helpers (cycle-safe) - required for export/download stability
# -----------------------------
def _v93_validate_before_download(obj, schema_path: str):
    """Return (ok, errors). Uses jsonschema if available, else fallback."""
    import json as _json
    from pathlib import Path
    try:
        sch = _json.loads(Path(schema_path).read_text(encoding="utf-8"))
        try:
            import jsonschema  # type: ignore
            jsonschema.validate(instance=obj, schema=sch)
            return True, []
        except ImportError:
            from tools.validate_schemas import _fallback_validate
            errs = _fallback_validate(obj, sch)
            return (len(errs) == 0), errs
    except Exception as e:
        return False, [repr(e)]

def _v93_paper_figures_pack_panel():
    import streamlit as st
    s = _v92_state_get()
    st.subheader("Paper Figures Pack")
    if not s.has_point():
        st.info("Run Point Designer first; pack is built from cached Point artifact.")
        return
    try:
        from tools.paper_figures_pack import build_figures_pack_bytes
        if st.button("Build paper figures pack", key="v93_build_figpack"):
            s.last_figures_pack_zip = build_figures_pack_bytes(s.last_point_artifact)
            st.success("Figures pack built.")
        b = getattr(s, "last_figures_pack_zip", None)
        if isinstance(b, (bytes, bytearray)) and len(b) > 0:
            st.download_button(
                "Download paper_figures_pack.zip",
                data=b,
                file_name="paper_figures_pack.zip",
                mime="application/zip",
                use_container_width=True,
                key="v93_dl_figpack",
            )
    except Exception:
        st.caption("Figures pack unavailable.")


def _v93_stateful_scan_panel():
    import streamlit as st
    s = _v92_state_get()
    st.subheader("Scan Ledger (cached artifact)")
    if s.last_scan_points is None:
        st.info("No cached scan yet.")
        return
    scan_obj = {"kind":"shams_feasible_set","meta": s.last_scan_meta or {}, "points": s.last_scan_points}
    ok, errs = _v93_validate_before_download(scan_obj, "schemas/shams_feasible_set.schema.json")
    if ok: st.success("Schema: PASS")
    else:
        st.warning("Schema: FAIL")
        for e in errs[:10]: st.write("- " + str(e))
    st.download_button("Download feasible_scan.json (stateful)",
                       data=_json.dumps(scan_obj, indent=2, sort_keys=True),
                       file_name="feasible_scan.json", mime="application/json",
                       use_container_width=True, key="v93_dl_scan_stateful")

def _v93_stateful_systems_panel():
    import streamlit as st
    s = _v92_state_get()
    st.subheader("Run Ledger (cached artifacts)")
    if s.last_systems_result is None:
        if s.has_point():
            st.info("No cached systems yet (no successful Systems solve). Point artifact is available - run a Systems solve to generate a Systems artifact.")
        else:
            st.info("No cached systems yet.")
        return
    ok, errs = _v93_validate_before_download(s.last_systems_result, "schemas/shams_run_artifact.schema.json")
    if ok: st.success("Schema: PASS")
    else:
        st.warning("Schema: FAIL")
        for e in errs[:10]: st.write("- " + str(e))
    st.download_button("Download systems_artifact.json (stateful)",
                       data=_shams_json_dumps(s.last_systems_result, indent=2, sort_keys=True),
                       file_name="systems_artifact.json", mime="application/json",
                       use_container_width=True, key="v93_dl_systems_stateful")


def _v182_render_latest_systems_solve_results(*, artifact: dict, point_artifact: dict | None = None, key_prefix: str = "v182_latest"):
    """Render the latest Systems *solve* results from a cached run artifact.

    This MUST be safe across Streamlit reruns (e.g., download_button triggers rerun).
    Therefore it takes a fully self-contained artifact and does not rely on locals.
    """
    import streamlit as st
    import json

    # Schema hardening: upgrade + validate (non-fatal; never blocks UI)
    try:
        try:
            from src.systems.schema import upgrade_systems_artifact, validate_systems_artifact
        except Exception:  # pragma: no cover
            from systems.schema import upgrade_systems_artifact, validate_systems_artifact
        artifact = upgrade_systems_artifact(artifact or {})
        _issues = validate_systems_artifact(artifact)
        if _issues:
            with st.expander("Schema warnings (non-fatal)", expanded=False):
                for msg in _issues:
                    st.warning(msg)
    except Exception:
        pass
    try:
        outs = (artifact or {}).get('headline') or (artifact or {}).get('outputs') or {}
    except Exception:
        outs = {}

    st.markdown("### Latest Systems solve results")
    # Unique suffix so this function can be called multiple times in the same Streamlit run
    try:
        _rid = (artifact or {}).get("rid") or (artifact or {}).get("run_id") or (artifact or {}).get("metadata", {}).get("rid")
    except Exception:
        _rid = None
    try:
        _suffix = str(_rid) if _rid else __import__("hashlib").sha1(__import__("json").dumps(artifact, sort_keys=True).encode("utf-8")).hexdigest()[:10]
    except Exception:
        _suffix = "na"
    _k_art = f"{key_prefix}_dl_systems_artifact_{_suffix}"
    _k_zip = f"{key_prefix}_dl_systems_bundle_{_suffix}"

    # Always-available downloads (do not depend on a button-click run path)
    st.download_button(
        "Download systems-mode run artifact JSON",
        data=_shams_json_dumps(artifact, indent=2, sort_keys=True),
        file_name="shams_run_artifact_systems.json",
        mime="application/json",
        use_container_width=True,
        key=_k_art,
    )

    # Optional export bundle (safe; uses cached artifacts)
    try:
        from tools.export.bundle import build_export_bundle_bytes
        bundle_bytes = build_export_bundle_bytes(
            repo_root=BASE_DIR,
            point_artifact=point_artifact,
            systems_artifact=artifact,
            scan_artifact=None,
            pareto_artifact=None,
            opt_artifact=None,
            feasible_search_artifact=None,
            include_readme=True,
        )
        st.download_button(
            "Download export bundle ZIP",
            data=bundle_bytes,
            file_name="shams_export_bundle_systems.zip",
            mime="application/zip",
            use_container_width=True,
            key=_k_zip,
        )
    except Exception as _e:
        st.caption(f"Export bundle unavailable: {_e}")

    # Key results
    kcols = st.columns(4)
    def _k(col, key, fmt="{:.3g}"):
        try:
            v = float(outs.get(key, float('nan')))
        except Exception:
            v = float('nan')
        with col:
            st.metric(key, fmt.format(v) if v == v else "NaN")

    _k(kcols[0], "Q_DT_eqv")
    _k(kcols[1], "H98")
    _k(kcols[2], "P_e_net_MW")
    _k(kcols[3], "q_div_MW_m2")

    # Constraints table (if present)
    try:
        rows = (artifact or {}).get('constraints')
        if isinstance(rows, list) and rows:
            with st.expander("Constraints & margins (cached)", expanded=False):
                import pandas as pd
                st.dataframe(pd.DataFrame(rows), use_container_width=True)
    except Exception:
        pass

def _v184_render_latest_feasible_search_results(*, report: dict, key_prefix: str = "v184_fs_latest") -> None:
    """Rerun-safe renderer for Feasible Design Search results.

    The search is easy to 'lose' in Streamlit after a button click because the app reruns and
    the workflow step may change. This renderer provides a stable, cached view.
    """
    import streamlit as st
    try:
        import pandas as pd
    except Exception:
        pd = None  # type: ignore

    if not isinstance(report, dict) or not report:
        st.info("No cached feasible-search results yet.")
        return

    ok = bool(report.get('ok'))
    reason = str(report.get('reason', ''))
    obj = str(report.get('objective', ''))
    ts_unix = report.get('ts_unix')
    try:
        ts_str = datetime.datetime.fromtimestamp(float(ts_unix)).strftime('%Y-%m-%d %H:%M:%S') if ts_unix else ''
    except Exception:
        ts_str = ''

    if ok:
        st.success(f"Feasible Search: **OK** - {reason} - best objective: {report.get('best_obj')}  {('(' + ts_str + ')') if ts_str else ''}")
    else:
        st.warning(f"Feasible Search: **NO RESULT** - {reason}  {('(' + ts_str + ')') if ts_str else ''}")

    # Compact summary
    st.json({k: report.get(k) for k in ['ok','reason','objective','budget','topk','multi_seed_runs','radius','seed','vars','start_feasible','best_obj','best_V'] if k in report})

    # Download JSON (stable key prefix avoids duplicates)
    try:
        st.download_button(
            label="Download feasible-search artifact JSON",
            data=_shams_json_dumps(report, indent=2, sort_keys=True),
            file_name="feasible_search_artifact.json",
            mime="application/json",
            key=f"{key_prefix}_dl_fs_artifact",
            use_container_width=True,
        )
    except Exception:
        pass

    # Candidates table
    try:
        cands = list(report.get('candidates', []) or [])
        if cands and pd is not None:
            rows = []
            for i, c in enumerate(cands):
                x = c.get('x', {}) or {}
                m = c.get('margins', {}) or {}
                row = {'rank': i+1, 'obj': c.get('obj'), 'V': c.get('V'), 'feasible': c.get('feasible')}
                # Common margins (if present)
                for nm in ['q95','q_div','P_SOL/R','B_peak','sigma_vm','HTS margin','TBR','NWL']:
                    if nm in m:
                        row[f'm_{nm}'] = m.get(nm)
                # Variables
                for k in (report.get('vars', []) or []):
                    row[k] = x.get(k)
                rows.append(row)
            if rows:
                st.dataframe(pd.DataFrame(rows), use_container_width=True)
    except Exception:
        pass

def _v93_stateful_sandbox_panel():
    import streamlit as st
    s = _v92_state_get()
    st.subheader("ðŸ“Œ Forge Cache (stateful sandbox)")
    if s.last_sandbox_run is None:
        st.info("No cached sandbox yet.")
        return
    st.download_button("Download sandbox_run.json (stateful)",
                       data=_json.dumps(s.last_sandbox_run, indent=2, sort_keys=True),
                       file_name="sandbox_run.json", mime="application/json",
                       use_container_width=True, key="v93_dl_sandbox_stateful")


# =====================
# v94 Run Records + Unified Export (append-only)
# =====================
def _v94_record_run(kind: str, payload: dict):
    import time
    s = _v92_state_get()
    try:
        s.run_history.append({
            "ts": time.strftime("%Y-%m-%d %H:%M:%S"),
            "kind": kind,
            "summary": {
                "feasible": payload.get("meta", {}).get("feasible", payload.get("feasible", None)),
                "version": payload.get("version", None),
            },
        })
    except Exception:
        pass

def _v94_run_records_page():
    import streamlit as st
    s = _v92_state_get()
    st.subheader("Run Records")
    st.caption("Timeline of actions in this session.")
    if not s.run_history:
        st.info("No run records yet.")
        return
    for i, r in enumerate(reversed(s.run_history[-50:]), 1):
        st.write(f"{i}. {r.get('ts','?')} - {r.get('kind','?')} - {r.get('summary',{})}")

def _v94_unified_export_bundle_panel():
    import streamlit as st
    from pathlib import Path
    s = _v92_state_get()
    st.subheader("Unified Export Bundle")
    st.caption("One zip: artifacts + capsule + schemas + figures pack. Best-effort.")
    if not (s.has_point() or s.last_systems_result or s.last_scan_points or s.last_sandbox_run):
        st.info("Nothing cached yet. Run at least one mode first.")
        return

    scan_obj = None
    if s.last_scan_points is not None:
        scan_obj = {"kind":"shams_feasible_set","meta": s.last_scan_meta or {}, "points": s.last_scan_points}

    # Validation summary (best-effort)
    if s.has_point():
        ok, errs = _v93_validate_before_download(s.last_point_artifact, "schemas/shams_run_artifact.schema.json")
        if ok: st.success("Point artifact schema: PASS")
        else:
            st.warning("Point artifact schema: FAIL")
            for e in errs[:8]: st.write("- " + str(e))
    if scan_obj is not None:
        ok, errs = _v93_validate_before_download(scan_obj, "schemas/shams_feasible_set.schema.json")
        if ok: st.success("Scan feasible set schema: PASS")
        else:
            st.warning("Scan feasible set schema: FAIL")
            for e in errs[:8]: st.write("- " + str(e))

    try:
        from tools.export.bundle import build_export_bundle_bytes
        if st.button("Build unified export bundle", key="v94_build_bundle"):
            b = build_export_bundle_bytes(
                repo_root=Path("."),
                point_artifact=s.last_point_artifact if s.has_point() else None,
                systems_artifact=s.last_systems_result,
                feasible_search_artifact=getattr(s, "last_feasible_search_artifact", None),
                certified_search_artifact=st.session_state.get("last_certified_search_artifact", None),
                repair_evidence_artifact=st.session_state.get("last_repair_evidence_artifact", None),
                interval_refinement_artifact=st.session_state.get("last_interval_refinement_artifact", None),
                scan_artifact=scan_obj,
                pareto_artifact=getattr(s, "last_pareto_artifact", None),
                opt_artifact=getattr(s, "last_opt_artifact", None),
                sandbox_run=s.last_sandbox_run,
                figures_pack_zip=getattr(s, "last_figures_pack_zip", None),
            )
            st.session_state["v94_bundle_bytes"] = b
            st.success("Bundle built.")
        b = st.session_state.get("v94_bundle_bytes", None)
        if isinstance(b, (bytes, bytearray)) and len(b) > 0:
            st.download_button("Download shams_export_bundle.zip", data=b,
                               file_name="shams_export_bundle.zip", mime="application/zip",
                               use_container_width=True, key="v94_dl_bundle")
    except Exception as e:
        st.error(f"Bundle builder unavailable: {e!r}")



# =====================
# v98 validation gate (append-only)
# =====================
def _v98_validation_gate_ui(title: str, ok: bool, errs):
    import streamlit as st
    if ok:
        st.success(f"{title}: schema PASS")
        return True
    st.error(f"{title}: schema FAIL (download allowed, but NOT publishable)")
    for e in (errs or [])[:12]:
        st.write("- " + str(e))
    return False


# =====================

# =====================
# v98 Run Ledger (append-only)
# =====================
def _v98_state_init_runlists():
    import streamlit as st
    s = _v92_state_get()
    if getattr(s, "run_history", None) is None: s.run_history = []
    if getattr(s, "pinned_run_ids", None) is None: s.pinned_run_ids = []
    # v130: persistent run vault (opt-in, default ON)
    if 'vault_enabled' not in st.session_state:
        st.session_state['vault_enabled'] = True
    if 'vault_limit' not in st.session_state:
        st.session_state['vault_limit'] = 50
    return s

def _v98_make_run_id(prefix: str) -> str:
    import time, random
    return f"{prefix}_{int(time.time())}_{random.randint(1000,9999)}"

def _v98_record_run(kind: str, payload, mode: str = "") -> str:
    import streamlit as st
    from tools import run_vault
    from pathlib import Path
    import time
    s = _v98_state_init_runlists()
    rid = _v98_make_run_id(kind)
    s.run_history.append({"id": rid, "ts": time.strftime("%Y-%m-%d %H:%M:%S"), "kind": kind, "mode": mode, "payload": payload})
    try:
        _alog(mode or kind, 'RecordRun', {'rid': rid, 'kind': kind})
    except Exception:
        pass  # ActivityLogRecordRun

    # v130: persist to vault (storage only) - must never break UI
    try:
        if bool(st.session_state.get("vault_enabled", True)):
            root = Path(__file__).resolve().parents[1]
            run_vault.write_entry(root=root, kind=kind, payload=payload, mode=mode, tags={"rid": rid})
    except Exception:
        pass

    return rid


def _v98_json_diff(a, b, path=""):
    diffs = []
    if type(a) != type(b):
        diffs.append(path or "<root>"); return diffs
    if isinstance(a, dict):
        keys = set(a.keys()) | set(b.keys())
        for k in sorted(keys):
            diffs += _v98_json_diff(a.get(k, "<missing>"), b.get(k, "<missing>"), (path + "/" + str(k)) if path else "/" + str(k))
        return diffs
    if isinstance(a, list):
        n = max(len(a), len(b))
        for i in range(n):
            diffs += _v98_json_diff(a[i] if i < len(a) else "<missing>", b[i] if i < len(b) else "<missing>", f"{path}[{i}]")
        return diffs
    if a != b: diffs.append(path or "<root>")
    return diffs

def _v98_run_ledger_page():
    import streamlit as st
    s = _v98_state_init_runlists()
    st.subheader("Run Ledger")
    st.caption("Session-persistent run artifacts with pin/compare.")
    if not s.run_history:
        st.info("No recorded runs yet.")
        return
    for r in reversed(s.run_history[-100:]):
        rid = r.get("id")
        c1,c2,c3 = st.columns([3,1,1])
        with c1: st.write(f"**{rid}** - {r.get('ts')} - {r.get('kind')} ({r.get('mode')})")
        with c2:
            pinned = rid in s.pinned_run_ids
            if st.button("Unpin" if pinned else "Pin", key=f"pin_{rid}"):
                if pinned: s.pinned_run_ids.remove(rid)
                else: s.pinned_run_ids.append(rid)
                st.rerun()
        with c3:
            st.download_button("JSON", data=_json.dumps(r.get("payload", {}), indent=2, sort_keys=True), file_name=f"{rid}.json", mime="application/json", key=f"dl_{rid}")
    st.divider()
    st.subheader("Compare two pinned runs")
    pins = list(s.pinned_run_ids)
    if len(pins) < 2:
        st.info("Pin at least two runs to compare.")
        return
    a_id = st.selectbox("Run A", pins, key="cmp_a")
    b_id = st.selectbox("Run B", pins, index=1, key="cmp_b")
    A = next((x for x in s.run_history if x.get("id")==a_id), None)
    B = next((x for x in s.run_history if x.get("id")==b_id), None)
    if A and B:
        diffs = _v98_json_diff(A.get("payload",{}), B.get("payload",{}))
        st.write(f"Changed fields: {len(diffs)}")
        for d in diffs[:200]: st.write("- " + d)


def _v98_process_handoff_panel():
    import streamlit as st
    s = _v92_state_get()
    st.subheader("external systems codes Handoff")
    st.caption("Exports a external systems codes-oriented handoff JSON (SHAMS upstream feasibility auditor).")
    if not s.has_point():
        st.info("Run Point Designer first.")
        return
    from tools.interoperability.process_handoff import make_process_handoff
    ho = make_process_handoff(s.last_point_artifact)
    ok, errs = _v93_validate_before_download(ho, "schemas/process_handoff.schema.json")
    _v98_validation_gate_ui("external systems codes handoff", ok, errs)
    st.download_button("Download process_handoff.json", data=_json.dumps(ho, indent=2, sort_keys=True),
                       file_name="process_handoff.json", mime="application/json", use_container_width=True, key="v98_dl_process_handoff")


# =====================
# v99 Session Report Export (append-only)
# =====================
def _v99_session_report_panel():
    import streamlit as st
    s = _v98_state_init_runlists()
    st.subheader("Session Report Export")
    st.caption("Exports a single zip: run ledger + pinned runs + diffs. Optional: include unified export bundle if built.")
    if not s.run_history:
        st.info("No runs recorded yet.")
        return

    include_bundle = st.checkbox("Include unified export bundle (if already built)", value=True, key="v99_inc_bundle")
    if st.button("Build session report zip", key="v99_build_report"):
        try:
            from tools.export.session_report import build_session_report_zip
            bundle = st.session_state.get("v94_bundle_bytes", None) if include_bundle else None
            b = build_session_report_zip(
                version=str(getattr(s, "version", None) or "v99"),
                run_history=list(s.run_history),
                pinned_ids=list(s.pinned_run_ids or []),
                unified_export_bundle_bytes=bundle if isinstance(bundle, (bytes, bytearray)) else None,
            )
            st.session_state["v99_session_report_zip"] = b
            st.success("Session report built.")
        except Exception as e:
            st.error(f"Failed to build session report: {e!r}")

    b = st.session_state.get("v99_session_report_zip", None)
    if isinstance(b, (bytes, bytearray)) and len(b) > 0:
        st.download_button(
            "Download shams_session_report.zip",
            data=b,
            file_name="shams_session_report.zip",
            mime="application/zip",
            use_container_width=True,
            key="v99_dl_report",
        )


# =====================
# v103 Audit Pack + Atlas + Sandbox Plus panels (append-only)
# =====================
def _v103_audit_pack_panel():
    import streamlit as st
    s = _v98_state_init_runlists()
    st.subheader("Audit Pack")
    st.caption("Single zip: selected artifacts + schemas + environment + manifest (journal/regulator-ready).")
    ids = [r.get("id") for r in (s.run_history or []) if r.get("id")]
    picked = st.multiselect("Include runs", options=ids, default=list(s.pinned_run_ids or []), key="v103_audit_pick")
    include_pf = st.checkbox("Include pip freeze (best-effort)", value=True, key="v103_audit_pf")
    if st.button("Build audit pack zip", key="v103_build_audit"):
        try:
            from tools.audit_pack import build_audit_pack_zip
            run_map = {r.get("id"): r for r in (s.run_history or []) if r.get("id")}
            arts = []
            for rid in picked:
                r = run_map.get(rid)
                if r and isinstance(r.get("payload"), dict):
                    arts.append(r["payload"])
            b = build_audit_pack_zip(version="v103", artifacts=arts, schema_dir="schemas", include_pip_freeze=include_pf)
            st.session_state["v103_audit_zip"] = b
            st.success("Audit pack built.")
        except Exception as e:
            st.error(f"Failed: {e!r}")
    b = st.session_state.get("v103_audit_zip")
    if isinstance(b, (bytes, bytearray)) and len(b) > 0:
        st.download_button("Download shams_audit_pack.zip", data=b, file_name="shams_audit_pack.zip", mime="application/zip", use_container_width=True, key="v103_dl_audit")

def _v103_atlas_panel():
    import streamlit as st
    st.subheader("Feasibility Boundary Atlas")
    st.caption("Deterministic nearest-feasible sweeps using SHAMS frontier search (audit-ready).")
    s = _v92_state_get()
    if not getattr(s, "last_point_inp", None):
        st.info("Run Point Designer first (uses last point inputs).")
        return
    base = s.last_point_inp
    # Default lever bounds (conservative, user-editable)
    levers = {
        "R0_m": (max(0.5, float(getattr(base, "R0_m", 2.0))*0.7), float(getattr(base, "R0_m", 2.0))*1.3),
        "a_m": (max(0.2, float(getattr(base, "a_m", 0.6))*0.7), float(getattr(base, "a_m", 0.6))*1.3),
        "Bt_T": (max(1.0, float(getattr(base, "Bt_T", 12.0))*0.7), float(getattr(base, "Bt_T", 12.0))*1.3),
        "Ip_MA": (max(0.1, float(getattr(base, "Ip_MA", 8.0))*0.7), float(getattr(base, "Ip_MA", 8.0))*1.3),
        "fG": (max(0.1, float(getattr(base, "fG", 0.8))*0.7), min(1.2, float(getattr(base, "fG", 0.8))*1.3)),
    }
    n_random = st.slider("Random samples", 20, 300, 80, 10, key="v103_atlas_n")
    seed = st.number_input("Seed", value=0, step=1, key="v103_atlas_seed")
    if st.button("Build Atlas", key="v103_build_atlas"):
        try:
            from tools.frontier_atlas import build_feasibility_atlas
            atlas = build_feasibility_atlas(base, levers=levers, targets=None, n_random=int(n_random), seed=int(seed), n_slices=5)
            st.session_state["v103_atlas"] = atlas
            _v98_record_run("atlas", atlas, mode="feasibility_atlas")
            st.success("Atlas built and recorded in Run Ledger.")
        except Exception as e:
            st.error(f"Atlas failed: {e!r}")
    atlas = st.session_state.get("v103_atlas")
    if isinstance(atlas, dict):
        st.write("Reports:", int(atlas.get("n_reports", 0)))
        st.download_button("Download feasibility_atlas.json", data=_json.dumps(atlas, indent=2, sort_keys=True),
                           file_name="feasibility_atlas.json", mime="application/json", use_container_width=True, key="v103_dl_atlas")

def _v103_sandbox_plus_panel():
    import streamlit as st
    st.subheader("Optimizer Sandbox Plus")
    st.caption("Safe orchestration layer: random/LHS exploration with feasibility-first behavior and full logs.")
    s = _v92_state_get()
    if not getattr(s, "last_point_inp", None):
        st.info("Run Point Designer first (uses last point inputs as baseline).")
        return
    base = s.last_point_inp
    strat = st.selectbox("Strategy", ["random", "lhs"], index=0, key="v103_sb_strat")
    obj = st.selectbox("Objective", ["min_R0", "min_Bpeak", "max_Pnet", "min_recirc"], index=0, key="v103_sb_obj")
    max_evals = st.slider("Max evals", 20, 1000, 200, 20, key="v103_sb_evals")
    seed = st.number_input("Seed", value=0, step=1, key="v103_sb_seed")
    levers = {
        "R0_m": (max(0.5, float(getattr(base, "R0_m", 2.0))*0.7), float(getattr(base, "R0_m", 2.0))*1.3),
        "a_m": (max(0.2, float(getattr(base, "a_m", 0.6))*0.7), float(getattr(base, "a_m", 0.6))*1.3),
        "Bt_T": (max(1.0, float(getattr(base, "Bt_T", 12.0))*0.7), float(getattr(base, "Bt_T", 12.0))*1.3),
        "Ip_MA": (max(0.1, float(getattr(base, "Ip_MA", 8.0))*0.7), float(getattr(base, "Ip_MA", 8.0))*1.3),
        "fG": (max(0.1, float(getattr(base, "fG", 0.8))*0.7), min(1.2, float(getattr(base, "fG", 0.8))*1.3)),
    }
    if st.button("Run Sandbox Plus", key="v103_run_sandbox_plus"):
        try:
            from tools.sandbox_plus import run_sandbox
            run = run_sandbox(base, levers=levers, objective=obj, max_evals=int(max_evals), seed=int(seed), strategy=strat)
            st.session_state["v103_sandbox_plus"] = run
            _v98_record_run("sandbox_plus", run, mode="optimizer_sandbox_plus")
            st.success("Sandbox run complete and recorded.")
        except Exception as e:
            st.error(f"Sandbox failed: {e!r}")
    run = st.session_state.get("v103_sandbox_plus")
    if isinstance(run, dict):
        st.download_button("Download sandbox_plus.json", data=_json.dumps(run, indent=2, sort_keys=True),
                           file_name="sandbox_plus.json", mime="application/json", use_container_width=True, key="v103_dl_sandbox_plus")


# =====================
# v104 Feasible Space Topology (append-only)
# =====================
def _v104_topology_panel():
    import streamlit as st
    from tools.topology import build_feasible_topology, extract_feasible_points_from_payload

    st.subheader("Feasible Space Topology")
    st.caption("Builds a connectivity graph of feasible designs (components = design 'islands').")

    s = _v98_state_init_runlists()
    if not s.run_history:
        st.info("No runs recorded yet.")
        return

    # pick source runs (default pinned)
    ids = [r.get("id") for r in (s.run_history or []) if r.get("id")]
    default_ids = list(s.pinned_run_ids or [])
    picked = st.multiselect("Source runs", options=ids, default=default_ids, key="v104_topology_pick")
    eps = st.slider("Connectivity threshold (scaled distance)", 0.05, 0.50, 0.18, 0.01, key="v104_topology_eps")
    max_pts = st.slider("Max points (cap)", 50, 600, 300, 10, key="v104_topology_cap")

    if st.button("Build topology", key="v104_build_topology"):
        try:
            run_map = {r.get("id"): r for r in (s.run_history or []) if r.get("id")}
            pts = []
            for rid in picked:
                payload = (run_map.get(rid) or {}).get("payload")
                if isinstance(payload, dict):
                    pts += extract_feasible_points_from_payload(payload)
            topo = build_feasible_topology(pts, eps=float(eps), max_points=int(max_pts))
            st.session_state["v104_topology"] = topo
            _v98_record_run("topology", topo, mode="feasible_topology")
            st.success(f"Topology built: {topo.get('n_points',0)} points, {len(topo.get('components',[]))} components.")
        except Exception as e:
            st.error(f"Topology build failed: {e!r}")

    topo = st.session_state.get("v104_topology")
    if isinstance(topo, dict):
        comps = topo.get("components", [])
        st.write({
            "points": int(topo.get("n_points", 0)),
            "edges": int(topo.get("n_edges", 0)),
            "components": int(len(comps) if isinstance(comps, list) else 0),
            "largest_component": int(len(comps[0]) if isinstance(comps, list) and comps else 0),
        })
        if isinstance(comps, list) and comps:
            st.write("Component sizes:", [len(c) for c in comps[:10]])
        st.download_button("Download feasible_topology.json",
                           data=_json.dumps(topo, indent=2, sort_keys=True),
                           file_name="feasible_topology.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v104_dl_topology")


# =====================
# v105 Constraint Dominance & Sensitivity (append-only)
# =====================
def _v105_constraint_dominance_panel():
    import streamlit as st
    from tools.constraint_dominance import build_constraint_dominance_report

    st.subheader("Constraint Dominance")
    st.caption("Ranks which constraints most strongly limit feasibility (failures + near-boundary).")

    s = _v98_state_init_runlists()
    if not s.run_history:
        st.info("No runs recorded yet.")
        return

    ids = [r.get("id") for r in (s.run_history or []) if r.get("id")]
    default_ids = list(s.pinned_run_ids or [])
    picked = st.multiselect("Source runs (run artifacts only)", options=ids, default=default_ids, key="v105_dom_pick")
    near = st.slider("Near-boundary threshold (margin_frac)", 0.00, 0.25, 0.05, 0.01, key="v105_dom_near")
    fail_w = st.slider("Failure weight", 1.0, 10.0, 4.0, 0.5, key="v105_dom_failw")

    if st.button("Build dominance report", key="v105_build_dom"):
        try:
            run_map = {r.get("id"): r for r in (s.run_history or []) if r.get("id")}
            payloads = []
            for rid in picked:
                payload = (run_map.get(rid) or {}).get("payload")
                if isinstance(payload, dict) and payload.get("kind") == "shams_run_artifact":
                    payloads.append(payload)
            rep = build_constraint_dominance_report(payloads, near_threshold=float(near), fail_weight=float(fail_w))
            st.session_state["v105_dom"] = rep
            _v98_record_run("dominance", rep, mode="constraint_dominance")
            st.success(f"Built dominance report for {rep.get('n_rows',0)} constraint rows.")
        except Exception as e:
            st.error(f"Dominance failed: {e!r}")

    rep = st.session_state.get("v105_dom")
    if isinstance(rep, dict):
        ranked = rep.get("constraints_ranked", [])
        if isinstance(ranked, list) and ranked:
            top = ranked[:8]
            st.write([{"name": r.get("name"), "score": r.get("dominance_score"), "fail_rate": r.get("fail_rate"), "near_rate": r.get("near_boundary_rate")} for r in top])
        st.download_button("Download constraint_dominance_report.json",
                           data=_json.dumps(rep, indent=2, sort_keys=True),
                           file_name="constraint_dominance_report.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v105_dl_dom")


# =====================
# v106 Failure Mode Taxonomy (append-only)
# =====================
def _v106_failure_taxonomy_panel():
    import streamlit as st
    from tools.failure_taxonomy import build_failure_taxonomy_report

    st.subheader("Failure Mode Taxonomy")
    st.caption("Classifies infeasible run artifacts by dominant failing constraint and aggregates failure modes.")

    s = _v98_state_init_runlists()
    if not s.run_history:
        st.info("No runs recorded yet.")
        return

    ids = [r.get("id") for r in (s.run_history or []) if r.get("id")]
    default_ids = list(s.pinned_run_ids or [])
    picked = st.multiselect("Source runs (run artifacts only)", options=ids, default=default_ids, key="v106_fail_pick")

    if st.button("Build failure taxonomy", key="v106_build_fail"):
        try:
            run_map = {r.get("id"): r for r in (s.run_history or []) if r.get("id")}
            payloads = []
            for rid in picked:
                payload = (run_map.get(rid) or {}).get("payload")
                if isinstance(payload, dict) and payload.get("kind") == "shams_run_artifact":
                    payloads.append(payload)
            rep = build_failure_taxonomy_report(payloads)
            st.session_state["v106_fail"] = rep
            _v98_record_run("failures", rep, mode="failure_taxonomy")
            st.success(f"Built failure taxonomy for {rep.get('n_failures',0)} failing runs.")
        except Exception as e:
            st.error(f"Failure taxonomy failed: {e!r}")

    rep = st.session_state.get("v106_fail")
    if isinstance(rep, dict):
        st.write({"failures": rep.get("n_failures"), "modes": len(rep.get("counts_by_mode",{}))})
        top = list(rep.get("counts_by_mode", {}).items())[:10]
        if top:
            st.write([{"mode": k, "count": v} for k,v in top])
        st.download_button("Download failure_taxonomy_report.json",
                           data=_json.dumps(rep, indent=2, sort_keys=True),
                           file_name="failure_taxonomy_report.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v106_dl_fail")


# =====================
# v107 Feasibility Science Pack (append-only)
# =====================
def _v107_science_pack_panel():
    import streamlit as st
    from tools.science_pack import build_feasibility_science_pack

    st.subheader("Feasibility Science Pack")
    st.caption("One-click export: topology + dominance + failures + publishable report + zip.")

    s = _v98_state_init_runlists()
    if not s.run_history:
        st.info("No runs recorded yet.")
        return

    # Pull latest artifacts from session state if present, else try to find from run history
    topo = st.session_state.get("v104_topology")
    dom = st.session_state.get("v105_dom")
    fail = st.session_state.get("v106_fail")

    # fallback: scan run history for latest by mode
    def _latest_by_mode(mode_name: str):
        for r in reversed(s.run_history or []):
            if (r.get("mode") == mode_name) and isinstance(r.get("payload"), dict):
                return r.get("payload")
        return None

    if topo is None:
        topo = _latest_by_mode("feasible_topology")
    if dom is None:
        dom = _latest_by_mode("constraint_dominance")
    if fail is None:
        fail = _latest_by_mode("failure_taxonomy")

    ready = isinstance(topo, dict) and isinstance(dom, dict) and isinstance(fail, dict)
    st.write({
        "topology_loaded": isinstance(topo, dict),
        "dominance_loaded": isinstance(dom, dict),
        "failures_loaded": isinstance(fail, dict),
        "ready": ready,
    })
    if not ready:
        st.info("Build topology, dominance, and failure taxonomy first.")
        return

    version = st.text_input("Pack version label", value="v107", key="v107_pack_version")
    if st.button("Build Feasibility Science Pack", key="v107_build_pack"):
        try:
            pack = build_feasibility_science_pack(
                topology=topo,
                dominance=dom,
                failures=fail,
                source_run_ids=list(s.pinned_run_ids or []),
                version=str(version),
            )
            st.session_state["v107_pack"] = pack
            # record summary only (zip bytes excluded from run ledger)
            _v98_record_run("science_pack", {"kind": "shams_feasibility_science_pack_summary", "summary": pack.get("summary")}, mode="feasibility_science_pack")
            st.success("Science pack built.")
        except Exception as e:
            st.error(f"Science pack failed: {e!r}")

    pack = st.session_state.get("v107_pack")
    if isinstance(pack, dict):
        st.write(pack.get("summary", {}))
        zip_bytes = pack.get("zip_bytes")
        if isinstance(zip_bytes, (bytes, bytearray)):
            st.download_button(
                "Download feasibility_science_pack.zip",
                data=bytes(zip_bytes),
                file_name="feasibility_science_pack.zip",
                mime="application/zip",
                use_container_width=True,
                key="v107_dl_pack_zip",
            )
        manifest = dict(pack)
        manifest.pop("zip_bytes", None)
        st.download_button(
            "Download science_pack_manifest.json",
            data=_json.dumps(manifest, indent=2, sort_keys=True),
            file_name="science_pack_manifest.json",
            mime="application/json",
            use_container_width=True,
            key="v107_dl_pack_manifest",
        )


# =====================
# v108 external systems codes Downstream Export (append-only)
# =====================
def _v108_process_downstream_panel():
    import streamlit as st
    from tools.process_downstream import build_process_downstream_bundle

    st.subheader("external systems codes Downstream Export")
    st.caption("Exports SHAMS run artifacts to a transparent (systems-code-inspired) table so external systems codes becomes downstream.")

    s = _v98_state_init_runlists()
    if not s.run_history:
        st.info("No runs recorded yet.")
        return

    ids = [r.get("id") for r in (s.run_history or []) if r.get("id")]
    default_ids = list(s.pinned_run_ids or [])
    picked = st.multiselect("Run artifacts to export", options=ids, default=default_ids, key="v108_proc_pick")
    version = st.text_input("Export version label", value="v108", key="v108_proc_version")

    if st.button("Build external systems codes downstream bundle", key="v108_proc_build"):
        try:
            run_map = {r.get("id"): r for r in (s.run_history or []) if r.get("id")}
            payloads = []
            for rid in picked:
                payload = (run_map.get(rid) or {}).get("payload")
                if isinstance(payload, dict) and payload.get("kind") == "shams_run_artifact":
                    payloads.append(payload)
            pack = build_process_downstream_bundle(payloads, version=str(version), source_run_ids=list(picked))
            st.session_state["v108_proc_pack"] = pack
            # record summary only
            _v98_record_run("process_downstream", {"kind":"shams_process_downstream_summary","summary":pack.get("summary")}, mode="process_downstream")
            st.success("external systems codes downstream bundle built.")
        except Exception as e:
            st.error(f"Export failed: {e!r}")

    pack = st.session_state.get("v108_proc_pack")
    if isinstance(pack, dict):
        st.write(pack.get("summary", {}))
        zip_bytes = pack.get("zip_bytes")
        if isinstance(zip_bytes, (bytes, bytearray)):
            st.download_button(
                "Download process_downstream_bundle.zip",
                data=bytes(zip_bytes),
                file_name="process_downstream_bundle.zip",
                mime="application/zip",
                use_container_width=True,
                key="v108_proc_dl_zip",
            )
        manifest = dict(pack)
        manifest.pop("zip_bytes", None)
        st.download_button(
            "Download process_downstream_manifest.json",
            data=_json.dumps(manifest, indent=2, sort_keys=True),
            file_name="process_downstream_manifest.json",
            mime="application/json",
            use_container_width=True,
            key="v108_proc_dl_manifest",
        )


# =====================
# v109 Island Inspector (append-only)
# =====================
def _v109_island_inspector_panel():
    import streamlit as st
    from tools.component_dominance import build_component_dominance_report

    st.subheader("Island Inspector")
    st.caption("Per-feasible-island dominance + boundary-near failure modes.")

    s = _v98_state_init_runlists()
    # Get topology + failure taxonomy from session/run ledger
    topo = st.session_state.get("v104_topology")
    dom = st.session_state.get("v105_dom")  # not required, but should exist for prior steps
    fail = st.session_state.get("v106_fail")

    def _latest_by_mode(mode_name: str):
        for r in reversed(s.run_history or []):
            if (r.get("mode") == mode_name) and isinstance(r.get("payload"), dict):
                return r.get("payload")
        return None

    if topo is None:
        topo = _latest_by_mode("feasible_topology")
    if fail is None:
        fail = _latest_by_mode("failure_taxonomy")

    st.write({"topology_loaded": isinstance(topo, dict), "failures_loaded": isinstance(fail, dict)})
    if not isinstance(topo, dict):
        st.info("Build topology first.")
        return

    # choose run artifacts to assign
    ids = [r.get("id") for r in (s.run_history or []) if r.get("id")]
    default_ids = list(s.pinned_run_ids or [])
    picked = st.multiselect("Run artifacts to use (feasible ones define islands)", options=ids, default=default_ids, key="v109_pick_runs")
    near = st.slider("Near-boundary threshold (margin_frac)", 0.00, 0.25, 0.05, 0.01, key="v109_near")
    fail_w = st.slider("Failure weight", 1.0, 10.0, 4.0, 0.5, key="v109_failw")

    if st.button("Build component dominance report", key="v109_build"):
        try:
            run_map = {r.get("id"): r for r in (s.run_history or []) if r.get("id")}
            payloads = []
            for rid in picked:
                payload = (run_map.get(rid) or {}).get("payload")
                if isinstance(payload, dict) and payload.get("kind") == "shams_run_artifact":
                    payloads.append(payload)
            rep = build_component_dominance_report(topology=topo, run_artifacts=payloads, failure_taxonomy=fail, near_threshold=float(near), fail_weight=float(fail_w))
            st.session_state["v109_components"] = rep
            _v98_record_run("islands", rep, mode="component_dominance")
            st.success(f"Built component report for {rep.get('n_components',0)} components.")
        except Exception as e:
            st.error(f"v109 failed: {e!r}")

    rep = st.session_state.get("v109_components")
    if isinstance(rep, dict):
        comps = rep.get("components", [])
        if isinstance(comps, list) and comps:
            st.write("Top components (by size):")
            st.write([{
                "component": c.get("component_index"),
                "size_pts": c.get("component_size_points"),
                "runs": c.get("n_feasible_runs_assigned"),
                "top_constraint": (c.get("dominance_top_constraints") or [{}])[0].get("name") if isinstance(c.get("dominance_top_constraints"), list) and c.get("dominance_top_constraints") else None,
                "top_failure_mode": (c.get("top_failure_modes_near_component") or [{}])[0].get("mode") if isinstance(c.get("top_failure_modes_near_component"), list) and c.get("top_failure_modes_near_component") else None,
            } for c in comps[:12]])
        st.download_button("Download component_dominance_report.json",
                           data=_json.dumps(rep, indent=2, sort_keys=True),
                           file_name="component_dominance_report.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v109_dl_report")


# =====================
# v110 Boundary Atlas v2 (append-only)
# =====================
def _v110_boundary_atlas_panel():
    import streamlit as st
    from tools.boundary_atlas_v2 import build_boundary_atlas_v2
    from tools.plot_boundary_atlas_v2 import main as _plot_cli  # not used directly

    st.subheader("Feasibility Boundary Atlas v2")
    st.caption("Extracts explicit feasible/infeasible boundaries for lever-pair slices, with failure-mode labels (best-effort).")

    s = _v98_state_init_runlists()
    topo = st.session_state.get("v104_topology")
    fail = st.session_state.get("v106_fail")

    def _latest_by_mode(mode_name: str):
        for r in reversed(s.run_history or []):
            if (r.get("mode") == mode_name) and isinstance(r.get("payload"), dict):
                return r.get("payload")
        return None

    if fail is None:
        fail = _latest_by_mode("failure_taxonomy")

    # Use pinned runs as sources; include atlas/sandbox/topology artifacts if present in history
    ids = [r.get("id") for r in (s.run_history or []) if r.get("id")]
    default_ids = list(s.pinned_run_ids or [])
    picked = st.multiselect("Source run artifacts (infeasible + feasible)", options=ids, default=default_ids, key="v110_pick_runs")
    q = st.slider("Boundary proximity quantile (lower = tighter)", 0.05, 0.80, 0.25, 0.05, key="v110_q")
    maxpairs = st.slider("Max lever pairs", 1, 8, 6, 1, key="v110_maxpairs")

    if st.button("Build Boundary Atlas v2", key="v110_build"):
        try:
            run_map = {r.get("id"): r for r in (s.run_history or []) if r.get("id")}
            payloads = []
            for rid in picked:
                payload = (run_map.get(rid) or {}).get("payload")
                if isinstance(payload, dict) and payload.get("kind") == "shams_run_artifact":
                    payloads.append(payload)
            rep = build_boundary_atlas_v2(payloads, failure_taxonomy=fail, max_pairs=int(maxpairs), proximity_quantile=float(q))
            st.session_state["v110_boundary_atlas"] = rep
            _v98_record_run("atlas_v2", rep, mode="boundary_atlas_v2")
            st.success(f"Built atlas slices: {len(rep.get('slices', []))}")
        except Exception as e:
            st.error(f"v110 failed: {e!r}")

    rep = st.session_state.get("v110_boundary_atlas")
    if isinstance(rep, dict):
        st.write({"slices": len(rep.get("slices", [])), "lever_pairs": rep.get("lever_pairs")})
        st.download_button("Download boundary_atlas_v2.json",
                           data=_json.dumps(rep, indent=2, sort_keys=True),
                           file_name="boundary_atlas_v2.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v110_dl_json")


# =====================
# v111 Design Family Explorer (append-only)
# =====================
def _v111_design_family_panel():
    import streamlit as st
    from tools.design_family import build_design_family_report

    st.subheader("Design Family Explorer")
    st.caption("Safe local exploration within a feasible island (no optimization).")

    s = _v98_state_init_runlists()
    topo = st.session_state.get("v104_topology")
    def _latest_by_mode(mode_name: str):
        for r in reversed(s.run_history or []):
            if (r.get("mode") == mode_name) and isinstance(r.get("payload"), dict):
                return r.get("payload")
        return None
    if topo is None:
        topo = _latest_by_mode("feasible_topology")

    if not isinstance(topo, dict):
        st.info("Build topology first.")
        return

    comps = topo.get("components", [])
    ncomp = len(comps) if isinstance(comps, list) else 0
    st.write({"n_components": ncomp})

    # choose baseline run (for full PointInputs)
    ids = [r.get("id") for r in (s.run_history or []) if r.get("id")]
    default_id = (list(s.pinned_run_ids or [])[:1] or [ids[0] if ids else None])[0]
    baseline_id = st.selectbox("Baseline run artifact (provides full inputs)", options=[None] + ids, index=(1 if default_id in ids else 0), key="v111_base_id")
    comp_idx = st.number_input("Component index", min_value=0, max_value=max(0, ncomp-1), value=0, step=1, key="v111_comp_idx")
    n_samples = st.slider("Samples", 10, 240, 120, 10, key="v111_ns")
    radius = st.slider("Local radius (fraction of lever span)", 0.01, 0.25, 0.08, 0.01, key="v111_rad")
    seed = st.number_input("Seed", min_value=0, max_value=10_000_000, value=0, step=1, key="v111_seed")

    if st.button("Run Design Family Explorer", key="v111_run"):
        try:
            run_map = {r.get("id"): r for r in (s.run_history or []) if r.get("id")}
            payload = (run_map.get(baseline_id) or {}).get("payload") if baseline_id else None
            if not (isinstance(payload, dict) and payload.get("kind") == "shams_run_artifact"):
                st.error("Baseline run artifact not found. Pin/select a valid run artifact first.")
                return
            base_inputs = payload.get("inputs", {})
            if not isinstance(base_inputs, dict):
                st.error("Baseline inputs invalid.")
                return

            rep = build_design_family_report(
                topology=topo,
                component_index=int(comp_idx),
                baseline_inputs=base_inputs,
                n_samples=int(n_samples),
                radius_frac=float(radius),
                seed=int(seed),
            )
            st.session_state["v111_family"] = rep
            _v98_record_run("design_family", rep, mode="design_family")
            st.success("Design family report built.")
        except Exception as e:
            st.error(f"v111 failed: {e!r}")

    rep = st.session_state.get("v111_family")
    if isinstance(rep, dict):
        st.write({
            "component_index": rep.get("component_index"),
            "n_samples": rep.get("n_samples"),
            "feasible_fraction": rep.get("feasible_fraction"),
            "top_worst_hard": (rep.get("worst_hard_ranked") or [{}])[0],
        })
        st.download_button("Download design_family_report.json",
                           data=_json.dumps(rep, indent=2, sort_keys=True),
                           file_name="design_family_report.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v111_dl_json")


# =====================
# v112 Literature Overlay (append-only)
# =====================
def _v112_literature_overlay_panel():
    import streamlit as st
    from tools.literature_overlay import template_payload, validate_literature_points
    from tools.literature_overlay import extract_xy_points

    st.subheader("Literature Overlay")
    st.caption("Upload a JSON of reference points (ITER/ARC/SPARC/etc.) and overlay on Boundary Atlas slices. SHAMS ships only a template.")

    if "v112_overlay" not in st.session_state:
        st.session_state["v112_overlay"] = template_payload(version="v112")

    c1, c2 = st.columns(2)
    with c1:
        st.download_button(
            "Download overlay template JSON",
            data=_json.dumps(template_payload(version="v112"), indent=2, sort_keys=True),
            file_name="literature_points_template.json",
            mime="application/json",
            use_container_width=True,
            key="v112_dl_template",
        )
    with c2:
        up = st.file_uploader("Upload literature_points.json", type=["json"], key="v112_upload")
        if up is not None:
            try:
                payload = _json.loads(up.getvalue().decode("utf-8"))
                errs = validate_literature_points(payload)
                st.session_state["v112_overlay"] = payload
                if errs:
                    st.warning(f"Loaded with warnings: {errs[:8]}")
                else:
                    st.success("Overlay loaded.")
            except Exception as e:
                st.error(f"Failed to parse JSON: {e!r}")

    overlay = st.session_state.get("v112_overlay")
    st.write({"n_points": len((overlay or {}).get("points", [])) if isinstance(overlay, dict) else None})

    atlas = st.session_state.get("v110_boundary_atlas")
    if isinstance(atlas, dict) and isinstance(overlay, dict):
        slices = atlas.get("slices", [])
        if isinstance(slices, list) and slices:
            st.write("Preview overlay-able slices (first 6):")
            prev = []
            for sl in slices[:6]:
                if not isinstance(sl, dict):
                    continue
                kx = sl.get("lever_x"); ky = sl.get("lever_y")
                if isinstance(kx, str) and isinstance(ky, str):
                    prev.append({"x": kx, "y": ky, "points_available": len(extract_xy_points(overlay, kx, ky))})
            st.write(prev)


# =====================
# v113 Design Decision Layer (append-only)
# =====================
def _v113_design_decision_panel():
    import streamlit as st
    from tools.design_decision_layer import build_design_candidates, build_design_decision_pack

    st.subheader("Design Decision Layer")
    st.caption("Build defensible design candidates + comparison table + exportable decision pack (no optimization).")

    s = _v98_state_init_runlists()

    topo = st.session_state.get("v104_topology")
    comp = st.session_state.get("v109_components")
    atlas = st.session_state.get("v110_boundary_atlas")
    fam = st.session_state.get("v111_family")
    overlay = st.session_state.get("v112_overlay")

    # pick candidate artifacts from pinned runs (feasible only)
    ids = [r.get("id") for r in (s.run_history or []) if r.get("id")]
    default_ids = list(s.pinned_run_ids or [])
    picked = st.multiselect("Candidate source run artifacts (feasible ones)", options=ids, default=default_ids, key="v113_pick")
    maxc = st.slider("Max candidates", 1, 20, 12, 1, key="v113_maxc")

    if st.button("Build candidates + decision pack", key="v113_build"):
        try:
            run_map = {r.get("id"): r for r in (s.run_history or []) if r.get("id")}
            artifacts = []
            for rid in picked:
                payload = (run_map.get(rid) or {}).get("payload")
                if isinstance(payload, dict) and payload.get("kind") == "shams_run_artifact":
                    artifacts.append(payload)

            candidates = build_design_candidates(
                artifacts=artifacts,
                topology=topo if isinstance(topo, dict) else None,
                component_dominance=comp if isinstance(comp, dict) else None,
                boundary_atlas_v2=atlas if isinstance(atlas, dict) else None,
                design_family_report=fam if isinstance(fam, dict) else None,
                literature_overlay=overlay if isinstance(overlay, dict) else None,
                max_candidates=int(maxc),
            )
            pack = build_design_decision_pack(candidates=candidates, version="v113")
            st.session_state["v113_candidates"] = candidates
            st.session_state["v113_pack"] = pack
            _v98_record_run("design_decision_pack", {"candidates": candidates, "manifest": pack.get("manifest")}, mode="design_decision_pack")
            st.success(f"Built {len(candidates)} candidates.")
        except Exception as e:
            st.error(f"v113 failed: {e!r}")

    candidates = st.session_state.get("v113_candidates")
    pack = st.session_state.get("v113_pack")
    if isinstance(candidates, list) and candidates:
        st.write("Candidate preview (first 5):")
        st.write([{
            "id": c.get("source_artifact_id"),
            "component": c.get("component_index"),
            "worst": (c.get("feasibility") or {}).get("worst_hard"),
            "margin": (c.get("feasibility") or {}).get("worst_hard_margin_frac"),
            "family_feas": (c.get("robustness") or {}).get("family_feasible_fraction"),
        } for c in candidates[:5]])
        st.download_button("Download candidates.json",
                           data=_json.dumps({"candidates": candidates}, indent=2, sort_keys=True),
                           file_name="candidates.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v113_dl_candidates")

    if isinstance(pack, dict) and isinstance(pack.get("zip_bytes"), (bytes, bytearray)):
        st.download_button("Download design_decision_pack.zip",
                           data=pack["zip_bytes"],
                           file_name="design_decision_pack.zip",
                           mime="application/zip",
                           use_container_width=True,
                           key="v113_dl_pack")


# =====================
# v114 Preference-Aware Decision Layer (append-only)
# =====================
def _v114_preference_panel():
    import streamlit as st
    from tools.preference_layer import template_preferences, annotate_candidates_with_preferences, pareto_sets_from_annotations
    from tools.design_decision_layer import build_design_decision_pack

    st.subheader("Preference-Aware Decision Layer")
    st.caption("Preference sliders annotate candidates (scores + Pareto sets). No optimization; no auto-selection.")

    # candidates from v113
    candidates = st.session_state.get("v113_candidates")
    if not (isinstance(candidates, list) and candidates):
        st.info("Build v113 candidates first (Design Decision Layer).")
        return

    if "v114_prefs" not in st.session_state:
        st.session_state["v114_prefs"] = template_preferences()

    prefs = st.session_state["v114_prefs"]
    weights = (prefs.get("weights") if isinstance(prefs, dict) else None)
    if not isinstance(weights, dict):
        weights = {}

    st.write("Weights (0 disables a metric).")
    c1, c2 = st.columns(2)
    with c1:
        weights["margin"] = st.slider("margin (worst constraint margin)", 0.0, 2.0, float(weights.get("margin", 1.0)), 0.1, key="v114_w_margin")
        weights["robustness"] = st.slider("robustness (family feasible fraction)", 0.0, 2.0, float(weights.get("robustness", 1.0)), 0.1, key="v114_w_rob")
        weights["boundary_clearance"] = st.slider("boundary clearance (2D proxy)", 0.0, 2.0, float(weights.get("boundary_clearance", 0.7)), 0.1, key="v114_w_bd")
    with c2:
        weights["compactness"] = st.slider("compactness (smaller R0)", 0.0, 2.0, float(weights.get("compactness", 0.3)), 0.1, key="v114_w_comp")
        weights["low_aux_power"] = st.slider("low aux power (smaller Paux)", 0.0, 2.0, float(weights.get("low_aux_power", 0.2)), 0.1, key="v114_w_paux")

    prefs["weights"] = weights
    st.session_state["v114_prefs"] = prefs

    metrics = st.multiselect("Pareto metrics (normalized)", options=["margin","robustness","boundary_clearance","compactness","low_aux_power"],
                             default=["margin","robustness","boundary_clearance"], key="v114_metrics")

    if st.button("Annotate + compute Pareto sets", key="v114_run"):
        try:
            ann = annotate_candidates_with_preferences(candidates, prefs)
            pareto = pareto_sets_from_annotations(ann, metrics=metrics, max_fronts=3)
            st.session_state["v114_ann"] = ann
            st.session_state["v114_pareto"] = pareto
            st.success("v114 annotations + Pareto computed.")
        except Exception as e:
            st.error(f"v114 failed: {e!r}")

    ann = st.session_state.get("v114_ann")
    pareto = st.session_state.get("v114_pareto")

    if isinstance(ann, dict):
        # show top composite scores (best-effort)
        cands = ann.get("candidates", [])
        preview = []
        if isinstance(cands, list):
            for c in cands:
                if not isinstance(c, dict):
                    continue
                pa = c.get("preference_annotation_v114", {})
                comp = pa.get("composite_score") if isinstance(pa, dict) else None
                preview.append({
                    "id": c.get("source_artifact_id"),
                    "composite_score": comp,
                    "worst": (c.get("feasibility") or {}).get("worst_hard"),
                    "margin": (c.get("feasibility") or {}).get("worst_hard_margin_frac"),
                })
            preview.sort(key=lambda r: (r.get("composite_score") is None, -(r.get("composite_score") or -1e9)))
        st.write("Preview (top 8 by composite score; annotation only):")
        st.write(preview[:8])

        st.download_button("Download preference_annotation_bundle.json",
                           data=_json.dumps(ann, indent=2, sort_keys=True),
                           file_name="preference_annotation_bundle_v114.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v114_dl_ann")

    if isinstance(pareto, dict):
        st.write({"pareto_metrics": pareto.get("metrics"), "n_points": pareto.get("n_points"), "front_sizes": [len(f) for f in (pareto.get("fronts") or []) if isinstance(f, list)]})
        st.download_button("Download pareto_sets.json",
                           data=_json.dumps(pareto, indent=2, sort_keys=True),
                           file_name="pareto_sets_v114.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v114_dl_pareto")

    if isinstance(ann, dict) and isinstance(pareto, dict):
        # build a justification + pack (adds decision_justification.json inside zip)
        justification = {
            "kind": "shams_decision_justification_v114",
            "created_utc": ann.get("created_utc"),
            "preferences": prefs,
            "pareto_sets": pareto,
            "n_candidates": len((ann.get("candidates") or [])) if isinstance(ann.get("candidates"), list) else None,
            "disclaimer": "Annotations only. No optimization. No auto-selected best design.",
        }
        pack = build_design_decision_pack(candidates=candidates, version="v114", decision_justification=justification)
        st.download_button("Download v114 design_decision_pack.zip (with justification)",
                           data=pack["zip_bytes"],
                           file_name="design_decision_pack_v114.zip",
                           mime="application/zip",
                           use_container_width=True,
                           key="v114_dl_pack")


# =====================
# v115 External Optimizer Sandbox (append-only)
# =====================
def _v115_optimizer_sandbox_panel():
    import streamlit as st
    from tools.optimizer_interface import template_request, template_response, evaluate_optimizer_proposal, build_optimizer_import_pack

    st.subheader("External Optimizer Sandbox")
    st.caption("Import external optimizer proposals as *read-only* candidates. SHAMS re-evaluates physics+constraints and records a run artifact.")

    c1, c2 = st.columns(2)
    with c1:
        st.download_button(
            "Download optimizer_request_template.json",
            data=_json.dumps(template_request(version="v115"), indent=2, sort_keys=True),
            file_name="optimizer_request_template.json",
            mime="application/json",
            use_container_width=True,
            key="v115_dl_req",
        )
    with c2:
        st.download_button(
            "Download optimizer_response_template.json",
            data=_json.dumps(template_response(version="v115"), indent=2, sort_keys=True),
            file_name="optimizer_response_template.json",
            mime="application/json",
            use_container_width=True,
            key="v115_dl_resp_tpl",
        )

    up = st.file_uploader("Upload optimizer_response.json (proposal)", type=["json"], key="v115_upload_resp")
    if up is None:
        st.info("Upload a proposal response to evaluate it inside SHAMS.")
        return

    try:
        payload = _json.loads(up.getvalue().decode("utf-8"))
    except Exception as e:
        st.error(f"Failed to parse JSON: {e!r}")
        return

    if payload.get("kind") != "shams_optimizer_response":
        st.error("JSON kind must be 'shams_optimizer_response'.")
        return

    if st.button("Evaluate proposal in SHAMS (frozen physics)", key="v115_eval"):
        try:
            out = evaluate_optimizer_proposal(payload)
            art = out["artifact"]
            ctx = out["context"]
            st.session_state["v115_last_artifact"] = art
            st.session_state["v115_last_context"] = ctx
            # record in ledger
            _v98_record_run("optimizer_proposal", {"artifact": art, "context": ctx}, mode="optimizer_import_v115")
            st.success("Proposal evaluated and recorded in Run Ledger.")
        except Exception as e:
            st.error(f"Evaluation failed: {e!r}")

    art = st.session_state.get("v115_last_artifact")
    ctx = st.session_state.get("v115_last_context")

    if isinstance(ctx, dict):
        st.write("Import context:")
        st.write({k: ctx.get(k) for k in ["result", "disclaimer"]})

    if isinstance(art, dict) and art.get("kind") == "shams_run_artifact":
        st.write("Evaluated artifact summary:")
        cs = art.get("constraints_summary", {})
        st.write({
            "id": art.get("id"),
            "feasible": (cs.get("feasible") if isinstance(cs, dict) else None),
            "worst_hard": (cs.get("worst_hard") if isinstance(cs, dict) else None),
            "worst_margin": (cs.get("worst_hard_margin_frac") if isinstance(cs, dict) else None),
        })
        st.download_button("Download evaluated_run_artifact.json",
                           data=_json.dumps(art, indent=2, sort_keys=True),
                           file_name="evaluated_run_artifact.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v115_dl_eval_art")

        pack = build_optimizer_import_pack(
            request_template=template_request(version="v115"),
            response_template=template_response(version="v115"),
            evaluated_artifact=art,
            import_context=ctx if isinstance(ctx, dict) else None,
            version="v115",
        )
        st.download_button("Download optimizer_import_pack.zip",
                           data=pack["zip_bytes"],
                           file_name="optimizer_import_pack.zip",
                           mime="application/zip",
                           use_container_width=True,
                           key="v115_dl_pack")


# =====================
# v116 Design Handoff Pack (append-only)
# =====================
def _v116_handoff_pack_panel():
    import streamlit as st
    from tools.handoff_pack import build_handoff_pack

    st.subheader("Design Handoff Pack")
    st.caption("Export an engineering-ready handoff bundle for any run artifact (inputs YAML, constraints CSV, figures, manifest).")

    s = _v98_state_init_runlists()
    ids = [r.get("id") for r in (s.run_history or []) if r.get("id")]
    if not ids:
        st.info("No runs in ledger yet. Run a point evaluation first.")
        return

    default_id = (s.pinned_run_ids[-1] if (s.pinned_run_ids and len(s.pinned_run_ids)>0) else ids[-1])
    rid = st.selectbox("Select run artifact", options=ids, index=ids.index(default_id) if default_id in ids else len(ids)-1, key="v116_pick_run")
    run_map = {r.get("id"): r for r in (s.run_history or []) if r.get("id")}
    payload = (run_map.get(rid) or {}).get("payload")
    art = payload if (isinstance(payload, dict) and payload.get("kind") == "shams_run_artifact") else None

    if art is None:
        st.error("Selected run does not contain a run artifact payload.")
        return

    if st.button("Build handoff pack", key="v116_build"):
        try:
            pack = build_handoff_pack(artifact=art, version="v116")
            st.session_state["v116_pack"] = pack
            _v98_record_run("handoff_pack", {"manifest": pack.get("manifest"), "source_artifact_id": art.get("id")}, mode="handoff_pack_v116")
            st.success("Handoff pack built.")
        except Exception as e:
            st.error(f"Failed: {e!r}")

    pack = st.session_state.get("v116_pack")
    if isinstance(pack, dict) and isinstance(pack.get("zip_bytes"), (bytes, bytearray)):
        st.download_button("Download handoff_pack.zip",
                           data=pack["zip_bytes"],
                           file_name=f"handoff_pack_{rid}.zip",
                           mime="application/zip",
                           use_container_width=True,
                           key="v116_dl_pack")
        st.download_button("Download handoff manifest.json",
                           data=_json.dumps(pack.get("manifest", {}), indent=2, sort_keys=True),
                           file_name="handoff_pack_manifest.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v116_dl_manifest")


# =====================
# v117 Tolerance Envelope (append-only)
# =====================
def _v117_tolerance_envelope_panel():
    import streamlit as st
    from tools.tolerance_envelope import template_tolerance_spec, evaluate_tolerance_envelope, envelope_summary_csv

    st.subheader("Tolerance Envelope")
    st.caption("Deterministic tolerance envelope around a selected run artifact. No Monte Carlo, no optimization.")

    s = _v98_state_init_runlists()
    ids = [r.get("id") for r in (s.run_history or []) if r.get("id")]
    if not ids:
        st.info("No runs in ledger yet. Run a point evaluation first.")
        return

    default_id = (s.pinned_run_ids[-1] if (s.pinned_run_ids and len(s.pinned_run_ids)>0) else ids[-1])
    rid = st.selectbox("Select baseline run artifact", options=ids, index=ids.index(default_id) if default_id in ids else len(ids)-1, key="v117_pick_run")
    run_map = {r.get("id"): r for r in (s.run_history or []) if r.get("id")}
    payload = (run_map.get(rid) or {}).get("payload")
    art = payload if (isinstance(payload, dict) and payload.get("kind") == "shams_run_artifact") else None
    if art is None:
        st.error("Selected run does not contain a run artifact payload.")
        return

    if "v117_spec" not in st.session_state:
        st.session_state["v117_spec"] = template_tolerance_spec()

    spec = st.session_state["v117_spec"]
    tmap = spec.get("tolerances") if isinstance(spec, dict) else {}
    if not isinstance(tmap, dict):
        tmap = {}

    mode = st.selectbox("Tolerance mode", options=["relative","absolute"], index=0 if spec.get("mode","relative")=="relative" else 1, key="v117_mode")
    include_mid = st.checkbox("Include edge midpoints", value=bool(spec.get("include_edge_midpoints", True)), key="v117_mid")
    max_samples = st.number_input("Max samples", min_value=10, max_value=500, value=200, step=10, key="v117_max")

    st.write("Tolerances (per lever):")
    cols = st.columns(3)
    keys = ["Bt_T","Ip_MA","R0_m","a_m","fG","Ti_keV","Paux_MW","kappa"]
    for i,k in enumerate(keys):
        with cols[i % 3]:
            default = float(tmap.get(k, 0.0) or 0.0)
            tmap[k] = st.number_input(f"{k} tol", min_value=0.0, max_value=1e6, value=default, step=0.005 if mode=="relative" else 0.5, key=f"v117_tol_{k}")

    spec["mode"] = mode
    spec["include_edge_midpoints"] = include_mid
    spec["tolerances"] = tmap
    st.session_state["v117_spec"] = spec

    st.download_button("Download tolerance_spec.json",
                       data=_json.dumps(spec, indent=2, sort_keys=True),
                       file_name="tolerance_spec_v117.json",
                       mime="application/json",
                       use_container_width=True,
                       key="v117_dl_spec")

    if st.button("Run tolerance envelope", key="v117_run"):
        try:
            rep = evaluate_tolerance_envelope(baseline_artifact=art, tolerance_spec=spec, version="v117", max_samples=int(max_samples))
            st.session_state["v117_report"] = rep
            _v98_record_run("tolerance_envelope", {"summary": rep.get("summary"), "source_artifact_id": rid}, mode="tolerance_envelope_v117")
            st.success("Tolerance envelope computed.")
        except Exception as e:
            st.error(f"Failed: {e!r}")

    rep = st.session_state.get("v117_report")
    if isinstance(rep, dict) and rep.get("kind") == "shams_tolerance_envelope_report":
        st.write("Summary:")
        st.write(rep.get("summary", {}))
        st.download_button("Download tolerance_envelope_report.json",
                           data=_json.dumps(rep, indent=2, sort_keys=True),
                           file_name="tolerance_envelope_report_v117.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v117_dl_report")
        st.download_button("Download tolerance_envelope_summary.csv",
                           data=envelope_summary_csv(rep),
                           file_name="tolerance_envelope_summary_v117.csv",
                           mime="text/csv",
                           use_container_width=True,
                           key="v117_dl_csv")


# =====================
# v118 Optimizer Downstream Workflow (append-only)
# =====================
def _v118_optimizer_downstream_panel():
    import streamlit as st
    from tools.optimizer_downstream import template_batch_response, evaluate_optimizer_batch, build_downstream_report_zip
    from tools.preference_layer import template_preferences
    from tools.tolerance_envelope import template_tolerance_spec

    st.subheader("Optimizer Downstream Workflow")
    st.caption("Upload a batch of optimizer proposals; SHAMS evaluates, filters feasible, runs tolerance envelopes, builds candidates, applies preferences+Pareto, and exports one bundle.")

    c1, c2, c3 = st.columns(3)
    with c1:
        st.download_button("Download optimizer_batch_template.json",
                           data=_json.dumps(template_batch_response(), indent=2, sort_keys=True),
                           file_name="optimizer_batch_template.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v118_dl_batch_tpl")
    with c2:
        st.download_button("Download preferences_template.json",
                           data=_json.dumps(template_preferences(), indent=2, sort_keys=True),
                           file_name="preferences_template.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v118_dl_prefs_tpl")
    with c3:
        st.download_button("Download tolerance_spec_template.json",
                           data=_json.dumps(template_tolerance_spec(), indent=2, sort_keys=True),
                           file_name="tolerance_spec_template.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v118_dl_tol_tpl")

    up = st.file_uploader("Upload optimizer_batch.json", type=["json"], key="v118_up_batch")
    if up is None:
        st.info("Upload a batch proposal JSON to run v118.")
        return

    try:
        batch = _json.loads(up.getvalue().decode("utf-8"))
    except Exception as e:
        st.error(f"Failed to parse JSON: {e!r}")
        return

    if batch.get("kind") != "shams_optimizer_batch_response":
        st.error("JSON kind must be 'shams_optimizer_batch_response'.")
        return

    max_env = st.number_input("Max envelope samples per feasible point", min_value=8, max_value=80, value=24, step=4, key="v118_max_env")
    max_cand = st.number_input("Max candidates", min_value=1, max_value=50, value=12, step=1, key="v118_max_cand")

    # Optional: use current v114 prefs if present, else template
    prefs = st.session_state.get("v114_prefs")
    if not isinstance(prefs, dict):
        prefs = template_preferences()
    spec = st.session_state.get("v117_spec")
    if not isinstance(spec, dict):
        spec = template_tolerance_spec()

    if st.button("Run v118 downstream report", key="v118_run"):
        try:
            out = evaluate_optimizer_batch(batch_payload=batch, tolerance_spec=spec, max_envelope_samples=int(max_env), max_candidates=int(max_cand), preferences=prefs)
            rep = out["report"]
            pack_bytes = out["decision_pack_zip_bytes"]
            st.session_state["v118_report"] = rep
            st.session_state["v118_pack_bytes"] = pack_bytes
            bundle = build_downstream_report_zip(report_obj=rep, decision_pack_zip_bytes=pack_bytes)
            st.session_state["v118_bundle"] = bundle
            _v98_record_run("optimizer_downstream", {"summary": rep.get("batch_meta"), "decision_pack_manifest": rep.get("decision_pack_manifest")}, mode="optimizer_downstream_v118")
            st.success("v118 downstream report built.")
        except Exception as e:
            st.error(f"v118 failed: {e!r}")

    rep = st.session_state.get("v118_report")
    bundle = st.session_state.get("v118_bundle")
    if isinstance(rep, dict):
        st.write("Batch meta:")
        st.write(rep.get("batch_meta", {}))
        st.write("Decision justification summary:")
        dj = rep.get("decision_justification", {}) if isinstance(rep.get("decision_justification"), dict) else {}
        st.write({k: dj.get(k) for k in ["n_proposals","n_feasible","n_candidates","n_envelopes"]})
        st.download_button("Download optimizer_downstream_report_v118.json",
                           data=_json.dumps(rep, indent=2, sort_keys=True),
                           file_name="optimizer_downstream_report_v118.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v118_dl_report")

    pack_bytes = st.session_state.get("v118_pack_bytes")
    if isinstance(pack_bytes, (bytes, bytearray)):
        st.download_button("Download design_decision_pack_v118.zip",
                           data=pack_bytes,
                           file_name="design_decision_pack_v118.zip",
                           mime="application/zip",
                           use_container_width=True,
                           key="v118_dl_pack")

    if isinstance(bundle, dict) and isinstance(bundle.get("zip_bytes"), (bytes, bytearray)):
        st.download_button("Download optimizer_downstream_bundle_v118.zip",
                           data=bundle["zip_bytes"],
                           file_name="optimizer_downstream_bundle_v118.zip",
                           mime="application/zip",
                           use_container_width=True,
                           key="v118_dl_bundle")


# =====================
# v119 Authority Pack (append-only)
# =====================
def _v119_authority_pack_panel():
    import streamlit as st
    from tools.authority_pack import build_authority_pack

    st.subheader("Authority Pack")
    st.caption("Build a single publishable evidence bundle: version, patch notes, requirements freeze (best-effort), command log, methods appendix, and selected artifacts.")

    # try to find latest generated artifacts from self-test run folder if present in session state
    audit_zip = st.session_state.get("last_audit_pack_zip_bytes")
    downstream_zip = None
    handoff_zip = None

    # If users ran v118/v116 panels in-session, these may exist:
    bundle = st.session_state.get("v118_bundle")
    if isinstance(bundle, dict) and isinstance(bundle.get("zip_bytes"), (bytes, bytearray)):
        downstream_zip = bytes(bundle["zip_bytes"])
    hp = st.session_state.get("v116_pack")
    if isinstance(hp, dict) and isinstance(hp.get("zip_bytes"), (bytes, bytearray)):
        handoff_zip = bytes(hp["zip_bytes"])

    cmdlog = st.text_area("Command log (editable)", value="\n".join([
        "python -m tools.ui_self_test --outdir out_ui_self_test",
        "python -m tools.verify_package",
        "python -m tools.verify_figures",
        "python -m tools.tests.test_plot_layout",
        "python -m tools.regression_suite",
    ]), height=140, key="v119_cmdlog")

    if st.button("Build Authority Pack", key="v119_build"):
        try:
            pack = build_authority_pack(
                repo_root=".",
                version="v119",
                audit_pack_zip=audit_zip,
                downstream_bundle_zip=downstream_zip,
                handoff_pack_zip=handoff_zip,
                command_log=[l for l in cmdlog.splitlines() if l.strip()],
            )
            st.session_state["v119_pack"] = pack
            _v98_record_run("authority_pack", {"manifest": pack.get("manifest")}, mode="authority_pack_v119")
            st.success("Authority pack built.")
        except Exception as e:
            st.error(f"Failed: {e!r}")

    pack = st.session_state.get("v119_pack")
    if isinstance(pack, dict) and isinstance(pack.get("zip_bytes"), (bytes, bytearray)):
        st.download_button("Download authority_pack_v119.zip",
                           data=pack["zip_bytes"],
                           file_name="authority_pack_v119.zip",
                           mime="application/zip",
                           use_container_width=True,
                           key="v119_dl_zip")
        st.download_button("Download authority manifest.json",
                           data=_json.dumps(pack.get("manifest", {}), indent=2, sort_keys=True),
                           file_name="authority_pack_manifest_v119.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v119_dl_manifest")


# =====================
# v120 Constitutional panels (append-only)
# =====================
def _v120_constitution_panel():
    import streamlit as st
    from tools.constitution import build_constitution_manifest
    from ui.layer_registry import get_layers

    st.subheader("Constitution & Layer Registry")
    st.caption("This panel exposes governance + architecture documents and a cryptographic integrity manifest (SHA256).")

    # Documents
    docs = [
        ("ARCHITECTURE.md", "Architecture (Constitution)"),
        ("GOVERNANCE.md", "Governance & Release Policy"),
        ("LAYER_MODEL.md", "Layer Model"),
        ("NON_OPTIMIZER_MANIFESTO.md", "Non-Optimizer Manifesto"),
        ("CITATION.cff", "Citation (CFF)"),
    ]
    for fn, label in docs:
        try:
            b = open(fn, "rb").read()
            st.download_button(f"Download {label}", data=b, file_name=fn, use_container_width=True, key=f"v120_dl_{fn}")
        except Exception:
            st.warning(f"Missing {fn}")

    st.write("Registered higher layers (UI-accessible):")
    st.table([{"layer": e.layer, "title": e.title, "description": e.description, "panel": e.panel_fn_name} for e in get_layers()])

    man = build_constitution_manifest(repo_root=".", version="v120")
    st.download_button("Download constitution_manifest.json",
                       data=_json.dumps(man, indent=2, sort_keys=True),
                       file_name="constitution_manifest_v120.json",
                       mime="application/json",
                       use_container_width=True,
                       key="v120_dl_manifest")
    st.write("Manifest preview:")
    st.json(man)

def _v120_mission_placeholder_panel():
    import streamlit as st
    st.subheader("Mission Context (v120 placeholder)")
    st.info("Mission contexts are schema-first and will be added as additive layer panels without changing physics.")

def _v120_explainability_placeholder_panel():
    import streamlit as st
    st.subheader("Explainability (v120 placeholder)")
    st.info("Explainability narratives will be added as additive post-processing panels consuming run artifacts.")



# =====================
# v121 Mission Context Layer (L3) - additive UI panel
# =====================
def _v121_mission_context_panel():
    import streamlit as st
    from tools.mission_context import list_builtin_missions, load_mission, apply_mission_overlays, mission_report_csv

    st.subheader("Mission Context")
    st.caption("Advisory mission overlay: evaluates alignment to mission targets and reports gaps. No physics/constraints are changed.")

    s = _v98_state_init_runlists()
    ids = [r.get("id") for r in (s.run_history or []) if r.get("id")]
    if not ids:
        st.info("No runs in ledger yet. Run a point evaluation first.")
        return

    default_id = (s.pinned_run_ids[-1] if (s.pinned_run_ids and len(s.pinned_run_ids)>0) else ids[-1])
    rid = st.selectbox("Select baseline run artifact", options=ids, index=ids.index(default_id) if default_id in ids else len(ids)-1, key="v121_pick_run")
    run_map = {r.get("id"): r for r in (s.run_history or []) if r.get("id")}
    payload = (run_map.get(rid) or {}).get("payload")
    art = payload if (isinstance(payload, dict) and payload.get("kind") == "shams_run_artifact") else None
    if art is None:
        st.error("Selected run does not contain a run artifact payload.")
        return

    missions = list_builtin_missions("missions")
    if not missions:
        st.error("No missions found in missions/ directory.")
        return

    mfile = st.selectbox("Mission", options=missions, index=0, key="v121_mission_file")
    mission = load_mission(str(Path("missions") / mfile))

    if st.button("Generate mission report", key="v121_run"):
        try:
            rep = apply_mission_overlays(run_artifact=art, mission=mission, version="v121")
            st.session_state["v121_mission_report"] = rep
            _v98_record_run("mission_context", {"mission": mission.get("name"), "gaps_n": len(rep.get("gaps", []))}, mode="mission_context_v121")
            st.success("Mission report generated.")
        except Exception as e:
            st.error(f"Failed: {e!r}")

    rep = st.session_state.get("v121_mission_report")
    if isinstance(rep, dict) and rep.get("kind") == "shams_mission_report":
        st.write("Alignment:")
        st.write(rep.get("alignment", {}))
        st.write("Gaps:")
        st.write(rep.get("gaps", []))
        st.download_button("Download mission_report.json",
                           data=_json.dumps(rep, indent=2, sort_keys=True),
                           file_name=f"mission_report_{mission.get('name','mission')}_v121.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v121_dl_json")
        st.download_button("Download mission_gaps.csv",
                           data=mission_report_csv(rep),
                           file_name=f"mission_gaps_{mission.get('name','mission')}_v121.csv",
                           mime="text/csv",
                           use_container_width=True,
                           key="v121_dl_csv")


# =====================
# v122 Explainability Layer (L4) - additive UI panel
# =====================
def _v122_explainability_panel():
    import streamlit as st
    from tools.explainability import build_explainability_report

    st.subheader("Explainability")
    st.caption("Post-processing narrative: why a design fails/succeeds, limiting constraints, robustness and mission context (if available).")

    s = _v98_state_init_runlists()
    ids = [r.get("id") for r in (s.run_history or []) if r.get("id")]
    if not ids:
        st.info("No runs in ledger yet. Run a point evaluation first.")
        return

    default_id = (s.pinned_run_ids[-1] if (s.pinned_run_ids and len(s.pinned_run_ids)>0) else ids[-1])
    rid = st.selectbox("Select baseline run artifact", options=ids, index=ids.index(default_id) if default_id in ids else len(ids)-1, key="v122_pick_run")
    run_map = {r.get("id"): r for r in (s.run_history or []) if r.get("id")}
    payload = (run_map.get(rid) or {}).get("payload")
    art = payload if (isinstance(payload, dict) and payload.get("kind") == "shams_run_artifact") else None
    if art is None:
        st.error("Selected run does not contain a run artifact payload.")
        return

    # Optional inputs: use latest in-session mission report / envelope report if present
    mission_rep = st.session_state.get("v121_mission_report")
    env_rep = st.session_state.get("v117_report")

    use_mission = st.checkbox("Use latest in-session mission report (if available)", value=isinstance(mission_rep, dict), key="v122_use_mission")
    use_env = st.checkbox("Use latest in-session tolerance envelope (if available)", value=isinstance(env_rep, dict), key="v122_use_env")

    if st.button("Generate explainability report", key="v122_run"):
        try:
            rep = build_explainability_report(
                run_artifact=art,
                mission_report=mission_rep if use_mission else None,
                tolerance_envelope_report=env_rep if use_env else None,
                version="v122",
            )
            st.session_state["v122_explainability_report"] = rep
            _v98_record_run("explainability", {"summary": rep.get("summary")}, mode="explainability_v122")
            st.success("Explainability report generated.")
        except Exception as e:
            st.error(f"Failed: {e!r}")

    rep = st.session_state.get("v122_explainability_report")
    if isinstance(rep, dict) and rep.get("kind") == "shams_explainability_report":
        st.write("Summary:")
        st.write(rep.get("summary", {}))
        st.text_area("Narrative", value=rep.get("narrative",""), height=260, key="v122_narr_view")
        st.download_button("Download explainability_report.json",
                           data=_json.dumps(rep, indent=2, sort_keys=True),
                           file_name="explainability_report_v122.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v122_dl_json")
        st.download_button("Download explainability_report.txt",
                           data=(rep.get("narrative","") or "").encode("utf-8"),
                           file_name="explainability_report_v122.txt",
                           mime="text/plain",
                           use_container_width=True,
                           key="v122_dl_txt")


# =====================
# v123 Evidence Graph + v123B Study Kit - additive UI panel
# =====================
def _v123_evidence_and_studykit_panel():
    import streamlit as st
    from tools.evidence_graph import build_evidence_graph, build_traceability_table, traceability_csv
    from tools.study_kit import build_study_kit_zip

    st.subheader("Evidence Graph & Design Study Kit (v123 / v123B)")
    st.caption("Build provenance graph + traceability table, and export a full publishable study kit zip (manifested with SHA256).")

    s = _v98_state_init_runlists()
    ids = [r.get("id") for r in (s.run_history or []) if r.get("id")]
    if not ids:
        st.info("No runs in ledger yet. Run a point evaluation first.")
        return

    default_id = (s.pinned_run_ids[-1] if (s.pinned_run_ids and len(s.pinned_run_ids)>0) else ids[-1])
    rid = st.selectbox("Select baseline run artifact", options=ids, index=ids.index(default_id) if default_id in ids else len(ids)-1, key="v123_pick_run")
    run_map = {r.get("id"): r for r in (s.run_history or []) if r.get("id")}
    payload = (run_map.get(rid) or {}).get("payload")
    art = payload if (isinstance(payload, dict) and payload.get("kind") == "shams_run_artifact") else None
    if art is None:
        st.error("Selected run does not contain a run artifact payload.")
        return

    # Optional context from session state
    mission_rep = st.session_state.get("v121_mission_report")
    env_rep = st.session_state.get("v117_report")
    expl_rep = st.session_state.get("v122_explainability_report")

    v118_bundle = st.session_state.get("v118_bundle")
    downstream_manifest = None
    downstream_zip = None
    if isinstance(v118_bundle, dict):
        downstream_manifest = v118_bundle.get("manifest")
        if isinstance(v118_bundle.get("zip_bytes"), (bytes, bytearray)):
            downstream_zip = bytes(v118_bundle["zip_bytes"])

    v119_pack = st.session_state.get("v119_pack")
    authority_manifest = None
    authority_zip = None
    if isinstance(v119_pack, dict):
        authority_manifest = v119_pack.get("manifest")
        if isinstance(v119_pack.get("zip_bytes"), (bytes, bytearray)):
            authority_zip = bytes(v119_pack["zip_bytes"])

    decision_manifest = None
    decision_zip = None
    pack_bytes = st.session_state.get("v118_pack_bytes")
    if isinstance(pack_bytes, (bytes, bytearray)):
        decision_zip = bytes(pack_bytes)
        # best-effort manifest from report if present
        rep = st.session_state.get("v118_report")
        if isinstance(rep, dict):
            decision_manifest = rep.get("decision_pack_manifest")

    use_mission = st.checkbox("Use latest in-session mission report", value=isinstance(mission_rep, dict), key="v123_use_mission")
    use_env = st.checkbox("Use latest in-session tolerance envelope", value=isinstance(env_rep, dict), key="v123_use_env")
    use_expl = st.checkbox("Use latest in-session explainability report", value=isinstance(expl_rep, dict), key="v123_use_expl")

    if st.button("Build evidence graph + traceability", key="v123_build_ev"):
        try:
            graph = build_evidence_graph(
                run_artifact=art,
                mission_report=mission_rep if use_mission else None,
                tolerance_envelope_report=env_rep if use_env else None,
                explainability_report=expl_rep if use_expl else None,
                decision_pack_manifest=decision_manifest,
                downstream_bundle_manifest=downstream_manifest,
                authority_pack_manifest=authority_manifest,
                version="v123",
            )
            tab = build_traceability_table(
                run_artifact=art,
                mission_report=mission_rep if use_mission else None,
                tolerance_envelope_report=env_rep if use_env else None,
                explainability_report=expl_rep if use_expl else None,
                version="v123",
            )
            st.session_state["v123_graph"] = graph
            st.session_state["v123_trace_table"] = tab
            _v98_record_run("evidence_graph", {"nodes": len(graph.get("nodes",[])), "edges": len(graph.get("edges",[]))}, mode="evidence_graph_v123")
            st.success("Evidence graph and traceability built.")
        except Exception as e:
            st.error(f"Failed: {e!r}")

    graph = st.session_state.get("v123_graph")
    tab = st.session_state.get("v123_trace_table")
    if isinstance(graph, dict) and graph.get("kind") == "shams_evidence_graph":
        st.download_button("Download evidence_graph.json", data=_json.dumps(graph, indent=2, sort_keys=True),
                           file_name="evidence_graph_v123.json", mime="application/json", use_container_width=True, key="v123_dl_graph")
        st.json({"nodes": len(graph.get("nodes",[])), "edges": len(graph.get("edges",[]))})

    if isinstance(tab, dict) and tab.get("kind") == "shams_traceability_table":
        st.download_button("Download traceability_table.json", data=_json.dumps(tab, indent=2, sort_keys=True),
                           file_name="traceability_table_v123.json", mime="application/json", use_container_width=True, key="v123_dl_tab")
        st.download_button("Download traceability.csv", data=traceability_csv(tab),
                           file_name="traceability_v123.csv", mime="text/csv", use_container_width=True, key="v123_dl_csv")
        st.write("Traceability rows:", len(tab.get("rows",[])))

    st.divider()
    st.write("Design Study Kit export (v123B): bundles run artifact + optional context + evidence graph + manifest.")
    if st.button("Build study kit zip", key="v123_build_kit"):
        try:
            if not (isinstance(graph, dict) and isinstance(tab, dict)):
                graph = build_evidence_graph(run_artifact=art, mission_report=mission_rep if use_mission else None,
                                             tolerance_envelope_report=env_rep if use_env else None,
                                             explainability_report=expl_rep if use_expl else None,
                                             decision_pack_manifest=decision_manifest,
                                             downstream_bundle_manifest=downstream_manifest,
                                             authority_pack_manifest=authority_manifest,
                                             version="v123")
                tab = build_traceability_table(run_artifact=art, mission_report=mission_rep if use_mission else None,
                                               tolerance_envelope_report=env_rep if use_env else None,
                                               explainability_report=expl_rep if use_expl else None,
                                               version="v123")
                st.session_state["v123_graph"] = graph
                st.session_state["v123_trace_table"] = tab

            kit = build_study_kit_zip(
                run_artifact=art,
                mission_report=mission_rep if use_mission else None,
                tolerance_envelope_report=env_rep if use_env else None,
                explainability_report=expl_rep if use_expl else None,
                evidence_graph=graph,
                traceability_table=tab,
                authority_pack_zip=authority_zip,
                optimizer_downstream_bundle_zip=downstream_zip,
                decision_pack_zip=decision_zip,
                version="v123B",
            )
            st.session_state["v123_study_kit"] = kit
            _v98_record_run("study_kit", {"files": len((kit.get("manifest") or {}).get("files", {}))}, mode="study_kit_v123B")
            st.success("Study kit built.")
        except Exception as e:
            st.error(f"Failed: {e!r}")

    kit = st.session_state.get("v123_study_kit")
    if isinstance(kit, dict) and isinstance(kit.get("zip_bytes"), (bytes, bytearray)):
        st.download_button("Download study_kit_v123B.zip",
                           data=kit["zip_bytes"], file_name="study_kit_v123B.zip",
                           mime="application/zip", use_container_width=True, key="v123_dl_kit")
        st.download_button("Download study_kit_manifest_v123B.json",
                           data=_json.dumps(kit.get("manifest", {}), indent=2, sort_keys=True),
                           file_name="study_kit_manifest_v123B.json", mime="application/json",
                           use_container_width=True, key="v123_dl_kitman")


# =====================
# v124 Feasibility Boundary Atlas - additive UI panel
# =====================
def _v124_feasibility_atlas_panel():
    import streamlit as st
    from tools.feasibility_atlas import build_feasibility_atlas_bundle, available_numeric_levers

    st.subheader("Feasibility Boundary Atlas")
    st.caption("Runs a 2D grid scan around a baseline run, extracts feasibility boundary, and exports a publishable atlas bundle. Additive only (no physics/solver changes).")

    s = _v98_state_init_runlists()
    ids = [r.get("id") for r in (s.run_history or []) if r.get("id")]
    if not ids:
        st.info("No runs in ledger yet. Run a point evaluation first.")
        return

    default_id = (s.pinned_run_ids[-1] if (s.pinned_run_ids and len(s.pinned_run_ids)>0) else ids[-1])
    rid = st.selectbox("Select baseline run artifact", options=ids, index=ids.index(default_id) if default_id in ids else len(ids)-1, key="v124_pick_run")
    run_map = {r.get("id"): r for r in (s.run_history or []) if r.get("id")}
    payload = (run_map.get(rid) or {}).get("payload")
    base = payload if (isinstance(payload, dict) and payload.get("kind") == "shams_run_artifact") else None
    if base is None:
        st.error("Selected run does not contain a run artifact payload.")
        return

    base_inputs = base.get("inputs", {}) if isinstance(base.get("inputs"), dict) else {}
    levers = available_numeric_levers(base_inputs)
    if len(levers) < 2:
        st.error("Baseline inputs do not contain enough numeric levers for an atlas.")
        return

    col1, col2 = st.columns(2)
    with col1:
        kx = st.selectbox("Lever X", options=levers, index=0, key="v124_kx")
    with col2:
        ky = st.selectbox("Lever Y", options=levers, index=1 if len(levers)>1 else 0, key="v124_ky")

    def _default_range(v):
        try:
            v=float(v)
        except Exception:
            return (0.0, 1.0)
        if abs(v) < 1e-9:
            return (-1.0, 1.0)
        # Â±20% around baseline
        return (v*0.8, v*1.2)

    x0 = base_inputs.get(kx)
    y0 = base_inputs.get(ky)
    xlo_def, xhi_def = _default_range(x0)
    ylo_def, yhi_def = _default_range(y0)

    st.write("Ranges (default Â±20% around baseline):")
    r1, r2 = st.columns(2)
    with r1:
        xlo = st.number_input("X min", value=float(xlo_def), key="v124_xlo")
        xhi = st.number_input("X max", value=float(xhi_def), key="v124_xhi")
    with r2:
        ylo = st.number_input("Y min", value=float(ylo_def), key="v124_ylo")
        yhi = st.number_input("Y max", value=float(yhi_def), key="v124_yhi")

    c1, c2 = st.columns(2)
    with c1:
        nx = st.slider("Grid NX", min_value=9, max_value=61, value=25, step=2, key="v124_nx")
    with c2:
        ny = st.slider("Grid NY", min_value=9, max_value=61, value=25, step=2, key="v124_ny")

    max_evals = int(nx) * int(ny)
    st.caption(f"Total evaluations: {max_evals} (pure point evaluations + constraints).")

    if st.button("Generate atlas bundle", key="v124_run"):
        try:
            outdir = "out_feasibility_atlas_v124"
            bundle = build_feasibility_atlas_bundle(
                baseline_run_artifact=base,
                lever_x=kx,
                lever_y=ky,
                x_range=(float(xlo), float(xhi)),
                y_range=(float(ylo), float(yhi)),
                nx=int(nx),
                ny=int(ny),
                outdir=outdir,
                version="v124",
            )
            st.session_state["v124_bundle"] = bundle
            _v98_record_run("feasibility_atlas", {"lever_x": kx, "lever_y": ky, "nx": int(nx), "ny": int(ny)}, mode="feasibility_atlas_v124")
            st.success("Atlas generated.")
        except Exception as e:
            st.error(f"Failed: {e!r}")

    bundle = st.session_state.get("v124_bundle")
    if isinstance(bundle, dict) and isinstance(bundle.get("zip_bytes"), (bytes, bytearray)):
        st.download_button("Download atlas_bundle_v124.zip",
                           data=bundle["zip_bytes"],
                           file_name="atlas_bundle_v124.zip",
                           mime="application/zip",
                           use_container_width=True,
                           key="v124_dl_zip")
        st.download_button("Download feasibility_atlas_v124.json",
                           data=_json.dumps(bundle, indent=2, sort_keys=True),
                           file_name="feasibility_atlas_v124.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v124_dl_json")
        st.write("Atlas summary:")
        st.json({"baseline_run_id": bundle.get("baseline_run_id"),
                 "lever_x": bundle.get("lever_x"), "lever_y": bundle.get("lever_y"),
                 "grid": bundle.get("grid"),
                 "n_slices": len((bundle.get("atlas_v2") or {}).get("slices", [])) if isinstance(bundle.get("atlas_v2"), dict) else None})


# =====================
# v125 One-Click Paper Pack - additive UI panel
# =====================
def _v125_paper_pack_panel():
    import streamlit as st
    from tools.mission_context import load_mission, apply_mission_overlays
    from tools.explainability import build_explainability_report
    from tools.evidence_graph import build_evidence_graph, build_traceability_table, traceability_csv
    from tools.feasibility_atlas import build_feasibility_atlas_bundle, available_numeric_levers
    from tools.study_kit import build_study_kit_zip
    from tools.study_orchestrator import build_paper_pack_zip

    st.subheader("One-Click Paper Pack")
    st.caption("Runs post-processing pipeline (mission â†’ explainability â†’ evidence/traceability â†’ atlas â†’ study kit) and exports a single publishable zip + manifest. No physics/solver changes.")

    s = _v98_state_init_runlists()
    ids = [r.get("id") for r in (s.run_history or []) if r.get("id")]
    if not ids:
        st.info("No runs in ledger yet. Run a point evaluation first.")
        return

    default_id = (s.pinned_run_ids[-1] if (s.pinned_run_ids and len(s.pinned_run_ids)>0) else ids[-1])
    rid = st.selectbox("Select baseline run artifact", options=ids, index=ids.index(default_id) if default_id in ids else len(ids)-1, key="v125_pick_run")
    run_map = {r.get("id"): r for r in (s.run_history or []) if r.get("id")}
    payload = (run_map.get(rid) or {}).get("payload")
    base = payload if (isinstance(payload, dict) and payload.get("kind") == "shams_run_artifact") else None
    if base is None:
        st.error("Selected run does not contain a run artifact payload.")
        return

    st.write("Pipeline options:")
    c1,c2,c3 = st.columns(3)
    with c1:
        do_mission = st.checkbox("Mission overlay", value=True, key="v125_do_mission")
        do_expl = st.checkbox("Explainability", value=True, key="v125_do_expl")
    with c2:
        do_ev = st.checkbox("Evidence+Traceability (v123)", value=True, key="v125_do_ev")
        do_atlas = st.checkbox("Feasibility Atlas (v124)", value=False, key="v125_do_atlas")
    with c3:
        do_kit = st.checkbox("Study Kit (v123B)", value=True, key="v125_do_kit")

    mission_name = None
    mission_rep = None
    if do_mission:
        missions = ["pilot","demo","powerplant"]
        msel = st.selectbox("Mission", options=missions, index=0, key="v125_mission")
        mission_name = msel

    atlas_cfg = None
    if do_atlas:
        base_inputs = base.get("inputs", {}) if isinstance(base.get("inputs"), dict) else {}
        levers = available_numeric_levers(base_inputs)
        if len(levers) >= 2:
            kx = st.selectbox("Atlas lever X", options=levers, index=0, key="v125_kx")
            ky = st.selectbox("Atlas lever Y", options=levers, index=1, key="v125_ky")

            def _default_range(v):
                try: v=float(v)
                except Exception: return (0.0, 1.0)
                if abs(v) < 1e-9: return (-1.0, 1.0)
                return (v*0.9, v*1.1)  # narrower defaults for v125
            x0 = base_inputs.get(kx); y0 = base_inputs.get(ky)
            xlo_def,xhi_def=_default_range(x0); ylo_def,yhi_def=_default_range(y0)
            r1,r2=st.columns(2)
            with r1:
                xlo = st.number_input("X min", value=float(xlo_def), key="v125_xlo")
                xhi = st.number_input("X max", value=float(xhi_def), key="v125_xhi")
            with r2:
                ylo = st.number_input("Y min", value=float(ylo_def), key="v125_ylo")
                yhi = st.number_input("Y max", value=float(yhi_def), key="v125_yhi")
            nx = st.slider("Atlas NX", min_value=9, max_value=41, value=15, step=2, key="v125_nx")
            ny = st.slider("Atlas NY", min_value=9, max_value=41, value=15, step=2, key="v125_ny")
            atlas_cfg = {"lever_x": kx, "lever_y": ky, "x_range": (float(xlo), float(xhi)), "y_range": (float(ylo), float(yhi)), "nx": int(nx), "ny": int(ny)}
        else:
            st.warning("Not enough numeric inputs to run atlas from this baseline.")
            do_atlas = False

    if st.button("Build Paper Pack", key="v125_run"):
        try:
            # Mission report (post-processing)
            if do_mission and mission_name:
                mspec = load_mission(mission_name)
                # apply overlay to baseline; function returns report dict
                mission_rep = apply_mission_overlays(run_artifact=base, mission=mspec, version="v121")
                st.session_state["v121_mission_report"] = mission_rep
            else:
                mission_rep = None

            # Explainability
            expl_rep = None
            if do_expl:
                env_rep = st.session_state.get("v117_report")  # optional
                expl_rep = build_explainability_report(run_artifact=base, mission_report=mission_rep, tolerance_envelope_report=env_rep if isinstance(env_rep, dict) else None, version="v122")
                st.session_state["v122_explainability_report"] = expl_rep

            # Evidence + traceability
            graph = None
            tab = None
            tcsv = None
            if do_ev:
                env_rep = st.session_state.get("v117_report")
                graph = build_evidence_graph(run_artifact=base, mission_report=mission_rep, tolerance_envelope_report=env_rep if isinstance(env_rep, dict) else None, explainability_report=expl_rep, version="v123")
                tab = build_traceability_table(run_artifact=base, mission_report=mission_rep, tolerance_envelope_report=env_rep if isinstance(env_rep, dict) else None, explainability_report=expl_rep, version="v123")
                tcsv = traceability_csv(tab)
                st.session_state["v123_graph"] = graph
                st.session_state["v123_trace_table"] = tab

            # Atlas
            atlas_meta = None
            atlas_zip = None
            if do_atlas and atlas_cfg:
                bundle = build_feasibility_atlas_bundle(
                    baseline_run_artifact=base,
                    lever_x=atlas_cfg["lever_x"],
                    lever_y=atlas_cfg["lever_y"],
                    x_range=atlas_cfg["x_range"],
                    y_range=atlas_cfg["y_range"],
                    nx=atlas_cfg["nx"],
                    ny=atlas_cfg["ny"],
                    outdir="out_feasibility_atlas_v125",
                    version="v124",
                )
                atlas_zip = bundle.get("zip_bytes")
                atlas_meta = dict(bundle)
                atlas_meta.pop("zip_bytes", None)
                st.session_state["v124_bundle"] = bundle

            # Study kit
            kit_zip = None
            if do_kit:
                kit = build_study_kit_zip(
                    run_artifact=base,
                    mission_report=mission_rep,
                    tolerance_envelope_report=st.session_state.get("v117_report") if isinstance(st.session_state.get("v117_report"), dict) else None,
                    explainability_report=expl_rep,
                    evidence_graph=graph,
                    traceability_table=tab,
                    authority_pack_zip=(st.session_state.get("v119_pack") or {}).get("zip_bytes") if isinstance(st.session_state.get("v119_pack"), dict) else None,
                    optimizer_downstream_bundle_zip=(st.session_state.get("v118_bundle") or {}).get("zip_bytes") if isinstance(st.session_state.get("v118_bundle"), dict) else None,
                    decision_pack_zip=st.session_state.get("v118_pack_bytes") if isinstance(st.session_state.get("v118_pack_bytes"), (bytes, bytearray)) else None,
                    version="v123B",
                )
                kit_zip = kit.get("zip_bytes")
                st.session_state["v123_study_kit"] = kit

            # Final pack
            pack = build_paper_pack_zip(
                baseline_run_artifact=base,
                mission_report=mission_rep,
                explainability_report=expl_rep,
                evidence_graph=graph,
                traceability_table=tab,
                traceability_csv=tcsv,
                feasibility_atlas_meta=atlas_meta,
                atlas_bundle_zip=atlas_zip,
                study_kit_zip=kit_zip,
                version="v125",
            )
            st.session_state["v125_paper_pack"] = pack
            _v98_record_run("paper_pack", {"files": len((pack.get("manifest") or {}).get("files", {}))}, mode="paper_pack_v125")
            st.success("Paper pack built.")
        except Exception as e:
            st.error(f"Failed: {e!r}")

    pack = st.session_state.get("v125_paper_pack")
    if isinstance(pack, dict) and isinstance(pack.get("zip_bytes"), (bytes, bytearray)):
        st.download_button("Download paper_pack_v125.zip",
                           data=pack["zip_bytes"],
                           file_name="paper_pack_v125.zip",
                           mime="application/zip",
                           use_container_width=True,
                           key="v125_dl_zip")
        st.download_button("Download paper_pack_manifest_v125.json",
                           data=_json.dumps(pack.get("manifest", {}), indent=2, sort_keys=True),
                           file_name="paper_pack_manifest_v125.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v125_dl_manifest")
        st.write("Pack summary:")
        st.json({"files": len((pack.get("manifest") or {}).get("files", {})),
                 "created_utc": pack.get("created_utc")})


# =====================
# v126 UI Smoke & Diagnostics - additive UI panel
# =====================
def _v126_ui_smoke_panel():
    import streamlit as st
    from pathlib import Path as _Path

    st.subheader("UI Smoke & Diagnostics")
    st.caption("Runs lightweight UI smoke checks and produces a report. This does not change physics/solvers.")

    scenarios = st.multiselect("Scenarios", options=["render_all", "paper_pack"], default=["render_all"], key="v126_scenarios")
    outdir = st.text_input("Output directory", value="out_ui_smoke_v126", key="v126_outdir")

    if st.button("Run UI smoke checks", key="v126_run"):
        try:
            cmd = [_sys.executable, "-m", "tools.cli_ui_smoke", "--outdir", outdir] + [ "--scenarios"] + list(scenarios)
            # note: argparse expects --scenarios then list; we pass that
            p = _subprocess.run(cmd, cwd=str(_Path(__file__).resolve().parents[1]), stdout=_subprocess.PIPE, stderr=_subprocess.STDOUT, text=True, timeout=300)
            st.text_area("Runner output", value=p.stdout, height=180, key="v126_out")
            rep_path = _Path(__file__).resolve().parents[1] / outdir / "ui_smoke_report.json"
            if rep_path.exists():
                rep = _json.loads(rep_path.read_text(encoding="utf-8"))
                st.session_state["v126_smoke_report"] = rep
                st.success("Smoke report generated.")
            else:
                st.warning("Report file not found after run.")
        except Exception as e:
            st.error(f"Failed: {e!r}")

    rep = st.session_state.get("v126_smoke_report")
    if isinstance(rep, dict):
        st.write("Summary:")
        st.json(rep.get("summary", {}))
        st.download_button("Download ui_smoke_report.json",
                           data=_json.dumps(rep, indent=2, sort_keys=True),
                           file_name="ui_smoke_report_v126.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v126_dl_json")


# =====================
# v127 Study Matrix - Batch Paper Packs (single UI)
# =====================
def _v127_study_matrix_panel():
    import streamlit as st
    from tools.study_matrix import build_cases_1d_sweep, build_study_matrix_bundle
    from tools.feasibility_atlas import available_numeric_levers

    st.subheader("Study Matrix + Batch Paper Packs")
    st.caption("Build multiple cases from a baseline and export a single study zip with per-case paper packs + index. Additive; no solver behavior changes.")

    s = _v98_state_init_runlists()
    ids = [r.get("id") for r in (s.run_history or []) if r.get("id")]
    if not ids:
        st.info("No runs in ledger yet. Run a point evaluation first.")
        return

    default_id = (s.pinned_run_ids[-1] if (s.pinned_run_ids and len(s.pinned_run_ids)>0) else ids[-1])
    rid = st.selectbox("Select baseline run artifact", options=ids, index=ids.index(default_id) if default_id in ids else len(ids)-1, key="v127_pick_run")
    run_map = {r.get("id"): r for r in (s.run_history or []) if r.get("id")}
    payload = (run_map.get(rid) or {}).get("payload")
    base = payload if (isinstance(payload, dict) and payload.get("kind") == "shams_run_artifact") else None
    if base is None:
        st.error("Selected run does not contain a run artifact payload.")
        return

    base_inputs = base.get("inputs", {}) if isinstance(base.get("inputs"), dict) else {}
    levers = available_numeric_levers(base_inputs)
    if not levers:
        st.warning("No numeric levers detected in baseline inputs.")
        return

    st.write("1D sweep builder (safe defaults):")
    c1,c2,c3 = st.columns(3)
    with c1:
        lever = st.selectbox("Sweep lever", options=levers, index=0, key="v127_lever")
    with c2:
        v0 = float(base_inputs.get(lever, 0.0) or 0.0)
        vmin = st.number_input("Min", value=(v0*0.95 if abs>1e-9 else -1.0), key="v127_vmin")
        vmax = st.number_input("Max", value=(v0*1.05 if abs>1e-9 else 1.0), key="v127_vmax")
    with c3:
        n = st.slider("N cases", min_value=3, max_value=15, value=5, step=1, key="v127_n")

    missions = st.multiselect("Missions (optional)", options=["pilot","demo","powerplant"], default=["pilot"], key="v127_missions")
    include_expl = st.checkbox("Include explainability (v122)", value=True, key="v127_expl")
    include_ev = st.checkbox("Include evidence/traceability (v123)", value=True, key="v127_ev")
    include_kit = st.checkbox("Include study kit (v123B)", value=True, key="v127_kit")

    if st.button("Build Study Matrix Zip", key="v127_run"):
        try:
            if int(n) < 2:
                raise ValueError("N must be >= 2")
            # linspace
            vals = [float(vmin) + (float(vmax)-float(vmin))*i/(int(n)-1) for i in range(int(n))]
            cases = build_cases_1d_sweep(baseline_run_artifact=base, lever=str(lever), values=vals, missions=(missions or None))
            bundle = build_study_matrix_bundle(
                baseline_run_artifact=base,
                cases=cases,
                outdir="out_study_matrix_v127",
                version="v127",
                include_explainability=bool(include_expl),
                include_evidence=bool(include_ev),
                include_study_kit=bool(include_kit),
            )
            st.session_state["v127_bundle"] = bundle
            _v98_record_run("study_matrix", {"cases": bundle.get("cases")}, mode="study_matrix_v127")
            st.success(f"Study matrix built: {bundle.get('cases')} cases.")
        except Exception as e:
            st.error(f"Failed: {e!r}")

    bun = st.session_state.get("v127_bundle")
    if isinstance(bun, dict) and isinstance(bun.get("zip_bytes"), (bytes, bytearray)):
        st.download_button("Download study_matrix_v127.zip",
                           data=bun["zip_bytes"],
                           file_name="study_matrix_v127.zip",
                           mime="application/zip",
                           use_container_width=True,
                           key="v127_dl_zip")
        st.download_button("Download study_matrix_manifest_v127.json",
                           data=_json.dumps(bun.get("manifest", {}), indent=2, sort_keys=True),
                           file_name="study_matrix_manifest_v127.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v127_dl_manifest")
        st.json({"cases": bun.get("cases"), "created_utc": bun.get("created_utc")})


# =====================
# v128 Study Explorer + Comparator (single UI)
# =====================
def _v128_study_explorer_panel():
    import streamlit as st
    from tools.study_explorer import load_study_zip, parse_study_index, filter_cases, load_case_run_artifact, compare_two_runs

    st.subheader("Study Explorer + Comparator")
    st.caption("Load a study_matrix zip and browse/filter cases, compare two cases, and export comparison JSON. Downstream-only.")

    up = st.file_uploader("Upload study_matrix zip", type=["zip"], key="v128_upl")
    if not up:
        st.info("Upload a v127 study_matrix zip (study_matrix_v127.zip).")
        return

    try:
        # write to temp bytes map
        import tempfile, os
        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".zip")
        tmp.write(up.getvalue()); tmp.flush(); tmp.close()
        files = load_study_zip(tmp.name)
        idx = parse_study_index(files)
        st.session_state["v128_files"] = files
        st.session_state["v128_index"] = {"created_utc": idx.created_utc, "rows": idx.rows}
        os.unlink(tmp.name)
    except Exception as e:
        st.error(f"Failed to load: {e!r}")
        return

    idxd = st.session_state.get("v128_index") or {}
    rows = list(idxd.get("rows") or [])
    if not rows:
        st.warning("No rows found in study index.")
        return

    missions = sorted({str(r.get("mission","")) for r in rows if str(r.get("mission",""))})
    feasible_only = st.checkbox("Feasible only", value=False, key="v128_feas")
    mission = st.selectbox("Mission filter", options=["(any)"] + missions, index=0, key="v128_msel")

    # simple KPI filters
    kf = {}
    with st.expander("KPI filters (optional)"):
        c1,c2,c3 = st.columns(3)
        with c1:
            qlo = st.number_input("Q min", value=0.0, key="v128_qlo")
            kf["Q"] = (float(qlo), None)
        with c2:
            pnlo = st.number_input("Pnet_MW min", value=-1e9, key="v128_pnlo")
            kf["Pnet_MW"] = (float(pnlo), None)
        with c3:
            pflo = st.number_input("Pfus_MW min", value=0.0, key="v128_pflo")
            kf["Pfus_MW"] = (float(pflo), None)

    idx_obj = parse_study_index(st.session_state["v128_files"])
    fr = filter_cases(idx_obj, feasible_only=feasible_only, mission=None if mission=="(any)" else mission, kpi_filters=kf)

    st.write(f"Filtered cases: {len(fr)} / {len(rows)}")
    show = fr[:200]  # avoid UI overload
    st.dataframe(show, use_container_width=True)

    case_ids = [str(r.get("case_id")) for r in fr if r.get("case_id") is not None]
    if len(case_ids) < 2:
        st.info("Need at least 2 filtered cases to compare.")
        return

    a_id = st.selectbox("Case A", options=case_ids, index=0, key="v128_a")
    b_id = st.selectbox("Case B", options=case_ids, index=1 if len(case_ids)>1 else 0, key="v128_b")

    if st.button("ðŸ†š Compare", key="v128_compare"):
        try:
            files = st.session_state["v128_files"]
            a_row = next(r for r in fr if str(r.get("case_id"))==a_id)
            b_row = next(r for r in fr if str(r.get("case_id"))==b_id)
            a_art = load_case_run_artifact(files, a_row)
            b_art = load_case_run_artifact(files, b_row)
            comp = compare_two_runs(a_art, b_art)
            st.session_state["v128_comp"] = comp
            st.success("Comparison built.")
        except Exception as e:
            st.error(f"Failed: {e!r}")

    comp = st.session_state.get("v128_comp")
    if isinstance(comp, dict):
        st.write("Comparison summary:")
        st.json(comp)
        st.download_button("Download comparison_v128.json",
                           data=_json.dumps(comp, indent=2, sort_keys=True),
                           file_name="comparison_v128.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v128_dl_comp")


# =====================
# v129 Pareto from Study (single UI)
# =====================
def _v129_pareto_panel():
    import streamlit as st
    from tools.pareto_from_study import build_pareto, pareto_bundle_zip

    st.subheader("Pareto from Study")
    st.caption("Compute Pareto fronts from a study_matrix zip. No optimizer. Downstream-only, publishable.")

    up = st.file_uploader("Upload study_matrix zip", type=["zip"], key="v129_upl")
    if not up:
        st.info("Upload a v127 study_matrix zip (study_matrix_v127.zip).")
        return

    # Save upload to temp file for existing loaders
    import tempfile, os
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".zip")
    tmp.write(up.getvalue()); tmp.flush(); tmp.close()
    study_path = tmp.name

    st.write("Objectives (choose senses):")
    c1,c2,c3 = st.columns(3)
    with c1:
        obj1 = st.text_input("Objective 1 column", value="Q", key="v129_o1")
        s1 = st.selectbox("Sense 1", options=["max","min"], index=0, key="v129_s1")
    with c2:
        obj2 = st.text_input("Objective 2 column", value="Pnet_MW", key="v129_o2")
        s2 = st.selectbox("Sense 2", options=["max","min"], index=0, key="v129_s2")
    with c3:
        obj3 = st.text_input("Objective 3 column (optional)", value="R0_m", key="v129_o3")
        s3 = st.selectbox("Sense 3", options=["min","max"], index=0, key="v129_s3")

    feasible_only = st.checkbox("Feasible only", value=True, key="v129_feas")
    mission = st.selectbox("Mission filter", options=["(any)","pilot","demo","powerplant"], index=0, key="v129_mis")

    objs=[{"k": obj1.strip(), "sense": s1}]
    if obj2.strip():
        objs.append({"k": obj2.strip(), "sense": s2})
    if obj3.strip():
        objs.append({"k": obj3.strip(), "sense": s3})

    if st.button("Compute Pareto", key="v129_run"):
        try:
            rep = build_pareto(study_path=study_path, objectives=objs, feasible_only=bool(feasible_only), mission=None if mission=="(any)" else mission, version="v129")
            bun = pareto_bundle_zip(rep)
            st.session_state["v129_rep"]=rep
            st.session_state["v129_bun"]=bun
            st.success(f"Computed Pareto: {rep.get('n_filtered')} rows, layers: {len(rep.get('layer_counts') or {})}")
        except Exception as e:
            st.error(f"Failed: {e!r}")
        finally:
            try: os.unlink(study_path)
            except Exception: pass

    rep = st.session_state.get("v129_rep")
    bun = st.session_state.get("v129_bun")
    if isinstance(rep, dict):
        st.write("Summary:")
        st.json({"n_filtered": rep.get("n_filtered"), "layer_counts": rep.get("layer_counts"), "objectives": rep.get("objectives")})
        st.dataframe((rep.get("rows") or [])[:200], use_container_width=True)
        st.download_button("Download pareto_report_v129.json",
                           data=_json.dumps(rep, indent=2, sort_keys=True),
                           file_name="pareto_report_v129.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v129_dl_rep")
    if isinstance(bun, dict) and isinstance(bun.get("zip_bytes"), (bytes,bytearray)):
        st.download_button("Download pareto_bundle_v129.zip",
                           data=bun["zip_bytes"],
                           file_name="pareto_bundle_v129.zip",
                           mime="application/zip",
                           use_container_width=True,
                           key="v129_dl_zip")


# =====================
# v130 Persistent Run Vault (single UI)
# =====================
def _v130_run_vault_panel():
    import streamlit as st
    from pathlib import Path
    from tools.run_vault import list_entries

    st.subheader("Persistent Run Vault")
    st.caption("Append-only, local vault of run artifacts and bundles. Prevents loss due to reruns/downloads. Storage-only; never changes physics/solvers.")

    c1,c2 = st.columns(2)
    with c1:
        enabled = st.toggle("Enable vault persistence", value=bool(st.session_state.get("vault_enabled", True)), key="vault_enabled")
    with c2:
        limit = st.slider("Show last N entries", min_value=10, max_value=200, value=int(st.session_state.get("vault_limit", 50)), step=10, key="vault_limit")

    root = Path(__file__).resolve().parents[1]
    entries = list_entries(root, limit=int(limit))
    st.write(f"Vault entries: showing last {len(entries)}")
    if entries:
        st.dataframe(entries, use_container_width=True)

        # allow downloading index file
        idx_path = root / "out_run_vault" / "INDEX.jsonl"
        if idx_path.exists():
            st.download_button("Download vault INDEX.jsonl",
                               data=idx_path.read_bytes(),
                               file_name="INDEX.jsonl",
                               mime="text/plain",
                               use_container_width=True,
                               key="v130_dl_index")
    else:
        st.info("No entries yet. Run a point evaluation or generate a bundle; entries will appear here.")


# =====================
# v131 Vault Restore + Session Replay (single UI)
# =====================
def _v131_vault_restore_panel():
    import streamlit as st
    from pathlib import Path
    from tools.vault_restore import list_entries, load_entry_payload, list_entry_files, read_entry_file

    st.subheader("Vault Restore + Session Replay")
    st.caption("Restore runs and bundles from the persistent vault back into the UI ledger. Downstream-only; no evaluation is performed.")

    root = Path(__file__).resolve().parents[1]
    entries = list_entries(root, limit=int(st.session_state.get("vault_limit", 50)))
    if not entries:
        st.info("No vault entries found yet. Enable the vault and run evaluations/bundles first.")
        return

    # select entry
    labels = []
    for e in entries:
        labels.append(f"{e.get('created_utc','')} | {e.get('record_kind','')} | {e.get('mode','')} | {str(e.get('sha256',''))[:10]}")
    idx = st.selectbox("Select vault entry", options=list(range(len(entries))), format_func=lambda i: labels[i], key="v131_pick")
    meta = entries[int(idx)]

    st.write("Entry meta:")
    st.json(meta)

    c1,c2,c3 = st.columns(3)
    with c1:
        if st.button("Restore payload to Run Ledger", key="v131_restore"):
            try:
                payload = load_entry_payload(root, meta)
                kind = str(meta.get("record_kind") or "vault_restore")
                mode = str(meta.get("mode") or "vault_restore_v131")
                _v98_record_run(kind, payload, mode=mode)
                st.success("Restored into run ledger.")
            except Exception as e:
                st.error(f"Restore failed: {e!r}")

    with c2:
        if st.button("Load payload into viewer (no restore)", key="v131_load"):
            try:
                payload = load_entry_payload(root, meta)
                st.session_state["v131_loaded_payload"] = payload
                st.success("Loaded.")
            except Exception as e:
                st.error(f"Load failed: {e!r}")

    with c3:
        show_files = st.toggle("Show attached files", value=False, key="v131_show_files")

    payload = st.session_state.get("v131_loaded_payload")
    if payload is not None:
        st.write("Loaded payload preview:")
        if isinstance(payload, (bytes, bytearray)):
            st.write(f"Binary payload: {len(payload)} bytes")
        else:
            try:
                st.json(payload)
            except Exception:
                st.write(repr(payload)[:2000])

    if show_files:
        fnames = list_entry_files(root, meta)
        if not fnames:
            st.info("No attached files for this entry.")
        else:
            st.write("Attached files:")
            for fn in fnames:
                b = read_entry_file(root, meta, fn)
                st.download_button(f"Download {fn}", data=b, file_name=fn, use_container_width=True, key=f"v131_dl_{fn}")


# =====================
# v132 Study Matrix Builder v2 (multi-lever sweep) (single UI)
# =====================
def _v132_study_matrix_v2_panel():
    import streamlit as st
    from tools.study_matrix_v2 import build_cases_multi_sweep, build_study_matrix_bundle_v2
    from tools.feasibility_atlas import available_numeric_levers

    st.subheader("Study Matrix Builder v2")
    st.caption("Multi-lever sweeps (2D/3D) with derived columns. Produces a study zip compatible with Study Explorer and Pareto.")

    s = _v98_state_init_runlists()
    ids = [r.get("id") for r in (s.run_history or []) if r.get("id")]
    if not ids:
        st.info("No runs in ledger yet. Run a point evaluation first.")
        return

    default_id = (s.pinned_run_ids[-1] if (s.pinned_run_ids and len(s.pinned_run_ids)>0) else ids[-1])
    rid = st.selectbox("Select baseline run artifact", options=ids, index=ids.index(default_id) if default_id in ids else len(ids)-1, key="v132_pick_run")
    run_map = {r.get("id"): r for r in (s.run_history or []) if r.get("id")}
    payload = (run_map.get(rid) or {}).get("payload")
    base = payload if (isinstance(payload, dict) and payload.get("kind") == "shams_run_artifact") else None
    if base is None:
        st.error("Selected run does not contain a run artifact payload.")
        return

    base_inputs = base.get("inputs", {}) if isinstance(base.get("inputs"), dict) else {}
    levers = available_numeric_levers(base_inputs)
    if not levers:
        st.warning("No numeric levers detected in baseline inputs.")
        return

    st.write("Define up to 3 sweeps (leave lever blank to disable a sweep):")
    sweeps=[]
    cols = st.columns(3)
    for i in range(3):
        with cols[i]:
            lever = st.selectbox(f"Sweep {i+1} lever", options=["(none)"]+levers, index=0, key=f"v132_lev_{i}")
            if lever != "(none)":
                v0 = float(base_inputs.get(lever, 0.0) or 0.0)
                vmin = st.number_input(f"Min {i+1}", value=(v0*0.95 if abs>1e-9 else -1.0), key=f"v132_min_{i}")
                vmax = st.number_input(f"Max {i+1}", value=(v0*1.05 if abs>1e-9 else 1.0), key=f"v132_max_{i}")
                n = st.slider(f"N {i+1}", min_value=2, max_value=10, value=4, step=1, key=f"v132_n_{i}")
                sweeps.append({"lever": str(lever), "min": float(vmin), "max": float(vmax), "n": int(n)})

    if not sweeps:
        st.info("Choose at least one sweep lever.")
        return

    missions = st.multiselect("Missions (optional)", options=["pilot","demo","powerplant"], default=["pilot"], key="v132_missions")
    derived = st.multiselect("Derived columns", options=["Pnet_per_R0","Q_per_Bt","margin_penalty"], default=["Pnet_per_R0","Q_per_Bt","margin_penalty"], key="v132_derived")

    include_expl = st.checkbox("Include explainability", value=True, key="v132_expl")
    include_ev = st.checkbox("Include evidence/traceability (v123)", value=True, key="v132_ev")
    include_kit = st.checkbox("Include study kit (v123B)", value=True, key="v132_kit")

    est = 1
    for sw in sweeps:
        est *= int(sw.get("n",1))
    est *= max(1, len(missions) if missions else 1)
    st.warning(f"Estimated cases: {est}. Keep it small for local runs.")

    if st.button("Build Study Matrix v132 Zip", key="v132_run"):
        try:
            cases = build_cases_multi_sweep(baseline_run_artifact=base, sweeps=sweeps, missions=(missions or None))
            bundle = build_study_matrix_bundle_v2(
                baseline_run_artifact=base,
                cases=cases,
                outdir="out_study_matrix_v132",
                version="v132",
                include_explainability=bool(include_expl),
                include_evidence=bool(include_ev),
                include_study_kit=bool(include_kit),
                derived=list(derived or []),
            )
            st.session_state["v132_bundle"] = bundle
            _v98_record_run("study_matrix_v2", {"cases": bundle.get("cases")}, mode="study_matrix_v132")
            st.success(f"Study matrix built: {bundle.get('cases')} cases.")
        except Exception as e:
            st.error(f"Failed: {e!r}")

    bun = st.session_state.get("v132_bundle")
    if isinstance(bun, dict) and isinstance(bun.get("zip_bytes"), (bytes, bytearray)):
        st.download_button("Download study_matrix_v132.zip",
                           data=bun["zip_bytes"],
                           file_name="study_matrix_v132.zip",
                           mime="application/zip",
                           use_container_width=True,
                           key="v132_dl_zip")
        st.download_button("Download study_matrix_manifest_v132.json",
                           data=_json.dumps(bun.get("manifest", {}), indent=2, sort_keys=True),
                           file_name="study_matrix_manifest_v132.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v132_dl_manifest")
        st.json({"cases": bun.get("cases"), "created_utc": bun.get("created_utc"), "version": bun.get("version")})


# =====================
# v133 Feasibility Completion (Partial Design Inference) (single UI)
# =====================
def _v133_fc_panel():
    import streamlit as st
    from tools.feasibility_completion import FCConfig, run_feasibility_completion, build_fc_bundle_zip
    from tools.feasibility_atlas import available_numeric_levers
    from tools import run_vault
    from pathlib import Path

    st.subheader("Feasibility Completion")
    st.caption("Partial design inference: fix a few major parameters, mark others as FREE/UNCERTAIN within bounds, and search for any feasible completions. Not an optimizer.")

    # baseline for lever discovery + default values
    s = _v98_state_init_runlists()
    ids = [r.get("id") for r in (s.run_history or []) if r.get("id")]
    if not ids:
        st.info("Run a point evaluation first so SHAMS has a baseline inputs dictionary to start from.")
        return

    default_id = (s.pinned_run_ids[-1] if (s.pinned_run_ids and len(s.pinned_run_ids)>0) else ids[-1])
    rid = st.selectbox("Baseline run (for defaults)", options=ids, index=ids.index(default_id) if default_id in ids else len(ids)-1, key="v133_pick_run")
    run_map = {r.get("id"): r for r in (s.run_history or []) if r.get("id")}
    payload = (run_map.get(rid) or {}).get("payload")
    base = payload if (isinstance(payload, dict) and payload.get("kind") == "shams_run_artifact") else None
    if base is None:
        st.error("Selected baseline does not contain a run artifact payload.")
        return

    base_inputs = base.get("inputs", {}) if isinstance(base.get("inputs"), dict) else {}
    levers = available_numeric_levers(base_inputs)
    if not levers:
        st.warning("No numeric levers detected.")
        return

    st.write("1) Choose FIXED parameters (set values).")
    fixed_choices = st.multiselect("Fixed parameters", options=levers, default=[x for x in ["R0_m","Bt_T","Ip_MA"] if x in levers], key="v133_fixed_list")
    fixed = {}
    if fixed_choices:
        cols = st.columns(min(3, len(fixed_choices)))
        for i,k in enumerate(fixed_choices):
            with cols[i % len(cols)]:
                v0 = float(base_inputs.get(k, 0.0) or 0.0)
                fixed[k] = st.number_input(f"Fixed {k}", value=v0, key=f"v133_fix_{k}")

    st.write("2) Choose FREE parameters (SHAMS searches within bounds). Up to 3 recommended.")
    free_choices = st.multiselect("Free parameters", options=[x for x in levers if x not in fixed_choices], default=[x for x in ["kappa","q95"] if x in levers and x not in fixed_choices], key="v133_free_list")

    st.write("3) Choose UNCERTAIN parameters (sampled within bounds). Optional.")
    uncertain_choices = st.multiselect("Uncertain parameters", options=[x for x in levers if x not in fixed_choices and x not in free_choices], default=[], key="v133_unc_list")

    vars_all = list(dict.fromkeys(list(free_choices) + list(uncertain_choices)))
    if not vars_all:
        st.info("Select at least one FREE or UNCERTAIN parameter.")
        return

    bounds = {}
    st.write("4) Bounds for FREE/UNCERTAIN parameters:")
    for k in vars_all:
        v0 = float(base_inputs.get(k, 0.0) or 0.0)
        c1,c2,c3 = st.columns([1,1,1])
        with c1:
            lo = st.number_input(f"{k} min", value=(v0*0.95 if abs>1e-9 else -1.0), key=f"v133_lo_{k}")
        with c2:
            hi = st.number_input(f"{k} max", value=(v0*1.05 if abs>1e-9 else 1.0), key=f"v133_hi_{k}")
        with c3:
            st.caption(f"baseline={v0:g}")
        bounds[k] = (float(lo), float(hi))

    st.write("5) Search method and budget:")
    method = st.selectbox("Method", options=["grid","random"], index=0, key="v133_method")
    if method == "grid":
        n_per_dim = st.slider("Grid points per dimension", min_value=2, max_value=10, value=4, step=1, key="v133_npd")
        est = 1
        for _ in vars_all: est *= int(n_per_dim)
        st.warning(f"Estimated evaluations: {est}")
        n_random = 0
    else:
        n_random = st.slider("Random samples", min_value=50, max_value=2000, value=300, step=50, key="v133_nr")
        n_per_dim = 0
        st.warning(f"Estimated evaluations: {n_random}")

    seed = st.number_input("Seed (determinism)", value=0, step=1, key="v133_seed")
    feasible_only_export = st.checkbox("Export only feasible evaluations", value=True, key="v133_fe_only")

    if st.button("Run Feasibility Completion", key="v133_run"):
        try:
            cfg = FCConfig(
                baseline_inputs=dict(base_inputs),
                fixed=dict(fixed),
                bounds=dict(bounds),
                free=list(free_choices),
                uncertain=list(uncertain_choices),
                method=str(method),
                n_per_dim=int(n_per_dim) if method=="grid" else 0,
                n_random=int(n_random) if method=="random" else 0,
                seed=int(seed),
                feasible_only_export=bool(feasible_only_export),
            )
            rep = run_feasibility_completion(cfg)
            bun = build_fc_bundle_zip(rep)
            st.session_state["v133_rep"] = rep
            st.session_state["v133_bun"] = bun
            _v98_record_run("feasibility_completion", {"exists_feasible": rep.get("exists_feasible"), "n_evals": rep.get("n_evals"), "n_feasible": rep.get("n_feasible")}, mode="fc_v133")
            st.success(f"Done. Feasible: {rep.get('exists_feasible')} (feasible points: {rep.get('n_feasible')}/{rep.get('n_evals')})")
        except Exception as e:
            st.error(f"Failed: {e!r}")

    rep = st.session_state.get("v133_rep")
    bun = st.session_state.get("v133_bun")
    if isinstance(rep, dict):
        st.write("Report summary:")
        st.json({"exists_feasible": rep.get("exists_feasible"),
                 "n_evals": rep.get("n_evals"),
                 "n_feasible": rep.get("n_feasible"),
                 "envelope": rep.get("envelope"),
                 "dominant_constraint_counts": rep.get("dominant_constraint_counts")})
        st.dataframe((rep.get("evaluations") or [])[:200], use_container_width=True)
        st.download_button("Download fc_report_v133.json",
                           data=_json.dumps(rep, indent=2, sort_keys=True),
                           file_name="fc_report_v133.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v133_dl_rep")

    if isinstance(bun, dict) and isinstance(bun.get("zip_bytes"), (bytes, bytearray)):
        st.download_button("Download fc_bundle_v133.zip",
                           data=bun["zip_bytes"],
                           file_name="fc_bundle_v133.zip",
                           mime="application/zip",
                           use_container_width=True,
                           key="v133_dl_zip")

        if st.button("Save FC bundle to Vault (attach zip)", key="v133_save_vault"):
            try:
                root = Path(__file__).resolve().parents[1]
                meta = run_vault.write_entry(root=root, kind="fc_bundle", payload=rep or {}, mode="v133", files={"fc_bundle_v133.zip": bun["zip_bytes"]})
                st.success(f"Saved to vault: {meta.get('entry_dir')}")
            except Exception as e:
                st.error(f"Vault save failed: {e!r}")


# =====================
# v134â€“v138 FC Superpanel (single UI)
# =====================
def _v138_fc_superpanel():
    import streamlit as st
    from pathlib import Path
    from tools import run_vault
    from tools.param_guidance import suggest_free_vars, suggest_bounds, sanity_check_bounds
    from tools.feasibility_completion import FCConfig, run_feasibility_completion, build_fc_bundle_zip
    from tools.fc_advanced import build_fc_atlas_bundle, repair_to_feasibility, RepairConfig, compress_feasible_set, completion_to_run_artifact

    st.subheader("Feasibility Completion Advanced (v134â€“v138)")
    st.caption("Atlas + guided setup + bounded repair + compression + handoff to study tools. All downstream/orchestration only.")

    # baseline for defaults
    s = _v98_state_init_runlists()
    ids = [r.get("id") for r in (s.run_history or []) if r.get("id")]
    if not ids:
        st.info("Run a point evaluation first to populate the run ledger.")
        return

    default_id = (s.pinned_run_ids[-1] if (s.pinned_run_ids and len(s.pinned_run_ids)>0) else ids[-1])
    rid = st.selectbox("Baseline run (defaults)", options=ids, index=ids.index(default_id) if default_id in ids else len(ids)-1, key="v138_pick_run")
    run_map = {r.get("id"): r for r in (s.run_history or []) if r.get("id")}
    payload = (run_map.get(rid) or {}).get("payload")
    base = payload if (isinstance(payload, dict) and payload.get("kind") == "shams_run_artifact") else None
    if base is None:
        st.error("Selected baseline does not contain a run artifact.")
        return
    base_inputs = base.get("inputs", {}) if isinstance(base.get("inputs"), dict) else {}

    # Use the v133 panel state if present
    rep = st.session_state.get("v133_rep")
    bun = st.session_state.get("v133_bun")

    tabs = st.tabs(["Guided Setup", "Run FC", "Atlas", "Repair (v136)", "Compress (v137)", "Handoff (v138)"])

    # ---------------- v135 ----------------
    with tabs[0]:
        st.write("Suggested FREE variables (heuristics):")
        avail = list(available_numeric_levers(base_inputs))
        fixed_guess = [x for x in ["R0_m","Bt_T"] if x in avail]
        suggested = suggest_free_vars(avail, fixed=fixed_guess, max_k=3)
        st.json({"suggested_free": suggested})
        st.write("Suggested bounds (editable):")
        b = {}
        for v in suggested:
            lo,hi = suggest_bounds(base_inputs, v)
            b[v] = [lo,hi]
            msgs = sanity_check_bounds(v, lo, hi)
            st.write(f"- {v}: [{lo:g}, {hi:g}]" + (f" âš  {', '.join(msgs)}" if msgs else ""))
        st.caption("You can copy these into the FC run tab.")

    # ---------------- v133 run ----------------
    with tabs[1]:
        st.write("Run Feasibility Completion using baseline defaults + your fixed/free/uncertain selections.")
        levers = list(available_numeric_levers(base_inputs))
        fixed_choices = st.multiselect("Fixed parameters", options=levers, default=[x for x in ["R0_m","Bt_T"] if x in levers], key="v138_fixed_list")
        fixed = {}
        for k in fixed_choices:
            v0 = float(base_inputs.get(k, 0.0) or 0.0)
            fixed[k] = st.number_input(f"Fixed {k}", value=v0, key=f"v138_fix_{k}")

        free_choices = st.multiselect("Free parameters", options=[x for x in levers if x not in fixed_choices], default=[x for x in ["Ip_MA","kappa","q95"] if x in levers and x not in fixed_choices], key="v138_free_list")
        uncertain_choices = st.multiselect("Uncertain parameters", options=[x for x in levers if x not in fixed_choices and x not in free_choices], default=[], key="v138_unc_list")
        vars_all = list(dict.fromkeys(list(free_choices)+list(uncertain_choices)))
        if vars_all:
            st.write("Bounds:")
        bounds={}
        for k in vars_all:
            lo0,hi0 = suggest_bounds(base_inputs, k)
            c1,c2 = st.columns(2)
            with c1:
                lo = st.number_input(f"{k} min", value=float(lo0), key=f"v138_lo_{k}")
            with c2:
                hi = st.number_input(f"{k} max", value=float(hi0), key=f"v138_hi_{k}")
            bounds[k]=(float(lo), float(hi))
            msgs = sanity_check_bounds(k, float(lo), float(hi))
            if msgs:
                st.warning(f"{k}: {', '.join(msgs)}")

        method = st.selectbox("Method", options=["grid","random"], index=0, key="v138_method")
        if method=="grid":
            n_per_dim = st.slider("Grid points per dimension", 2, 10, 4, 1, key="v138_npd")
            n_random = 0
        else:
            n_random = st.slider("Random samples", 50, 2000, 300, 50, key="v138_nr")
            n_per_dim = 0
        seed = st.number_input("Seed", value=0, step=1, key="v138_seed")
        feasible_only = st.checkbox("Export only feasible points", value=True, key="v138_fe_only")

        if st.button("Run FC", key="v138_run_fc"):
            cfg = FCConfig(
                baseline_inputs=dict(base_inputs),
                fixed=dict(fixed),
                bounds=dict(bounds),
                free=list(free_choices),
                uncertain=list(uncertain_choices),
                method=str(method),
                n_per_dim=int(n_per_dim) if method=="grid" else 0,
                n_random=int(n_random) if method=="random" else 0,
                seed=int(seed),
                feasible_only_export=bool(feasible_only),
            )
            rep = run_feasibility_completion(cfg)
            bun = build_fc_bundle_zip(rep)
            st.session_state["v133_rep"] = rep
            st.session_state["v133_bun"] = bun
            _v98_record_run("feasibility_completion", {"exists_feasible": rep.get("exists_feasible"), "n_evals": rep.get("n_evals"), "n_feasible": rep.get("n_feasible")}, mode="fc_v133")
            st.success(f"Feasible: {rep.get('exists_feasible')} (feasible points: {rep.get('n_feasible')}/{rep.get('n_evals')})")

        rep = st.session_state.get("v133_rep")
        bun = st.session_state.get("v133_bun")
        if isinstance(rep, dict):
            st.json({"exists_feasible": rep.get("exists_feasible"), "n_evals": rep.get("n_evals"), "n_feasible": rep.get("n_feasible"), "envelope": rep.get("envelope")})
            st.download_button("Download fc_report_v133.json", data=_json.dumps(rep, indent=2, sort_keys=True), file_name="fc_report_v133.json", mime="application/json", use_container_width=True)
        if isinstance(bun, dict) and isinstance(bun.get("zip_bytes"), (bytes, bytearray)):
            st.download_button("Download fc_bundle_v133.zip", data=bun["zip_bytes"], file_name="fc_bundle_v133.zip", mime="application/zip", use_container_width=True)
            if st.button("Save FC bundle to Vault", key="v138_save_fc_vault"):
                root = Path(__file__).resolve().parents[1]
                run_vault.write_entry(root=root, kind="fc_bundle", payload=rep or {}, mode="v138", files={"fc_bundle_v133.zip": bun["zip_bytes"]})
                st.success("Saved.")

    # ---------------- v134 atlas ----------------
    with tabs[2]:
        rep = st.session_state.get("v133_rep")
        if not isinstance(rep, dict):
            st.info("Run FC first (Run FC tab).")
        else:
            bounds = rep.get("config", {}).get("bounds", {}) if isinstance(rep.get("config"), dict) else {}
            vars_ = list(bounds.keys())
            if len(vars_) < 2:
                st.info("Need at least two bounded variables in FC config to build an atlas.")
            else:
                x = st.selectbox("X axis", options=vars_, index=0, key="v138_atlas_x")
                y = st.selectbox("Y axis", options=[v for v in vars_ if v != x], index=0, key="v138_atlas_y")
                if st.button("Build FC Atlas", key="v138_build_atlas"):
                    try:
                        bun_at = build_fc_atlas_bundle(rep, x_var=str(x), y_var=str(y))
                        st.session_state["v134_atlas"] = bun_at
                        _v98_record_run("fc_atlas", {"x": x, "y": y, "n": bun_at.get("manifest", {}).get("files", {}).get("atlas_points.csv", {}).get("bytes")}, mode="v134")
                        st.success("Atlas built.")
                    except Exception as e:
                        st.error(f"Atlas failed: {e!r}")

                bun_at = st.session_state.get("v134_atlas")
                if isinstance(bun_at, dict) and isinstance(bun_at.get("zip_bytes"), (bytes, bytearray)):
                    st.download_button("Download fc_atlas_v134.zip", data=bun_at["zip_bytes"], file_name="fc_atlas_v134.zip", mime="application/zip", use_container_width=True, key="v138_dl_atlas")
                    st.json(bun_at.get("manifest", {}))

    # ---------------- v136 repair ----------------
    with tabs[3]:
        rep = st.session_state.get("v133_rep")
        if not isinstance(rep, dict):
            st.info("Run FC first (Run FC tab).")
        else:
            evals = list(rep.get("evaluations") or [])
            infeas = [r for r in evals if r.get("feasible") is not True]
            cfg0 = rep.get("config", {}) if isinstance(rep.get("config"), dict) else {}
            bounds = cfg0.get("bounds", {}) if isinstance(cfg0.get("bounds"), dict) else {}
            free = list(cfg0.get("free") or [])
            if not free or not bounds:
                st.info("Repair requires FREE variables with bounds.")
            elif not infeas:
                st.success("No infeasible points in current export. Disable 'feasible-only export' if you want to repair infeasible samples.")
            else:
                st.write(f"Infeasible points available: {len(infeas)}")
                idx = st.slider("Pick infeasible sample index", 0, len(infeas)-1, 0, 1, key="v138_rep_idx")
                start = infeas[int(idx)].get("inputs", {}) if isinstance(infeas[int(idx)].get("inputs"), dict) else {}
                max_steps = st.slider("Max steps", 5, 30, 12, 1, key="v138_rep_steps")
                step_frac = st.slider("Step fraction", 0.05, 0.50, 0.15, 0.05, key="v138_rep_frac")
                seed = st.number_input("Repair seed", value=0, step=1, key="v138_rep_seed")
                if st.button("Run bounded repair", key="v138_run_repair"):
                    try:
                        bnd = {k: (float(v[0]), float(v[1])) for k,v in bounds.items()}
                        cfg = RepairConfig(bounds=bnd, free=list(free), max_steps=int(max_steps), step_frac=float(step_frac), seed=int(seed))
                        tr = repair_to_feasibility(baseline_inputs=dict(base_inputs), start_inputs=dict(start), cfg=cfg)
                        st.session_state["v136_trace"] = tr
                        st.success(f"Repair done. Feasible={tr.get('final',{}).get('feasible')}")
                    except Exception as e:
                        st.error(f"Repair failed: {e!r}")
                tr = st.session_state.get("v136_trace")
                if isinstance(tr, dict):
                    st.json({"final": tr.get("final"), "final_inputs": tr.get("final_inputs")})
                    st.dataframe(tr.get("trace", [])[:200], use_container_width=True)
                    st.download_button("Download repair_trace_v136.json", data=_json.dumps(tr, indent=2, sort_keys=True), file_name="repair_trace_v136.json", mime="application/json", use_container_width=True)

    # ---------------- v137 compress ----------------
    with tabs[4]:
        rep = st.session_state.get("v133_rep")
        if not isinstance(rep, dict):
            st.info("Run FC first.")
        else:
            k = st.slider("Representatives K", 5, 200, 25, 5, key="v138_k")
            if st.button("Compress feasible set", key="v138_compress"):
                comp = compress_feasible_set(rep, k=int(k))
                st.session_state["v137_comp"] = comp
                st.success("Compressed.")
            comp = st.session_state.get("v137_comp")
            if isinstance(comp, dict):
                st.json({"k": comp.get("k"), "n_feasible": comp.get("n_feasible")})
                st.dataframe(comp.get("representatives", [])[:200], use_container_width=True)
                st.download_button("Download fc_compressed_v137.json", data=_json.dumps(comp, indent=2, sort_keys=True), file_name="fc_compressed_v137.json", mime="application/json", use_container_width=True)

    # ---------------- v138 handoff ----------------
    with tabs[5]:
        comp = st.session_state.get("v137_comp")
        rep = st.session_state.get("v133_rep")
        if not isinstance(rep, dict):
            st.info("Run FC first.")
        else:
            # pick best feasible from compressed or report
            best_inputs = None
            if isinstance(comp, dict) and (comp.get("representatives")):
                best_inputs = (comp["representatives"][0].get("inputs") if isinstance(comp["representatives"][0], dict) else None)
            if best_inputs is None:
                feas = [r for r in (rep.get("evaluations") or []) if isinstance(r, dict) and r.get("feasible") is True]
                if feas:
                    best_inputs = feas[0].get("inputs")
            if not isinstance(best_inputs, dict):
                st.info("No feasible point available to hand off. Try expanding bounds or budget.")
            else:
                st.write("Best feasible completion inputs (preview):")
                st.json({k: best_inputs.get(k) for k in list(best_inputs.keys())[:40]})
                if st.button("Evaluate completion as new run + pin", key="v138_handoff_eval"):
                    try:
                        art = completion_to_run_artifact(baseline_inputs=dict(base_inputs), completion_inputs=dict(best_inputs))
                        rid2 = _v98_record_run("handoff_run_artifact", art, mode="fc_handoff_v138")
                        s.pinned_run_ids.append(rid2)
                        st.success(f"Created new run artifact in ledger: {rid2} (pinned)")
                    except Exception as e:
                        st.error(f"Handoff failed: {e!r}")
                st.caption("After creating the pinned run, go to Study Matrix Builder v2 and select the pinned baseline.")


# =====================
# v139 Feasibility Certificate
# =====================
def _v139_feasibility_certificate_panel():
    import streamlit as st
    import json as _json
    from pathlib import Path
    from tools import run_vault
    from tools.feasibility_certificate import generate_feasibility_certificate

    st.subheader("Feasibility Certificate")
    st.caption("Generate an immutable, audit-ready certificate for a specific run artifact.")

    s = _v98_state_init_runlists()
    ids = [r.get("id") for r in (s.run_history or []) if r.get("id")]
    if not ids:
        st.info("Run a point evaluation first to populate the run ledger.")
        return

    default_id = (s.pinned_run_ids[-1] if (s.pinned_run_ids and len(s.pinned_run_ids) > 0) else ids[-1])
    rid = st.selectbox("Select run", options=ids, index=ids.index(default_id) if default_id in ids else len(ids) - 1, key="v139_pick_run")
    run_map = {r.get("id"): r for r in (s.run_history or []) if r.get("id")}
    payload = (run_map.get(rid) or {}).get("payload")
    art = payload if (isinstance(payload, dict) and payload.get("kind") == "shams_run_artifact") else None
    if art is None:
        st.error("Selected entry does not contain a run artifact.")
        return

    origin = st.selectbox("Origin label", options=["point","fc_handoff","study_matrix",""], index=0, key="v139_origin")

    if st.button("Generate certificate", key="v139_gen"):
        root = Path(__file__).resolve().parents[1]
        cert = generate_feasibility_certificate(run_artifact=art, repo_root=root, run_id=str(rid), origin=str(origin))
        st.session_state["v139_cert"] = cert
        _v98_record_run("feasibility_certificate", {"certificate_id": cert.get("certificate_id"), "worst": cert.get("dominance",{}).get("worst_constraint"), "worst_margin_frac": cert.get("dominance",{}).get("worst_margin_frac")}, mode="v139")
        st.success("Certificate generated.")

    cert = st.session_state.get("v139_cert")
    if isinstance(cert, dict):
        st.json({
            "certificate_id": cert.get("certificate_id"),
            "issued_utc": cert.get("issued_utc"),
            "worst_constraint": (cert.get("dominance") or {}).get("worst_constraint"),
            "worst_margin_frac": (cert.get("dominance") or {}).get("worst_margin_frac"),
            "n_hard": len((cert.get("constraints") or {}).get("hard") or {}),
        })
        st.download_button(
            "Download feasibility_certificate_v139.json",
            data=_json.dumps(cert, indent=2, sort_keys=True),
            file_name="feasibility_certificate_v139.json",
            mime="application/json",
            use_container_width=True,
        )
        if st.button("Save certificate to Vault", key="v139_save_vault"):
            root = Path(__file__).resolve().parents[1]
            run_vault.write_entry(root=root, kind="feasibility_certificate", payload=cert, mode="v139", files={"feasibility_certificate_v139.json": _json.dumps(cert, indent=2, sort_keys=True).encode("utf-8")})
            st.success("Saved.")


# =====================
# v140 Sensitivity Maps (certificate -> fragility envelopes)
# =====================
def _v140_sensitivity_panel():
    import streamlit as st
    from pathlib import Path
    from tools.sensitivity_maps import SensitivityConfig, run_sensitivity, build_sensitivity_bundle
    from tools.feasibility_atlas import available_numeric_levers
    from tools import run_vault

    st.subheader("Constraint Sensitivity Maps")
    st.caption("Finite perturbation sensitivity: how much each variable can vary (+/-) before feasibility breaks. Auditable, no gradients/optimizers.")

    s = _v98_state_init_runlists()
    ids = [r.get("id") for r in (s.run_history or []) if r.get("id")]
    if not ids:
        st.info("Run a point evaluation first.")
        return
    default_id = (s.pinned_run_ids[-1] if (s.pinned_run_ids and len(s.pinned_run_ids)>0) else ids[-1])
    rid = st.selectbox("Baseline run", options=ids, index=ids.index(default_id) if default_id in ids else len(ids)-1, key="v140_pick_run")
    run_map = {r.get("id"): r for r in (s.run_history or []) if r.get("id")}
    payload = (run_map.get(rid) or {}).get("payload")
    base = payload if (isinstance(payload, dict) and payload.get("kind") == "shams_run_artifact") else None
    if base is None:
        st.error("Selected baseline does not contain a run artifact.")
        return
    base_inputs = base.get("inputs", {}) if isinstance(base.get("inputs"), dict) else {}

    levers = list(available_numeric_levers(base_inputs))
    default_vars = [v for v in ["Ip_MA","kappa","q95","a_m","beta_N","f_GW"] if v in levers]
    vars_sel = st.multiselect("Variables to probe", options=levers, default=(default_vars or levers[:4]), key="v140_vars")
    max_rel = st.slider("Max relative change (+/-)", 0.05, 1.0, 0.40, 0.05, key="v140_max_rel")
    n_expand = st.slider("Expansion steps", 3, 20, 8, 1, key="v140_nexp")
    n_bisect = st.slider("Bisection steps", 3, 20, 10, 1, key="v140_nbis")
    require_feas = st.checkbox("Require baseline feasible", value=True, key="v140_req_feas")

    if st.button("Run sensitivity", key="v140_run"):
        try:
            cfg = SensitivityConfig(
                baseline_inputs=dict(base_inputs),
                fixed_overrides={},
                vars=list(vars_sel),
                bounds={},  # intentionally empty by default; user can constrain later if needed
                max_rel=float(max_rel),
                max_abs=0.0,
                n_expand=int(n_expand),
                n_bisect=int(n_bisect),
                require_baseline_feasible=bool(require_feas),
            )
            rep = run_sensitivity(cfg)
            bun = build_sensitivity_bundle(rep)
            st.session_state["v140_rep"] = rep
            st.session_state["v140_bun"] = bun
            _v98_record_run("sensitivity_maps", {"n_vars": len(vars_sel), "max_rel": max_rel}, mode="v140")
            st.success("Sensitivity run complete.")
        except Exception as e:
            st.error(f"Failed: {e!r}")

    rep = st.session_state.get("v140_rep")
    bun = st.session_state.get("v140_bun")
    if isinstance(rep, dict):
        st.json({"baseline": rep.get("baseline"), "n_vars": len(rep.get("results") or [])})
        st.dataframe(rep.get("results", [])[:200], use_container_width=True)
        st.download_button("Download sensitivity_report_v140.json", data=_json.dumps(rep, indent=2, sort_keys=True, default=str),
                           file_name="sensitivity_report_v140.json", mime="application/json", use_container_width=True, key="v140_dl_rep")

    if isinstance(bun, dict) and isinstance(bun.get("zip_bytes"), (bytes, bytearray)):
        st.download_button("Download sensitivity_bundle_v140.zip", data=bun["zip_bytes"], file_name="sensitivity_bundle_v140.zip",
                           mime="application/zip", use_container_width=True, key="v140_dl_zip")
        if st.button("Save sensitivity bundle to Vault", key="v140_save_vault"):
            try:
                root = Path(__file__).resolve().parents[1]
                run_vault.write_entry(root=root, kind="sensitivity_bundle", payload=rep or {}, mode="v140",
                                      files={"sensitivity_bundle_v140.zip": bun["zip_bytes"]})
                st.success("Saved to vault.")
            except Exception as e:
                st.error(f"Vault save failed: {e!r}")


# =====================
# v141 Robustness Certificate
# =====================
def _v141_robustness_panel():
    import streamlit as st
    from pathlib import Path
    from tools.feasibility_certificate import generate_feasibility_certificate
    from tools.sensitivity_maps import SensitivityConfig, run_sensitivity
    from tools.robustness_certificate import generate_robustness_certificate
    from tools.feasibility_atlas import available_numeric_levers
    from tools import run_vault

    st.subheader("Robustness Certificate")
    st.caption("Derives a robustness certificate from v139 Feasibility Certificate + v140 Sensitivity Report. No optimizers/gradients.")

    s = _v98_state_init_runlists()
    ids = [r.get("id") for r in (s.run_history or []) if r.get("id")]
    if not ids:
        st.info("Run a point evaluation first.")
        return

    default_id = (s.pinned_run_ids[-1] if (s.pinned_run_ids and len(s.pinned_run_ids)>0) else ids[-1])
    rid = st.selectbox("Baseline run (for certificate + sensitivity)", options=ids, index=ids.index(default_id) if default_id in ids else len(ids)-1, key="v141_pick_run")
    run_map = {r.get("id"): r for r in (s.run_history or []) if r.get("id")}
    payload = (run_map.get(rid) or {}).get("payload")
    base = payload if (isinstance(payload, dict) and payload.get("kind") == "shams_run_artifact") else None
    if base is None:
        st.error("Selected baseline does not contain a run artifact.")
        return
    base_inputs = base.get("inputs", {}) if isinstance(base.get("inputs"), dict) else {}

    # v140 settings (re-run sensitivity here for coherence)
    levers = list(available_numeric_levers(base_inputs))
    default_vars = [v for v in ["Ip_MA","kappa","q95","a_m","beta_N","f_GW"] if v in levers]
    vars_sel = st.multiselect("Variables for robustness probing", options=levers, default=(default_vars or levers[:4]), key="v141_vars")
    max_rel = st.slider("Max relative change (+/-) for sensitivity", 0.05, 1.0, 0.40, 0.05, key="v141_max_rel")
    n_expand = st.slider("Expansion steps", 3, 20, 8, 1, key="v141_nexp")
    n_bisect = st.slider("Bisection steps", 3, 20, 10, 1, key="v141_nbis")
    require_feas = st.checkbox("Require baseline feasible", value=True, key="v141_req_feas")

    policy_json = st.text_area("Policy JSON (optional)", value="{}", height=120, key="v141_policy")

    if st.button("Generate Robustness Certificate", key="v141_run"):
        try:
            # v139 cert from current artifact (source of truth)
            fc = generate_feasibility_certificate(base)

            # v140 report (computed here for coherence; also available via v140 panel)
            cfg = SensitivityConfig(
                baseline_inputs=dict(base_inputs),
                fixed_overrides={},
                vars=list(vars_sel),
                bounds={},
                max_rel=float(max_rel),
                max_abs=0.0,
                n_expand=int(n_expand),
                n_bisect=int(n_bisect),
                require_baseline_feasible=bool(require_feas),
            )
            sr = run_sensitivity(cfg)

            policy = _json.loads(policy_json) if policy_json.strip() else {}
            rc = generate_robustness_certificate(fc, sr, policy=policy)

            st.session_state["v141_fc"] = fc
            st.session_state["v141_sr"] = sr
            st.session_state["v141_rc"] = rc
            _v98_record_run("robustness_certificate", {"n_vars": len(vars_sel), "max_rel": max_rel, "index": rc.get("robustness", {}).get("index_min_bounded_rel")}, mode="v141")
            st.success("Robustness certificate generated.")
        except Exception as e:
            st.error(f"Failed: {e!r}")

    rc = st.session_state.get("v141_rc")
    fc = st.session_state.get("v141_fc")
    sr = st.session_state.get("v141_sr")

    if isinstance(rc, dict):
        st.json({
            "robustness_index_min_bounded_rel": (rc.get("robustness", {}) or {}).get("index_min_bounded_rel"),
            "fragility_top": ((rc.get("robustness", {}) or {}).get("fragility_ranking") or [])[:5],
        })
        st.download_button("Download robustness_certificate_v141.json",
                           data=_json.dumps(rc, indent=2, sort_keys=True, default=str),
                           file_name="robustness_certificate_v141.json", mime="application/json", use_container_width=True, key="v141_dl_rc")
        with st.expander("See underlying v139 feasibility certificate"):
            st.download_button("Download feasibility_certificate_v139.json",
                               data=_json.dumps(fc or {}, indent=2, sort_keys=True, default=str),
                               file_name="feasibility_certificate_v139.json", mime="application/json", use_container_width=True, key="v141_dl_fc")
        with st.expander("See underlying v140 sensitivity report"):
            st.download_button("Download sensitivity_report_v140.json",
                               data=_json.dumps(sr or {}, indent=2, sort_keys=True, default=str),
                               file_name="sensitivity_report_v140.json", mime="application/json", use_container_width=True, key="v141_dl_sr")

        if st.button("Save robustness certificate to Vault", key="v141_save_vault"):
            try:
                root = Path(__file__).resolve().parents[1]
                run_vault.write_entry(root=root, kind="robustness_certificate", payload=rc, mode="v141",
                                      files={"robustness_certificate_v141.json": _json.dumps(rc, indent=2, sort_keys=True, default=str).encode("utf-8")})
                st.success("Saved to vault.")
            except Exception as e:
                st.error(f"Vault save failed: {e!r}")


# =====================
# v142â€“v144 Feasibility Deep Dive
# =====================
def _v144_deepdive_panel():
    import streamlit as st
    from pathlib import Path
    from tools.feasibility_atlas import available_numeric_levers
    from tools.feasibility_deepdive import (
        SampleConfig, sample_and_evaluate, topology_from_dataset, bundle_topology,
        interactions_from_dataset, bundle_interactions,
        IntervalConfig, interval_certificate, bundle_interval_certificate,
    )
    from tools import run_vault

    st.subheader("Feasibility Deep Dive (v142â€“v144)")
    st.caption("Topology maps + constraint interaction structure + interval feasibility certificates. Downstream analysis only.")

    s = _v98_state_init_runlists()
    ids = [r.get("id") for r in (s.run_history or []) if r.get("id")]
    if not ids:
        st.info("Run a point evaluation first.")
        return
    default_id = (s.pinned_run_ids[-1] if (s.pinned_run_ids and len(s.pinned_run_ids)>0) else ids[-1])
    rid = st.selectbox("Baseline run", options=ids, index=ids.index(default_id) if default_id in ids else len(ids)-1, key="v144_pick_run")
    run_map = {r.get("id"): r for r in (s.run_history or []) if r.get("id")}
    payload = (run_map.get(rid) or {}).get("payload")
    base = payload if (isinstance(payload, dict) and payload.get("kind") == "shams_run_artifact") else None
    if base is None:
        st.error("Selected baseline does not contain a run artifact.")
        return
    base_inputs = base.get("inputs", {}) if isinstance(base.get("inputs"), dict) else {}

    levers = list(available_numeric_levers(base_inputs))
    tabs = st.tabs(["Topology Maps", "Constraint Interactions", "Interval Certificates"])

    # -------- v142
    with tabs[0]:
        st.write("Build feasible topology (islands) by sampling within bounds for selected variables.")
        vars_sel = st.multiselect("Variables (2â€“6 recommended)", options=levers, default=[v for v in ["Ip_MA","kappa"] if v in levers] or levers[:2], key="v142_vars")
        n_samples = st.slider("Samples", 100, 2000, 300, 50, key="v142_ns")
        seed = st.number_input("Seed", value=0, step=1, key="v142_seed")
        k = st.slider("kNN neighbors", 2, 20, 6, 1, key="v142_k")
        eps = st.number_input("Distance cutoff eps (0 = no cutoff)", value=0.0, key="v142_eps")

        bounds = {}
        for v in vars_sel:
            v0 = float(base_inputs.get(v, 0.0) or 0.0)
            lo = st.number_input(f"{v} min", value=(v0*0.9 if v0 else 0.0), key=f"v142_lo_{v}")
            hi = st.number_input(f"{v} max", value=(v0*1.1 if v0 else 1.0), key=f"v142_hi_{v}")
            bounds[v] = (float(lo), float(hi))

        if st.button("Run topology", key="v142_run"):
            try:
                cfg = SampleConfig(baseline_inputs=dict(base_inputs), vars=list(vars_sel), bounds=dict(bounds), n_samples=int(n_samples), seed=int(seed))
                ds = sample_and_evaluate(cfg)
                topo = topology_from_dataset(ds, k=int(k), eps=float(eps))
                bun = bundle_topology(ds, topo)
                st.session_state["v142_ds"] = ds
                st.session_state["v142_topo"] = topo
                st.session_state["v142_bun"] = bun
                _v98_record_run("feasible_topology", {"n_samples": n_samples, "n_feasible": topo.get("n_feasible_points"), "n_islands": len(topo.get("islands") or [])}, mode="v142")
                st.success(f"Topology built: feasible={topo.get('n_feasible_points')} islands={len(topo.get('islands') or [])}")
            except Exception as e:
                st.error(f"Topology failed: {e!r}")

        topo = st.session_state.get("v142_topo")
        bun = st.session_state.get("v142_bun")
        if isinstance(topo, dict):
            st.json({"n_feasible_points": topo.get("n_feasible_points"), "islands": topo.get("islands")[:10]})
        if isinstance(bun, dict) and isinstance(bun.get("zip_bytes"), (bytes, bytearray)):
            st.download_button("Download topology_bundle_v142.zip", data=bun["zip_bytes"], file_name="topology_bundle_v142.zip", mime="application/zip", use_container_width=True, key="v142_dl")
            if st.button("Save topology bundle to Vault", key="v142_save_vault"):
                root = Path(__file__).resolve().parents[1]
                run_vault.write_entry(root=root, kind="topology_bundle", payload=topo or {}, mode="v142", files={"topology_bundle_v142.zip": bun["zip_bytes"]})
                st.success("Saved.")

    # -------- v143
    with tabs[1]:
        st.write("Compute constraint co-failure and dominance statistics from the last deep-dive dataset.")
        ds = st.session_state.get("v142_ds")
        if not isinstance(ds, dict):
            st.info("Run topology sampling first (v142 tab) to generate a dataset.")
        else:
            if st.button("Compute interactions", key="v143_run"):
                try:
                    inter = interactions_from_dataset(ds, top_n=20)
                    bun2 = bundle_interactions(inter)
                    st.session_state["v143_inter"] = inter
                    st.session_state["v143_bun"] = bun2
                    _v98_record_run("constraint_interactions", {"top_constraints": len(inter.get("top_constraints") or [])}, mode="v143")
                    st.success("Interactions computed.")
                except Exception as e:
                    st.error(f"Interactions failed: {e!r}")
            inter = st.session_state.get("v143_inter")
            bun2 = st.session_state.get("v143_bun")
            if isinstance(inter, dict):
                st.json({"top_constraints": inter.get("top_constraints")[:12], "dominance_top": sorted((inter.get("dominance_counts") or {}).items(), key=lambda kv: kv[1], reverse=True)[:10]})
            if isinstance(bun2, dict) and isinstance(bun2.get("zip_bytes"), (bytes, bytearray)):
                st.download_button("Download interactions_bundle_v143.zip", data=bun2["zip_bytes"], file_name="interactions_bundle_v143.zip", mime="application/zip", use_container_width=True, key="v143_dl")
                if st.button("Save interactions bundle to Vault", key="v143_save_vault"):
                    root = Path(__file__).resolve().parents[1]
                    run_vault.write_entry(root=root, kind="interactions_bundle", payload=inter or {}, mode="v143", files={"interactions_bundle_v143.zip": bun2["zip_bytes"]})
                    st.success("Saved.")

    # -------- v144
    with tabs[2]:
        st.write("Certify a hyper-rectangle interval (conservative): checks all corners + random interior probes.")
        vars_sel = st.multiselect("Interval variables", options=levers, default=[v for v in ["Ip_MA","kappa"] if v in levers] or levers[:2], key="v144_ivars")
        bounds = {}
        for v in vars_sel:
            v0 = float(base_inputs.get(v, 0.0) or 0.0)
            lo = st.number_input(f"{v} min", value=(v0*0.95 if v0 else 0.0), key=f"v144_lo_{v}")
            hi = st.number_input(f"{v} max", value=(v0*1.05 if v0 else 1.0), key=f"v144_hi_{v}")
            bounds[v] = (float(lo), float(hi))
        n_random = st.slider("Random interior probes", 0, 500, 60, 10, key="v144_nr")
        seed = st.number_input("Seed", value=0, step=1, key="v144_seed")
        if st.button("Generate interval certificate (v144)", key="v144_run"):
            try:
                cert = interval_certificate(IntervalConfig(baseline_inputs=dict(base_inputs), bounds=dict(bounds), n_random=int(n_random), seed=int(seed)))
                bun = bundle_interval_certificate(cert)
                st.session_state["v144_cert"] = cert
                st.session_state["v144_bun"] = bun
                _v98_record_run("interval_certificate", {"interval_certified": cert.get("verdict",{}).get("interval_certified"), "n_vars": len(bounds)}, mode="v144")
                st.success(f"Interval certified = {cert.get('verdict',{}).get('interval_certified')}")
            except Exception as e:
                st.error(f"Interval certificate failed: {e!r}")

        cert = st.session_state.get("v144_cert")
        bun = st.session_state.get("v144_bun")
        if isinstance(cert, dict):
            st.json(cert.get("verdict", {}))
            st.download_button("Download interval_certificate_v144.json", data=_json.dumps(cert, indent=2, sort_keys=True, default=str),
                               file_name="interval_certificate_v144.json", mime="application/json", use_container_width=True, key="v144_dl_json")
        if isinstance(bun, dict) and isinstance(bun.get("zip_bytes"), (bytes, bytearray)):
            st.download_button("Download interval_bundle_v144.zip", data=bun["zip_bytes"], file_name="interval_bundle_v144.zip", mime="application/zip", use_container_width=True, key="v144_dl_zip")
            if st.button("Save interval bundle to Vault", key="v144_save_vault"):
                root = Path(__file__).resolve().parents[1]
                run_vault.write_entry(root=root, kind="interval_bundle", payload=cert or {}, mode="v144", files={"interval_bundle_v144.zip": bun["zip_bytes"]})
                st.success("Saved.")


# =====================
# v145 Topology Certificate
# =====================
def _v145_topology_certificate_panel():
    import streamlit as st
    from pathlib import Path
    from tools.topology_certificate import generate_topology_certificate
    from tools import run_vault

    st.subheader("Topology Certificate")
    st.caption("Citable certificate summarizing feasible-set topology (islands) for a declared domain and sampling protocol.")

    s = _v98_state_init_runlists()
    ids = [r.get("id") for r in (s.run_history or []) if r.get("id")]
    if not ids:
        st.info("Run a point evaluation first.")
        return
    default_id = (s.pinned_run_ids[-1] if (s.pinned_run_ids and len(s.pinned_run_ids)>0) else ids[-1])
    rid = st.selectbox("Baseline run", options=ids, index=ids.index(default_id) if default_id in ids else len(ids)-1, key="v145_pick_run")
    run_map = {r.get("id"): r for r in (s.run_history or []) if r.get("id")}
    payload = (run_map.get(rid) or {}).get("payload")
    base = payload if (isinstance(payload, dict) and payload.get("kind") == "shams_run_artifact") else None
    if base is None:
        st.error("Selected baseline does not contain a run artifact.")
        return

    topo = st.session_state.get("v142_topo")
    ds = st.session_state.get("v142_ds")
    if not isinstance(topo, dict):
        st.info("Run v142 Topology Maps first (Feasibility Deep Dive panel) to generate feasible topology.")
        return

    policy_json = st.text_area("Policy JSON (optional)", value="{}", height=120, key="v145_policy")

    if st.button("Generate Topology Certificate", key="v145_run"):
        try:
            policy = _json.loads(policy_json) if policy_json.strip() else {}
            cert = generate_topology_certificate(base, topo, deepdive_dataset=(ds if isinstance(ds, dict) else None), policy=policy)
            st.session_state["v145_cert"] = cert
            _v98_record_run("topology_certificate", {"n_islands": (cert.get("topology_summary", {}) or {}).get("n_islands")}, mode="v145")
            st.success("Topology certificate generated.")
        except Exception as e:
            st.error(f"Failed: {e!r}")

    cert = st.session_state.get("v145_cert")
    if isinstance(cert, dict):
        st.json(cert.get("topology_summary", {}))
        st.download_button("Download topology_certificate_v145.json",
                           data=_json.dumps(cert, indent=2, sort_keys=True, default=str),
                           file_name="topology_certificate_v145.json", mime="application/json",
                           use_container_width=True, key="v145_dl")
        if st.button("Save topology certificate to Vault", key="v145_save_vault"):
            try:
                root = Path(__file__).resolve().parents[1]
                run_vault.write_entry(root=root, kind="topology_certificate", payload=cert, mode="v145",
                                      files={"topology_certificate_v145.json": _json.dumps(cert, indent=2, sort_keys=True, default=str).encode("utf-8")})
                st.success("Saved to vault.")
            except Exception as e:
                st.error(f"Vault save failed: {e!r}")


# =====================
# v146â€“v147 Feasibility Completion
# =====================
def _v147_feasibility_completion_panel():
    import streamlit as st
    from pathlib import Path
    from tools.feasibility_atlas import available_numeric_levers
    from tools.feasibility_bridge import BridgeConfig, run_bridge, bridge_certificate
    from tools.safe_domain_shrink import ShrinkConfig, run_safe_domain_shrink
    from tools import run_vault
    import hashlib

    st.subheader("Feasibility Completion (v146â€“v147)")
    st.caption("v146: topology bridge witness between two points. v147: auto-shrink to a certified safe interval box (uses v144).")

    s = _v98_state_init_runlists()
    runs = [r for r in (s.run_history or []) if r.get("id")]
    ids = [r.get("id") for r in runs]
    if len(ids) < 1:
        st.info("Run at least one point evaluation first.")
        return

    run_map = {r.get("id"): r for r in runs}
    def _get_art(rid):
        payload = (run_map.get(rid) or {}).get("payload")
        return payload if (isinstance(payload, dict) and payload.get("kind") == "shams_run_artifact") else None

    tabs = st.tabs(["Bridge Certificate", "Safe Domain Shrink"])

    # ---------- v146
    with tabs[0]:
        if len(ids) < 2:
            st.info("Need at least two runs in history to bridge (A and B).")
        else:
            ridA = st.selectbox("Point A (baseline)", options=ids, index=max(0, len(ids)-2), key="v146_A")
            ridB = st.selectbox("Point B (target)", options=ids, index=len(ids)-1, key="v146_B")
            artA = _get_art(ridA); artB = _get_art(ridB)
            if artA is None or artB is None:
                st.error("Selected runs must be run artifacts.")
            else:
                Ain = artA.get("inputs", {}) if isinstance(artA.get("inputs"), dict) else {}
                Bin = artB.get("inputs", {}) if isinstance(artB.get("inputs"), dict) else {}
                levers = sorted(set(available_numeric_levers(Ain)).intersection(set(available_numeric_levers(Bin))))
                default_vars = [v for v in ["Ip_MA","kappa"] if v in levers] or levers[:2]
                vars_sel = st.multiselect("Bridge variables", options=levers, default=default_vars, key="v146_vars")
                n_steps = st.slider("Coarse steps", 5, 101, 21, 2, key="v146_steps")
                bis = st.slider("Max bisection depth", 0, 10, 6, 1, key="v146_bis")
                req_end = st.checkbox("Require endpoints feasible", value=True, key="v146_req")
                if st.button("Run bridge", key="v146_run"):
                    try:
                        cfg = BridgeConfig(inputs_A=dict(Ain), inputs_B=dict(Bin), vars=list(vars_sel), n_steps=int(n_steps), max_bisect_depth=int(bis), require_endpoints_feasible=bool(req_end))
                        rep = run_bridge(cfg)
                        # baseline hash from A inputs
                        h = hashlib.sha256(_json.dumps(Ain, sort_keys=True, default=str).encode("utf-8")).hexdigest()
                        cert = bridge_certificate(rep, baseline_inputs_sha256=h)
                        st.session_state["v146_rep"] = rep
                        st.session_state["v146_cert"] = cert
                        _v98_record_run("bridge_certificate", {"bridge_exists": rep.get("bridge_exists"), "n_points": len(rep.get("path") or [])}, mode="v146")
                        st.success(f"Bridge exists = {rep.get('bridge_exists')}")
                    except Exception as e:
                        st.error(f"Bridge failed: {e!r}")

                cert = st.session_state.get("v146_cert")
                rep = st.session_state.get("v146_rep")
                if isinstance(cert, dict):
                    st.json(cert.get("summary", {}))
                    st.download_button("Download bridge_certificate_v146.json", data=_json.dumps(cert, indent=2, sort_keys=True, default=str),
                                       file_name="bridge_certificate_v146.json", mime="application/json", use_container_width=True, key="v146_dl_cert")
                    st.download_button("Download bridge_report_v146.json", data=_json.dumps(rep or {}, indent=2, sort_keys=True, default=str),
                                       file_name="bridge_report_v146.json", mime="application/json", use_container_width=True, key="v146_dl_rep")
                    if st.button("Save v146 to Vault", key="v146_save_vault"):
                        root = Path(__file__).resolve().parents[1]
                        run_vault.write_entry(root=root, kind="bridge_certificate", payload={"certificate": cert, "report": rep}, mode="v146",
                                              files={
                                                  "bridge_certificate_v146.json": _json.dumps(cert, indent=2, sort_keys=True, default=str).encode("utf-8"),
                                                  "bridge_report_v146.json": _json.dumps(rep or {}, indent=2, sort_keys=True, default=str).encode("utf-8"),
                                              })
                        st.success("Saved.")

    # ---------- v147
    with tabs[1]:
        rid = st.selectbox("Baseline run for safe box", options=ids, index=len(ids)-1, key="v147_base")
        art = _get_art(rid)
        if art is None:
            st.error("Selected run must be a run artifact.")
        else:
            base_inputs = art.get("inputs", {}) if isinstance(art.get("inputs"), dict) else {}
            levers = list(available_numeric_levers(base_inputs))
            vars_sel = st.multiselect("Interval variables", options=levers, default=[v for v in ["Ip_MA","kappa"] if v in levers] or levers[:2], key="v147_vars")
            bounds={}
            for v in vars_sel:
                v0=float(base_inputs.get(v, 0.0) or 0.0)
                lo = st.number_input(f"{v} min", value=(v0*0.9 if v0 else 0.0), key=f"v147_lo_{v}")
                hi = st.number_input(f"{v} max", value=(v0*1.1 if v0 else 1.0), key=f"v147_hi_{v}")
                bounds[v]=(float(lo), float(hi))
            shrink_factor = st.slider("Shrink factor per iteration", 0.50, 0.98, 0.85, 0.01, key="v147_sf")
            max_iter = st.slider("Max iterations", 1, 30, 10, 1, key="v147_mi")
            n_random = st.slider("Random probes per iteration", 0, 300, 40, 10, key="v147_nr")
            seed = st.number_input("Seed", value=0, step=1, key="v147_seed")

            if st.button("Run safe-domain shrink", key="v147_run"):
                try:
                    cfg = ShrinkConfig(baseline_inputs=dict(base_inputs), bounds=dict(bounds), shrink_factor=float(shrink_factor),
                                       max_iter=int(max_iter), n_random=int(n_random), seed=int(seed))
                    rep = run_safe_domain_shrink(cfg)
                    st.session_state["v147_rep"] = rep
                    _v98_record_run("safe_domain_shrink", {"final_certified": rep.get("final_certified"), "iters": len(rep.get("history") or [])}, mode="v147")
                    st.success(f"Final certified = {rep.get('final_certified')} (iters={len(rep.get('history') or [])})")
                except Exception as e:
                    st.error(f"Shrink failed: {e!r}")

            rep = st.session_state.get("v147_rep")
            if isinstance(rep, dict):
                st.json({"final_certified": rep.get("final_certified"), "final_bounds": rep.get("final_bounds"), "last": (rep.get("history") or [])[-1] if (rep.get("history") or []) else None})
                st.download_button("Download safe_domain_shrink_report_v147.json",
                                   data=_json.dumps(rep, indent=2, sort_keys=True, default=str),
                                   file_name="safe_domain_shrink_report_v147.json", mime="application/json",
                                   use_container_width=True, key="v147_dl_rep")
                cert = rep.get("interval_certificate_v144")
                if isinstance(cert, dict):
                    st.download_button("Download interval_certificate_v144.json",
                                       data=_json.dumps(cert, indent=2, sort_keys=True, default=str),
                                       file_name="interval_certificate_v144.json", mime="application/json",
                                       use_container_width=True, key="v147_dl_cert")
                if st.button("Save v147 to Vault", key="v147_save_vault"):
                    root = Path(__file__).resolve().parents[1]
                    files={"safe_domain_shrink_report_v147.json": _json.dumps(rep, indent=2, sort_keys=True, default=str).encode("utf-8")}
                    if isinstance(cert, dict):
                        files["interval_certificate_v144.json"]=_json.dumps(cert, indent=2, sort_keys=True, default=str).encode("utf-8")
                    run_vault.write_entry(root=root, kind="safe_domain_shrink", payload=rep, mode="v147", files=files)
                    st.success("Saved.")


# =====================
# v148â€“v150 Publishable Study Kit
# =====================
def _v150_publishable_study_kit_panel():
    import streamlit as st
    from pathlib import Path
    from tools.design_study_kit import PaperPackConfig, build_paper_pack
    from tools import run_vault

    st.subheader("Publishable Study Kit (v148â€“v150)")
    st.caption("One-click paper pack (zip) with study registry + integrity manifest. Downstream-only.")

    s = _v98_state_init_runlists()
    runs = [r for r in (s.run_history or []) if r.get("id")]
    if not runs:
        st.info("Run at least one evaluation first.")
        return
    ids=[r.get("id") for r in runs]
    run_map={r.get("id"): r for r in runs}

    def _get_art(rid):
        payload = (run_map.get(rid) or {}).get("payload")
        return payload if (isinstance(payload, dict) and payload.get("kind") == "shams_run_artifact") else None

    sel = st.multiselect("Select run(s) to include", options=ids, default=[ids[-1]], key="v150_sel_runs")
    title = st.text_input("Study title", value="SHAMS design study", key="v150_title")
    authors = st.text_input("Authors (comma-separated)", value="", key="v150_authors")
    desc = st.text_area("Description", value="", height=120, key="v150_desc")

    # auto-pick latest certificates present in session_state (best effort)
    certs=[]
    for key, fn in [
        ("v139_fc", "feasibility_certificate_v139.json"),
        ("v141_rc", "robustness_certificate_v141.json"),
        ("v144_ic", "interval_certificate_v144.json"),
        ("v145_tc", "topology_certificate_v145.json"),
        ("v146_bc", "bridge_certificate_v146.json"),
        ("v147_sd", "safe_domain_shrink_report_v147.json"),
    ]:
        obj = st.session_state.get(key)
        if isinstance(obj, dict):
            certs.append((fn, obj))

    st.checkbox("Include best-effort certificates from current session", value=True, key="v150_include_session_certs")
    st.checkbox("Include figures/tables from session bundles", value=True, key="v151_include_session_bundles")
    methods_json = st.text_area("Methods JSON (optional)", value="{}", height=120, key="v150_methods")
    policy_json = st.text_area("Policy JSON (optional)", value='{"mode":"paper_pack"}', height=120, key="v150_policy")

    if st.button("Build Paper Pack", key="v150_run"):
        try:
            run_arts=[]
            for rid in sel:
                art=_get_art(rid)
                if art is not None:
                    run_arts.append(art)
            if not run_arts:
                st.error("No valid run artifacts selected.")
                return

            methods=_json.loads(methods_json) if methods_json.strip() else {}
            policy=_json.loads(policy_json) if policy_json.strip() else {}
            # v154 captions override
            caps = st.session_state.get("v154_captions")
            if isinstance(caps, dict) and caps:
                policy = dict(policy)
                policy["captions_override"] = caps

            cfg=PaperPackConfig(
                shams_version=str((_json.loads((Path(__file__).resolve().parents[1]/"VERSION").read_text(encoding="utf-8").strip().splitlines()[0]) if True else "unknown")),
                title=title,
                authors=[a.strip() for a in authors.split(",") if a.strip()],
                description=desc,
                run_artifacts=run_arts,
                certificates=certs if st.session_state.get("v150_include_session_certs") else [],
                figures=figs,
                tables=tabs,
                methods=methods,
                policy=policy,
            )
            bun=build_paper_pack(cfg)
            st.session_state["v150_pack"]=bun
            _v98_record_run("paper_pack", {"n_runs": len(run_arts), "n_certs": len(cfg.certificates)}, mode="v150")
            st.success("Paper pack built.")
        except Exception as e:
            st.error(f"Failed: {e!r}")

    bun=st.session_state.get("v150_pack")
    if isinstance(bun, dict) and isinstance(bun.get("zip_bytes"), (bytes, bytearray)):
        st.download_button("Download paper_pack_v150.zip", data=bun["zip_bytes"], file_name="paper_pack_v150.zip",
                           mime="application/zip", use_container_width=True, key="v150_dl_zip")
        st.download_button("Download study_registry_v149.json", data=_json.dumps(bun.get("study_registry") or {}, indent=2, sort_keys=True, default=str),
                           file_name="study_registry_v149.json", mime="application/json", use_container_width=True, key="v150_dl_reg")
        st.download_button("Download captions.json", data=_json.dumps({"note":"captions.json is included inside paper_pack_v150.zip"}, indent=2), file_name="captions_note.json", mime="application/json", use_container_width=True, key="v151_dl_caps_note")
        
        st.download_button("Download integrity_manifest_v150.json", data=_json.dumps((bun.get("manifest") or {}), indent=2, sort_keys=True, default=str),
                           file_name="integrity_manifest_v150.json", mime="application/json", use_container_width=True, key="v150_dl_mf")

        if st.button("Save paper pack to Vault", key="v150_save_vault"):
            root = Path(__file__).resolve().parents[1]
            run_vault.write_entry(root=root, kind="paper_pack", payload={"study_registry": bun.get("study_registry"), "manifest": bun.get("manifest")}, mode="v150",
                                  files={
                                      "paper_pack_v150.zip": bun["zip_bytes"],
                                      "study_registry_v149.json": _json.dumps(bun.get("study_registry") or {}, indent=2, sort_keys=True, default=str).encode("utf-8"),
                                      "integrity_manifest_v150.json": _json.dumps(bun.get("manifest") or {}, indent=2, sort_keys=True, default=str).encode("utf-8"),
                                  })
            st.success("Saved.")


# =====================
# v152 Integrity Lock helpers
# =====================
def _v152_artifact_sha(payload):
    import json, hashlib
    b = json.dumps(payload, indent=2, sort_keys=True, default=str).encode("utf-8")
    return hashlib.sha256(b).hexdigest()

def _v152_get_lock_for_run(run_id: str):
    import streamlit as st
    locks = st.session_state.get("v152_locks") or {}
    return locks.get(run_id) if isinstance(locks, dict) else None

def _v152_set_lock_for_run(run_id: str, lock_obj: dict):
    import streamlit as st
    locks = st.session_state.get("v152_locks")
    if not isinstance(locks, dict):
        locks = {}
    locks[str(run_id)] = lock_obj
    st.session_state["v152_locks"] = locks

def _v152_integrity_status(run_id: str, payload: dict):
    lock = _v152_get_lock_for_run(run_id)
    if not isinstance(lock, dict):
        return ("UNLOCKED", None)
    # compare current artifact sha with stored
    expected = ((lock.get("files") or {}).get("run_artifact.json") or {}).get("sha256")
    cur = _v152_artifact_sha(payload)
    if expected and cur == expected:
        return ("VERIFIED", cur)
    return ("MODIFIED", cur)

def _v152_integrity_panel():
    import streamlit as st
    from pathlib import Path
    from tools.run_integrity_lock import lock_run, verify_run
    from tools import run_vault

    st.subheader("Run Integrity Lock")
    st.caption("Lock a run artifact hash and later verify whether it changed. Downstream-only.")

    s = _v98_state_init_runlists()
    runs = [r for r in (s.run_history or []) if r.get("id")]
    if not runs:
        st.info("Run at least one evaluation first.")
        return
    ids=[r.get("id") for r in runs]
    run_map={r.get("id"): r for r in runs}
    rid = st.selectbox("Select run to lock/verify", options=ids, index=len(ids)-1, key="v152_pick_run")
    payload = (run_map.get(rid) or {}).get("payload")
    if not (isinstance(payload, dict) and payload.get("kind")=="shams_run_artifact"):
        st.error("Selected run payload missing.")
        return

    status, cur_sha = _v152_integrity_status(rid, payload)
    st.write(f"Status: **{status}**")
    if cur_sha:
        st.code(cur_sha)

    if st.button("Lock integrity for this run", key="v152_lock"):
        out = lock_run(run_id=str(rid), run_artifact=payload, extras=None, policy={"mode":"ui"})
        lock_obj = out["lock"]
        _v152_set_lock_for_run(rid, lock_obj)
        st.session_state["v152_last_lock"] = lock_obj
        st.success("Locked.")
        _v98_record_run("integrity_lock", {"status":"locked"}, mode="v152")

    lock_obj = _v152_get_lock_for_run(rid) or st.session_state.get("v152_last_lock")
    if isinstance(lock_obj, dict):
        rep = verify_run(lock_obj, payload, extras=None)
        st.json(rep)
        st.download_button("Download run_integrity_lock_v152.json", data=_json.dumps(lock_obj, indent=2, sort_keys=True, default=str),
                           file_name="run_integrity_lock_v152.json", mime="application/json", use_container_width=True, key="v152_dl_lock")
        st.download_button("Download run_integrity_verify_v152.json", data=_json.dumps(rep, indent=2, sort_keys=True, default=str),
                           file_name="run_integrity_verify_v152.json", mime="application/json", use_container_width=True, key="v152_dl_rep")

        if st.button("Save integrity lock to Vault", key="v152_save_vault"):
            root = Path(__file__).resolve().parents[1]
            run_vault.write_entry(root=root, kind="run_integrity_lock", payload={"lock": lock_obj, "verify": rep}, mode="v152",
                                  files={
                                      "run_integrity_lock_v152.json": _json.dumps(lock_obj, indent=2, sort_keys=True, default=str).encode("utf-8"),
                                      "run_integrity_verify_v152.json": _json.dumps(rep, indent=2, sort_keys=True, default=str).encode("utf-8"),
                                  })
            st.success("Saved.")


# =====================
# v153â€“v155 Study Kit Extensions
# =====================
def _v153_doi_export_panel():
    import streamlit as st
    from tools.doi_export import zenodo_metadata_from_registry, crossref_minimal_from_registry

    st.subheader("DOI Export Helper")
    st.caption("Export Zenodo/Crossref-style metadata from the latest study registry built in this session.")

    bun = st.session_state.get("v150_pack")
    reg = (bun or {}).get("study_registry") if isinstance(bun, dict) else None
    if not isinstance(reg, dict):
        st.info("Build a Paper Pack first (Publishable Study Kit panel).")
        return

    comm = st.text_input("Zenodo communities (comma-separated identifiers)", value="", key="v153_comm")
    kws = st.text_input("Keywords (comma-separated)", value="", key="v153_kws")
    doi = st.text_input("DOI (optional)", value="", key="v153_doi")
    publisher = st.text_input("Publisher (optional)", value="SHAMS", key="v153_pub")
    url = st.text_input("Resource URL (optional)", value="", key="v153_url")

    communities=[c.strip() for c in comm.split(",") if c.strip()] if comm else []
    keywords=[k.strip() for k in kws.split(",") if k.strip()] if kws else []

    zen = zenodo_metadata_from_registry(reg, communities=communities, keywords=keywords)
    cr  = crossref_minimal_from_registry(reg, doi=doi, publisher=publisher, resource_url=url)

    st.download_button("Download zenodo_metadata_v153.json", data=_json.dumps(zen, indent=2, sort_keys=True, default=str),
                       file_name="zenodo_metadata_v153.json", mime="application/json", use_container_width=True, key="v153_dl_zen")
    st.download_button("Download crossref_minimal_v153.json", data=_json.dumps(cr, indent=2, sort_keys=True, default=str),
                       file_name="crossref_minimal_v153.json", mime="application/json", use_container_width=True, key="v153_dl_cr")

def _v154_caption_editor_panel():
    import streamlit as st
    st.subheader("Caption Editor")
    st.caption("Edit captions for figures/tables included in the paper pack. Captions are stored in session and exported into paper packs.")

    # Captions live in session state and are applied when building the paper pack.
    caps = st.session_state.get("v154_captions")
    if not isinstance(caps, dict):
        caps = {}
        st.session_state["v154_captions"] = caps

    # Show detected figure/table filenames from latest built pack, if any
    bun = st.session_state.get("v150_pack")
    reg = (bun or {}).get("study_registry") if isinstance(bun, dict) else None
    names=[]
    if isinstance(reg, dict):
        for ref in (reg.get("figures") or []):
            if isinstance(ref, dict) and ref.get("name"):
                names.append(ref["name"])
        for ref in (reg.get("tables") or []):
            if isinstance(ref, dict) and ref.get("name"):
                names.append(ref["name"])
    names = sorted(set(names))

    if not names:
        st.info("No figures/tables detected yet. Build a Paper Pack with v151 session-bundle harvesting enabled.")
    else:
        pick = st.selectbox("Select figure/table", options=names, key="v154_pick")
        cur = caps.get(pick, f"Figure/Table: {pick}.")
        new = st.text_area("Caption text", value=cur, height=120, key="v154_text")
        if st.button("Save caption", key="v154_save"):
            caps[pick] = new
            st.session_state["v154_captions"] = caps
            st.success("Saved.")
        st.download_button("Download captions_override_v154.json", data=_json.dumps({"captions": caps}, indent=2, sort_keys=True, default=str),
                           file_name="captions_override_v154.json", mime="application/json", use_container_width=True, key="v154_dl")

def _v155_multi_study_pack_panel():
    import streamlit as st
    from tools.multi_study_pack import build_multi_study_pack

    st.subheader("Multi-Study Comparison Pack")
    st.caption("Upload multiple paper packs and export a comparison bundle with a multi-pack integrity manifest.")

    files = st.file_uploader("Upload paper_pack_v150.zip files", type=["zip"], accept_multiple_files=True, key="v155_upload")
    if not files:
        st.info("Upload 2+ paper packs to compare.")
        return
    packs=[]
    for f in files:
        packs.append((f.name, f.getvalue()))

    bun = build_multi_study_pack(packs, policy={"source":"ui"})
    st.json({"n_packs": len(packs), "manifest_sha256": (bun.get("manifest") or {}).get("hashes",{}).get("manifest_sha256")})
    st.download_button("Download multi_study_pack_v155.zip", data=bun["zip_bytes"], file_name="multi_study_pack_v155.zip",
                       mime="application/zip", use_container_width=True, key="v155_dl_zip")
    st.download_button("Download comparison_report_v155.json", data=_json.dumps(bun.get("bundle") or {}, indent=2, sort_keys=True, default=str),
                       file_name="comparison_report_v155.json", mime="application/json", use_container_width=True, key="v155_dl_rep")


# =====================
# v156 / v160 Design Space Authority (Atlas + Certificates)
# =====================
def _v156_feasibility_atlas_panel():
    import streamlit as st
    from tools.feasibility_field import build_feasibility_field

    st.subheader("Feasibility Atlas")
    st.caption("Sample a 2D design space slice and export a feasibility field + atlas bundle.")

    s = _v98_state_init_runlists()
    runs = [r for r in (s.run_history or []) if r.get("id")]
    if not runs:
        st.info("Run at least one evaluation first (Point Designer / Systems Mode).")
        return
    ids=[r.get("id") for r in runs]
    run_map={r.get("id"): r for r in runs}
    rid = st.selectbox("Baseline run", options=ids, index=len(ids)-1, key="v156_base_run")
    payload = (run_map.get(rid) or {}).get("payload")
    if not (isinstance(payload, dict) and payload.get("kind")=="shams_run_artifact"):
        st.error("Selected baseline run payload missing.")
        return
    baseline_inputs = payload.get("inputs") or payload.get("_inputs") or {}
    if not isinstance(baseline_inputs, dict) or not baseline_inputs:
        st.error("Baseline inputs missing.")
        return

    keys=sorted([k for k in baseline_inputs.keys() if isinstance(k,str)])
    col1,col2=st.columns(2)
    with col1:
        a1 = st.selectbox("Axis 1 parameter", options=keys, index=0 if keys else 0, key="v156_a1")
        a1_start = st.number_input("Axis 1 start", value=float(baseline_inputs.get(a1, 0.0) or 0.0), key="v156_a1s")
        a1_stop  = st.number_input("Axis 1 stop", value=float(baseline_inputs.get(a1, 1.0) or 1.0)+1.0, key="v156_a1e")
        a1_n     = st.number_input("Axis 1 N", value=25, min_value=2, max_value=250, step=1, key="v156_a1n")
    with col2:
        a2 = st.selectbox("Axis 2 parameter", options=keys, index=1 if len(keys)>1 else 0, key="v156_a2")
        a2_start = st.number_input("Axis 2 start", value=float(baseline_inputs.get(a2, 0.0) or 0.0), key="v156_a2s")
        a2_stop  = st.number_input("Axis 2 stop", value=float(baseline_inputs.get(a2, 1.0) or 1.0)+1.0, key="v156_a2e")
        a2_n     = st.number_input("Axis 2 N", value=25, min_value=2, max_value=250, step=1, key="v156_a2n")

    fixed_json = st.text_area("Fixed overrides (JSON list of {name,value})", value="[]", height=90, key="v156_fixed")
    assumptions_json = st.text_area("Assumption set (JSON object)", value="{}", height=90, key="v156_assumptions")
    margin_eps = st.number_input("Margin epsilon for feasible", value=1e-6, format="%.1e", key="v156_eps")
    run_btn = st.button("Build Feasibility Field", use_container_width=True, key="v156_run")

    if run_btn:
        try:
            fixed = _json.loads(fixed_json) if fixed_json.strip() else []
            assumptions = _json.loads(assumptions_json) if assumptions_json.strip() else {}
        except Exception as e:
            st.error(f"JSON parse error: {e}")
            return

        axis1={"name": a1, "role":"axis", "grid":{"type":"linspace","start": float(a1_start), "stop": float(a1_stop), "n": int(a1_n)}}
        axis2={"name": a2, "role":"axis", "grid":{"type":"linspace","start": float(a2_start), "stop": float(a2_stop), "n": int(a2_n)}}
        with st.spinner("Sampling feasibility field..."):
            out = build_feasibility_field(
                baseline_inputs=baseline_inputs,
                axis1=axis1,
                axis2=axis2,
                fixed=fixed if isinstance(fixed, list) else [],
                assumption_set=assumptions if isinstance(assumptions, dict) else {},
                sampling={"generator":"ui","strategy":"grid"},
                solver_meta={"label":"feasibility_field_v156"},
                margin_eps=float(margin_eps),
            )
        st.session_state["v156_field"] = out["field"]
        st.session_state["v156_atlas_zip"] = out["zip_bytes"]
        summ = (((out["field"].get("payload") or {}).get("field") or {}).get("summaries") or {})
        st.success("Built.")
        st.json(summ)
        st.download_button("Download feasibility_atlas_bundle_v156.zip", data=out["zip_bytes"], file_name="feasibility_atlas_bundle_v156.zip",
                           mime="application/zip", use_container_width=True, key="v156_dl_zip")
        st.download_button("Download feasibility_field_v156.json", data=_json.dumps(out["field"], indent=2, sort_keys=True, default=str),
                           file_name="feasibility_field_v156.json", mime="application/json", use_container_width=True, key="v156_dl_json")

def _v160_authority_certificate_panel():
    import streamlit as st
    from tools.feasibility_authority_certificate import issue_certificate_from_field

    st.subheader("Feasibility Authority Certificate")
    st.caption("Issue an authority certificate from a feasibility field (dense sampling basis).")

    field = st.session_state.get("v156_field")
    up = st.file_uploader("Optional: upload feasibility_field_v156.json", type=["json"], key="v160_upload")
    if up is not None:
        try:
            field = _json.loads(up.getvalue().decode("utf-8"))
        except Exception as e:
            st.error(f"Invalid JSON: {e}")
            return

    if not isinstance(field, dict):
        st.info("Build a Feasibility Field first or upload one here.")
        return

    claim = st.selectbox("Claim type", options=["feasible_region","excluded_region","boundary_surface","completion_existence"], index=1, key="v160_claim")
    default_stmt = "Under assumption_set SHA256=..., region sampled is excluded/feasible under dense sampling evidence."
    stmt = st.text_area("Claim statement (human-readable, publishable)", value=default_stmt, height=110, key="v160_stmt")
    conf = st.number_input("Confidence level", value=0.95, min_value=0.5, max_value=0.999, step=0.01, key="v160_conf")
    grade = st.selectbox("Grade", options=["A","B","C"], index=1, key="v160_grade")

    if st.button("Issue Certificate", use_container_width=True, key="v160_issue"):
        cert = issue_certificate_from_field(field=field, claim_type=claim, statement=stmt, confidence_level=float(conf), confidence_grade=str(grade), policy={"mode":"ui"})
        st.session_state["v160_cert"] = cert
        st.success("Issued.")
        st.json(cert.get("payload") or {})
        st.download_button("Download feasibility_authority_certificate_v160.json",
                           data=_json.dumps(cert, indent=2, sort_keys=True, default=str),
                           file_name="feasibility_authority_certificate_v160.json",
                           mime="application/json", use_container_width=True, key="v160_dl")


# =====================
# v157 Feasibility Boundary Extractor
# =====================
def _v157_feasibility_boundary_panel():
    import streamlit as st
    from tools.feasibility_boundary import build_feasibility_boundary

    st.subheader("Feasibility Boundary")
    st.caption("Extract a feasibility boundary curve from a v156 feasibility field (grid interpolation).")

    field = st.session_state.get("v156_field")
    up = st.file_uploader("Optional: upload feasibility_field_v156.json", type=["json"], key="v157_upload")
    if up is not None:
        try:
            field = _json.loads(up.getvalue().decode("utf-8"))
        except Exception as e:
            st.error(f"Invalid JSON: {e}")
            return
    if not isinstance(field, dict):
        st.info("Build a Feasibility Field first or upload one here.")
        return

    prefer_lowest = st.checkbox("Prefer lowest Axis2 crossing (typical min required)", value=True, key="v157_low")
    if st.button("Extract Boundary", use_container_width=True, key="v157_run"):
        b = build_feasibility_boundary(field=field, prefer_lowest_axis2=bool(prefer_lowest))
        st.session_state["v157_boundary"] = b
        st.success(f"Extracted {len(((b.get('payload') or {}).get('boundary') or {}).get('samples') or [])} samples.")
        st.json((b.get("payload") or {}).get("boundary_definition") or {})
        st.download_button("Download feasibility_boundary_v157.json",
                           data=_json.dumps(b, indent=2, sort_keys=True, default=str),
                           file_name="feasibility_boundary_v157.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v157_dl")


# =====================
# v158 Constraint Dominance Topology
# =====================
def _v158_constraint_dominance_panel():
    import streamlit as st
    from tools.constraint_dominance import build_constraint_dominance

    st.subheader("Constraint Dominance Topology")
    st.caption("Explain why regions fail: dominant violated constraint maps + connected components (grid topology).")

    field = st.session_state.get("v156_field")
    up = st.file_uploader("Optional: upload feasibility_field_v156.json", type=["json"], key="v158_upload")
    if up is not None:
        try:
            field = _json.loads(up.getvalue().decode("utf-8"))
        except Exception as e:
            st.error(f"Invalid JSON: {e}")
            return
    if not isinstance(field, dict):
        st.info("Build a Feasibility Field first or upload one here.")
        return

    include_all = st.checkbox("Include feasible points in dominance map (larger JSON)", value=False, key="v158_all")
    if st.button("Compute Dominance Topology", use_container_width=True, key="v158_run"):
        dom = build_constraint_dominance(field=field, only_infeasible=(not include_all))
        st.session_state["v158_dom"] = dom
        summ = (((dom.get("payload") or {}).get("dominance") or {}).get("summary") or {})
        st.success("Computed.")
        st.json(summ)
        st.download_button("Download constraint_dominance_v158.json",
                           data=_json.dumps(dom, indent=2, sort_keys=True, default=str),
                           file_name="constraint_dominance_v158.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v158_dl")


# =====================
# v159 Feasibility Completion Evidence
# =====================
def _v159_feasibility_completion_panel():
    import streamlit as st
    from tools.feasibility_completion_evidence import build_feasibility_completion_evidence

    st.subheader("Feasibility Completion Evidence")
    st.caption("Given partial inputs + bounds on unknowns, search for a feasible completion witness and report bottlenecks.")

    s = _v98_state_init_runlists()
    runs = [r for r in (s.run_history or []) if r.get("id")]
    if not runs:
        st.info("Run at least one evaluation first (Point Designer / Systems Mode).")
        return
    ids=[r.get("id") for r in runs]
    run_map={r.get("id"): r for r in runs}
    rid = st.selectbox("Baseline run (provides default known inputs)", options=ids, index=len(ids)-1, key="v159_base_run")
    payload = (run_map.get(rid) or {}).get("payload")
    if not (isinstance(payload, dict) and payload.get("kind")=="shams_run_artifact"):
        st.error("Selected baseline run payload missing.")
        return
    baseline_inputs = payload.get("inputs") or payload.get("_inputs") or {}
    if not isinstance(baseline_inputs, dict) or not baseline_inputs:
        st.error("Baseline inputs missing.")
        return

    st.markdown("### Unknowns to search")
    keys=sorted([k for k in baseline_inputs.keys() if isinstance(k,str)])
    unk = st.multiselect("Select unknown parameters (will be randomized within bounds)", options=keys, default=[k for k in keys if k in ("R0_m","B0_T")], key="v159_unk_keys")
    st.caption('Provide bounds as JSON list: [{"name":"R0_m","bounds":[2.5,3.5]}, ...]')
    default_bounds=_json.dumps([{"name":k, "bounds":[float(baseline_inputs.get(k,0.0) or 0.0)*0.9, float(baseline_inputs.get(k,0.0) or 0.0)*1.1+1e-9]} for k in (unk or [])][:6], indent=2)
    bounds_json = st.text_area("Unknown bounds (JSON)", value=default_bounds, height=140, key="v159_bounds")
    fixed_json = st.text_area("Fixed overrides (JSON list of {name,value})", value="[]", height=80, key="v159_fixed")
    assumptions_json = st.text_area("Assumption set (JSON object)", value="{}", height=80, key="v159_assumptions")

    col1,col2,col3=st.columns(3)
    with col1:
        n_samples = st.number_input("Samples", value=400, min_value=20, max_value=20000, step=20, key="v159_n")
    with col2:
        seed = st.number_input("Seed", value=0, min_value=0, max_value=10_000_000, step=1, key="v159_seed")
    with col3:
        strategy = st.selectbox("Strategy", options=["random","lhs"], index=0, key="v159_strategy")

    if st.button("Search Completion Witness", use_container_width=True, key="v159_run"):
        try:
            unknowns = _json.loads(bounds_json) if bounds_json.strip() else []
            fixed = _json.loads(fixed_json) if fixed_json.strip() else []
            assumptions = _json.loads(assumptions_json) if assumptions_json.strip() else {}
        except Exception as e:
            st.error(f"JSON parse error: {e}")
            return

        # known = baseline minus unknown keys
        known = {k:v for k,v in baseline_inputs.items() if (k not in set(unk or []))}
        with st.spinner("Sampling for feasibility completion..."):
            ev = build_feasibility_completion_evidence(
                known=known,
                unknowns=unknowns,
                fixed=fixed if isinstance(fixed,list) else [],
                assumption_set=assumptions if isinstance(assumptions,dict) else {},
                n_samples=int(n_samples),
                seed=int(seed),
                strategy=str(strategy),
                policy={"generator":"ui"},
            )
        st.session_state["v159_completion"] = ev
        res=((ev.get("payload") or {}).get("result") or {})
        st.success(f"Verdict: {res.get('verdict')}")
        st.json(res.get("bottleneck") or {})
        st.download_button("Download feasibility_completion_evidence_v159.json",
                           data=_json.dumps(ev, indent=2, sort_keys=True, default=str),
                           file_name="feasibility_completion_evidence_v159.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v159_dl")


# =====================
# v161 Completion Frontier + Minimal Change Distance
# =====================
def _v161_completion_frontier_panel():
    import streamlit as st
    from tools.completion_frontier import build_completion_frontier

    st.subheader("Completion Frontier")
    st.caption("Quantify how far a baseline guess is from feasibility: minimal-change feasible witness + distanceâ€“margin frontier.")

    s = _v98_state_init_runlists()
    runs = [r for r in (s.run_history or []) if r.get("id")]
    if not runs:
        st.info("Run at least one evaluation first (Point Designer / Systems Mode).")
        return
    ids=[r.get("id") for r in runs]
    run_map={r.get("id"): r for r in runs}
    rid = st.selectbox("Baseline run (baseline guess x0)", options=ids, index=len(ids)-1, key="v161_base_run")
    payload = (run_map.get(rid) or {}).get("payload")
    if not (isinstance(payload, dict) and payload.get("kind")=="shams_run_artifact"):
        st.error("Selected baseline run payload missing.")
        return
    baseline_inputs = payload.get("inputs") or payload.get("_inputs") or {}
    if not isinstance(baseline_inputs, dict) or not baseline_inputs:
        st.error("Baseline inputs missing.")
        return

    st.markdown("### Decision variables (vary to reach feasibility)")
    keys=sorted([k for k in baseline_inputs.keys() if isinstance(k,str)])
    dv = st.multiselect("Select decision variables", options=keys, default=[k for k in keys if k in ("R0_m","B0_T","q95","kappa")], key="v161_dv_keys")
    default_bounds=_json.dumps([{"name":k, "bounds":[float(baseline_inputs.get(k,0.0) or 0.0)*0.9, float(baseline_inputs.get(k,0.0) or 0.0)*1.1+1e-9]} for k in (dv or [])][:8], indent=2)
    st.caption('Bounds JSON example: [{"name":"R0_m","bounds":[2.5,3.5]}, ...]')
    bounds_json = st.text_area("Decision variable bounds (JSON)", value=default_bounds, height=160, key="v161_bounds")
    fixed_json = st.text_area("Fixed overrides (JSON list of {name,value})", value="[]", height=80, key="v161_fixed")
    assumptions_json = st.text_area("Assumption set (JSON object)", value="{}", height=80, key="v161_assumptions")

    col1,col2,col3=st.columns(3)
    with col1:
        n_samples = st.number_input("Samples", value=800, min_value=40, max_value=50000, step=40, key="v161_n")
    with col2:
        seed = st.number_input("Seed", value=0, min_value=0, max_value=10_000_000, step=1, key="v161_seed")
    with col3:
        strategy = st.selectbox("Strategy", options=["random","lhs"], index=0, key="v161_strategy")

    if st.button("Compute Completion Frontier", use_container_width=True, key="v161_run"):
        try:
            vars_spec = _json.loads(bounds_json) if bounds_json.strip() else []
            fixed = _json.loads(fixed_json) if fixed_json.strip() else []
            assumptions = _json.loads(assumptions_json) if assumptions_json.strip() else {}
        except Exception as e:
            st.error(f"JSON parse error: {e}")
            return
        with st.spinner("Sampling and evaluating frontier..."):
            out = build_completion_frontier(
                baseline=baseline_inputs,
                decision_vars=vars_spec,
                fixed=fixed if isinstance(fixed,list) else [],
                assumption_set=assumptions if isinstance(assumptions,dict) else {},
                n_samples=int(n_samples),
                seed=int(seed),
                strategy=str(strategy),
                policy={"generator":"ui"},
            )
        st.session_state["v161_frontier"] = out
        res=((out.get("payload") or {}).get("result") or {})
        st.success("Computed.")
        st.markdown("**Minimal-change feasible witness**")
        st.json(res.get("minimal_change_feasible") or {})
        st.markdown("**Frontier (preview)**")
        st.json((res.get("frontier") or [])[:20])
        st.download_button("Download completion_frontier_v161.json",
                           data=_json.dumps(out, indent=2, sort_keys=True, default=str),
                           file_name="completion_frontier_v161.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v161_dl")


# =====================
# v162 Directed Local Search (safe)
# =====================
def _v162_directed_local_search_panel():
    import streamlit as st
    from tools.directed_local_search import build_directed_local_search

    st.subheader("Directed Local Search")
    st.caption("A safe, bounded local search that tries to reach feasibility with minimal evaluations (downstream-only).")

    s = _v98_state_init_runlists()
    runs = [r for r in (s.run_history or []) if r.get("id")]
    if not runs:
        st.info("Run at least one evaluation first (Point Designer / Systems Mode).")
        return
    ids=[r.get("id") for r in runs]
    run_map={r.get("id"): r for r in runs}
    rid = st.selectbox("Baseline run (starting guess)", options=ids, index=len(ids)-1, key="v162_base_run")
    payload = (run_map.get(rid) or {}).get("payload")
    if not (isinstance(payload, dict) and payload.get("kind")=="shams_run_artifact"):
        st.error("Selected baseline run payload missing.")
        return
    baseline_inputs = payload.get("inputs") or payload.get("_inputs") or {}
    if not isinstance(baseline_inputs, dict) or not baseline_inputs:
        st.error("Baseline inputs missing.")
        return

    st.markdown("### Decision variables")
    keys=sorted([k for k in baseline_inputs.keys() if isinstance(k,str)])
    dv = st.multiselect("Select decision variables to adjust", options=keys, default=[k for k in keys if k in ("R0_m","B0_T","q95","kappa")], key="v162_dv_keys")
    default_bounds=_json.dumps([{"name":k, "bounds":[float(baseline_inputs.get(k,0.0) or 0.0)*0.9, float(baseline_inputs.get(k,0.0) or 0.0)*1.1+1e-9]} for k in (dv or [])][:8], indent=2)
    st.caption('Bounds JSON example: [{"name":"R0_m","bounds":[2.5,3.5]}, ...]')
    bounds_json = st.text_area("Decision variable bounds (JSON)", value=default_bounds, height=160, key="v162_bounds")
    fixed_json = st.text_area("Fixed overrides (JSON list of {name,value})", value="[]", height=80, key="v162_fixed")
    assumptions_json = st.text_area("Assumption set (JSON object)", value="{}", height=80, key="v162_assumptions")

    col1,col2,col3,col4=st.columns(4)
    with col1:
        max_evals = st.number_input("Max evals", value=200, min_value=30, max_value=5000, step=10, key="v162_maxeval")
    with col2:
        seed = st.number_input("Seed", value=0, min_value=0, max_value=10_000_000, step=1, key="v162_seed")
    with col3:
        init_step = st.number_input("Initial step (norm)", value=0.12, min_value=0.01, max_value=0.50, step=0.01, key="v162_initstep")
    with col4:
        min_step = st.number_input("Min step (norm)", value=0.004, min_value=0.0005, max_value=0.10, step=0.0005, format="%.4f", key="v162_minstep")

    shrink = st.slider("Step shrink factor", min_value=0.2, max_value=0.9, value=0.5, step=0.05, key="v162_shrink")

    if st.button("Run Directed Search", use_container_width=True, key="v162_run"):
        try:
            vars_spec = _json.loads(bounds_json) if bounds_json.strip() else []
            fixed = _json.loads(fixed_json) if fixed_json.strip() else []
            assumptions = _json.loads(assumptions_json) if assumptions_json.strip() else {}
        except Exception as e:
            st.error(f"JSON parse error: {e}")
            return
        with st.spinner("Searching..."):
            out = build_directed_local_search(
                baseline=baseline_inputs,
                decision_vars=vars_spec,
                fixed=fixed if isinstance(fixed,list) else [],
                assumption_set=assumptions if isinstance(assumptions,dict) else {},
                max_evals=int(max_evals),
                seed=int(seed),
                initial_step_norm=float(init_step),
                min_step_norm=float(min_step),
                step_shrink=float(shrink),
                policy={"generator":"ui"},
            )
        st.session_state["v162_local_search"] = out
        res=((out.get("payload") or {}).get("result") or {})
        st.success(f"Stop reason: {res.get('stop_reason')}")
        st.json(res.get("final") or {})
        st.download_button("Download directed_local_search_v162.json",
                           data=_json.dumps(out, indent=2, sort_keys=True, default=str),
                           file_name="directed_local_search_v162.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v162_dl")


# =====================
# v163 Completion Pack (Actionable Recipe)
# =====================
def _v163_completion_pack_panel():
    import streamlit as st
    from tools.completion_pack import build_completion_pack, render_completion_pack_markdown

    st.subheader("Completion Pack")
    st.caption("Bundle completion evidence into an actionable recipe: witness + knob ranking + bounds recommendations.")

    v159 = st.session_state.get("v159_completion")
    v161 = st.session_state.get("v161_frontier")
    v162 = st.session_state.get("v162_local_search")

    st.markdown("### Inputs")
    colA,colB,colC = st.columns(3)
    with colA:
        up159 = st.file_uploader("Optional: upload v159 evidence JSON", type=["json"], key="v163_up159")
    with colB:
        up161 = st.file_uploader("Optional: upload v161 frontier JSON", type=["json"], key="v163_up161")
    with colC:
        up162 = st.file_uploader("Optional: upload v162 local search JSON", type=["json"], key="v163_up162")

    def _load(up):
        if up is None:
            return None
        try:
            return _json.loads(up.getvalue().decode("utf-8"))
        except Exception:
            return None

    v159 = _load(up159) or v159
    v161 = _load(up161) or v161
    v162 = _load(up162) or v162

    tighten = st.slider("Bounds tighten factor (heuristic)", min_value=0.05, max_value=0.45, value=0.25, step=0.05, key="v163_tighten")

    if st.button("Build Completion Pack", use_container_width=True, key="v163_run"):
        pack = build_completion_pack(v159=v159, v161=v161, v162=v162, policy={"generator":"ui", "tighten": float(tighten)})
        st.session_state["v163_pack"] = pack
        st.success(f"Witness provenance: {((pack.get('payload') or {}).get('witness_provenance') or '')}")
        st.markdown("### Preview")
        st.json((pack.get("payload") or {}) )
        st.download_button("Download completion_pack_v163.json",
                           data=_json.dumps(pack, indent=2, sort_keys=True, default=str),
                           file_name="completion_pack_v163.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v163_dl_json")
        md = render_completion_pack_markdown(pack)
        st.download_button("Download completion_pack_summary_v163.md",
                           data=md,
                           file_name="completion_pack_summary_v163.md",
                           mime="text/markdown",
                           use_container_width=True,
                           key="v163_dl_md")


# =====================
# v164 Sensitivity + Bottleneck Attribution
# =====================
def _v164_sensitivity_panel():
    import streamlit as st
    from tools.sensitivity_v164 import build_sensitivity_report, render_sensitivity_markdown

    st.subheader("Sensitivity + Bottleneck Attribution")
    st.caption("Local finite-difference sensitivities around a witness: ranked leverage variables and dominant-constraint changes.")

    # Prefer completion pack witness if present, else last run
    pack = st.session_state.get("v163_pack")
    witness = None
    if isinstance(pack, dict):
        witness = ((pack.get("payload") or {}).get("recommended_witness"))
    if not isinstance(witness, dict):
        # fallback to baseline last run
        s = _v98_state_init_runlists()
        runs = [r for r in (s.run_history or []) if r.get("id")]
        if runs:
            payload = (runs[-1].get("payload") or {})
            if isinstance(payload, dict) and payload.get("kind")=="shams_run_artifact":
                witness = payload.get("inputs") or payload.get("_inputs")

    if not isinstance(witness, dict) or not witness:
        st.info("No witness found yet. Run v159/v161/v162 or build v163 completion pack first.")
        return

    st.markdown("### Witness source")
    st.code(f"Using witness keys: {len(witness)}", language="text")

    st.markdown("### Variables to perturb")
    keys=sorted([k for k in witness.keys() if isinstance(k,str)])
    dv = st.multiselect("Select variables", options=keys, default=[k for k in keys if k in ("R0_m","B0_T","q95","kappa")], key="v164_dv")
    default_bounds=_json.dumps([{"name":k, "bounds":[float(witness.get(k,0.0) or 0.0)*0.9, float(witness.get(k,0.0) or 0.0)*1.1+1e-9]} for k in (dv or [])][:10], indent=2)
    st.caption('Bounds JSON example: [{"name":"R0_m","bounds":[2.5,3.5]}, ...]')
    bounds_json = st.text_area("Variable bounds (JSON)", value=default_bounds, height=160, key="v164_bounds")
    assumptions_json = st.text_area("Assumption set (JSON object)", value="{}", height=80, key="v164_assumptions")

    col1,col2=st.columns(2)
    with col1:
        rel_step = st.number_input("Relative step (fraction of span)", value=0.01, min_value=0.001, max_value=0.10, step=0.001, format="%.3f", key="v164_relstep")
    with col2:
        abs_step_min = st.number_input("Absolute step min", value=1e-6, min_value=0.0, max_value=1e-2, step=1e-6, format="%.6f", key="v164_absmin")

    if st.button("Compute Sensitivity", use_container_width=True, key="v164_run"):
        try:
            vars_spec = _json.loads(bounds_json) if bounds_json.strip() else []
            assumptions = _json.loads(assumptions_json) if assumptions_json.strip() else {}
        except Exception as e:
            st.error(f"JSON parse error: {e}")
            return
        with st.spinner("Evaluating baseline and perturbations..."):
            rep = build_sensitivity_report(
                witness=witness,
                variables=vars_spec,
                assumption_set=assumptions if isinstance(assumptions,dict) else {},
                rel_step=float(rel_step),
                abs_step_min=float(abs_step_min),
                policy={"generator":"ui"},
            )
        st.session_state["v164_sensitivity"] = rep
        ranked=((rep.get("payload") or {}).get("ranked") or [])
        st.success(f"Computed. Ranked variables: {len(ranked)}")
        st.markdown("### Ranked leverage (preview)")
        st.json(ranked[:15])
        st.download_button("Download sensitivity_v164.json",
                           data=_json.dumps(rep, indent=2, sort_keys=True, default=str),
                           file_name="sensitivity_v164.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v164_dl_json")
        md = render_sensitivity_markdown(rep)
        st.download_button("Download sensitivity_v164.md",
                           data=md,
                           file_name="sensitivity_v164.md",
                           mime="text/markdown",
                           use_container_width=True,
                           key="v164_dl_md")


# =====================
# v165 Study Protocol Generator
# =====================
def _v165_study_protocol_panel():
    import streamlit as st
    from tools.study_protocol_v165 import build_study_protocol, render_study_protocol_markdown

    st.subheader("Study Protocol Generator")
    st.caption("Generate journal-ready, audit-ready study protocol (Methods) with protocol SHA-256. Reporting-only.")

    s = _v98_state_init_runlists()
    runs = [r for r in (s.run_history or []) if r.get("id")]
    if not runs:
        st.info("Run at least one evaluation first (Point Designer / Systems Mode) to generate a run artifact.")
        return
    ids=[r.get("id") for r in runs]
    run_map={r.get("id"): r for r in runs}
    rid = st.selectbox("Select run artifact", options=ids, index=len(ids)-1, key="v165_run_id")
    run_art = (run_map.get(rid) or {}).get("payload")
    if not (isinstance(run_art, dict) and run_art.get("kind")=="shams_run_artifact"):
        st.error("Selected run is missing shams_run_artifact payload.")
        return

    col1,col2=st.columns(2)
    with col1:
        title = st.text_input("Study title", value="SHAMS Design Study", key="v165_title")
    with col2:
        study_id = st.text_input("Study ID (optional)", value="", key="v165_study_id")

    objective = st.text_area("Objective (paper-ready)", value="Feasibility characterization and completion under explicit constraints.", height=80, key="v165_obj")
    notes = st.text_area("Notes (one per line)", value="", height=60, key="v165_notes")

    st.markdown("### Optional: variables varied / scan definition")
    vars_json = st.text_area("variables_varied (JSON list)", value="[]", height=120, key="v165_vars")
    artifacts_json = st.text_area("artifacts_generated (JSON list)", value='["study_protocol_v165.json","study_protocol_v165.md"]', height=80, key="v165_artifacts")
    seed = st.number_input("Seed (optional)", value=0, min_value=0, max_value=10_000_000, step=1, key="v165_seed")

    if st.button("Generate Study Protocol", use_container_width=True, key="v165_run"):
        try:
            vv = _json.loads(vars_json) if vars_json.strip() else []
            ag = _json.loads(artifacts_json) if artifacts_json.strip() else []
        except Exception as e:
            st.error(f"JSON parse error: {e}")
            return
        overrides={
            "title": title,
            "study_id": study_id,
            "objective": objective,
            "notes": [ln.strip() for ln in notes.splitlines() if ln.strip()],
            "variables_varied": vv if isinstance(vv, list) else [],
            "artifacts_generated": ag if isinstance(ag, list) else [],
            "seed": int(seed),
        }
        prot = build_study_protocol(run_artifact=run_art, protocol_overrides=overrides)
        st.session_state["v165_protocol"] = prot
        sha = ((prot.get("payload") or {}).get("integrity") or {}).get("protocol_sha256")
        st.success(f"Generated. Protocol SHA-256: {sha}")
        st.download_button("Download study_protocol_v165.json",
                           data=_json.dumps(prot, indent=2, sort_keys=True, default=str),
                           file_name="study_protocol_v165.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v165_dl_json")
        md = render_study_protocol_markdown(prot)
        st.download_button("Download study_protocol_v165.md",
                           data=md,
                           file_name="study_protocol_v165.md",
                           mime="text/markdown",
                           use_container_width=True,
                           key="v165_dl_md")


# =====================
# v166 Reproducibility Lock + Replay Checker
# =====================
def _v166_repro_lock_panel():
    import streamlit as st
    from tools.repro_lock_v166 import build_repro_lock, replay_check

    st.subheader("Reproducibility Lock + Replay Checker")
    st.caption("Freeze a run (inputs + assumptions + solver meta) into a lockfile and verify replay matches within tolerances.")

    s = _v98_state_init_runlists()
    runs = [r for r in (s.run_history or []) if r.get("id")]
    if not runs:
        st.info("Run at least one evaluation first (Point Designer / Systems Mode).")
        return
    ids=[r.get("id") for r in runs]
    run_map={r.get("id"): r for r in runs}
    rid = st.selectbox("Select run artifact to lock", options=ids, index=len(ids)-1, key="v166_run_id")
    run_art = (run_map.get(rid) or {}).get("payload")
    if not (isinstance(run_art, dict) and run_art.get("kind")=="shams_run_artifact"):
        st.error("Selected run is missing shams_run_artifact payload.")
        return

    st.markdown("### Tolerances (edit JSON)")
    tol_default = _json.dumps({
        "min_margin_abs": 1e-8,
        "constraint_margin_abs": 1e-6,
        "metric_rel": 1e-6,
        "metric_abs": 1e-9,
    }, indent=2)
    tol_json = st.text_area("tolerances JSON", value=tol_default, height=140, key="v166_tol")

    notes = st.text_area("Notes (one per line)", value="", height=60, key="v166_notes")

    col1,col2=st.columns(2)
    with col1:
        if st.button("Create Lock", use_container_width=True, key="v166_lock"):
            try:
                tol=_json.loads(tol_json) if tol_json.strip() else {}
            except Exception as e:
                st.error(f"JSON parse error: {e}")
                return
            lock = build_repro_lock(run_artifact=run_art, lock_overrides={"tolerances": tol, "notes":[ln.strip() for ln in notes.splitlines() if ln.strip()]})
            st.session_state["v166_lock"] = lock
            sha = ((lock.get("payload") or {}).get("integrity") or {}).get("lock_sha256")
            st.success(f"Lock created. lock_sha256: {sha}")
            st.download_button("Download repro_lock_v166.json",
                               data=_json.dumps(lock, indent=2, sort_keys=True, default=str),
                               file_name="repro_lock_v166.json",
                               mime="application/json",
                               use_container_width=True,
                               key="v166_dl_lock")
    with col2:
        lock_up = st.file_uploader("Or upload existing lock JSON", type=["json"], key="v166_up_lock")

    lock = st.session_state.get("v166_lock")
    if lock_up is not None:
        try:
            lock = _json.loads(lock_up.getvalue().decode("utf-8"))
            st.session_state["v166_lock"] = lock
        except Exception:
            st.warning("Could not parse uploaded lock JSON.")

    st.markdown("### Replay check")
    ao_json = st.text_area("assumption_set override (JSON, optional)", value="{}", height=80, key="v166_ao")
    if st.button("Run Replay Check", use_container_width=True, key="v166_replay"):
        if not isinstance(lock, dict) or lock.get("kind")!="shams_repro_lock":
            st.error("No valid lock loaded. Create or upload a lock first.")
            return
        try:
            ao = _json.loads(ao_json) if ao_json.strip() else {}
        except Exception as e:
            st.error(f"JSON parse error: {e}")
            return
        rep = replay_check(lock=lock, assumption_set_override=ao if isinstance(ao, dict) else None, policy={"generator":"ui"})
        st.session_state["v166_replay"] = rep
        ok = ((rep.get("payload") or {}).get("ok"))
        st.success("Replay OK" if ok else "Replay NOT OK")
        st.json((rep.get("payload") or {}).get("checks") or {})
        st.download_button("Download replay_report_v166.json",
                           data=_json.dumps(rep, indent=2, sort_keys=True, default=str),
                           file_name="replay_report_v166.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v166_dl_rep")


# =====================
# v167 Design Study Authority Pack
# =====================
def _v167_authority_pack_panel():
    import streamlit as st
    from tools.authority_pack_v167 import build_authority_pack

    st.subheader("Design Study Authority Pack")
    st.caption("One downloadable ZIP bundling protocol + lock + replay + completion + sensitivity (+ certificate if available).")

    s = _v98_state_init_runlists()
    runs = [r for r in (s.run_history or []) if r.get("id")]
    if not runs:
        st.info("Run at least one evaluation first (Point Designer / Systems Mode).")
        return
    ids=[r.get("id") for r in runs]
    run_map={r.get("id"): r for r in runs}
    rid = st.selectbox("Select run artifact to include", options=ids, index=len(ids)-1, key="v167_run_id")
    run_art = (run_map.get(rid) or {}).get("payload")
    if not (isinstance(run_art, dict) and run_art.get("kind")=="shams_run_artifact"):
        st.error("Selected run is missing shams_run_artifact payload.")
        return

    st.markdown("### Auto-pick from session state (recommended)")
    prot = st.session_state.get("v165_protocol")
    lock = st.session_state.get("v166_lock")
    replay = st.session_state.get("v166_replay")
    comp = st.session_state.get("v163_pack")
    sens = st.session_state.get("v164_sensitivity")
    cert = st.session_state.get("v160_certificate")

    st.markdown("### Optional: upload any missing JSONs")
    col1,col2,col3 = st.columns(3)
    with col1:
        up_prot = st.file_uploader("upload study_protocol_v165.json", type=["json"], key="v167_up_prot")
        up_lock = st.file_uploader("upload repro_lock_v166.json", type=["json"], key="v167_up_lock")
    with col2:
        up_rep = st.file_uploader("upload replay_report_v166.json", type=["json"], key="v167_up_rep")
        up_comp = st.file_uploader("upload completion_pack_v163.json", type=["json"], key="v167_up_comp")
    with col3:
        up_sens = st.file_uploader("upload sensitivity_v164.json", type=["json"], key="v167_up_sens")
        up_cert = st.file_uploader("upload certificate_v160.json", type=["json"], key="v167_up_cert")

    def _load(up):
        if up is None:
            return None
        try:
            return _json.loads(up.getvalue().decode("utf-8"))
        except Exception:
            return None

    prot = _load(up_prot) or prot
    lock = _load(up_lock) or lock
    replay = _load(up_rep) or replay
    comp = _load(up_comp) or comp
    sens = _load(up_sens) or sens
    cert = _load(up_cert) or cert

    if st.button("Build Authority Pack ZIP", use_container_width=True, key="v167_build"):
        res = build_authority_pack(
            run_artifact=run_art,
            study_protocol_v165=prot,
            repro_lock_v166=lock,
            replay_report_v166=replay,
            completion_pack_v163=comp,
            sensitivity_v164=sens,
            certificate_v160=cert,
            policy={"generator":"ui"},
        )
        st.session_state["v167_manifest"] = res["manifest"]
        st.success(f"Built authority_pack_v167.zip (sha256={res['pack']['integrity']['zip_sha256']})")
        st.json(res["manifest"])
        st.download_button("Download authority_pack_v167.zip",
                           data=res["zip_bytes"],
                           file_name="authority_pack_v167.zip",
                           mime="application/zip",
                           use_container_width=True,
                           key="v167_dl_zip")


# =====================
# v168 Citation-Grade Study Reference
# =====================
def _v168_citation_panel():
    import streamlit as st
    from tools.citation_v168 import build_citation_bundle

    st.subheader("Citation-Grade Study Reference")
    st.caption("Generate Study ID + CITATION.cff + BibTeX from protocol/lock/authority-pack manifest.")

    prot = st.session_state.get("v165_protocol")
    lock = st.session_state.get("v166_lock")
    manifest = st.session_state.get("v167_manifest")

    st.markdown("### Inputs (auto-pick from session state or upload)")
    col1,col2,col3 = st.columns(3)
    with col1:
        up_prot = st.file_uploader("upload study_protocol_v165.json", type=["json"], key="v168_up_prot")
    with col2:
        up_lock = st.file_uploader("upload repro_lock_v166.json", type=["json"], key="v168_up_lock")
    with col3:
        up_man = st.file_uploader("upload authority_pack_manifest_v167.json", type=["json"], key="v168_up_man")

    def _load(up):
        if up is None:
            return None
        try:
            return _json.loads(up.getvalue().decode("utf-8"))
        except Exception:
            return None

    prot = _load(up_prot) or prot
    lock = _load(up_lock) or lock
    manifest = _load(up_man) or manifest

    st.markdown("### Metadata (optional)")
    title = st.text_input("Title override (optional)", value="", key="v168_title")
    repo_url = st.text_input("Repository/URL (optional)", value="", key="v168_repo")
    doi = st.text_input("DOI (optional)", value="", key="v168_doi")
    author = st.text_input("Author name (optional)", value="SHAMSâ€“FUSION-X Contributors", key="v168_author")

    if st.button("Generate Citation Bundle", use_container_width=True, key="v168_run"):
        if not (isinstance(prot, dict) and prot.get("kind")=="shams_study_protocol"):
            st.error("Need a valid study_protocol_v165.json (generate in v165).")
            return
        meta={
            "title": title or None,
            "repository": repo_url or None,
            "url": repo_url or None,
            "doi": doi or None,
            "authors": [{"name": author}] if author else [{"name":"SHAMSâ€“FUSION-X Contributors"}],
            "version": "v168",
        }
        res = build_citation_bundle(
            study_protocol_v165=prot,
            repro_lock_v166=lock if isinstance(lock, dict) else None,
            authority_pack_manifest_v167=manifest if isinstance(manifest, dict) else None,
            metadata=meta,
        )
        st.session_state["v168_citation"] = res
        sid = ((res.get("payload") or {}).get("study_id"))
        st.success(f"Study ID: {sid}")
        st.download_button("Download citation_bundle_v168.json",
                           data=_json.dumps(res, indent=2, sort_keys=True, default=str),
                           file_name="citation_bundle_v168.json",
                           mime="application/json",
                           use_container_width=True,
                           key="v168_dl_json")
        st.download_button("Download CITATION.cff",
                           data=(res.get("payload") or {}).get("citation_cff_text",""),
                           file_name="CITATION.cff",
                           mime="text/yaml",
                           use_container_width=True,
                           key="v168_dl_cff")
        st.download_button("Download study_citation_v168.bib",
                           data=(res.get("payload") or {}).get("bibtex_text",""),
                           file_name="study_citation_v168.bib",
                           mime="text/plain",
                           use_container_width=True,
                           key="v168_dl_bib")
        st.download_button("Download study_reference_v168.md",
                           data=(res.get("payload") or {}).get("reference_markdown",""),
                           file_name="study_reference_v168.md",
                           mime="text/markdown",
                           use_container_width=True,
                           key="v168_dl_md")


# =====================
# v169 Feasibility Boundary Atlas (figure pack)
# =====================
def _v169_atlas_panel():
    import streamlit as st
    from tools.atlas_v169 import build_atlas_pack

    st.subheader("Feasibility Boundary Atlas")
    st.caption("Generate a publishable atlas-style figure pack with consistent captions and a hash manifest.")

    sens = st.session_state.get("v164_sensitivity")
    st.markdown("### Input")
    up_sens = st.file_uploader("Upload sensitivity_v164.json (optional)", type=["json"], key="v169_up_sens")

    if up_sens is not None:
        try:
            sens = _json.loads(up_sens.getvalue().decode("utf-8"))
        except Exception:
            st.warning("Could not parse uploaded JSON.")

    if not isinstance(sens, dict):
        st.info("Run v164 Sensitivity first (or upload sensitivity_v164.json) to build the initial atlas.")
        return

    if st.button("Build Atlas Pack ZIP", use_container_width=True, key="v169_build"):
        res = build_atlas_pack(sensitivity_v164=sens, policy={"generator":"ui"})
        st.session_state["v169_manifest"] = res["manifest"]
        st.success(f"Built atlas_pack_v169.zip (sha256={res['pack']['integrity']['zip_sha256']})")
        st.json(res["manifest"])
        st.download_button("Download atlas_pack_v169.zip",
                           data=res["zip_bytes"],
                           file_name="atlas_pack_v169.zip",
                           mime="application/zip",
                           use_container_width=True,
                           key="v169_dl_zip")


# =====================
# v170 SHAMS â†’ external systems codes Downstream Export
# =====================
def _v170_process_export_panel():
    import streamlit as st
    from tools.process_export_v170 import build_process_export_pack

    st.subheader("SHAMS â†’ external systems codes Downstream Export")
    st.caption("Export SHAMS study outputs into transparent (systems-code-inspired) tables, keeping SHAMS as upstream authority.")

    s = _v98_state_init_runlists()
    runs = [r for r in (s.run_history or []) if r.get("id")]
    if not runs:
        st.info("Run at least one evaluation first (Point Designer / Systems Mode).")
        return
    ids=[r.get("id") for r in runs]
    run_map={r.get("id"): r for r in runs}
    rid = st.selectbox("Select run artifact to export", options=ids, index=len(ids)-1, key="v170_run_id")
    run_art = (run_map.get(rid) or {}).get("payload")
    if not (isinstance(run_art, dict) and run_art.get("kind")=="shams_run_artifact"):
        st.error("Selected run is missing shams_run_artifact payload.")
        return

    comp = st.session_state.get("v163_pack")
    cite = st.session_state.get("v168_citation")

    st.markdown("### Optional attachments")
    st.write("- completion_pack_v163.json:", "yes" if isinstance(comp, dict) else "no")
    st.write("- citation_bundle_v168.json:", "yes" if isinstance(cite, dict) else "no")

    if st.button("Build external systems codes Export Pack ZIP", use_container_width=True, key="v170_build"):
        res = build_process_export_pack(
            run_artifact=run_art,
            completion_pack_v163=comp if isinstance(comp, dict) else None,
            citation_bundle_v168=cite if isinstance(cite, dict) else None,
            policy={"generator":"ui"},
        )
        st.session_state["v170_manifest"] = res["manifest"]
        st.success(f"Built process_export_pack_v170.zip (sha256={res['pack']['integrity']['zip_sha256']})")
        st.json(res["manifest"])
        st.download_button("Download process_export_pack_v170.zip",
                           data=res["zip_bytes"],
                           file_name="process_export_pack_v170.zip",
                           mime="application/zip",
                           use_container_width=True,
                           key="v170_dl_zip")


# =====================
# v172 Demo seed + hydration
# =====================
def _v172_demo_loader():
    import streamlit as st
    from tools.demo_seed_v172 import install_demo_bundle
    st.subheader("Demo seed")
    st.caption("Loads synthetic demo artifacts into session state so every panel shows content offline. Not authoritative.")
    col1,col2 = st.columns([1,2])
    with col1:
        if st.button("Load demo artifacts", use_container_width=True, key="v172_load_demo"):
            install_demo_bundle(st.session_state)
            st.success("Demo artifacts loaded into session state.")
    with col2:
        if st.button("Clear demo artifacts", use_container_width=True, key="v172_clear_demo"):
            for k in ["v164_sensitivity","v165_protocol","v166_lock","v166_replay","v167_manifest","v168_citation","v163_pack",
                      "pd_last_outputs","pd_last_artifact","demo_run_artifact"]:
                if k in st.session_state:
                    del st.session_state[k]
            st.success("Demo artifacts cleared.")


# --- Deferred PAM render ---
try:
    if 'pam_placeholder' in globals() and pam_placeholder is not None:
        with pam_placeholder.container():
            _v175_panel_availability_map_panel()
except Exception as _e:
    # Never fail the whole app due to PAM rendering
    try:
        st.warning(f'PAM render failed: {_e}')
    except Exception:
        pass

# --- Render Activity Log late so it includes events logged during this run ---
try:
    _render_activity_log_sidebar()
except Exception:
    pass

_render_footer()